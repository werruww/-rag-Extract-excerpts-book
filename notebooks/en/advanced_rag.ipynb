{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUCaGdAj9-9F"
      },
      "source": [
        "# Advanced RAG on Hugging Face documentation using LangChain\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKv51c_h9-9H"
      },
      "source": [
        "This notebook demonstrates how you can build an advanced RAG (Retrieval Augmented Generation) for answering a user's question about a specific knowledge base (here, the HuggingFace documentation), using LangChain.\n",
        "\n",
        "For an introduction to RAG, you can check [this other cookbook](rag_zephyr_langchain)!\n",
        "\n",
        "RAG systems are complex, with many moving parts: here is a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
        "\n",
        "> ğŸ’¡ As you can see, there are many steps to tune in this architecture: tuning the system properly will yield significant performance gains.\n",
        "\n",
        "In this notebook, we will take a look into many of these blue notes to see how to tune your RAG system and get the best performance.\n",
        "\n",
        "__Let's dig into the model building!__ First, we install the required model dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NSX0p0rV9-9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703aedf8-d3a8-4905-df1d-44224c59c379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.7/313.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eoujYMwW9-9J"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\n",
        "    \"display.max_colwidth\", None\n",
        ")  # This will be helpful when visualizing retriever outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr6rN10U9-9J"
      },
      "source": [
        "### Load your knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token XXX"
      ],
      "metadata": {
        "id": "1a_3Q8S9TIiF",
        "outputId": "aa4b20d4-3b05-4657-f927-c0671127625d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qZLVIEVW9-9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "73a29ffc-6830-446b-f7de-9f5016565b92"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid pattern: '**' can only be an entire path component",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e65787c70b30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m-ric/huggingface_doc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2113\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m                     ) from None\n\u001b[0;32m-> 1495\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m                     \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m                     \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   1480\u001b[0m         except (\n\u001b[1;32m   1481\u001b[0m             \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         )\n\u001b[1;32m   1036\u001b[0m         data_files = DataFilesDict.from_patterns(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mget_data_patterns\u001b[0;34m(base_path, download_config)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolve_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_data_files_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The directory at {base_path} doesn't contain any data files\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_path_and_storage_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fs_token_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0mfs_base_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_marker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mfs_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expand_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"expand_info\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     def find(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mallpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mends_with_sep\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/utils.py\u001b[0m in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"**\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    732\u001b[0m                 \u001b[0;34m\"Invalid pattern: '**' can only be an entire path component\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ]
    },
    {
      "source": [
        "!pip install -U datasets fsspec"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tlMjrwbYTRS1",
        "outputId": "e2aadccf-f64c-4a22-b3e8-84817e7ba932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets"
                ]
              },
              "id": "b7ffe90dbbf2413f8f13bf9e63b9025a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "import datasets\n",
        "\n",
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "R3MZ5F0iTRjN",
        "outputId": "1b94618a-e58e-4c19-cdbb-752bc1b2487e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "836Q7vF49-9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "99df283d-5c80-470d-84ca-b9b67b4fe034"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tqdm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-683ac2352a2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m RAW_KNOWLEDGE_BASE = [\n\u001b[1;32m      4\u001b[0m     \u001b[0mLangchainDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m ]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
          ]
        }
      ],
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]"
      ]
    },
    {
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from tqdm.notebook import tqdm # Import tqdm here\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DeITG8R3Tfl9",
        "outputId": "2a9a4bb4-747a-43f9-d741-deb364c667bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c92afbcf51b64d0e946e920bfddad38e",
            "5ccb7ea417bd4371adeed10bb7809673",
            "b20ff3fd1bb347618cfeacacf0493bd4",
            "cacc7cabba454ead9c5020a625dfa338",
            "2c99550d032f4a5281de616f2b1ed1f6",
            "2513e870326845339341141dafcb428e",
            "1e9d881821c740fda788b4d8a20be00c",
            "6bc4a0e7f6ef43e0b916dac3da75925f",
            "cba8586519714de68ec14df9dfa1c15d",
            "4a88baee21ed4cd2b9e683c88f33954a",
            "b39d7eb01b0b447b8fa31c0fd039ef1b"
          ]
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c92afbcf51b64d0e946e920bfddad38e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_LxjD5h9-9K"
      },
      "source": [
        "# 1. Retriever - embeddings ğŸ—‚ï¸\n",
        "The __retriever acts like an internal search engine__: given the user query, it returns a few relevant snippets from your knowledge base.\n",
        "\n",
        "These snippets will then be fed to the Reader Model to help it generate its answer.\n",
        "\n",
        "So __our objective here is, given a user question, to find the most relevant snippets from our knowledge base to answer that question.__\n",
        "\n",
        "This is a wide objective, it leaves open some questions. How many snippets should we retrieve? This parameter will be named `top_k`.\n",
        "\n",
        "How long should these snippets be? This is called the `chunk size`. There's no one-size-fits-all answers, but here are a few elements:\n",
        "- ğŸ”€ Your `chunk size` is allowed to vary from one snippet to the other.\n",
        "- Since there will always be some noise in your retrieval, increasing the `top_k` increases the chance to get relevant elements in your retrieved snippets. ğŸ¯ Shooting more arrows increases your probability of hitting your target.\n",
        "- Meanwhile, the summed length of your retrieved documents should not be too high: for instance, for most current models 16k tokens will probably drown your Reader model in information due to [Lost-in-the-middle phenomenon](https://huggingface.co/papers/2307.03172). ğŸ¯ Give your reader model only the most relevant insights, not a huge pile of books!\n",
        "\n",
        "\n",
        "> In this notebook, we use Langchain library since __it offers a huge variety of options for vector databases and allows us to keep document metadata throughout the processing__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uS6Mv8O9-9L"
      },
      "source": [
        "### 1.1 Split the documents into chunks\n",
        "\n",
        "- In this part, __we split the documents from our knowledge base into smaller chunks__ which will be the snippets on which the reader LLM will base its answer.\n",
        "- The goal is to prepare a collection of **semantically relevant snippets**. So their size should be adapted to precise ideas: too small will truncate ideas, and too large will dilute them.\n",
        "\n",
        "ğŸ’¡ _Many options exist for text splitting: splitting on words, on sentence boundaries, recursive chunking that processes documents in a tree-like way to preserve structure information... To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt._\n",
        "\n",
        "\n",
        "- **Recursive chunking** breaks down the text into smaller parts step by step using a given list of separators sorted from the most important to the least important separator. If the first split doesn't give the right size or shape of chunks, the method repeats itself on the new chunks using a different separator. For instance with the list of separators `[\"\\n\\n\", \"\\n\", \".\", \"\"]`:\n",
        "    - The method will first break down the document wherever there is a double line break `\"\\n\\n\"`.\n",
        "    - Resulting documents will be split again on simple line breaks `\"\\n\"`, then on sentence ends `\".\"`.\n",
        "    - Finally, if some chunks are still too big, they will be split whenever they overflow the maximum size.\n",
        "\n",
        "- With this method, the global structure is well preserved, at the expense of getting slight variations in chunk size.\n",
        "\n",
        "> [This space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
        "\n",
        "ğŸ”¬ Let's experiment a bit with chunk sizes, beginning with an arbitrary size, and see how splits work. We use Langchain's implementation of recursive chunking with `RecursiveCharacterTextSplitter`.\n",
        "- Parameter `chunk_size` controls the length of individual chunks: this length is counted by default as the number of characters in the chunk.\n",
        "- Parameter `chunk_overlap` lets adjacent chunks get a bit of overlap on each other. This reduces the probability that an idea could be cut in half by the split between two adjacent chunks. We ~arbitrarily set this to 1/10th of the chunk size, you could try different values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M4m6TwDJ9-9L"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "# This list is taken from LangChain's MarkdownTextSplitter class\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
        "    chunk_overlap=100,  # The number of characters to overlap between chunks\n",
        "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "    separators=MARKDOWN_SEPARATORS,\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5jJUMgb9-9M"
      },
      "source": [
        "We also have to keep in mind that when embedding documents, we will use an embedding model that accepts a certain maximum sequence length `max_seq_length`.\n",
        "\n",
        "So we should make sure that our chunk sizes are below this limit because any longer chunk will be truncated before processing, thus losing relevancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8fa22cd3a3a142ffaf0953a358256bea",
            "40113e410df44dc48b524e77dc2a930a",
            "f9205b86e9e84b888722224c5c12b63e",
            "732e26a898b34458a1e25a206fdd96a5",
            "90d7d9c827554de3a876c3791f9a0a14",
            "861db66f8a4e46628d9877d2a521e79c",
            "033d139f1a7e4df4b8ac13bc5d929271",
            "ac8bfc67b1f54b0c960d2d664ba7fd17",
            "d6d019c7f36841a399a82cf62c666553",
            "a95d67ea9e4745c2af96705ece91885e",
            "97c860e8d11c4edd8277e9289582282f",
            "46319f2020c4484c87fd0ee044d11154",
            "877d4a725a944be4b7a5885885973750",
            "89364c7bb0ad4af98c3c4beedf6c8310",
            "0da3d95ac0bc4d2399d9c0cad4f998f7",
            "84a2c079c32b41e7bdc4351db25a881c",
            "e68224d3707e4602b40fb5ea6cda4c13",
            "a55c621ce4a844b7ba4989e51feb06d8",
            "d83ff7370df64a238653c81d8e6c74a5",
            "d14e34c7bc14417fa0a8d76509c13741",
            "98b05baf2dfe4fc5a8c90b1c27a07717",
            "feff46742d4e4252b69731f7bc957670",
            "6433164578154666a197c63079c4704d",
            "d14a420d791946f39f437fbb4176bf69",
            "c317c8747f514579bb6469f3e6c644d9",
            "7432d11752d04280b063c798061e6ff8",
            "3f70dc5340a440caadc32dc5ee1ae94c",
            "438613c7eae14075be433d9a72c70edc",
            "351a3ddc6c37490d95d7556927c4d573",
            "9f5d203fd0b94266870b36159c7d9f59",
            "a6470dc9ad674bc89a4f3a91e12aa269",
            "e72aa9440380409d8c5255b939455057",
            "2b85407534af473db459161367bf52eb",
            "a6680c52c24b44318832f13507688580",
            "cd2eb63ea9394fd6b0d1cbe5d052f19e",
            "bc7ce257adee4c3491a478e4117e8dea",
            "199055a7b6ca43fe90dfd7c9fcdf5744",
            "ac88f488ca554fcfaa06cd25a62134a8",
            "10825a808ecf43f4bfd84e2e6779fdd0",
            "d26a7f1cabeb4fb29a68b0649a5b596a",
            "2ee6d36ebcb344818988e2837d4c4202",
            "847d2f0c03d541d7942dc3429cb908ad",
            "32089ee021cf456381edde2a6c28adcd",
            "073f0aee47a64818864ed9eca235f646",
            "db6ec575efaa4866833ffa7df4898d5e",
            "9c1804a77a264112a4625d3b3ebc9eff",
            "d31e87be2bdc40aa9e2c11110ead05ce",
            "9a2654c2dfd24fc7a8f1daf27a68be43",
            "9560ea595e1b4a18b773344030fc6017",
            "fe74ab37d3e649c3bbf68fd74dab9bc4",
            "5e7d2aa5e2774529bdc02202c4cb546f",
            "d69628cd98cf4c78bdd3c20777ae1517",
            "a02c01305cbe4b94ba12754b44ac3d8d",
            "3580abd2c32d4357892730148470a001",
            "762273b4dee54ef49ed0570f69992349",
            "a057640b556c43549efd0c2c333891ef",
            "5e3d18047c0c4f6685722bf2dc3c546f",
            "dc4986b64a824501970527e66735cfdd",
            "a739c8fe9ec74a489b9ebc2b4dfea733",
            "e5f783adf1cf406e84013b2e53767b36",
            "bc0c882af0794af698e353b51e154639",
            "781710943d1e4ce3b2124307dc4fb634",
            "2d0178a713bf4268a86ce8e0744c5d5f",
            "ad9df7c3eb9c4017b2baebd30cf978a5",
            "8b6607a65b8e43cfaac9c32e2282fa2b",
            "c695fa9b175743c7acc972490200dc88",
            "1c4be8013fe04dceb145dbed4ea414bf",
            "bc92dcc01841465eb8d5e191b0fc5567",
            "297bd42339a249c892586fe279f444fc",
            "671b82d3d4e44a389424a5680479bf48",
            "34946db0d59746d9acb78689c435c5bb",
            "efe9d1eaa3524a1ebc9e4c64503e8239",
            "b4ba7f9d62e94885a7afd536f8717fc1",
            "4addb452efe2476da0f2f9d24a14b811",
            "bccbff19155d4e568c5e7a474764cbb7",
            "39c8713aa3564bdbbf537935f90bce30",
            "d24f05c1c3f44394afa51ed1be714e50",
            "1af5e2fc153d4e318fa6e50c1887798f",
            "2fb2954f79764fd0ba84eec07ca048b1",
            "ea41dc9ceacb478f85b9be965b5afe38",
            "edf5479146d0442ab1120a2ec4f68fd5",
            "90a7a948c2fd4706bbe74d702911aafa",
            "c604f230cf6f4ab9b661ca37e79184e7",
            "accfc18e92aa473a99220e39fade35c9",
            "3f5849382e11483ab66b58243230221c",
            "2a34e7d19f934c529f0fb04a923dd74c",
            "ef75e56db1fc48d59f3fe4da72c5a9f2",
            "601b195ceab84040886859486565fa94",
            "1f6a6b881d7d4c38ae42a97a22e01eb5",
            "7f7583fe51b546198ec901c2dec3b6ca",
            "2fc63d0d9ad146c5a11fd96a8d265966",
            "6fd92beb5c7e4c0781d40b090a723133",
            "e26802d6860d4f49b37a586281320e8e",
            "49ff2d6a04a147899a171c20fb51a4a0",
            "9dcc78a1063a4d2ba6d62c3ad096db0f",
            "ff12cce13b664d35b83c7e54300d3df4",
            "6797225b28334783a8b2f8d2b15ad1e1",
            "b6a494af70e84505b4b43b620e4c8ce0",
            "d59115c0e9374144baa07282052ba903",
            "fb85d9aef8be44ebb1186a58c501c54b",
            "b394afb8c9dd44ca9619c73620650199",
            "90da8f25f914451fa40491dec9d4c552",
            "67d0f0a2453f4a73912a13006e031f3d",
            "f1c37e9658e746e889ebef3db607e7be",
            "40734e8590e6413483cc96bbc659d61d",
            "13c7fbd39f30460d9a832edcbe7fd9c8",
            "3571aa91285c4c19a2a947b4c3d4fc1c",
            "089b3bd121b54b06a95eaf6c95f5c6db",
            "43b46ac6580a43dd96581a7919cd7b5b",
            "3f2c7a5ca1a549e89d0424bc64190f31",
            "2267dca102e346548fbaca4f5ef2f455",
            "b2235d81bd394df08bffd909a3a4991d",
            "4b59ed049be84b25ad17725bbff25140",
            "33807852497d448da2979f290da318b7",
            "00fb0e954c8443bb89f55d34b9f53fd4",
            "c4594e84e4bf4af99cd66bba3f456c03",
            "7eb70d98bf6f4381843a96cc37d184bd",
            "a36ac9a7444f40c98df94205cc268263",
            "8afae79c18f347a0b1a8dab34abf3849",
            "834b697beca14883b0cbddc9f19fb092",
            "7eb31c03fd61491dba8e556033aa3046"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "B4hoki349-9M",
        "outputId": "1f3e338f-86f4-4698-9592-f63be8fce5a2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fa22cd3a3a142ffaf0953a358256bea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46319f2020c4484c87fd0ee044d11154"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6433164578154666a197c63079c4704d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6680c52c24b44318832f13507688580"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db6ec575efaa4866833ffa7df4898d5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a057640b556c43549efd0c2c333891ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c4be8013fe04dceb145dbed4ea414bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1af5e2fc153d4e318fa6e50c1887798f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f6a6b881d7d4c38ae42a97a22e01eb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb85d9aef8be44ebb1186a58c501c54b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's maximum sequence length: 512\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/31085 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2267dca102e346548fbaca4f5ef2f455"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-979afccad2e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Plot the distribution of document lengths, counted as the number of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distribution of document lengths in the knowledge base (in count of tokens)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\"\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "\n",
        "# Plot the distribution of document lengths, counted as the number of tokens\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd # Import pandas and define the alias pd\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\"\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "\n",
        "# Plot the distribution of document lengths, counted as the number of tokens\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "A-e4OnugT1Ed",
        "outputId": "1316cf93-e558-4643-dd6a-90cfac79277c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501,
          "referenced_widgets": [
            "adf709a94f63457590fbd0f6ecc1dce7",
            "f5de3ceed147446fabc1862fd9620c78",
            "e3bf1dc095844bf7aadf67e188287c7a",
            "757d7b2fb1984f388ca24ed1e329165d",
            "61e8a3c008384612b1947cf92543d58e",
            "d2c8464c9a3a4b93a676c28bd1945b3d",
            "907d4bb764554778a49fcceebaebc1d3",
            "72101cbe483b4d78b8b0c4244413a653",
            "75d8bf5808514831bde7329f500c5e92",
            "4b5ed4c3264a459fbc89c902ead0deb5",
            "edee2264920042ada4d9554f43ef6ebe"
          ]
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's maximum sequence length: 512\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/31085 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adf709a94f63457590fbd0f6ecc1dce7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd # Import pandas and define the alias pd\n",
        "import matplotlib.pyplot as plt # Import matplotlib.pyplot here\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\"\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "\n",
        "# Plot the distribution of document lengths, counted as the number of tokens\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zW-BoHfOT_qu",
        "outputId": "305255f9-688f-4d42-8bf3-22deb774fe14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501,
          "referenced_widgets": [
            "1484f4dc9cae4b9d820f789c7b44f291",
            "98cdf507861245ecae4bde37a21e6db0",
            "0b30b713a8fa47bda943f1b49f138d3c",
            "837c385289ca4ae3b0be5afb85853215",
            "a4db75e28ffe4e5ebd448b1175db99db",
            "b63d9cbff0334b859c885668bda47ec3",
            "dcbe0862509d477ab3f440ddb9adb991",
            "7005f98fe22e4446a9978dc767afeac9",
            "b98f796b2f27410f90c8739d8141a95e",
            "694eb29da59e4577a48bebd7eb8236a8",
            "ba8cce4a12974d288000549afa143e7a"
          ]
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's maximum sequence length: 512\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/31085 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1484f4dc9cae4b9d820f789c7b44f291"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAGzCAYAAAChApYOAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVLhJREFUeJzt3XlcFfXi//E3yK4CogmiqFwt9y0sxb1EcE3LJdPSTPOWetMsLSvNpXIrc03zdtMWvZZW5rVScSk1yS1xS83KsjSgRMQVET6/P/qd+XoEdFA4QL6ej4ePOp/5nM985jNzZt5nZs7gZowxAgAAAK7BvaA7AAAAgKKB4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwJd+D49ixY+Xm5pbfs5EktWrVSq1atbJef/nll3Jzc9OyZctcMv+HH35YlStXdsm8rteZM2c0YMAAhYSEyM3NTcOGDct1G25ubho7dmye9+1mVLlyZT388MMF3Y1revjhh1WiRIl8nYertitX7Rdcvf+5UT///LPc3Ny0cOHCPGtz4cKFcnNz088//5xnbdpVuXJldezY0eXzvVFnzpxR2bJltWjRIqvMlcfRv7u8OAba5dj+d+zYkW/zuF49e/ZUjx49ruu9uQqOjkFw/PPx8VFoaKhiYmI0c+ZMnT59+ro6caXjx49r7Nixio+Pz5P28lJh7psdr7zyihYuXKjHH39c7733nh566KGC7tLfyuLFizV9+vSC7sZ1OXfunMaOHasvv/yyoLuSJ4ryusDNa8aMGSpZsqR69uxZ0F0pUK+88oqWL1+eL+3aPQbmVx8Kg2eeeUYfffSRdu/enev3XtcZx/Hjx+u9997T3Llz9a9//UuSNGzYMNWpU0d79uxxqvvCCy/o/PnzuWr/+PHjGjduXK7D2Zo1a7RmzZpcvSe3rta3f//73zp06FC+zv9GrV+/Xo0bN9aLL76oBx98UBEREQXdpb+VohxWzp07p3HjxhVYcDx//rxeeOGFPGuvKK8L3JzS09M1Y8YMDRgwQMWKFbPKr+c4WtTlV2jLzTHw7xwcGzRooIYNG+q1117L9XuvKzi2a9dODz74oPr166dRo0Zp9erVWrt2rZKSknTPPfc4beAeHh7y8fG5ntnYdu7cOUmSl5eXvLy88nVeV+Pp6Slvb+8Cm78dSUlJCgwMLOhuAFn4+PjIw8OjoLsBFJiVK1fqjz/+yHIJ0RXH0ZsFx8D/06NHD3388cc6c+ZMrt6XZ/c43n333Ro9erR++eUXvf/++1Z5dvdmxMbGqlmzZgoMDFSJEiVUrVo1Pffcc5L+ui/ojjvukCT169fPuizuuO+mVatWql27tnbu3KkWLVrIz8/Peu+V9zg6ZGRk6LnnnlNISIiKFy+ue+65R7/++qtTnZzuNbu8zWv1Lbt7HM+ePaunnnpKYWFh8vb2VrVq1fTqq6/KGONUz83NTUOGDNHy5ctVu3ZteXt7q1atWlq1alX2A36FpKQk9e/fX8HBwfLx8VG9evX0zjvvWNMd91sdOXJEn332mdX3q917lJaWpieffFK33HKLSpYsqXvuuUe//fZbtnV37dqldu3ayd/fXyVKlFDr1q31zTffZKmXkpKiJ598UpUrV5a3t7cqVKigPn366M8//5SU8z1Rjv5ffjbMsS3s2bNHLVu2lJ+fn6pWrWrdU/bVV1+pUaNG8vX1VbVq1bR27dos/Tl27JgeeeQRBQcHW2P+9ttvZzvvDz/8UC+//LIqVKggHx8ftW7dWj/88INTfz777DP98ssv1vhezz2vKSkpGjZsmLXNVK1aVZMnT1ZmZqZVx3E/2quvvqr58+erSpUq8vb21h133KHt27dnaXPp0qWqWbOmfHx8VLt2bX3yySdO2+vPP/+sW265RZI0btw4q/9X3nN47NgxdenSRSVKlNAtt9yip59+WhkZGU51lixZooiICJUsWVL+/v6qU6eOZsyYcc3lvnJ+jn3HDz/8oIcffliBgYEKCAhQv379rC+LObGzLjIzM6+6Ph22bt2qtm3bKiAgQH5+fmrZsqW+/vrray5PdtLS0tSxY0cFBARoy5YtuV7OS5cuacKECdb6rly5sp577jmlpaVZdYYPH67SpUs77WP+9a9/yc3NTTNnzrTKEhMT5ebmprlz5161zwcPHlS3bt0UFBQkHx8fNWzYUCtWrMhSb//+/br77rvl6+urChUq6KWXXnLaZh0yMzM1duxYhYaGys/PT3fddZe+++67bPfBdj4L17JmzRrVr19fPj4+qlmzpj7++GOn6cnJyXr66adVp04dlShRQv7+/mrXrl22l/BmzZqlWrVqyc/PT6VKlVLDhg21ePFipzp29ik5Wb58uSpXrqwqVao4lWd3HL3RY8aFCxc0duxY3XbbbfLx8VG5cuV033336ccff7Tq2Dl+Xe3e2Ov9TLu5uens2bN65513rM/vte4Fz+tj4LX6YPeYd6WTJ0/qzjvvVIUKFawrlGlpaXrxxRdVtWpVeXt7KywsTCNHjnT6XDv6ZGednz59WsOGDbOOs2XLllWbNm307bffOtVr06aNzp49q9jY2Gv2+3J5+vX+oYce0nPPPac1a9bo0UcfzbbO/v371bFjR9WtW1fjx4+Xt7e3fvjhB2tHXKNGDY0fP15jxozRwIED1bx5c0lSkyZNrDZOnDihdu3aqWfPnnrwwQcVHBx81X69/PLLcnNz0zPPPKOkpCRNnz5dUVFRio+Pl6+vr+3ls9O3yxljdM8992jDhg3q37+/6tevr9WrV2vEiBE6duyYXn/9daf6mzdv1scff6xBgwapZMmSmjlzprp27aqjR4+qdOnSOfbr/PnzatWqlX744QcNGTJE4eHhWrp0qR5++GGlpKRo6NChqlGjht577z09+eSTqlChgp566ilJssJCdgYMGKD3339fvXr1UpMmTbR+/Xp16NAhS739+/erefPm8vf318iRI+Xp6ak333xTrVq1ssKb9NdNyc2bN9eBAwf0yCOP6Pbbb9eff/6pFStW6LffflOZMmWuvgKycfLkSXXs2FE9e/ZU9+7dNXfuXPXs2VOLFi3SsGHD9Nhjj6lXr16aOnWqunXrpl9//VUlS5aU9NeBs3HjxtaH8ZZbbtEXX3yh/v37KzU1NctN05MmTZK7u7uefvppnTp1SlOmTFHv3r21detWSdLzzz+vU6dO6bfffrPWbW5/UHLu3Dm1bNlSx44d0z//+U9VrFhRW7Zs0ahRo/T7779nufS6ePFinT59Wv/85z/l5uamKVOm6L777tNPP/0kT09PSdJnn32m+++/X3Xq1NHEiRN18uRJ9e/fX+XLl7faueWWWzR37lw9/vjjuvfee3XfffdJkurWrWvVycjIUExMjBo1aqRXX31Va9eu1WuvvaYqVaro8ccfl/TXl8IHHnhArVu31uTJkyVJBw4c0Ndff62hQ4fmaiwcevToofDwcE2cOFHffvut3nrrLZUtW9ZqPzt21sW11qf012Wtdu3aKSIiQi+++KLc3d21YMEC3X333dq0aZPuvPNO28tx/vx5de7cWTt27NDatWutL6G5Wc4BAwbonXfeUbdu3fTUU09p69atmjhxog4cOKBPPvlEktS8eXO9/vrr2r9/v2rXri1J2rRpk9zd3bVp0yY98cQTVpkktWjRIsc+79+/X02bNlX58uX17LPPqnjx4vrwww/VpUsXffTRR7r33nslSQkJCbrrrrt06dIlq978+fOz3b+OGjVKU6ZMUadOnRQTE6Pdu3crJiZGFy5ccKqX289Cdg4fPqz7779fjz32mPr27asFCxaoe/fuWrVqldq0aSNJ+umnn7R8+XJ1795d4eHhSkxM1JtvvqmWLVvqu+++U2hoqKS/bkV64okn1K1bNw0dOlQXLlzQnj17tHXrVvXq1UtS7vcpV9qyZYtuv/32ay6Xw/UeMzIyMtSxY0etW7dOPXv21NChQ3X69GnFxsZq3759qlKlSq6PX7lxrW39vffe04ABA3TnnXdq4MCBkpQlTF8uP46BV+uD3WPelf7880+1adNGycnJ+uqrr1SlShVlZmbqnnvu0ebNmzVw4EDVqFFDe/fu1euvv67vv/8+y6VyO+v8scce07JlyzRkyBDVrFlTJ06c0ObNm3XgwAGn7atmzZry9fXV119/bX2WbTG5sGDBAiPJbN++Pcc6AQEBpkGDBtbrF1980Vw+m9dff91IMn/88UeObWzfvt1IMgsWLMgyrWXLlkaSmTdvXrbTWrZsab3esGGDkWTKly9vUlNTrfIPP/zQSDIzZsywyipVqmT69u17zTav1re+ffuaSpUqWa+XL19uJJmXXnrJqV63bt2Mm5ub+eGHH6wyScbLy8upbPfu3UaSmTVrVpZ5XW769OlGknn//fetsosXL5rIyEhTokQJp2WvVKmS6dChw1XbM8aY+Ph4I8kMGjTIqbxXr15GknnxxRetsi5duhgvLy/z448/WmXHjx83JUuWNC1atLDKxowZYySZjz/+OMv8MjMzjTH/t40dOXLEabpjXW7YsMEqc2wLixcvtsoOHjxoJBl3d3fzzTffWOWrV6/Ost769+9vypUrZ/7880+nefXs2dMEBASYc+fOOc27Ro0aJi0tzao3Y8YMI8ns3bvXKuvQoYPTNnAtV253EyZMMMWLFzfff/+9U71nn33WFCtWzBw9etQYY8yRI0eMJFO6dGmTnJxs1fv000+NJPO///3PKqtTp46pUKGCOX36tFX25ZdfGklOff3jjz+yrFuHvn37Gklm/PjxTuUNGjQwERER1uuhQ4caf39/c+nSJdtj4HDlvB37jkceecSp3r333mtKly59zfZyWhd212dmZqa59dZbTUxMjLV9GmPMuXPnTHh4uGnTps1V5++Yz9KlS83p06dNy5YtTZkyZcyuXbuc6tldTsdncsCAAU71nn76aSPJrF+/3hhjTFJSkpFk3njjDWOMMSkpKcbd3d10797dBAcHW+974oknTFBQkLVsjm3q8s9I69atTZ06dcyFCxessszMTNOkSRNz6623WmXDhg0zkszWrVutsqSkJBMQEOD0eU5ISDAeHh6mS5cuTsswduxYI+m6Pgs5qVSpkpFkPvroI6vs1KlTply5ck7HqAsXLpiMjAyn9x45csR4e3s7be+dO3c2tWrVuuo87e5TspOenm7c3NzMU089lWXalcdRY27smPH2228bSWbatGlZpjm2B7vHr+y2m8v7eL2f6eLFi2d7TM5OfhwDr9YHu8e8yzPT77//bmrVqmX+8Y9/mJ9//tmq89577xl3d3ezadMmp3nMmzfPSDJff/21VWZ3nQcEBJjBgwfbWsbbbrvNtGvXzlZdhzx/HE+JEiWu+utqx70Fn376aa4uN1zO29tb/fr1s12/T58+1lkmSerWrZvKlSunzz///Lrmb9fnn3+uYsWKWd/wHZ566ikZY/TFF184lUdFRTl9q6pbt678/f31008/XXM+ISEheuCBB6wyT09PPfHEEzpz5oy++uqr6+q7pCx9v/Ibc0ZGhtasWaMuXbroH//4h1Verlw59erVS5s3b1Zqaqok6aOPPlK9evWy/WZzvY+aKFGihNOvD6tVq6bAwEDVqFHD6Vuf4/8dY2mM0UcffaROnTrJGKM///zT+hcTE6NTp05lOa3fr18/p3toHWecr7V+cmPp0qVq3ry5SpUq5dSnqKgoZWRkaOPGjU7177//fpUqVSrHPh0/flx79+5Vnz59nM64tWzZUnXq1Ml1/x577DGn182bN3da/sDAwOu69JHbeZ44ccLarq7XtdZnfHy8Dh8+rF69eunEiRPWujh79qxat26tjRs32tqHnTp1StHR0Tp48KC+/PJL1a9fP9t611pOx2dy+PDhTvUcZ04+++wzSX+dQalevbq1rXz99dcqVqyYRowYocTERB0+fFjSX2ccmzVrluNnLzk5WevXr1ePHj10+vRpa/lPnDihmJgYHT58WMeOHbP61rhxY6czsLfccot69+7t1Oa6det06dIlDRo0yKnc8SPLy+X2s5Cd0NBQp/2Nv7+/+vTpo127dikhIUHSX8cTd/e/DoUZGRk6ceKEdQvV5fuAwMBA/fbbb9neCiJd3z7lcsnJyTLGOH2er+V6jxkfffSRypQpk+24O7aH3B6/ciOvP9P5cQzMSW6OeQ6//fabWrZsqfT0dG3cuFGVKlWypi1dulQ1atRQ9erVnbaZu+++W5K0YcMGp7bsrPPAwEBt3bpVx48fv+byOD5fuZHnd6I7nkGVk/vvv19vvfWWBgwYoGeffVatW7fWfffdp27dulkf3mspX758rn4Ec+uttzq9dnNzU9WqVfP92WK//PKLQkNDnUKr9Nclb8f0y1WsWDFLG6VKldLJkyevOZ9bb701y/jlNB+7fXd3d89yeaBatWpOr//44w+dO3cuS7lj/pmZmfr1119Vq1Yt/fjjj+ratWuu+3I1FSpUyHLgCwgIUFhYWJYySdZY/vHHH0pJSdH8+fM1f/78bNtOSkpyen3l+nHs4K+1fnLj8OHD2rNnT46XT3LbJ8e6r1q1apa2qlatetUD2ZV8fHyy9OvK7XPQoEH68MMP1a5dO5UvX17R0dHq0aOH2rZta3s+V7raMvr7++dLu5KsgNW3b98c2zh16tQ1D/TDhg3ThQsXtGvXLtWqVeu6+uPv7299Jq9clyEhIQoMDHT6nDdv3twKmps2bVLDhg3VsGFDBQUFadOmTQoODtbu3butS6zZ+eGHH2SM0ejRozV69Ohs6yQlJal8+fL65Zdfsr08d+V+IaftMSgoKMs45vazkJ2qVatm2T/cdtttkv66Ny8kJESZmZmaMWOG3njjDR05csTpnt3LL/c+88wzWrt2re68805VrVpV0dHR6tWrl5o2bSrp+vYp2TFX3P9+Ndd7zPjxxx9VrVq1q/4YLbfHr9zI6890fhwDc5KbY57DQw89JA8PDx04cEAhISFO7zl8+LAOHDhw3ft8Kes6nzJlivr27auwsDBFRESoffv26tOnj1PQdTDG5PrETZ4Gx99++02nTp3K9iDl4Ovrq40bN2rDhg367LPPtGrVKn3wwQe6++67tWbNGqdHEFytjbyW08BlZGTY6lNeyGk+udmRFHVXWw/ZyWnMrjWWjjNFDz74YI7B4PL7++y0mRcyMzPVpk0bjRw5MtvpjoOeK/t0rXldrmzZsoqPj9fq1av1xRdf6IsvvtCCBQvUp08fpxvV82K+N7qMdreRqVOn5niW0M49rJ07d9aSJUs0adIkvfvuuzl+Qba7nHZ28s2aNdO///1v/fTTT9q0aZOaN28uNzc3NWvWTJs2bVJoaKgyMzOts6zZcSz/008/rZiYmGzrXG1ff6Ny+1m4Xq+88opGjx6tRx55RBMmTFBQUJDc3d01bNgwpzPKNWrU0KFDh7Ry5UqtWrVKH330kd544w2NGTNG48aNu659yuWCgoLk5uaWqy+iheGYkdt9tlQ4+u1K9913n959913NmDFDEydOdJqWmZmpOnXqaNq0adm+98qTIHbGrkePHmrevLk++eQTrVmzRlOnTtXkyZP18ccfq127dk7vO3nyZJaTa9eSp8Hxvffek6QcdzIO7u7uat26tVq3bq1p06bplVde0fPPP68NGzYoKioqz5+Q7zhz4GCM0Q8//OD0IS5VqpRSUlKyvPeXX35xSum56VulSpW0du1anT592ulb28GDB63peaFSpUras2ePMjMznQ5KNzKfSpUqKTMz0/pm6nDlcypvueUW+fn5Zfv8yoMHD8rd3d3a8KtUqaJ9+/Zddb6Ob55Xrou8/MYoyfqleEZGhqKiovKs3RvddqtUqaIzZ87kWZ8c6z67XwtfWZZXnzsvLy916tRJnTp1UmZmpgYNGqQ333xTo0ePztegcaW8WBfSX5c3b2R9dOnSRdHR0Xr44YdVsmTJa/6KOSeOz+Thw4etMynSXz/ISElJcfqcOwJhbGystm/frmeffVbSXz+EmTt3rkJDQ1W8ePGrPsPOsd/z9PS85vJXqlQpy35Wyrq/uHx7DA8Pt8pPnDiRJTDlxWfBcdb08m3h+++/lyTrV/bLli3TXXfdpf/85z9O701JScnyg73ixYvr/vvv1/3336+LFy/qvvvu08svv6xRo0bd8D7Fw8NDVapU0ZEjR3L93tyqUqWKtm7dqvT0dOtHdFeye/zKr312bo+1eX0MzKkPuTnmOfzrX/9S1apVNWbMGAUEBFifR+mvdbF79261bt06T7NPuXLlNGjQIA0aNEhJSUm6/fbb9fLLLzsFx0uXLunXX3/VPffck6u28+wex/Xr12vChAkKDw/Pcl/L5ZKTk7OUOb7NO356Xrx4cUlZN8Tr9e677zrdd7ls2TL9/vvvTgNYpUoVffPNN7p48aJVtnLlyiyP7clN39q3b6+MjAzNnj3bqfz111+Xm5tbluR/vdq3b6+EhAR98MEHVtmlS5c0a9YslShRQi1btsx1m46+Xf74DklZfslYrFgxRUdH69NPP3W69J+YmKjFixerWbNm1qWHrl27avfu3davPy/n+LbkOFhffv9SRkZGjpd+rlexYsXUtWtXffTRR9mG2T/++OO62i1evLhOnTp13f3q0aOH4uLitHr16izTUlJSdOnSpVy1Fxoaqtq1a+vdd991elbXV199pb179zrV9fPzs+ZzvU6cOOH02t3d3fqCduWjJfLbja6LiIgIValSRa+++mq2zznLzTbSp08fzZw5U/PmzdMzzzxzXf1p3769pKyfQceZisufeBAeHq7y5cvr9ddfV3p6unU5tXnz5vrxxx+1bNkyNW7c+KqXKsuWLatWrVrpzTff1O+//55l+uXL3759e33zzTfatm2b0/TL/2yeJLVu3VoeHh5ZwvOV+0gpbz4Lx48fd9rfpKam6t1331X9+vWtS4bFihXLcqZr6dKl1v2bDldu215eXqpZs6aMMUpPT8+TfUpkZKRL/jxd165d9eeff2Y77o6xsHv88vf3V5kyZbLcc/rGG2/cUB+LFy9ue1+UH8fAnPqQm2Pe5UaPHq2nn35ao0aNctr+e/TooWPHjunf//53lvecP39eZ8+ezVWfMzIysuz3ypYtq9DQ0Cz74O+++04XLlzI8ckwObmuM45ffPGFDh48qEuXLikxMVHr169XbGysKlWqpBUrVlz1QaXjx4/Xxo0b1aFDB1WqVElJSUl64403VKFCBTVr1kzSX+EhMDBQ8+bNU8mSJVW8eHE1atTI6RtqbgQFBalZs2bq16+fEhMTNX36dFWtWtXpkUEDBgzQsmXL1LZtW/Xo0UM//vij3n///Sz3+OWmb506ddJdd92l559/Xj///LPq1aunNWvW6NNPP9WwYcOu+niB3Bg4cKDefPNNPfzww9q5c6cqV66sZcuW6euvv9b06dOz3KNiR/369fXAAw/ojTfe0KlTp9SkSROtW7cu2zNXL730kvVszkGDBsnDw0Nvvvmm0tLSNGXKFKveiBEjtGzZMnXv3l2PPPKIIiIilJycrBUrVmjevHmqV6+eatWqpcaNG2vUqFFKTk5WUFCQlixZkuvAZMekSZO0YcMGNWrUSI8++qhq1qyp5ORkffvtt1q7dm22X3KuJSIiQh988IGGDx+uO+64QyVKlFCnTp1sv3/EiBFasWKFOnbsqIcfflgRERE6e/as9u7dq2XLlunnn3/O9WOLXnnlFXXu3FlNmzZVv379dPLkSc2ePVu1a9d2CkS+vr6qWbOmPvjgA912220KCgpS7dq1rUe62DFgwAAlJyfr7rvvVoUKFfTLL79o1qxZql+/vtNZMle40XXh7u6ut956S+3atVOtWrXUr18/lS9fXseOHdOGDRvk7++v//3vf7bbGzJkiFJTU/X8888rICDAev6sXfXq1VPfvn01f/58paSkqGXLltq2bZveeecddenSRXfddZdT/ebNm2vJkiWqU6eOdVbo9ttvV/HixfX9999f9f5Ghzlz5qhZs2aqU6eOHn30Uf3jH/9QYmKi4uLi9Ntvv1nPOhw5cqTee+89tW3bVkOHDrUex+M4E+QQHBysoUOH6rXXXtM999yjtm3bavfu3friiy9UpkwZpzMuefFZuO2229S/f39t375dwcHBevvtt5WYmKgFCxZYdTp27Kjx48erX79+atKkifbu3atFixZluR8sOjpaISEhatq0qYKDg3XgwAHNnj1bHTp0sPaxN7pP6dy5s9577z19//33eXYpPjt9+vTRu+++q+HDh2vbtm1q3ry5zp49q7Vr12rQoEHq3Llzro5fAwYM0KRJkzRgwAA1bNhQGzdutM7sXq+IiAitXbtW06ZNU2hoqMLDw3N8zE1+HAOv1ge7x7wrTZ06VadOndLgwYNVsmRJPfjgg3rooYf04Ycf6rHHHtOGDRvUtGlTZWRk6ODBg/rwww+1evVqNWzY0HafT58+rQoVKqhbt26qV6+eSpQoobVr12r79u1Z/kpMbGys/Pz8rEdT2Zabn2A7flru+Ofl5WVCQkJMmzZtzIwZM5x+8u5w5WME1q1bZzp37mxCQ0ONl5eXCQ0NNQ888ECWRy58+umnpmbNmsbDw8Ppp/4tW7bM8ZEIOT2O57///a8ZNWqUKVu2rPH19TUdOnQwv/zyS5b3v/baa6Z8+fLG29vbNG3a1OzYsSNLm1fr25WP4zHGmNOnT5snn3zShIaGGk9PT3PrrbeaqVOnOj3ew5i/fmaf3c/nc3pM0JUSExNNv379TJkyZYyXl5epU6dOto9HyM2jCM6fP2+eeOIJU7p0aVO8eHHTqVMn8+uvv2b7yJZvv/3WxMTEmBIlShg/Pz9z1113mS1btmRp88SJE2bIkCGmfPnyxsvLy1SoUMH07dvX6fEVP/74o4mKijLe3t4mODjYPPfccyY2Njbbx/Fkty3ktIzZjXFiYqIZPHiwCQsLM56eniYkJMS0bt3azJ8/36pz+WNVLpfdYyjOnDljevXqZQIDA7M87iY72a3f06dPm1GjRpmqVasaLy8vU6ZMGdOkSRPz6quvmosXLzrNe+rUqdku55XrZ8mSJaZ69erG29vb1K5d26xYscJ07drVVK9e3aneli1bTEREhPHy8nJqp2/fvqZ48eJZ5nXl53vZsmUmOjralC1b1nh5eZmKFSuaf/7zn+b333+/6jhk129H21c+uiunRzZdKad1kZv1aYwxu3btMvfdd58pXbq08fb2NpUqVTI9evQw69atu+r8c5rPyJEjjSQze/bsXC9nenq6GTdunAkPDzeenp4mLCzMjBo1yulxOQ5z5swxkszjjz/uVB4VFWUkZel/Tsv/448/mj59+piQkBDj6elpypcvbzp27GiWLVvmVG/Pnj2mZcuWxsfHx5QvX95MmDDB/Oc//8myDJcuXTKjR482ISEhxtfX19x9993mwIEDpnTp0uaxxx5zatPOZyEnjv3A6tWrTd26dY23t7epXr16lvVx4cIF89RTT5ly5coZX19f07RpUxMXF5dl3//mm2+aFi1aWNtBlSpVzIgRI8ypU6ec2rOzT8lJWlqaKVOmjJkwYYJTeU6P47mRY8a5c+fM888/b21LISEhplu3bk6PmLF7/Dp37pzp37+/CQgIMCVLljQ9evSwHgt1vZ/pgwcPmhYtWhhfX98sj2rKTn4cA6/WBzvHvOweYZiRkWEeeOAB4+HhYZYvX26M+evRQZMnTza1atUy3t7eplSpUiYiIsKMGzfOafuys87T0tLMiBEjTL169UzJkiVN8eLFTb169azHc12uUaNG5sEHH7Q1Fpdz+/+dAXCTqV+/vm655ZY8fXQOcD1SUlJUqlQpvfTSS3r++ecLujsFasKECVqwYIEOHz7ssh9m4uYTHx+v22+/Xd9++22OP/7LSZ4/xxFA4ZKenp7lUv+XX36p3bt3Z/snOoH8dP78+Sxljvs22R6lJ598UmfOnNGSJUsKuiv4G5s0aZK6deuW69AoSZxxBP7mfv75Z0VFRenBBx9UaGioDh48qHnz5ikgIED79u276p8mA/LawoULtXDhQrVv314lSpTQ5s2b9d///lfR0dHZ/hAGQOGS5w8AB1C4lCpVShEREXrrrbf0xx9/qHjx4urQoYMmTZpEaITL1a1bVx4eHpoyZYpSU1OtH8y89NJLBd01ADZwxhEAAAC2cI8jAAAAbCE4AgAAwBbucbxOmZmZOn78uEqWLJnnfyIRAADkD2OMTp8+rdDQ0Bz/djxyRnC8TsePH8/y9ygBAEDR8Ouvv6pChQoF3Y0ih+B4nRx/wujXX3/N9u9SXo/09HStWbNG0dHROf7heeQNxto1GGfXYaxdg3F2nfwa69TUVIWFhV33nyK82REcr5Pj8rS/v3+eBkc/Pz/5+/uzQ8pnjLVrMM6uw1i7BuPsOvk91txmdn24uA8AAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwxaOgOwAUpNpjVystw62gu5ErP0/qUNBdAADcpDjjCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFtcGhw3btyoTp06KTQ0VG5ublq+fLk1LT09Xc8884zq1Kmj4sWLKzQ0VH369NHx48ed2khOTlbv3r3l7++vwMBA9e/fX2fOnHGqs2fPHjVv3lw+Pj4KCwvTlClTsvRl6dKlql69unx8fFSnTh19/vnn+bLMAAAAfxcuDY5nz55VvXr1NGfOnCzTzp07p2+//VajR4/Wt99+q48//liHDh3SPffc41Svd+/e2r9/v2JjY7Vy5Upt3LhRAwcOtKanpqYqOjpalSpV0s6dOzV16lSNHTtW8+fPt+ps2bJFDzzwgPr3769du3apS5cu6tKli/bt25d/Cw8AAFDEufQvx7Rr107t2rXLdlpAQIBiY2OdymbPnq0777xTR48eVcWKFXXgwAGtWrVK27dvV8OGDSVJs2bNUvv27fXqq68qNDRUixYt0sWLF/X222/Ly8tLtWrVUnx8vKZNm2YFzBkzZqht27YaMWKEJGnChAmKjY3V7NmzNW/evHwcAQAAgKKrUP/JwVOnTsnNzU2BgYGSpLi4OAUGBlqhUZKioqLk7u6urVu36t5771VcXJxatGghLy8vq05MTIwmT56skydPqlSpUoqLi9Pw4cOd5hUTE+N06fxKaWlpSktLs16npqZK+usSe3p6eh4srax28qo95Mwxxt7upoB7kntFaftgm3Ydxto1GGfXya+xZt3dmEIbHC9cuKBnnnlGDzzwgPz9/SVJCQkJKlu2rFM9Dw8PBQUFKSEhwaoTHh7uVCc4ONiaVqpUKSUkJFhll9dxtJGdiRMnaty4cVnK16xZIz8/v9wv4FVceeYV+WdCw8yC7kKuFcX7cdmmXYexdg3G2XXyeqzPnTuXp+3dbAplcExPT1ePHj1kjNHcuXMLujuSpFGjRjmdpUxNTVVYWJiio6OtYHuj0tPTFRsbqzZt2sjT0zNP2kT2HGM9eoe70jLdCro7ubJvbExBd8E2tmnXYaxdg3F2nfwaa8cVQ1yfQhccHaHxl19+0fr1651CWUhIiJKSkpzqX7p0ScnJyQoJCbHqJCYmOtVxvL5WHcf07Hh7e8vb2ztLuaenZ57vPPKjTWQvLdNNaRlFKzgWxW2Dbdp1GGvXYJxdJ6/HmvV2YwrVcxwdofHw4cNau3atSpcu7TQ9MjJSKSkp2rlzp1W2fv16ZWZmqlGjRladjRs3Ot3DEBsbq2rVqqlUqVJWnXXr1jm1HRsbq8jIyPxaNAAAgCLPpcHxzJkzio+PV3x8vCTpyJEjio+P19GjR5Wenq5u3bppx44dWrRokTIyMpSQkKCEhARdvHhRklSjRg21bdtWjz76qLZt26avv/5aQ4YMUc+ePRUaGipJ6tWrl7y8vNS/f3/t379fH3zwgWbMmOF0mXno0KFatWqVXnvtNR08eFBjx47Vjh07NGTIEFcOBwAAQJHi0uC4Y8cONWjQQA0aNJAkDR8+XA0aNNCYMWN07NgxrVixQr/99pvq16+vcuXKWf+2bNlitbFo0SJVr15drVu3Vvv27dWsWTOnZzQGBARozZo1OnLkiCIiIvTUU09pzJgxTs96bNKkiRYvXqz58+erXr16WrZsmZYvX67atWu7bjAAAACKGJfe49iqVSsZk/PjT642zSEoKEiLFy++ap26detq06ZNV63TvXt3de/e/ZrzAwAAwF8K1T2OAAAAKLwIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwxaXBcePGjerUqZNCQ0Pl5uam5cuXO003xmjMmDEqV66cfH19FRUVpcOHDzvVSU5OVu/eveXv76/AwED1799fZ86ccaqzZ88eNW/eXD4+PgoLC9OUKVOy9GXp0qWqXr26fHx8VKdOHX3++ed5vrwAAAB/Jy4NjmfPnlW9evU0Z86cbKdPmTJFM2fO1Lx587R161YVL15cMTExunDhglWnd+/e2r9/v2JjY7Vy5Upt3LhRAwcOtKanpqYqOjpalSpV0s6dOzV16lSNHTtW8+fPt+ps2bJFDzzwgPr3769du3apS5cu6tKli/bt25d/Cw8AAFDEebhyZu3atVO7du2ynWaM0fTp0/XCCy+oc+fOkqR3331XwcHBWr58uXr27KkDBw5o1apV2r59uxo2bChJmjVrltq3b69XX31VoaGhWrRokS5evKi3335bXl5eqlWrluLj4zVt2jQrYM6YMUNt27bViBEjJEkTJkxQbGysZs+erXnz5mXbv7S0NKWlpVmvU1NTJUnp6elKT0/Pk/FxtJNX7SFnjjH2djcF3JPcK0rbB9u06zDWrsE4u05+jTXr7sa4NDhezZEjR5SQkKCoqCirLCAgQI0aNVJcXJx69uypuLg4BQYGWqFRkqKiouTu7q6tW7fq3nvvVVxcnFq0aCEvLy+rTkxMjCZPnqyTJ0+qVKlSiouL0/Dhw53mHxMTk+XS+eUmTpyocePGZSlfs2aN/Pz8bmDJs4qNjc3T9pCzCQ0zC7oLuVYUb6tgm3Ydxto1GGfXyeuxPnfuXJ62d7MpNMExISFBkhQcHOxUHhwcbE1LSEhQ2bJlnaZ7eHgoKCjIqU54eHiWNhzTSpUqpYSEhKvOJzujRo1yCpupqakKCwtTdHS0/P39c7OoOUpPT1dsbKzatGkjT0/PPGkT2XOM9egd7krLdCvo7uTKvrExBd0F29imXYexdg3G2XXya6wdVwxxfQpNcCzsvL295e3tnaXc09Mzz3ce+dEmspeW6aa0jKIVHIvitsE27TqMtWswzq6T12PNersxheZxPCEhIZKkxMREp/LExERrWkhIiJKSkpymX7p0ScnJyU51smvj8nnkVMcxHQAAAFkVmuAYHh6ukJAQrVu3zipLTU3V1q1bFRkZKUmKjIxUSkqKdu7cadVZv369MjMz1ahRI6vOxo0bnW5+jY2NVbVq1VSqVCmrzuXzcdRxzAcAAABZuTQ4njlzRvHx8YqPj5f01w9i4uPjdfToUbm5uWnYsGF66aWXtGLFCu3du1d9+vRRaGiounTpIkmqUaOG2rZtq0cffVTbtm3T119/rSFDhqhnz54KDQ2VJPXq1UteXl7q37+/9u/frw8++EAzZsxwuj9x6NChWrVqlV577TUdPHhQY8eO1Y4dOzRkyBBXDgcAAECR4tJ7HHfs2KG77rrLeu0Ic3379tXChQs1cuRInT17VgMHDlRKSoqaNWumVatWycfHx3rPokWLNGTIELVu3Vru7u7q2rWrZs6caU0PCAjQmjVrNHjwYEVERKhMmTIaM2aM07MemzRposWLF+uFF17Qc889p1tvvVXLly9X7dq1XTAKAAAARZNLg2OrVq1kTM7PzXNzc9P48eM1fvz4HOsEBQVp8eLFV51P3bp1tWnTpqvW6d69u7p37371DgMAAMBSaO5xBAAAQOFGcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2OJR0B3A30flZz8r6C7Y5l3MaMqdBd0LAACKFs44AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMCWQhUcMzIyNHr0aIWHh8vX11dVqlTRhAkTZIyx6hhjNGbMGJUrV06+vr6KiorS4cOHndpJTk5W79695e/vr8DAQPXv319nzpxxqrNnzx41b95cPj4+CgsL05QpU1yyjAAAAEVVoQqOkydP1ty5czV79mwdOHBAkydP1pQpUzRr1iyrzpQpUzRz5kzNmzdPW7duVfHixRUTE6MLFy5YdXr37q39+/crNjZWK1eu1MaNGzVw4EBrempqqqKjo1WpUiXt3LlTU6dO1dixYzV//nyXLi8AAEBR4lHQHbjcli1b1LlzZ3Xo0EGSVLlyZf33v//Vtm3bJP11tnH69Ol64YUX1LlzZ0nSu+++q+DgYC1fvlw9e/bUgQMHtGrVKm3fvl0NGzaUJM2aNUvt27fXq6++qtDQUC1atEgXL17U22+/LS8vL9WqVUvx8fGaNm2aU8AEAADA/ylUwbFJkyaaP3++vv/+e912223avXu3Nm/erGnTpkmSjhw5ooSEBEVFRVnvCQgIUKNGjRQXF6eePXsqLi5OgYGBVmiUpKioKLm7u2vr1q269957FRcXpxYtWsjLy8uqExMTo8mTJ+vkyZMqVapUlr6lpaUpLS3Nep2amipJSk9PV3p6ep4sv6OdvGrP1byLmWtXKiS83Y3Tf4uSorR9FPVtuihhrF2DcXad/Bpr1t2NKVTB8dlnn1VqaqqqV6+uYsWKKSMjQy+//LJ69+4tSUpISJAkBQcHO70vODjYmpaQkKCyZcs6Tffw8FBQUJBTnfDw8CxtOKZlFxwnTpyocePGZSlfs2aN/Pz8rmdxcxQbG5un7bnKlDsLuge5N6FhZkF3Idc+//zzgu5CrhXVbbooYqxdg3F2nbwe63PnzuVpezebQhUcP/zwQy1atEiLFy+2Lh8PGzZMoaGh6tu3b4H2bdSoURo+fLj1OjU1VWFhYYqOjpa/v3+ezCM9PV2xsbFq06aNPD0986RNV6o9dnVBd8E2b3ejCQ0zNXqHu9Iy3Qq6O7myb2xMQXfBtqK+TRcljLVrMM6uk19j7bhiiOtTqILjiBEj9Oyzz6pnz56SpDp16uiXX37RxIkT1bdvX4WEhEiSEhMTVa5cOet9iYmJql+/viQpJCRESUlJTu1eunRJycnJ1vtDQkKUmJjoVMfx2lHnSt7e3vL29s5S7unpmec7j/xo0xXSMopWAJOktEy3ItfvorhtFNVtuihirF2DcXadvB5r1tuNKVS/qj537pzc3Z27VKxYMWVm/nU5MTw8XCEhIVq3bp01PTU1VVu3blVkZKQkKTIyUikpKdq5c6dVZ/369crMzFSjRo2sOhs3bnS6zyE2NlbVqlXL9jI1AAAACllw7NSpk15++WV99tln+vnnn/XJJ59o2rRpuvfeeyVJbm5uGjZsmF566SWtWLFCe/fuVZ8+fRQaGqouXbpIkmrUqKG2bdvq0Ucf1bZt2/T1119ryJAh6tmzp0JDQyVJvXr1kpeXl/r376/9+/frgw8+0IwZM5wuRQMAAMBZobpUPWvWLI0ePVqDBg1SUlKSQkND9c9//lNjxoyx6owcOVJnz57VwIEDlZKSombNmmnVqlXy8fGx6ixatEhDhgxR69at5e7urq5du2rmzJnW9ICAAK1Zs0aDBw9WRESEypQpozFjxvAoHgAAgKsoVMGxZMmSmj59uqZPn55jHTc3N40fP17jx4/PsU5QUJAWL1581XnVrVtXmzZtut6uAgAA3HQK1aVqAAAAFF4ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYUuiC47Fjx/Tggw+qdOnS8vX1VZ06dbRjxw5rujFGY8aMUbly5eTr66uoqCgdPnzYqY3k5GT17t1b/v7+CgwMVP/+/XXmzBmnOnv27FHz5s3l4+OjsLAwTZkyxSXLBwAAUFQVquB48uRJNW3aVJ6envriiy/03Xff6bXXXlOpUqWsOlOmTNHMmTM1b948bd26VcWLF1dMTIwuXLhg1endu7f279+v2NhYrVy5Uhs3btTAgQOt6ampqYqOjlalSpW0c+dOTZ06VWPHjtX8+fNdurwAAABFiUdBd+BykydPVlhYmBYsWGCVhYeHW/9vjNH06dP1wgsvqHPnzpKkd999V8HBwVq+fLl69uypAwcOaNWqVdq+fbsaNmwoSZo1a5bat2+vV199VaGhoVq0aJEuXryot99+W15eXqpVq5bi4+M1bdo0p4AJAACA/1OoguOKFSsUExOj7t2766uvvlL58uU1aNAgPfroo5KkI0eOKCEhQVFRUdZ7AgIC1KhRI8XFxalnz56Ki4tTYGCgFRolKSoqSu7u7tq6davuvfdexcXFqUWLFvLy8rLqxMTEaPLkyTp58qTTGU6HtLQ0paWlWa9TU1MlSenp6UpPT8+T5Xe0k1ftuZp3MVPQXbDN2904/bcoKUrbR1HfposSxto1GGfXya+xZt3dmEIVHH/66SfNnTtXw4cP13PPPaft27friSeekJeXl/r27auEhARJUnBwsNP7goODrWkJCQkqW7as03QPDw8FBQU51bn8TOblbSYkJGQbHCdOnKhx48ZlKV+zZo38/Pyuc4mzFxsbm6ftucqUOwu6B7k3oWFmQXch1z7//POC7kKuFdVtuihirF2DcXadvB7rc+fO5Wl7N5tCFRwzMzPVsGFDvfLKK5KkBg0aaN++fZo3b5769u1boH0bNWqUhg8fbr1OTU1VWFiYoqOj5e/vnyfzSE9PV2xsrNq0aSNPT888adOVao9dXdBdsM3b3WhCw0yN3uGutEy3gu5OruwbG1PQXbCtqG/TRQlj7RqMs+vk11g7rhji+hSq4FiuXDnVrFnTqaxGjRr66KOPJEkhISGSpMTERJUrV86qk5iYqPr161t1kpKSnNq4dOmSkpOTrfeHhIQoMTHRqY7jtaPOlby9veXt7Z2l3NPTM893HvnRpiukZRStACZJaZluRa7fRXHbKKrbdFHEWLsG4+w6eT3WrLcbU6h+Vd20aVMdOnTIqez7779XpUqVJP31Q5mQkBCtW7fOmp6amqqtW7cqMjJSkhQZGamUlBTt3LnTqrN+/XplZmaqUaNGVp2NGzc63ecQGxuratWqZXuZGgAAAIUsOD755JP65ptv9Morr+iHH37Q4sWLNX/+fA0ePFiS5ObmpmHDhumll17SihUrtHfvXvXp00ehoaHq0qWLpL/OULZt21aPPvqotm3bpq+//lpDhgxRz549FRoaKknq1auXvLy81L9/f+3fv18ffPCBZsyY4XQpGgAAAM4K1aXqO+64Q5988olGjRql8ePHKzw8XNOnT1fv3r2tOiNHjtTZs2c1cOBApaSkqFmzZlq1apV8fHysOosWLdKQIUPUunVrubu7q2vXrpo5c6Y1PSAgQGvWrNHgwYMVERGhMmXKaMyYMTyKBwAA4CoKVXCUpI4dO6pjx445Tndzc9P48eM1fvz4HOsEBQVp8eLFV51P3bp1tWnTpuvuJwAAwM2mUF2qBgAAQOFFcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANhCcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwBAAAgC0ERwAAANjiUdAdQPYqP/tZQXcBAADACWccAQAAYAvBEQAAALYU6kvVkyZN0qhRozR06FBNnz5dknThwgU99dRTWrJkidLS0hQTE6M33nhDwcHB1vuOHj2qxx9/XBs2bFCJEiXUt29fTZw4UR4e/7e4X375pYYPH679+/crLCxML7zwgh5++GEXLyGQe0XpNgbvYkZT7izoXgAA8kqhPeO4fft2vfnmm6pbt65T+ZNPPqn//e9/Wrp0qb766isdP35c9913nzU9IyNDHTp00MWLF7Vlyxa98847WrhwocaMGWPVOXLkiDp06KC77rpL8fHxGjZsmAYMGKDVq1e7bPkAAACKmkIZHM+cOaPevXvr3//+t0qVKmWVnzp1Sv/5z380bdo03X333YqIiNCCBQu0ZcsWffPNN5KkNWvW6LvvvtP777+v+vXrq127dpowYYLmzJmjixcvSpLmzZun8PBwvfbaa6pRo4aGDBmibt266fXXXy+Q5QUAACgKCuWl6sGDB6tDhw6KiorSSy+9ZJXv3LlT6enpioqKssqqV6+uihUrKi4uTo0bN1ZcXJzq1KnjdOk6JiZGjz/+uPbv368GDRooLi7OqQ1HnWHDhuXYp7S0NKWlpVmvU1NTJUnp6elKT0+/0UW22nL817uYyZM2kT1vd+P0X+QPx/jm1WcEObt8/4H8wzi7Tn6NNevuxhS64LhkyRJ9++232r59e5ZpCQkJ8vLyUmBgoFN5cHCwEhISrDqXh0bHdMe0q9VJTU3V+fPn5evrm2XeEydO1Lhx47KUr1mzRn5+fvYX0IbY2FjuC3ORCQ0zC7oLN4XY2NiC7sJNg7F2DcbZdfJ6rM+dO5en7d1sClVw/PXXXzV06FDFxsbKx8enoLvjZNSoURo+fLj1OjU1VWFhYYqOjpa/v3+ezCM9PV2xsbFq06aNGry8Pk/aRPa83Y0mNMzU6B3uSst0K+ju/G05xrlNmzby9PQs6O78rV2+/2Cs8w/j7Dr5NdaOK4a4PoUqOO7cuVNJSUm6/fbbrbKMjAxt3LhRs2fP1urVq3Xx4kWlpKQ4nXVMTExUSEiIJCkkJETbtm1zajcxMdGa5vivo+zyOv7+/tmebZQkb29veXt7Zyn39PTM852Hp6en0jIIM66QlunGWLtAfnxOkD3G2jUYZ9fJ67Fmvd2YQvXjmNatW2vv3r2Kj4+3/jVs2FC9e/e2/t/T01Pr1q2z3nPo0CEdPXpUkZGRkqTIyEjt3btXSUlJVp3Y2Fj5+/urZs2aVp3L23DUcbQBAACArArVGceSJUuqdu3aTmXFixdX6dKlrfL+/ftr+PDhCgoKkr+/v/71r38pMjJSjRs3liRFR0erZs2aeuihhzRlyhQlJCTohRde0ODBg60zho899phmz56tkSNH6pFHHtH69ev14Ycf6rPPis7z8QAAAFytUAVHO15//XW5u7ura9euTg8AdyhWrJhWrlypxx9/XJGRkSpevLj69u2r8ePHW3XCw8P12Wef6cknn9SMGTNUoUIFvfXWW4qJiSmIRQIAACgSCn1w/PLLL51e+/j4aM6cOZozZ06O76lUqZI+//zzq7bbqlUr7dq1Ky+6CAAAcFMoVPc4AgAAoPAiOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAlkIVHCdOnKg77rhDJUuWVNmyZdWlSxcdOnTIqc6FCxc0ePBglS5dWiVKlFDXrl2VmJjoVOfo0aPq0KGD/Pz8VLZsWY0YMUKXLl1yqvPll1/q9ttvl7e3t6pWraqFCxfm9+IBAAAUaYUqOH711VcaPHiwvvnmG8XGxio9PV3R0dE6e/asVefJJ5/U//73Py1dulRfffWVjh8/rvvuu8+anpGRoQ4dOujixYvasmWL3nnnHS1cuFBjxoyx6hw5ckQdOnTQXXfdpfj4eA0bNkwDBgzQ6tWrXbq8AAAARYlHQXfgcqtWrXJ6vXDhQpUtW1Y7d+5UixYtdOrUKf3nP//R4sWLdffdd0uSFixYoBo1auibb75R48aNtWbNGn333Xdau3atgoODVb9+fU2YMEHPPPOMxo4dKy8vL82bN0/h4eF67bXXJEk1atTQ5s2b9frrrysmJibbvqWlpSktLc16nZqaKklKT09Xenp6niy/o5309HR5FzN50iay5+1unP6L/OEY37z6jCBnl+8/kH8YZ9fJr7Fm3d2YQhUcr3Tq1ClJUlBQkCRp586dSk9PV1RUlFWnevXqqlixouLi4tS4cWPFxcWpTp06Cg4OturExMTo8ccf1/79+9WgQQPFxcU5teGoM2zYsBz7MnHiRI0bNy5L+Zo1a+Tn53cji5lFbGysptyZp00iBxMaZhZ0F24KsbGxBd2FmwZj7RqMs+vk9VifO3cuT9u72RTa4JiZmalhw4apadOmql27tiQpISFBXl5eCgwMdKobHByshIQEq87lodEx3THtanVSU1N1/vx5+fr6ZunPqFGjNHz4cOt1amqqwsLCFB0dLX9//xtb2P8vPT1dsbGxatOmjRq8vD5P2kT2vN2NJjTM1Ogd7krLdCvo7vxtOca5TZs28vT0LOju/K1dvv9grPMP4+w6+TXWjiuGuD6FNjgOHjxY+/bt0+bNmwu6K5Ikb29veXt7Zyn39PTM852Hp6en0jIIM66QlunGWLtAfnxOkD3G2jUYZ9fJ67Fmvd2YQvXjGIchQ4Zo5cqV2rBhgypUqGCVh4SE6OLFi0pJSXGqn5iYqJCQEKvOlb+ydry+Vh1/f/9szzYCAACgkAVHY4yGDBmiTz75ROvXr1d4eLjT9IiICHl6emrdunVW2aFDh3T06FFFRkZKkiIjI7V3714lJSVZdWJjY+Xv76+aNWtadS5vw1HH0QYAAACyKlSXqgcPHqzFixfr008/VcmSJa17EgMCAuTr66uAgAD1799fw4cPV1BQkPz9/fWvf/1LkZGRaty4sSQpOjpaNWvW1EMPPaQpU6YoISFBL7zwggYPHmxdan7sscc0e/ZsjRw5Uo888ojWr1+vDz/8UJ999lmBLTsAAEBhV6jOOM6dO1enTp1Sq1atVK5cOevfBx98YNV5/fXX1bFjR3Xt2lUtWrRQSEiIPv74Y2t6sWLFtHLlShUrVkyRkZF68MEH1adPH40fP96qEx4ers8++0yxsbGqV6+eXnvtNb311ls5PooHAAAAheyMozHXfqaej4+P5syZozlz5uRYp1KlSvr888+v2k6rVq20a9euXPcRAADgZlWozjgCAACg8CI4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFoIjAAAAbCE4AgAAwBaCIwAAAGwhOAIAAMAWgiMAAABsITgCAADAFo+C7gCAv7/aY1crLcOtoLuRKz9P6lDQXQCAQoczjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsIXgCAAAAFsIjgAAALCF4AgAAABbCI4AAACwheAIAAAAWwiOAAAAsMWjoDsAAIVR5Wc/K+gu5Ip3MaMpdxZ0LwD83XHGEQAAALYQHAEAAGDLTR8c58yZo8qVK8vHx0eNGjXStm3bCrpLAAAAhdJNHRw/+OADDR8+XC+++KK+/fZb1atXTzExMUpKSirorgEAABQ6N3VwnDZtmh599FH169dPNWvW1Lx58+Tn56e33367oLsGAABQ6Ny0v6q+ePGidu7cqVGjRlll7u7uioqKUlxcXJb6aWlpSktLs16fOnVKkpScnKz09PQ86VN6errOnTunEydOyOPS2TxpE9nzyDQ6dy5THunuysh0K+ju/G0xzq7jGOv6z3+stCI01ltHtS7oLuTK5ftpT0/Pgu7O31p+jfXp06clScaYPGvzZnLTBsc///xTGRkZCg4OdioPDg7WwYMHs9SfOHGixo0bl6U8PDw83/qI/NWroDtwk2CcXacojnWZ1wq6B7hZnT59WgEBAQXdjSLnpg2OuTVq1CgNHz7cep2Zmank5GSVLl1abm558+0+NTVVYWFh+vXXX+Xv758nbSJ7jLVrMM6uw1i7BuPsOvk11sYYnT59WqGhoXnW5s3kpg2OZcqUUbFixZSYmOhUnpiYqJCQkCz1vb295e3t7VQWGBiYL33z9/dnh+QijLVrMM6uw1i7BuPsOvkx1pxpvH437Y9jvLy8FBERoXXr1lllmZmZWrdunSIjIwuwZwAAAIXTTXvGUZKGDx+uvn37qmHDhrrzzjs1ffp0nT17Vv369SvorgEAABQ6N3VwvP/++/XHH39ozJgxSkhIUP369bVq1aosP5hxFW9vb7344otZLokj7zHWrsE4uw5j7RqMs+sw1oWTm+H36AAAALDhpr3HEQAAALlDcAQAAIAtBEcAAADYQnAEAACALQRHAAAA2EJwLETmzJmjypUry8fHR40aNdK2bdsKuktFysSJE3XHHXeoZMmSKlu2rLp06aJDhw451blw4YIGDx6s0qVLq0SJEuratWuWvx509OhRdejQQX5+fipbtqxGjBihS5cuuXJRipRJkybJzc1Nw4YNs8oY57xz7NgxPfjggypdurR8fX1Vp04d7dixw5pujNGYMWNUrlw5+fr6KioqSocPH3ZqIzk5Wb1795a/v78CAwPVv39/nTlzxtWLUmhlZGRo9OjRCg8Pl6+vr6pUqaIJEybo8oeOMM7XZ+PGjerUqZNCQ0Pl5uam5cuXO03Pq3Hds2ePmjdvLh8fH4WFhWnKlCn5vWg3L4NCYcmSJcbLy8u8/fbbZv/+/ebRRx81gYGBJjExsaC7VmTExMSYBQsWmH379pn4+HjTvn17U7FiRXPmzBmrzmOPPWbCwsLMunXrzI4dO0zjxo1NkyZNrOmXLl0ytWvXNlFRUWbXrl3m888/N2XKlDGjRo0qiEUq9LZt22YqV65s6tata4YOHWqVM855Izk52VSqVMk8/PDDZuvWreann34yq1evNj/88INVZ9KkSSYgIMAsX77c7N6929xzzz0mPDzcnD9/3qrTtm1bU69ePfPNN9+YTZs2mapVq5oHHnigIBapUHr55ZdN6dKlzcqVK82RI0fM0qVLTYkSJcyMGTOsOozz9fn888/N888/bz7++GMjyXzyySdO0/NiXE+dOmWCg4NN7969zb59+8x///tf4+vra958801XLeZNheBYSNx5551m8ODB1uuMjAwTGhpqJk6cWIC9KtqSkpKMJPPVV18ZY4xJSUkxnp6eZunSpVadAwcOGEkmLi7OGPPXTs7d3d0kJCRYdebOnWv8/f1NWlqaaxegkDt9+rS59dZbTWxsrGnZsqUVHBnnvPPMM8+YZs2a5Tg9MzPThISEmKlTp1plKSkpxtvb2/z3v/81xhjz3XffGUlm+/btVp0vvvjCuLm5mWPHjuVf54uQDh06mEceecSp7L777jO9e/c2xjDOeeXK4JhX4/rGG2+YUqVKOe07nnnmGVOtWrV8XqKbE5eqC4GLFy9q586dioqKssrc3d0VFRWluLi4AuxZ0Xbq1ClJUlBQkCRp586dSk9Pdxrn6tWrq2LFitY4x8XFqU6dOk5/PSgmJkapqanav3+/C3tf+A0ePFgdOnRwGk+Jcc5LK1asUMOGDdW9e3eVLVtWDRo00L///W9r+pEjR5SQkOA01gEBAWrUqJHTWAcGBqphw4ZWnaioKLm7u2vr1q2uW5hCrEmTJlq3bp2+//57SdLu3bu1efNmtWvXThLjnF/yalzj4uLUokULeXl5WXViYmJ06NAhnTx50kVLc/O4qf/kYGHx559/KiMjI8ufOgwODtbBgwcLqFdFW2ZmpoYNG6amTZuqdu3akqSEhAR5eXkpMDDQqW5wcLASEhKsOtmtB8c0/GXJkiX69ttvtX379izTGOe889NPP2nu3LkaPny4nnvuOW3fvl1PPPGEvLy81LdvX2usshvLy8e6bNmyTtM9PDwUFBTEWP9/zz77rFJTU1W9enUVK1ZMGRkZevnll9W7d29JYpzzSV6Na0JCgsLDw7O04ZhWqlSpfOn/zYrgiL+lwYMHa9++fdq8eXNBd+Vv59dff9XQoUMVGxsrHx+fgu7O31pmZqYaNmyoV155RZLUoEED7du3T/PmzVPfvn0LuHd/Hx9++KEWLVqkxYsXq1atWoqPj9ewYcMUGhrKOANX4FJ1IVCmTBkVK1Ysy69OExMTFRISUkC9KrqGDBmilStXasOGDapQoYJVHhISoosXLyolJcWp/uXjHBISku16cEzDX5eik5KSdPvtt8vDw0MeHh766quvNHPmTHl4eCg4OJhxziPlypVTzZo1ncpq1Kiho0ePSvq/sbraviMkJERJSUlO0y9duqTk5GTG+v8bMWKEnn32WfXs2VN16tTRQw89pCeffFITJ06UxDjnl7waV/YnrkVwLAS8vLwUERGhdevWWWWZmZlat26dIiMjC7BnRYsxRkOGDNEnn3yi9evXZ7l0ERERIU9PT6dxPnTokI4ePWqNc2RkpPbu3eu0o4qNjZW/v3+WA/jNqnXr1tq7d6/i4+Otfw0bNlTv3r2t/2ec80bTpk2zPFLq+++/V6VKlSRJ4eHhCgkJcRrr1NRUbd261WmsU1JStHPnTqvO+vXrlZmZqUaNGrlgKQq/c+fOyd3d+XBYrFgxZWZmSmKc80tejWtkZKQ2btyo9PR0q05sbKyqVavGZer8UNC/zsFflixZYry9vc3ChQvNd999ZwYOHGgCAwOdfnWKq3v88cdNQECA+fLLL83vv/9u/Tt37pxV57HHHjMVK1Y069evNzt27DCRkZEmMjLSmu54TEx0dLSJj483q1atMrfccguPibmGy39VbQzjnFe2bdtmPDw8zMsvv2wOHz5sFi1aZPz8/Mz7779v1Zk0aZIJDAw0n376qdmzZ4/p3Llzto8zadCggdm6davZvHmzufXWW2/6x8Rcrm/fvqZ8+fLW43g+/vhjU6ZMGTNy5EirDuN8fU6fPm127dpldu3aZSSZadOmmV27dplffvnFGJM345qSkmKCg4PNQw89ZPbt22eWLFli/Pz8eBxPPiE4FiKzZs0yFStWNF5eXubOO+8033zzTUF3qUiRlO2/BQsWWHXOnz9vBg0aZEqVKmX8/PzMvffea37//Xendn7++WfTrl074+vra8qUKWOeeuopk56e7uKlKVquDI6Mc9753//+Z2rXrm28vb1N9erVzfz5852mZ2ZmmtGjR5vg4GDj7e1tWrdubQ4dOuRU58SJE+aBBx4wJUqUMP7+/qZfv37m9OnTrlyMQi01NdUMHTrUVKxY0fj4+Jh//OMf5vnnn3d6vAvjfH02bNiQ7X65b9++xpi8G9fdu3ebZs2aGW9vb1O+fHkzadIkVy3iTcfNmMsejQ8AAADkgHscAQAAYAvBEQAAALYQHAEAAGALwREAAAC2EBwBAABgC8ERAAAAthAcAQAAYAvBEQAAALYQHAEAAGALwREAAAC2EBwBAABgy/8DOrVd80PekuAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3teXczl9-9M"
      },
      "source": [
        "ğŸ‘€ As you can see, __the chunk lengths are not aligned with our limit of 512 tokens__, and some documents are above the limit, thus some part of them will be lost in truncation!\n",
        " - So we should change the `RecursiveCharacterTextSplitter` class to count length in number of tokens instead of number of characters.\n",
        " - Then we can choose a specific chunk size, here we would choose a lower threshold than 512:\n",
        "    - Smaller documents could allow the split to focus more on specific ideas.\n",
        "    - But too small chunks would split sentences in half, thus losing meaning again: the proper tuning is a matter of balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9hvIL2jO9-9M",
        "outputId": "74213af7-bf11-4a07-90dd-64c3271ae00b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'List' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-97f3cd5ee555>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m def split_documents(\n\u001b[1;32m      8\u001b[0m     \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mknowledge_base\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLangchainDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtokenizer_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMBEDDING_MODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m ) -> List[LangchainDocument]:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    512,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n",
        "\n",
        "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Optional, List # Import List and Optional from typing\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    512,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n",
        "\n",
        "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fJUVZj4jUNBe",
        "outputId": "9a6e74d1-443f-4ce9-dfa2-b40af6bb79e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "60598824aa664b81b88ff30622f3de7a",
            "cb4ea892be7f40fba941f9b02c930f4c",
            "223b31681720423db92dcf844a5e52c6",
            "178e696659f94c888f0703b3b265c5d6",
            "de9e8e18aa4e4bc199e93979d4e08c8c",
            "991985d888f54512b020e9cae80a7475",
            "9e652fc588074b5ea9e26d1448bc34b9",
            "e74498868eb049d2882a26356a98477c",
            "9db2e9e27e5e4832b2aa0ce1e76f6f78",
            "da8bbef218ba4784b9873446a822583b",
            "3b8bb4140712421087e1e47e80a6558b"
          ]
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16776 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60598824aa664b81b88ff30622f3de7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc3riwX39-9M"
      },
      "source": [
        "â¡ï¸ Now the chunk length distribution looks better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ho-UKM9-9M"
      },
      "source": [
        "### 1.2 Building the vector database\n",
        "\n",
        "We want to compute the embeddings for all the chunks of our knowledge base: to learn more about sentence embeddings, we recommend reading [this guide](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/).\n",
        "\n",
        "#### How does retrieval work?\n",
        "\n",
        "Once the chunks are all embedded, we store them in a vector database. When the user types in a query, it gets embedded by the same model previously used, and a similarity search returns the closest documents from the vector database.\n",
        "\n",
        "The technical challenge is thus, given a query vector, to quickly find the nearest neighbors of this vector in the vector database. To do this, we need to choose two things: a distance, and a search algorithm to find the nearest neighbors quickly within a database of thousands of records.\n",
        "\n",
        "##### Nearest Neighbor search algorithm\n",
        "\n",
        "There are plentiful choices for the nearest neighbor search algorithm: we go with Facebook's [FAISS](https://github.com/facebookresearch/faiss) since FAISS is performant enough for most use cases, and it is well known and thus widely implemented.\n",
        "\n",
        "##### Distances\n",
        "\n",
        "Regarding distances, you can find a good guide [here](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/#distance-between-embeddings). In short:\n",
        "\n",
        "- **Cosine similarity** computes the similarity between two vectors as the cosinus of their relative angle: it allows us to compare vector directions regardless of their magnitude. Using it requires normalizing all vectors, to rescale them into unit norm.\n",
        "- **Dot product** takes into account magnitude, with the sometimes undesirable effect that increasing a vector's length will make it more similar to all others.\n",
        "- **Euclidean distance** is the distance between the ends of vectors.\n",
        "\n",
        "You can try [this small exercise](https://developers.google.com/machine-learning/clustering/similarity/check-your-understanding) to check your understanding of these concepts. But once vectors are normalized, [the choice of a specific distance does not matter much](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use).\n",
        "\n",
        "Our particular model works well with cosine similarity, so choose this distance, and we set it up both in the Embedding model, and in the `distance_strategy` argument of our FAISS index. With cosine similarity, we have to normalize our embeddings.\n",
        "\n",
        "ğŸš¨ğŸ‘‡ The cell below takes a few minutes to run on A10G!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dalledM99-9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52c3cd8-f583-488c-ef3f-b7ade56e480d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-86d3e6721df3>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zM-wfiJ9-9N"
      },
      "source": [
        "ğŸ‘€ To visualize the search for the closest documents, let's project our embeddings from 384 dimensions down to 2 dimensions using PaCMAP.\n",
        "\n",
        "ğŸ’¡ _We chose PaCMAP rather than other techniques such as t-SNE or UMAP, since [it is efficient (preserves local and global structure), robust to initialization parameters and fast](https://www.nature.com/articles/s42003-022-03628-x#Abs1)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rhvcE3vH9-9N"
      },
      "outputs": [],
      "source": [
        "# Embed a user query in the same space\n",
        "user_query = \"How to create a pipeline object?\"\n",
        "query_vector = embedding_model.embed_query(user_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l8nz5FYC9-9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d96b4e8-c95a-4275-fac5-ad90aafc48b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pacmap.pacmap:Warning: random state is set to 1.\n"
          ]
        }
      ],
      "source": [
        "import pacmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "embedding_projector = pacmap.PaCMAP(\n",
        "    n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1\n",
        ")\n",
        "\n",
        "embeddings_2d = [\n",
        "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0])\n",
        "    for idx in range(len(docs_processed))\n",
        "] + [query_vector]\n",
        "\n",
        "# Fit the data (the index of transformed data corresponds to the index of the original data)\n",
        "documents_projected = embedding_projector.fit_transform(\n",
        "    np.array(embeddings_2d), init=\"pca\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7Cl9Fw2A9-9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "6d30d187-6034-4cd5-c12d-8dd1939e3b14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7263b162-0362-4a40-a9d0-021e6d957803\" class=\"plotly-graph-div\" style=\"height:700px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7263b162-0362-4a40-a9d0-021e6d957803\")) {                    Plotly.newPlot(                        \"7263b162-0362-4a40-a9d0-021e6d957803\",                        [{\"customdata\":[[\"Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](htt...\"],[\"## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n\\u003cim...\"],[\"Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through t...\"],[\"Hugging Face Inference Endpoints documentation\\n\\n## Setup\\n\\n```bash\\npip install hf-doc-builder==0.4.0 ...\"],[\"Pricing\\n\\n\\u003cdiv class=\\\"flex md:justify-start mb-2 text-gray-400 items-center\\\"\\u003e\\n  \\u003ca href=\\\"https:\\u002f\\u002fui.e...\"],[\"| Provider | Instance Size | hourly rate | vCPUs | Memory | Architecture          |\\n| -------- | ---...\"],[\"| Provider | Instance Size | hourly rate | GPUs | Memory | Architecture |\\n| -------- | -------------...\"],[\"```\\ninstance hourly rate * ((hours * # min replica) + (scale-up hrs * # additional replicas))\\n```\\n\\n#...\"],[\"Supported Transformers & Diffusers Tasks\\n\\nInference Endpoints offers out-of-the-box support for Mach...\"],[\"```\\n\\n### Text Classification\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"This sound track was beautiful! It paints the s...\"],[\"```\\n\\n### Text Generation\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"This sound track was beautiful! It paints the sener...\"],[\"```\\n\\n**Binary**\\n```bash\\ncurl --request POST \\\\\\n  --url https:\\u002f\\u002f{ENDPOINT}\\u002f \\\\\\n  --header 'Content-Type...\"],[\"```\\n\\n### Table Question Answering\\n\\n```json\\n{\\n  \\\"inputs\\\": {\\n    \\\"query\\\": \\\"How many stars does the tra...\"],[\"Access and view Metrics\\n\\nHugging Face Endpoints provides access to the metrics and analytics of your...\"],[\"# FAQs \\n\\n\\n\\n### Q: In which regions are Inference Endpoints available?\\n\\nA: Inference Endpoints are cu...\"],[\"A: Yes, your Endpoint will always stay available\\u002fup with the number of min replicas defined in the A...\"],[\"### Q: What if I would like to deploy to a different instance type that is not listed?\\n\\nA: Please co...\"],[\"Help & Support \\n\\nWe have a variety of Inference Endpoints blog posts to help you at https:\\u002f\\u002fhuggingf...\"],[\"Pause and Resume your Endpoint\\n\\nYou can `pause` & `resume` endpoints to save cost and configurations...\"],[\"After that your replicas will be set to 0 and your endpoint will be paused. You can see the status c...\"],[\"API Reference (Swagger)\\n\\nğŸ¤— Inference Endpoints can be used through the [UI](https:\\u002f\\u002fui.endpoints.hug...\"],[\"Use a custom Container Image\\n\\n\\nInference Endpoints not only allows you to [customize your inference ...\"],[\"Autoscaling\\n\\nAutoscaling allows you to dynamically adjust the number of endpoint replicas running yo...\"],[\"Scaling to 0 replicas helps optimize cost savings by minimizing resource usage during periods of ina...\"],[\"Create a Private Endpoint with AWS PrivateLink\\n\\nSecurity and secure inference are key principles of ...\"],[\"After the VPC Endpoint status changes from **pending** to **available**, you should see a Endpoint U...\"],[\"Security & Compliance\\n\\nğŸ¤— Inference Endpoints is built with security and secure inference at its core...\"],[\"Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, y...\"],[\"Send Requests to Endpoints\\n\\nYou can send requests to Inference Endpoints using the UI leveraging the...\"],[\"```\\n\\nThe Endpoints API offers the same API definitions as the [Inference API](https:\\u002f\\u002fhuggingface.co...\"],[\"```\\n\\n### Custom handler\\n\\n`@huggingface\\u002finference` supports tasks from https:\\u002f\\u002fhuggingface.co\\u002ftasks, ...\"],[\"Change Organization or Account\\n\\nInference Endpoints uses your [Hugging Face](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"Update your Endpoint\\n\\nYou can update `running` Endpoints to change some of the configurations. Howev...\"],[\"Advanced Setup (Instance Types, Auto Scaling, Versioning)\\n\\nWe have seen how fast and easy it is to d...\"],[\"_Default: The most recent commit._\\n\\n**Image**\\n\\nAllows you to provide a custom container image you wa...\"],[\"Inference Endpoints Version\\n\\nHugging Face Inference Endpoints comes with a default serving container...\"],[\"### GPU\\n\\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\\n- `diffusers`: `0.14.0`\\n- `a...\"],[\"Serialization & Deserialization for Requests\\n\\nHugging Face Inference Endpount comes with a default s...\"],[\"| Content-Type           | Payload                        | \\n| ---------------------- | ------------...\"],[\"| audio\\u002fAMR-WB+          | `{\\\"inputs\\\": bytes(body)}`                     |\\n| audio\\u002fm4a              ...\"],[\"Below is a list of supported `accept` headers and the serialized payload is returned.\\n\\n\\n| Accept    ...\"],[\"ğŸ¤— Inference Endpoints\\n\\nğŸ¤— Inference Endpoints offers a secure production solution to easily deploy an...\"],[\"### Guides\\n\\n* [Access the solution (UI)](\\u002fdocs\\u002finference-endpoints\\u002fguides\\u002faccess)\\n* [Create your fir...\"],[\"Access ğŸ¤— Inference Endpoints\\n\\nTo access the [Inference Endpoints web application](https:\\u002f\\u002fui.endpoin...\"],[\"Add custom Dependencies\\n\\nInference Endpointsâ€™ base image includes all required libraries to run infe...\"],[\"Create custom Inference Handler\\n\\nHugging Face Endpoints supports all of the Transformers and Sentenc...\"],[\"Included examples are for:\\n\\n* [Optimum and ONNX Runtime](https:\\u002f\\u002fhuggingface.co\\u002fphilschmid\\u002fdistilber...\"],[\"The code can also be found in this [Notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1hANJeRa1PK1gZ...\"],[\"```\\n# install git-lfs to interact with the repository\\nsudo apt-get update\\nsudo apt-get install git-l...\"],[\"```\\n!cd distilbert-base-uncased-emotion && touch handler.py\\n```\\n\\nIn there, you define your `Endpoint...\"],[\"```\\n!echo \\\"holidays\\\" \\u003e\\u003e requirements.txt\\n!pip install -r requirements.txt\\n```\\n\\nNext, we have to adju...\"],[\"```\\n\\n### 4. Test EndpointHandler\\n\\nTo test our EndpointHandler, we can simplify import, initialize an...\"],[\"```\\n# add all our new files\\n!git add *\\n# commit our files\\n!git commit -m \\\"add custom handler\\\"\\n# push...\"]],\"hovertemplate\":\"source=hf-endpoints-documentation\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hf-endpoints-documentation, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hf-endpoints-documentation, circle\",\"showlegend\":true,\"x\":[4.231147,3.9997277,4.6607127,4.406969,3.7495582,-3.7913227,-3.6499069,-4.108069,-5.9284754,-4.1494665,-6.5526705,9.565641,-7.0428495,4.613403,3.4962826,3.830976,4.9932885,3.8134756,4.157048,4.133463,3.9801047,1.2910857,-2.7351003,3.059997,4.098753,4.3771343,3.9845166,4.3348603,3.858915,3.7492013,3.8787575,4.4056845,3.8090723,3.342112,2.3050292,2.9466493,-3.622604,3.755972,9.591686,9.806206,9.519392,3.6918738,3.84372,4.002044,3.1375577,3.336239,-7.3122826,3.653521,3.2007358,3.0552616,0.6084071,2.2465386,3.626158],\"xaxis\":\"x\",\"y\":[-0.5856562,-0.9156023,-0.16356778,0.40043443,-1.5033137,-2.1737344,-2.329736,-2.0240107,1.4267001,4.9258914,6.9025044,3.6507492,-0.21207896,-0.5801558,-1.6641791,-1.337963,0.7465002,-1.4376833,-0.37182453,-0.25785837,-0.9362977,-0.021445889,-2.4020915,0.8519151,-0.7630186,-0.45956826,-1.2328538,-0.4849082,-0.9445754,-0.89745057,-0.80031663,-0.07072788,-0.6736401,-1.4790905,0.28295895,-1.5596194,-1.0915284,-1.076714,3.636933,3.8122585,3.3791332,-1.7449607,-0.8233818,-1.322968,-0.8255107,-1.0813371,0.5788134,-1.040322,0.6682196,-0.578933,2.0058904,1.6055579,0.64405787],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Choosing a metric for your task\\n\\n**So you've trained your model and want to see how well itâ€™s doing ...\"],[\"```\\n\\u003e\\u003e\\u003e precision_metric = evaluate.load(\\\"precision\\\")\\n\\u003e\\u003e\\u003e results = precision_metric.compute(referen...\"],[\"```\\n\\n### Task-specific metrics\\n\\nPopular ML tasks like Machine Translation and Named Entity Recogniti...\"],[\"\\u003cTip warning={true}\\u003e\\nğŸ’¡\\nGLUE is actually a collection of different subsets on different tasks, so fir...\"],[\"```\\n\\u003e\\u003e\\u003e from evaluate import load\\n\\u003e\\u003e\\u003e squad_metric = load(\\\"squad\\\")\\n\\u003e\\u003e\\u003e predictions = [{'prediction_t...\"],[\"--\\ntitle: poseval\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"```python\\n\\u003e\\u003e\\u003e predictions = [['INTJ', 'ADP', 'PROPN', 'NOUN', 'PUNCT', 'INTJ', 'ADP', 'PROPN', 'VERB...\"],[\"```\\n\\n## Output values\\n\\nThis metric returns a a classification report as a dictionary with a summary ...\"],[\"`f1`: the average [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff1), on a scale between 0.0 and 1.0.\\n\\n\\n#...\"],[\"```\\n\\n## Limitations and bias\\n\\nIn contrast to [seqeval](https:\\u002f\\u002fgithub.com\\u002fchakki-works\\u002fseqeval), the...\"],[\"--\\ntitle: MAPE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mape': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popu...\"],[\"```\\n\\n## Limitations and Bias\\nOne limitation of MAPE is that it cannot be used if the ground truth is...\"],[\"--\\ntitle: ROUGE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n\\nOne can also pass a custom tokenizer which is especially useful for non-latin languages.\\n```pyt...\"],[\"```\\n```\\n\\n### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n      ...\"],[\"```\\n\\nThe ROUGE values are in the range of 0 to 1.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nA...\"],[\"```\\n\\n## Limitations and Bias\\nSee [Schluter (2017)](https:\\u002f\\u002faclanthology.org\\u002fE17-2007\\u002f) for an in-dep...\"],[\"--\\ntitle: Word Length\\nemoji: ğŸ¤—\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_f...\"],[\"```\\n\\nExample for a multiple strings\\n```python\\n\\u003e\\u003e\\u003e data = [\\\"hello sun and goodbye moon\\\", \\\"foo bar foo...\"],[\"Working with Keras and Tensorflow\\n\\n\\n\\nEvaluate can be easily intergrated into your Keras and Tensorfl...\"],[\"x_train = np.expand_dims(x_train, -1)\\nx_test = np.expand_dims(x_test, -1)\\n\\n\\nmodel = keras.Sequential...\"],[\"```\\n\\n## Callbacks\\n\\nSuppose we want to keep track of model metrics while a model is training. We can ...\"],[\"```\\n\\n```python\\nprint(\\\"Test accuracy is : \\\", acc.compute(predictions = test_preds, references = test_...\"],[\"--\\ntitle: CharCut\\nemoji: ğŸ”¤\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"```\\n\\n## Citation\\n```bibtex\\n@inproceedings{lardilleux-lepage-2017-charcut,\\n    title = \\\"{CHARCUT}: Hu...\"],[\"--\\ntitle: IndicGLUE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo...\"],[\"```\\n    \\n## Output values\\n\\nThe output of the metric depends on the IndicGLUE subset chosen, consisti...\"],[\"```\\n\\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\\n\\n```python\\n\\u003e\\u003e\\u003e indi...\"],[\"```\\n    \\n## Further References \\n- [IndicNLP website](https:\\u002f\\u002findicnlp.ai4bharat.org\\u002fhome\\u002f)...\"],[\"--\\ntitle: Google BLEU\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"The minimum value of precision and recall is then returned as the score.\\n\\n\\n## Intended Uses\\nThis met...\"],[\"```\\n\\n### Inputs\\n- **predictions** (list of str): list of translations to score.\\n- **references** (li...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nExample with one reference per sample:\\n```python...\"],[\"```\\n\\nExample with multiple references for the first sample, and with `min_len` adjusted to `2`, inst...\"],[\"```\\n\\nExample with multiple references for the first sample, with `min_len` adjusted to `2`, instead ...\"],[\"```\\n\\n## Limitations and Bias\\n\\nThe GoogleBLEU metric does not come with a predefined tokenization fun...\"],[\"--\\ntitle: \\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\np...\"],[\"```\\n\\nFrugalScore calculates how good are the predictions given some references, based on a set of sc...\"],[\"```\\n\\n## Limitations and bias\\n\\nFrugalScore is based on [BertScore](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fber...\"],[\"| FrugalScore                                        | Student     | Teacher        | Method     |\\n|...\"],[\"| [moussaKam\\u002ffrugalscore_tiny_deberta_bert-score](https:\\u002f\\u002fhuggingface.co\\u002fmoussaKam\\u002ffrugalscore_tiny_...\"],[\"Depending on the size of the model picked, the loading time will vary: the `tiny` models will load v...\"],[\"```\\n\\n## Further References\\n- [Original FrugalScore code](https:\\u002f\\u002fgithub.com\\u002fmoussaKam\\u002fFrugalScore)\\n-...\"],[\"--\\ntitle: Mean IoU\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"```\\n\\n### Inputs\\n**Mandatory inputs**\\n- `predictions` (`List[ndarray]`): List of predicted segmentati...\"],[\"The values of all of the scores reported range from from `0.0` (minimum) and `1.0` (maximum).\\n\\nOutpu...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\nThe [leaderboard for the CityScapes dataset](https:\\u002f\\u002fpaperswit...\"],[\"### Examples\\n\\n```python\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e mean_iou = evaluate.load(\\\"mean_iou\\\")\\n\\u003e\\u003e\\u003e # suppos...\"],[\"## Limitations and Bias\\nMean IOU is an average metric, so it will not show you where model predictio...\"],[\"```\\n\\n\\n## Further References\\n- [Wikipedia article - Jaccard Index](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fJacc...\"],[\"--\\ntitle: SuperGLUE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the SuperGLUE subset chosen, consisting of...\"],[\"```\\n\\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfro...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as the [...\"],[\"ğŸ¤— Transformers\\n\\nTo run the ğŸ¤— Transformers examples make sure you have installed the following librar...\"],[\"```\\n\\n## Trainer\\n\\nThe metrics in `evaluate` can be easily integrated with the [`~transformers.Trainer...\"],[\"trainer.train()...\"],[\"```\\n\\n## Seq2SeqTrainer\\n\\nWe can use the [`~transformers.Seq2SeqTrainer`] for sequence-to-sequence tas...\"],[\"# Setup evaluation\\nnltk.download(\\\"punkt\\\", quiet=True)\\nmetric = evaluate.load(\\\"rouge\\\")\\n\\ndef compute_m...\"],[\"```\\n\\nYou can use any `evaluate` metric with the `Trainer` and `Seq2SeqTrainer` as long as they are c...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nYou can adapt the `--build_dir` to set any temporary folder that you prefer. This command will ...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"```\\n## XXXConfig\\n\\n[[autodoc]] XXXConfig\\n```\\n\\nThis will include every public method of the configurat...\"],[\"```\\n## XXXTokenizer\\n\\n[[autodoc]] XXXTokenizer\\n    - all\\n    - __call__\\n```\\n\\n### Writing source docum...\"],[\"```\\n\\nIf the description is too long to fit in one line, another indentation is necessary before writ...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\nWe follow the [doctest](https:\\u002f\\u002fdocs.pyth...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"--\\ntitle: Spearman Correlation Coefficient Metric \\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradi...\"],[\"## How to Use\\nAt minimum, this metric only requires a `list` of predictions and a `list` of referenc...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `float`): Predicted labels, as returned by a model.\\n-...\"],[\"```\\n\\nThe same example, but that also returns the pvalue:\\n```python\\n\\u003e\\u003e\\u003e spearmanr_metric = evaluate.l...\"],[\"```\\n\\n## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@book{kokoska2000crc,\\n  title={CRC standard pro...\"],[\"--\\ntitle: TREC Eval\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Inputs\\n- **predictions** *(dict): a single retrieval run.*\\n    - **query** *(int): Query ID...\"],[\"### Examples\\n\\nA minimal example of looks as follows:\\n```Python\\nqrel = {\\n    \\\"query\\\": [0],\\n    \\\"q0\\\": ...\"],[\"```\\n\\nA more realistic use case with an examples from [`trectools`](https:\\u002f\\u002fgithub.com\\u002fjoaopalotti\\u002ftr...\"],[\"```\\n\\n```python\\nresult\\n\\n{'runid': 'InexpC2',\\n 'num_ret': 100000,\\n 'num_rel': 6074,\\n 'num_rel_ret': 31...\"],[\"```\\n\\n## Limitations and Bias\\nThe `trec_eval` metric requires the inputs to be in the TREC run and qr...\"],[\"A quick tour\\n\\nğŸ¤— Evaluate provides access to a wide range of evaluation tools. It covers a range of m...\"],[\"```\\n\\nIf you want to make sure you are loading the right type of evaluation (especially if there are ...\"],[\"```\\n\\n## Module attributes\\n\\nAll evalution modules come with a range of useful attributes that help to...\"],[\"```\\n\\nYou can see that it describes how the metric works in theory. If you use this metric for your w...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNote that features always describe the type of a single input element. In general we wil...\"],[\"```\\nEvaluation modules return the results in a dictionary. However, in some instances you build up t...\"],[\"```\\n\\n### Distributed evaluation\\n\\nComputing metrics in a distributed environment can be tricky. Metri...\"],[\"```\\n\\nThe `combine` function accepts both the list of names of the metrics as well as an instantiated...\"],[\"```\\n\\nThe content of the JSON file look like the following:\\n\\n```json\\n{\\n    \\\"experiment\\\": \\\"run 42\\\",\\n  ...\"],[\"```\\n\\n## Evaluator\\n\\nThe [`evaluate.evaluator`] provides automated evaluation and only requires a mode...\"],[\"```\\n\\nCalculating the value of the metric alone is often not enough to know if a model performs signi...\"],[\"```\\n\\nThe evaluator expects a `\\\"text\\\"` and `\\\"label\\\"` column for the data input. If your dataset diffe...\"],[\"```\\n\\nWhich lets you visually compare the 4 models and choose the optimal one for you, based on one o...\"],[\"```\\n\\nEvaluation can be run by loading the `EvaluationSuite` and calling the `run()` method with a mo...\"],[\"Using the `evaluator` with custom pipelines\\n\\nThe evaluator is designed to work with `transformer` pi...\"],[\"```\\n\\nFollowing the convention in the `TextClassificationPipeline` of `transformers` our pipeline sho...\"],[\"```\\n\\nThis snippet shows how we can use the `polarity` feature added with `spacytextblob` to get the ...\"],[\"--\\ntitle: Matthews Correlation Coefficient\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_ve...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `int`s): Predicted class labels.\\n- **`references`** (...\"],[\"```\\n\\nThe same example as above, with sample weights that cause a negative correlation:\\n```python\\n\\u003e\\u003e\\u003e...\"],[\"Evaluator\\n\\nThe evaluator classes for automatic evaluation.\\n\\n## Evaluator classes\\n\\nThe main entry poi...\"],[\"Using the `evaluator`\\n\\nThe `Evaluator` classes allow to evaluate a  triplet of model, dataset, and m...\"],[\"## Text classification\\n\\nThe text classification evaluator can be used to evaluate text models on cla...\"],[\"# 2. Pass an instantiated model\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\"lvwerra\\u002f...\"],[\"```\\n\\u003cTip\\u003e\\n\\nWithout specifying a device, the default for model inference will be the first GPU on the...\"],[\"```\\n\\nNext let's have a look at token classification.\\n\\n## Token Classification\\n\\nWith the token classi...\"],[\"data = load_dataset(\\\"conll2003\\\", split=\\\"validation\\\").shuffle().select(range(1000))\\ntask_evaluator = ...\"],[\"```\\n\\nThe result is a table that looks like this:...\"],[\"|   model                                                            |   overall_f1 |   overall_accu...\"],[\"| philschmid\\u002fdistilroberta-base-ner-conll2003                        |        0.961 |              0...\"],[\"### Visualizing results\\n\\nYou can feed in the `results` list above into the `plot_radar()` function t...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002f...\"],[\"```python\\nfrom datasets import load_dataset\\nfrom evaluate import evaluator\\n\\ntask_evaluator = evaluat...\"],[\"```\\n\\nResults include confidence intervals as well as error estimates as follows:\\n\\n```python\\n{\\n    'e...\"],[\"```\\n\\nSince we are using `datasets` to store data we make use of a technique called memory mappings. ...\"],[\"--\\ntitle: Exact Match\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `str`): List of predicted texts.\\n- **`references`** (...\"],[\"```\\n\\nThis metric's range is 0-1, inclusive. Here, 0.0 means no prediction\\u002freference pairs were match...\"],[\"```\\nNote that in the example above, because the regexes are ignored before the case is normalized, \\\"...\"],[\"```\\n\\nAn example that includes sentences:\\n```python\\n\\u003e\\u003e\\u003e exact_match = evaluate.load(\\\"exact_match\\\")\\n\\u003e\\u003e...\"],[\"--\\ntitle: Wilcoxon\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file:...\"],[\"--\\ntitle: SQuAD v2\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"*References*: List of question-answers dictionaries with the following key-value pairs:\\n* `'id'`: id...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary with 13 values: \\n* `'exact'`: Exact match (th...\"],[\"For more recent model performance, see the [dataset leaderboard](https:\\u002f\\u002fpaperswithcode.com\\u002fdataset\\u002f...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom evaluate import load\\nsqu...\"],[\"```\\n\\nPartial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom evaluate import load\\nsquad_metric...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with the datasets in the same format as the [SQu...\"],[\"--\\ntitle: BLEU\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"## How to Use\\n\\nThis metric takes as input a list of predicted sentences and a list of lists of refer...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list` of `str`s): Translations to score.\\n- **references** (`list...\"],[\"Output Example:\\n```python\\n{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, ...\"],[\"```\\n\\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidat...\"],[\"```\\n\\nExample where the second prediction has 2 references:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\n...     [\\\"h...\"],[\"```\\n\\n## Limitations and Bias\\nThis metric has multiple known limitations:\\n- BLEU compares overlap in ...\"],[\"--\\ntitle: Pearson Correlation Coefficient \\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_ve...\"],[\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels, as returned by a model...\"],[\"```\\n\\nExample 2-The same as Example 1, but that also returns the `p-value`.\\n```python\\n\\u003e\\u003e\\u003e pearsonr_me...\"],[\"```\\n\\n\\n## Limitations and Bias\\n\\nAs stated above, the calculation of the p-value relies on the assumpt...\"],[\"--\\ntitle: Code Eval\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\nN.B.\\nThis metric exists to run untrusted model-generated code. Users are strongly encouraged no...\"],[\"```\\n\\nPartial match at k=1, full match at k=2:\\n\\n```python\\nfrom evaluate import load\\ncode_eval = load(...\"],[\"```\\n\\n## Limitations and bias\\n\\nAs per the warning included in the metric code itself:\\n\\u003e This program ...\"],[\"More information about the limitations of the code can be found on the [Human Eval Github repository...\"],[\"```\\n    \\n## Further References \\n\\n- [Human Eval Github repository](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fhuman-ev...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002fresolve\\u002fmain...\"],[\"ğŸ¤— Evaluate is a library that makes evaluating and comparing models and reporting their performance e...\"],[\"```\\n\\n# Usage\\n\\nğŸ¤— Evaluate's main methods are:\\n\\n- `evaluate.list_evaluation_modules()` to list the ava...\"],[\"Types of Evaluations in ğŸ¤— Evaluate\\n\\nThe goal of the ğŸ¤— Evaluate library is to support different types...\"],[\"Comparisons have yet to be systematically used when comparing and reporting model performance, howev...\"],[\"--\\ntitle: WER\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"This problem is solved by first aligning the recognized word sequence with the reference (spoken) wo...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the word error rate.\\n\\n```\\nprint(wer_s...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\nwer = load(\\\"wer...\"],[\"--\\ntitle: chrF\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"See the [sacreBLEU README.md](https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#chrf--chrf) for more information.\\n...\"],[\"```\\n\\nThe chrF(++) score can be any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Pop...\"],[\"```\\n\\nThe same chrF++ example as above, but with `lowercase=True` to normalize all case:\\n```python\\n\\u003e\\u003e...\"],[\"--\\ntitle: Regard\\nemoji: ğŸ¤—\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"```\\n\\n### Inputs\\n- **data** (list of `str`): prediction\\u002fcandidate sentences, e.g. sentences describin...\"],[\"```\\n\\nWith the `aggregation='maximum'` option, this measurement will output the maximum regard for ea...\"],[\"```\\n\\nExample 3 (returns the maximum regard score):\\n```python\\n\\u003e\\u003e\\u003e regard = evaluate.load(\\\"regard\\\", \\\"c...\"],[\"```\\n\\n## Citation(s)\\n@article{https:\\u002f\\u002fdoi.org\\u002f10.48550\\u002farxiv.1909.01326,\\n  doi = {10.48550\\u002fARXIV.1909...\"],[\"--\\ntitle: Honest\\nemoji: ğŸ¤—\\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ap...\"],[\"```\\n\\nArguments:\\n    **predictions** (list of list of `str`): a list of completions to [HONEST prompt...\"],[\"| Model Name       | Top K =1 | Top K =5 |Top K =20 |\\n| ---------------- | -------- | -------- | ---...\"],[\"## Examples\\n\\nExample 1: Calculating HONEST without groups\\n\\n```python\\n\\u003e\\u003e\\u003e honest = evaluate.load('hon...\"],[\"```\\n\\nExample 2: Calculating HONEST with 2 groups (e.g. male\\u002ffemale)\\n```python\\n\\u003e\\u003e\\u003e honest = evaluate....\"],[\"```\\n\\n## Citation\\n\\n```bibtex\\n@inproceedings{nozza-etal-2021-honest,\\n    title = {\\\"{HONEST}: Measuring...\"],[\"--\\ntitle: SQuAD\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n{'exact_match': 100.0, 'f1': 100.0}\\n```\\n\\nThe range of `exact_match` is 0-100, where 0.0 means no...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom evaluate import load\\nsqu...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as [SQuA...\"],[\"--\\ntitle: Recall\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"```\\n```python\\n{'recall': array([1., 0., 0.])}\\n```\\n\\nThis metric outputs a dictionary with one entry, ...\"],[\"```\\n\\nExample 4-A multiclass example, using different averages.\\n```python\\n\\u003e\\u003e\\u003e recall_metric = evaluat...\"],[\"--\\ntitle: BERT Score\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file...\"],[\"```python\\nfrom evaluate import load\\nbertscore = load(\\\"bertscore\\\")\\npredictions = [\\\"hello there\\\", \\\"gen...\"],[\"### Values from popular papers\\nThe [original BERTScore paper](https:\\u002f\\u002fopenreview.net\\u002fpdf?id=SkeHuCVF...\"],[\"```\\n\\nPartial match with the `distilbert-base-uncased` model:\\n\\n```python\\nfrom evaluate import load\\nbe...\"],[\"--\\ntitle: Competition MATH\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\nap...\"],[\"```\\n\\nN.B. To be able to use Competition MATH, you need to install the `math_equivalence` dependency ...\"],[\"```\\n\\n## Limitations and bias\\n\\nThis metric is limited to datasets with the same format as the [Mathem...\"],[\"--\\ntitle: MSE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mse': array([0.41666667, 1. ])}\\n```\\n\\n#### Values fro...\"],[\"--\\ntitle: WikiSplit\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Values from popular papers\\n\\nThis metric was initially used by [Rothe et al.(2020)](https:\\u002f\\u002f...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\n\\u003e\\u003e\\u003e wiki_split = evaluate.load(\\\"wiki_spli...\"],[\"--\\ntitle: r_squared\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"```\\n\\n### How to Use Examples:\\n\\nThe R2 class in the evaluate module can be used to compute the R^2 va...\"],[\"```\\n\\n## Further References\\n\\n- [The Open University: R-Squared](https:\\u002f\\u002fwww.open.edu\\u002fopenlearn\\u002focw\\u002fmo...\"],[\"--\\ntitle: Precision\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n```python\\n{'precision': array([0.66666667, 0.0, 0.0])}\\n```\\n\\n\\n\\n\\n#### Values from Popular Papers\\n\\n...\"],[\"--\\ntitle: XTREME-S\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"```\\n\\n2. **Calculating the metric**: the metric takes two inputs : \\n\\n- `predictions`: a list of predi...\"],[\"```\\n\\nIt also has two optional arguments: \\n\\n- `bleu_kwargs`: a `dict` of keywords to be passed when c...\"],[\"- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. T...\"],[\"```\\n\\nFor the `covost2` subset (which outputs `bleu`):\\n\\n```python\\n\\u003e\\u003e\\u003e xtreme_s_metric = evaluate.load...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as the [...\"],[\"--\\ntitle: CER\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the character error rate.\\n\\n```\\nprint(...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\ncer = load(\\\"cer...\"],[\"--\\ntitle: {{ cookiecutter.module_name }}\\ndatasets:\\n- {{ cookiecutter.dataset_name }} \\ntags:\\n- evalua...\"],[\"#### Values from Popular Papers\\n*Give examples, preferrably with links to leaderboards or publicatio...\"],[\"--\\ntitle: McNemar\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"`references`: a list of the ground truth reference labels.\\n\\n## Output values\\n\\nThe McNemar comparison...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe McNemar test is a non-parametric test, so it has relatively few as...\"],[\"--\\ntitle: F1\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py...\"],[\"```\\n```python\\n{'f1': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a si...\"],[\"Considerations for model evaluation\\n\\nDeveloping an ML model is rarely a one-shot deal: it often invo...\"],[\"## The impact of class imbalance\\n\\nWhile many academic datasets, such as the [IMDb dataset](https:\\u002f\\u002fh...\"],[\"![Imbalanced Labels](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002fresolve\\u002fmain\\u002fimbalanced-classes....\"],[\"Other metrics, such as [BLEU](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fevaluate-metric\\u002fexact_match) are harder ...\"],[\"When doing online model evaluation, there is often a trade-off to be done between inference speed an...\"],[\"--\\ntitle: Mahalanobis Distance\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19....\"],[\"```\\n\\n#### Values from Popular Papers\\n*N\\u002fA*\\n\\n### Example\\n\\n```python\\n\\u003e\\u003e\\u003e mahalanobis_metric = evaluate...\"],[\"--\\ntitle: MAUVE\\nemoji: ğŸ¤—\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\nIt also has several optional arguments:\\n\\n`num_buckets`: the size of the histogram to quantize P...\"],[\"`frontier_integral`: Frontier Integral, which ranges between 0 and 1. **Smaller** values indicate th...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\nmauve = lo...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe [original MAUVE paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.01454) did not a...\"],[\"See the [official implementation](https:\\u002f\\u002fgithub.com\\u002fkrishnap25\\u002fmauve#best-practices-for-mauve) for ...\"],[\"```\\n\\n## Further References\\n- [Official MAUVE implementation](https:\\u002f\\u002fgithub.com\\u002fkrishnap25\\u002fmauve)\\n- ...\"],[\"--\\ntitle: BLEURT\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list` of `str`s): List of generated sentences to score.\\n- **refe...\"],[\"```\\n\\nBLEURT's output is always a number between 0 and (approximately 1). This value indicates how si...\"],[\"```\\n\\n## Limitations and Bias\\nThe [original BLEURT paper](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2004.04696.pdf) showe...\"],[\"--\\ntitle: Label Distribution\\nemoji: ğŸ¤—\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0....\"],[\"```\\n\\nIf skewness is 0, the dataset is perfectly balanced; if it is less than -1 or greater than 1, t...\"],[\"```\\n\\n## Limitations and Bias\\nWhile label distribution can be a useful signal for analyzing datasets ...\"],[\"--\\ntitle: XNLI\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\n## Output values\\n\\nThe output of the XNLI metric is simply the `accuracy`, i.e. the proportion o...\"],[\"```\\n\\n## Limitations and bias\\n\\nWhile accuracy alone does give a certain indication of performance, it...\"],[\"--\\ntitle: Text Duplicates\\nemoji: ğŸ¤—\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\na...\"],[\"```\\n\\nExample with multiple duplicates and `list_duplicates=True`:\\n```python\\n\\u003e\\u003e\\u003e data = [\\\"hello sun\\\",...\"],[\"Creating an EvaluationSuite\\n\\nIt can be useful to evaluate models on a variety of different tasks to ...\"],[\"The mandatory attributes for a new `SubTask` are `task_type` and `data`.\\n1. [`task_type`] maps to th...\"],[\"```\\n\\nAn `EvaluationSuite` can be loaded by name from the Hugging Face Hub, or locally by providing a...\"],[\"--\\ntitle: MAE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mae': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popul...\"],[\"```\\n\\n## Limitations and Bias\\nOne limitation of MAE is that the relative size of the error is not alw...\"],[\"--\\ntitle: GLUE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"### Values from popular papers\\nThe [original GLUE paper](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fglue) repor...\"],[\"```\\n\\nMinimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfrom e...\"],[\"Scikit-Learn\\n\\nTo run the scikit-learn examples make sure you have installed the following library:\\n\\n...\"],[\"```\\n\\nAlternatively X and y can be obtained directly from the frame attribute:\\n\\n```python\\nX = titanic...\"],[\"```\\n\\nYou can use any suitable `evaluate` metric with the estimators as long as they are compatible w...\"],[\"Logging methods\\n\\nğŸ¤— Evaluate strives to be transparent and explicit about how it works, but this can ...\"],[\"```\\n\\nAll the methods of this logging module are documented below. The main ones are:\\n\\n- [`logging.ge...\"],[\"### evaluate.logging.DEBUG\\n\\nevaluate.logging.DEBUG = 10\\n\\n### evaluate.logging.ERROR\\n\\nevaluate.loggin...\"],[\"Installation\\n\\nBefore you start, you will need to setup your environment and install the appropriate ...\"],[\"--\\ntitle: Word Count\\nemoji: ğŸ¤—\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_fi...\"],[\"```\\n\\nExample for a multiple strings\\n```python\\n\\u003e\\u003e\\u003e data = [\\\"hello sun and goodbye moon\\\", \\\"foo bar foo...\"],[\"--\\ntitle: seqeval\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"`mode`: whether to count correct entity labels with incorrect I\\u002fB tags as true positives or not. If ...\"],[\"## Examples \\n\\nMaximal values (full match) :\\n\\n```python\\n\\u003e\\u003e\\u003e seqeval = evaluate.load('seqeval')\\n\\u003e\\u003e\\u003e pr...\"],[\"```\\n\\nMinimal values (no match):\\n\\n```python\\n\\u003e\\u003e\\u003e seqeval = evaluate.load('seqeval')\\n\\u003e\\u003e\\u003e predictions = ...\"],[\"```\\n\\nPartial match:\\n\\n```python\\n\\u003e\\u003e\\u003e seqeval = evaluate.load('seqeval')\\n\\u003e\\u003e\\u003e predictions = [['O', 'O', ...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002fresolve\\u002fmain...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eHigh-level explanations for building a better understanding of important to...\"],[\"--\\ntitle: SARI\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"`sari = ( F1_add + F1_keep + P_del) \\u002f 3`\\n\\nwhere \\n\\n`F1_add` is the n-gram F1 score for add operations...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary with the SARI score:\\n\\n```\\nprint(sari_score)\\n{...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\nsari = loa...\"],[\"--\\ntitle: METEOR\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"It also has several optional parameters:\\n\\n`alpha`: Parameter for controlling relative weights of pre...\"],[\"```\\n\\n## Output values\\n\\nThe metric outputs a dictionary containing the METEOR score. Its values range...\"],[\"```\\n\\nMultiple `references` per `prediction`, partial match:\\n\\n```python\\n\\u003e\\u003e\\u003e meteor = evaluate.load('m...\"],[\"```\\n    \\n## Further References \\n- [METEOR -- Wikipedia](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fMETEOR)\\n- [MET...\"],[\"--\\ntitle: CharacTER\\nemoji: ğŸ”¤\\ncolorFrom: orange\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file...\"],[\"```\\n\\n### Inputs\\n- **predictions**: a single prediction or a list of predictions to score. Each predi...\"],[\"```\\n\\n## Further References\\n- Repackaged version that is used in this HF implementation: [https:\\u002f\\u002fgit...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Submitting a new issue or feature request\\n\\nFollowing these guidelines when submitting an issue or...\"],[\"### Did you find a bug?\\n\\nThank you for reporting an issue. If the bug is related to a community metr...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   $ git checkout -b a-des...\"],[\"```\\n\\n   Then, make sure you have all the dependencies to be able to build the doc with:\\n   \\n   ```ba...\"],[\"```\\n\\n6. Once you are satisfied, go to the webpage of your fork on GitHub. Click on 'Pull request' to...\"],[\"### Develop on Windows\\n\\nOn Windows, you need to configure git to transform Windows `CRLF` line endin...\"],[\"```\\n$ git checkout -b your-branch-for-syncing\\n$ git pull --squash --no-commit upstream main\\n$ git co...\"],[\"--\\ntitle: MASE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mase': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popu...\"],[\"```\\n\\n## Limitations and Bias\\n\\n\\n## Citation(s)\\n\\n```bibtex\\n@article{HYNDMAN2006679,\\n    title = {Anoth...\"],[\"--\\ntitle: sMAPE\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'smape': array([0.5, 1.5 ])}\\n```\\n\\n#### Values from Po...\"],[\"```\\n\\n## Further References\\n- [Symmetric Mean absolute percentage error - Wikipedia](https:\\u002f\\u002fen.wikip...\"],[\"Saving methods\\n\\nMethods for saving evaluations results:\\n\\n## Save\\n\\n[[autodoc]] evaluate.save...\"],[\"--\\ntitle: Exact Match \\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_f...\"],[\"```\\n\\n\\n## Limitations and bias\\n\\n## Citations...\"],[\"--\\ntitle: Perplexity\\nemoji: ğŸ¤—\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_fi...\"],[\"```\\n\\n### Inputs\\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on...\"],[\"```\\n\\nThe range of this metric is [0, inf). A lower score is better.\\n\\n#### Values from Popular Papers...\"],[\"--\\ntitle: Accuracy\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"```\\n\\nThis metric outputs a dictionary, containing the accuracy score.\\n\\n\\n#### Values from Popular Pap...\"],[\"--\\ntitle: RL Reliability\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_...\"],[\"```\\n\\n\\n### Inputs\\n- **timesteps** *(List[int]): For each run a an list\\u002farray with its timesteps.*\\n- *...\"],[\"### Output Values\\n\\nIn `\\\"online\\\"` mode:\\n- HighFreqEnergyWithinRuns: High Frequency across Time (DT)\\n-...\"],[\"```\\n\\nLoad the sample data:\\n```python\\ndfs = [pd.read_csv(f\\\".\\u002fcsv_data\\u002fsac_humanoid_{i}_train.csv\\\") fo...\"],[\"--\\ntitle: \\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\np...\"],[\"9\\tWord sense\\tThis is the word sense of the word in Column 3.\\n  10\\tSpeaker\\u002fAuthor\\tThis is the speaker...\"],[\"## Metric description\\n\\nCoVal is a coreference evaluation tool for the [CoNLL](https:\\u002f\\u002fhuggingface.co...\"],[\"```python\\nfrom evaluate import load\\ncoval = load('coval')\\nwords = ['bc\\u002fcctv\\u002f00\\u002fcctv_0005   0   0    ...\"],[\"## Examples \\n\\nMaximal values\\n\\n```python\\nfrom evaluate import load\\ncoval = load('coval')\\nwords = ['bc...\"],[\"--\\ntitle: CUAD\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"`references`: a list of question-answer dictionaries with the following key-values:\\n - `id`: the id ...\"],[\"For more recent model performance, see the [dataset leaderboard](https:\\u002f\\u002fpaperswithcode.com\\u002fdataset\\u002f...\"],[\"```\\n\\nMinimal values:\\n\\n```python\\nfrom evaluate import load\\ncuad_metric = load(\\\"cuad\\\")\\npredictions = [...\"],[\"```\\n\\nPartial match: \\n\\n```python\\nfrom evaluate import load\\ncuad_metric = load(\\\"cuad\\\")\\npredictions = [...\"],[\"Visualization methods\\n\\nMethods for visualizing evaluations results:\\n\\n## Radar Plot\\n\\n[[autodoc]] eval...\"],[\"--\\ntitle: TER\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"See the README.md file at https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#ter for more information.\\n\\n\\n## How to ...\"],[\"```\\n\\n### Inputs\\nThis metric takes the following as input:\\n- **`predictions`** (`list` of `str`): The...\"],[\"```\\n\\nThe metric can take on any value `0` and above. `0` is a perfect score, meaning the predictions...\"],[\"```\\n\\nExample ignoring punctuation and capitalization, and everything matches:\\n```python\\n\\u003e\\u003e\\u003e predicti...\"],[\"```\\n\\n\\n## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@inproceedings{snover-etal-2006-study,\\n    tit...\"],[\"--\\ntitle: Toxicity\\nemoji: ğŸ¤—\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ap...\"],[\"```\\n        In this case, the `toxic_label` would be `offensive`.\\n    `aggregation` (optional): dete...\"],[\"```\\n    Example 3 (returns the maximum toxicity score):\\n```python\\n\\u003e\\u003e\\u003e toxicity = evaluate.load(\\\"toxi...\"],[\"Hub methods\\n\\nMethods for using the Hugging Face Hub:\\n\\n## Push to hub \\n\\n[[autodoc]] evaluate.push_to_...\"],[\"--\\ntitle: COMET\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n\\nIt has several configurations, named after the COMET model to be used. For versions below 2.0 i...\"],[\"## Output values\\n\\nThe COMET metric outputs two lists:\\n\\n`scores`: a list of COMET scores for each of ...\"],[\"```\\n\\nPartial match:\\n\\n```python\\nfrom evaluate import load\\ncomet_metric = load('comet') \\nsource = [\\\"De...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe models provided for calculating the COMET metric are built on top ...\"],[\"However, for the latest COMET models like `Unbabel\\u002fwmt22-comet-da`, we have introduced a new trainin...\"],[\"```\\n\\n```bibtex\\n@inproceedings{rei-EtAl:2020:WMT,\\n   author    = {Rei, Ricardo  and  Stewart, Craig  ...\"],[\"```\\n\\n## Further References\\n\\n- [COMET website](https:\\u002f\\u002funbabel.github.io\\u002fCOMET\\u002fhtml\\u002findex.html)\\n- [Hu...\"],[\"--\\ntitle: Brier Score\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n## Limitations and Bias\\nThe [brier_score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fbrier_score) is appropr...\"],[\"--\\ntitle: Perplexity\\nemoji: ğŸ¤—\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Inputs\\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on...\"],[\"```\\n\\nThe range of this metric is [0, inf). A lower score is better.\\n\\n#### Values from Popular Papers...\"],[\"```\\n\\n## Limitations and Bias\\nNote that the output value is based heavily on what text the model was ...\"],[\"Loading methods\\n\\nMethods for listing and loading evaluation modules:\\n\\n## List\\n\\n[[autodoc]] evaluate....\"],[\"Creating and sharing a new evaluation\\n\\n## Setup\\n\\nBefore you can create a new metric make sure you ha...\"],[\"```\\n\\nThis will create a new Space on the ğŸ¤— Hub, clone it locally, and populate it with a template. I...\"],[\"```\\n\\nOr if you need to download the NLTK `\\\"punkt\\\"` resources:\\n\\n```py\\ndef _download_and_prepare(self,...\"],[\"```\\ncd PATH_TO_MODULE\\ngit add .\\ngit commit -m \\\"Add my new, shiny module.\\\"\\ngit push\\n```\\nTada ğŸ‰! Your ...\"],[\"Main classes\\n\\n## EvaluationModuleInfo\\n\\nThe base class `EvaluationModuleInfo` implements a the logic ...\"],[\"--\\ntitle: SacreBLEU\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\nThe score can take any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Popular Pap...\"],[\"--\\ntitle: ROC AUC\\nemoji: ğŸ¤— \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"This metric has three separate use cases:\\n- **binary**: The case in which there are only two differe...\"],[\"```\\n\\nThe default implementation of this metric is the **binary** implementation. If employing the **...\"],[\"```\\n\\nIn contrast, though, the output takes the following format in the multilabel case when `average...\"],[\"```\\n\\nExample 3, the **multilabel** use case:\\n```python\\n\\u003e\\u003e\\u003e roc_auc_score = evaluate.load(\\\"roc_auc\\\", ...\"],[\"--\\ntitle: NIST_MT\\nemoji: ğŸ¤— \\ncolorFrom: purple\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Inputs\\n- **predictions**: tokenized predictions to score. For sentence-level NIST, a list o...\"]],\"hovertemplate\":\"source=evaluate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"evaluate, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"evaluate, circle\",\"showlegend\":true,\"x\":[0.9938256,0.6254144,-0.036962762,-4.806239,0.548042,-0.20721446,0.42408955,0.18440089,0.43517557,-0.30057424,-0.16901664,0.86226827,0.6375683,-0.1922333,-0.2367186,0.024503225,0.15186553,0.36782238,-0.8232353,-0.2054905,-0.6394004,0.67100745,-1.3068659,0.31242576,0.5261132,-0.22379948,-1.3339897,-0.23146272,0.4099392,0.19117148,-0.31211865,7.1996713,-0.2404997,-0.1870333,0.11279469,0.3271324,-0.08249229,-0.07190274,-1.2707157,-0.23698352,0.3536784,-0.28833458,7.7676377,7.796004,-5.521267,7.3068833,-0.17756958,0.48598456,0.20698534,-0.5000277,0.32603747,0.021705372,7.223757,-0.23606668,0.35844597,0.28933153,0.5608131,-0.49056393,0.86984587,0.5572842,-1.1242146,-1.6526924,0.17747192,0.41033083,5.1488447,5.224331,4.6787477,2.049459,0.07489501,-0.45303607,-1.4393594,4.634078,-0.14394914,0.48806566,0.46781307,0.5353664,0.28123912,-0.19305004,-0.19545934,-0.14888287,-0.057551295,-0.22225745,-0.38927674,1.0369158,0.9062535,0.80610687,0.2736073,0.5831865,0.62085366,0.88965434,0.7679215,1.5197812,0.5118195,0.606306,0.19571579,1.1563189,0.776615,0.28717944,-3.006911,-3.71632,-0.12731914,0.6177812,0.2747983,0.51711404,0.34450758,0.18412443,-1.2152178,-2.1764321,-1.9499425,-0.08847965,1.044773,-4.6124334,7.960721,0.283007,-0.059701417,0.3175813,-0.23745136,1.1088257,-0.19692558,0.0988619,0.1404162,0.0836648,0.101255216,-0.15533108,-0.35127127,-0.030938093,0.049426068,0.31824762,0.44561523,0.4174905,-0.6014579,-0.24189687,0.2406064,0.008579684,-0.16668423,-0.52169377,0.2044395,-0.675335,-0.11959089,0.58129525,0.59280044,0.23497963,-0.21009411,0.45293865,0.3897279,0.33526164,-6.4584255,-0.38816795,2.0626185,1.1564167,0.8585533,1.022029,0.7606171,-0.1825077,-0.18206292,-0.026696483,-0.15167534,-0.21327172,0.1587749,0.03278326,-0.02418607,-0.21297114,-0.17139877,0.12871768,0.08440437,-1.024448,-0.21760413,-0.53319806,-4.994942,-0.34305725,-0.34425306,-1.3576056,-0.254009,0.26316103,0.5320641,-0.51157933,-0.18430458,0.6535104,0.465248,-0.6104877,0.0910876,-4.5951586,-0.16319637,-0.18766178,0.46287468,-0.2058491,-0.18087283,0.68021244,-0.23226613,-0.22651158,-0.13992095,-0.14164296,0.72292924,-0.5149308,-0.18174693,0.6373936,-0.2446993,0.41235515,-0.16659527,-0.15454647,0.5072925,-0.8063084,-0.16525131,-0.06963333,-0.22285329,0.54233664,7.221166,-0.16143899,0.47745585,-0.51201254,-0.18196726,0.5977445,1.2219743,0.521552,0.8013064,0.7599391,0.52362263,-0.15087691,-0.09610891,-0.28162876,0.09478946,-0.76391625,0.2625412,-1.1692154,-0.9828191,7.423494,-0.2597555,0.23281766,-0.24301061,-0.8548645,-0.13952468,0.38169736,0.35309476,-0.22750093,0.36970913,-0.9080289,-0.14101519,-0.19385645,1.2880098,0.8122921,0.7742324,-0.17571707,0.84502757,0.74564165,-0.14103554,-0.23397586,0.10691052,-0.115024425,0.3621485,0.4634867,0.76568186,1.2777275,1.1141974,0.8379674,1.3101879,-0.18876228,-0.20991209,-0.2412308,0.23983872,0.35799837,0.45351794,0.45066354,1.2922511,8.608197,8.62769,-0.30296,-0.0018473275,0.36174628,-0.1564281,-0.28589776,-0.17062178,0.3993964,-0.45162776,7.000008,-0.26830062,0.18869792,7.6523414,5.788054,5.4804463,3.3655329,3.196494,3.3702567,3.9754777,3.7287579,3.8142896,-0.15382448,0.74043614,0.68548054,-0.3819046,-0.17911215,0.8730547,0.6882175,6.972732,1.1197021,-0.17424017,-0.5938279,-0.20892608,-1.1816401,0.29833546,-0.16915593,0.3508338,-0.17898232,-0.29546028,-0.0055513564,0.30192205,-0.22589122,-1.5000395,-0.30265155,0.32354647,0.26577318,-0.21515587,0.33728302,0.44882008,0.47116035,0.46150768,10.352386,-0.23086736,0.100453325,0.021115653,0.11690867,0.026178904,-1.1786349,-0.2314878,-0.074445225,-0.7617495,4.2818737,-0.22976698,-4.485303,-0.020743556,0.45616722,-0.83789724,-0.75091344,-1.1120148,7.391492,-0.18856476,0.5536733,-0.36065236,-0.24130127,-1.1306403,0.2918895,-0.81849164,1.3052877,1.2151762,2.0221133,0.9423131,3.3548837,0.7960226,-0.21608031,0.053341974,-0.06262189,0.39258444,0.34147507,0.33546495,0.2035971,-0.21651964,-0.5594487],\"xaxis\":\"x\",\"y\":[6.492542,7.9271593,7.1333594,1.8772153,7.45695,9.851973,8.003087,7.77409,7.9232006,7.752813,10.049934,7.377782,7.9267745,6.929271,9.9219675,8.1038885,8.049263,7.7929096,7.5478044,9.643454,6.925048,6.0406594,1.5370467,6.115,7.710823,9.91524,7.645543,9.910539,7.9064083,8.171845,8.395412,3.0991013,9.837572,8.320101,8.275133,7.8610516,7.2979865,7.4027815,7.8754926,9.862797,7.965659,7.361398,3.222998,2.8538356,0.84774125,2.9942212,9.99655,6.9661646,7.391405,6.4378653,6.7658215,6.9235497,3.035417,9.915594,8.009458,8.1014595,8.325291,7.611727,0.8536982,6.282565,-0.00712617,4.3112516,6.0482793,5.9773436,0.16602589,2.220644,1.9683788,1.4984732,2.1933503,3.0296977,4.2141857,1.3431371,8.910698,8.023638,7.8719325,7.9518743,6.2140765,9.952153,6.8574166,6.6429276,6.0500836,5.4197693,7.632207,6.089133,5.2334,5.942793,7.6936297,6.4223785,6.310317,6.3546844,6.63393,3.4877844,5.765582,6.3322754,5.129963,5.8871603,5.207747,5.7307568,3.8306065,4.047509,9.396194,7.607779,7.5972295,5.056125,5.48928,5.292871,3.251542,-1.0256873,4.25409,4.4343066,4.7278843,0.83808166,4.0236917,4.9037943,5.084952,4.997768,5.037173,4.347457,9.907736,8.487554,8.45291,8.639031,8.561755,9.344139,8.905282,7.547232,7.8680716,8.2592325,8.389162,8.385366,7.543729,9.96229,8.15689,8.497251,6.154885,8.244517,8.058523,8.271896,9.413303,7.838858,7.823247,6.415513,9.933091,6.684866,8.1312685,6.621002,0.89190894,7.013269,5.4581723,6.1057267,5.8037124,6.3892574,6.3531475,8.923732,8.778939,8.50871,8.582002,9.892462,8.333995,8.306599,8.151708,9.845285,7.2365203,7.266868,7.256245,7.598399,9.938787,6.40508,0.8980494,6.747263,6.5438986,7.478239,9.699904,8.288921,8.5665,7.534323,9.991335,7.991485,7.8359423,8.926133,7.7379947,1.8310076,7.9638467,9.946251,8.145048,7.2948537,10.012801,7.9378448,9.924969,8.460561,8.549979,9.261012,7.0928965,7.6989365,10.066509,7.902746,9.685127,8.104004,8.480061,8.699617,8.155766,7.8198814,9.225167,8.755014,8.673794,5.7687483,2.8114066,9.272771,7.8661437,7.588666,10.0143175,7.9835043,6.0389757,6.3388567,6.2919507,6.5408955,6.4940786,9.630792,7.3941975,9.674349,5.2575865,7.742703,8.347532,7.615753,7.683509,1.6916085,9.846779,7.9671936,8.147697,8.035635,9.585529,5.7201095,5.8862762,9.911072,8.060836,7.797512,9.604242,7.3390064,5.52163,4.686852,5.1752315,10.031243,7.471143,7.9359097,7.1187835,9.932051,8.06878,8.227701,5.72951,6.6086497,6.433432,2.7465963,2.2882376,2.1244617,3.9871247,9.851863,6.8127513,9.701578,6.947325,8.0902195,8.246909,8.441401,6.1022873,2.775184,2.7836897,9.596998,8.363888,8.030739,8.444405,9.863166,3.789758,7.865932,8.148511,3.2427676,10.095761,8.232696,2.6973262,0.5361962,1.1014295,0.8996253,1.1832476,1.121655,1.2449191,0.9892037,0.90994465,9.769215,7.281074,7.964669,7.385259,10.029569,7.532425,7.768543,3.506857,3.747504,9.739813,7.4513927,9.715362,6.846499,8.006919,9.910516,8.034298,9.950691,3.4424353,3.1346278,6.392592,9.778926,6.474636,7.999474,7.708119,7.7092123,9.764927,8.12029,8.342848,8.300895,8.465472,4.04716,9.805773,8.4148245,8.475002,8.444761,8.465361,7.577672,9.816702,6.683542,6.056042,0.5627126,9.919482,0.13321713,8.085029,8.601506,8.1772785,8.064394,7.7550373,1.1714517,9.844656,7.695729,7.257847,9.717216,6.8146944,8.104003,7.1412883,4.217566,4.9278097,4.0512815,5.0017443,1.2607006,5.768843,9.930131,8.441605,8.985192,7.421594,7.4318986,7.548626,7.3498454,9.913979,8.222191],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"ä¸»è¦ç‰¹ç‚¹\\n\\nè®©æˆ‘ä»¬æ¥ä»‹ç»ä¸€ä¸‹ Gradio æœ€å—æ¬¢è¿çš„ä¸€äº›åŠŸèƒ½ï¼è¿™é‡Œæ˜¯ Gradio çš„ä¸»è¦ç‰¹ç‚¹ï¼š\\n\\n1. [æ·»åŠ ç¤ºä¾‹è¾“å…¥](#example-inputs)\\n2. [ä¼ é€’è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯](#erro...\"],[\"ç»§ç»­äº†è§£ç¤ºä¾‹ï¼Œè¯·å‚é˜…[æ›´å¤šç¤ºä¾‹](https:\\u002f\\u002fgradio.app\\u002fmore-on-examples)æŒ‡å—ã€‚\\n\\n## é”™è¯¯\\n\\næ‚¨å¸Œæœ›å‘ç”¨æˆ·ä¼ é€’è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯ã€‚ä¸ºæ­¤ï¼Œwith `gr.Error(\\\"...\"],[\"```python\\ngr.Number(label='å¹´é¾„', info='ä»¥å¹´ä¸ºå•ä½ï¼Œå¿…é¡»å¤§äº0')...\"],[\"```\\n\\n## æ——æ ‡\\n\\né»˜è®¤æƒ…å†µä¸‹ï¼Œ\\\"Interface\\\" å°†æœ‰ä¸€ä¸ª \\\"Flag\\\" æŒ‰é’®ã€‚å½“ç”¨æˆ·æµ‹è¯•æ‚¨çš„ `Interface` æ—¶ï¼Œå¦‚æœçœ‹åˆ°æœ‰è¶£çš„è¾“å‡ºï¼Œä¾‹å¦‚é”™è¯¯æˆ–æ„å¤–çš„æ¨¡å‹è¡Œä¸ºï¼Œä»–ä»¬å¯ä»¥å°†è¾“å…¥æ ‡è®°ä¸º...\"],[\"```\\n\\n_flagged\\u002flogs.csv_\\n\\n```csv\\nim,Output\\nim\\u002f0.png,Output\\u002f0.png\\nim\\u002f1.png,Output\\u002f1.png\\n```\\n\\nå¦‚æœæ‚¨å¸Œæœ›ç”¨æˆ·æä¾›...\"],[\"```\\n\\nç›¸åï¼Œè¿™é‡Œæˆ‘ä»¬ä¿ç•™å›¾åƒçš„åŸå§‹å¤§å°ï¼Œä½†åœ¨å°†å…¶è½¬æ¢ä¸º numpy æ•°ç»„ä¹‹å‰åè½¬é¢œè‰²ï¼š\\n\\n```py\\nimg = gr.Image(invert_colors=True, type=\\\"numpy\\\"...\"],[\"```\\n\\n## é˜Ÿåˆ— (Queuing)\\n\\nå¦‚æœæ‚¨çš„åº”ç”¨ç¨‹åºé¢„è®¡ä¼šæœ‰å¤§é‡æµé‡ï¼Œè¯· with `queue()` æ–¹æ³•æ¥æ§åˆ¶å¤„ç†é€Ÿç‡ã€‚è¿™å°†æ’é˜Ÿå¤„ç†è°ƒç”¨ï¼Œå› æ­¤ä¸€æ¬¡åªå¤„ç†ä¸€å®šæ•°é‡çš„è¯·æ±‚ã€‚é˜Ÿåˆ—ä½¿ç”¨ Webso...\"],[\"```\\n\\n## è¿­ä»£è¾“å‡º (Iterative Outputs)\\n\\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¼ è¾“ä¸€ç³»åˆ—è¾“å‡ºè€Œä¸æ˜¯ä¸€æ¬¡æ˜¾ç¤ºå•ä¸ªè¾“å‡ºã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½æœ‰ä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¸Œæœ›æ˜¾ç¤ºç”Ÿæˆçš„æ¯ä¸ªæ­¥éª¤çš„å›¾åƒï¼Œç›´åˆ°æœ€ç»ˆ...\"],[\"```\\n\\næ‚¨ä»¥ä¸å¸¸è§„å‡½æ•°ç›¸åŒçš„æ–¹å¼å°†ç”Ÿæˆå™¨æä¾›ç»™ Gradioã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªï¼ˆè™šæ‹Ÿçš„ï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå®ƒåœ¨è¾“å‡ºå›¾åƒä¹‹å‰ç”Ÿæˆæ•°ä¸ªæ­¥éª¤çš„å™ªéŸ³ï¼š\\n\\n$code_fake_diffusion\\n$demo_fa...\"],[\"## æ‰¹å¤„ç†å‡½æ•° (Batch Functions)\\n\\nGradio æ”¯æŒä¼ é€’*æ‰¹å¤„ç†*å‡½æ•°ã€‚æ‰¹å¤„ç†å‡½æ•°åªæ˜¯æ¥å—è¾“å…¥åˆ—è¡¨å¹¶è¿”å›é¢„æµ‹åˆ—è¡¨çš„å‡½æ•°ã€‚\\n\\nä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰¹å¤„ç†å‡½æ•°ï¼Œå®ƒæ¥å—ä¸¤ä¸ªè¾“å…¥åˆ—è¡¨ï¼ˆä¸€ä¸ªå•è¯...\"],[\"```\\n\\nä½¿ç”¨æ‰¹å¤„ç†å‡½æ•°çš„ä¼˜ç‚¹æ˜¯ï¼Œå¦‚æœå¯ç”¨äº†é˜Ÿåˆ—ï¼ŒGradio æœåŠ¡å™¨å¯ä»¥è‡ªåŠ¨*æ‰¹å¤„ç†*ä¼ å…¥çš„è¯·æ±‚å¹¶å¹¶è¡Œå¤„ç†å®ƒä»¬ï¼Œä»è€Œå¯èƒ½åŠ å¿«æ¼”ç¤ºé€Ÿåº¦ã€‚ä»¥ä¸‹æ˜¯ Gradio ä»£ç çš„ç¤ºä¾‹ï¼ˆè¯·æ³¨æ„ `batch=True...\"],[\"```\\n\\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç† 16 ä¸ªè¯·æ±‚ï¼ˆæ€»æ¨ç†æ—¶é—´ä¸º 5 ç§’ï¼‰ï¼Œè€Œä¸æ˜¯åˆ†åˆ«å¤„ç†æ¯ä¸ªè¯·æ±‚ï¼ˆæ€»æ¨ç†æ—¶é—´ä¸º 80 ç§’ï¼‰ã€‚è®¸å¤š Hugging Face çš„ `transformers` å’Œ `...\"],[\"Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef...\"],[\"State in Blocks\\n\\nWe covered [State in Interfaces](https:\\u002f\\u002fgradio.app\\u002finterface-state), this guide ta...\"],[\"å¦‚ä½•ä½¿ç”¨åœ°å›¾ç»„ä»¶ç»˜åˆ¶å›¾è¡¨\\n\\nRelated spaces:\\nTags: PLOTS, MAPS\\n\\n## ç®€ä»‹\\n\\næœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Gradio çš„ `Plot` ç»„ä»¶åœ¨åœ°å›¾ä¸Šç»˜åˆ¶åœ°ç†æ•°æ®ã€‚Gradi...\"],[\"dataset = load_dataset(\\\"gradio\\u002fNYC-Airbnb-Open-Data\\\", split=\\\"train\\\")\\ndf = dataset.to_pandas()\\n\\ndef f...\"],[\"```\\n\\nåœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å…ˆå°† CSV æ•°æ®åŠ è½½åˆ°ä¸€ä¸ª pandas dataframe ä¸­ã€‚è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿™å°†ä½œä¸º gradio åº”ç”¨ç¨‹åºçš„é¢„æµ‹å‡½æ•°ã€‚è¯¥å‡½æ•°å°†æ¥å—æœ€ä½ä»·æ ¼ã€æœ€é«˜ä»·æ ¼èŒƒå›´...\"],[\"```\\n\\nä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¼ å…¥ç»çº¬åº¦åˆ—è¡¨æ¥åˆ›å»ºä¸€ä¸ªæ•£ç‚¹å›¾ã€‚æˆ‘ä»¬è¿˜ä¼ å…¥äº†åç§°å’Œä»·æ ¼çš„è‡ªå®šä¹‰æ•°æ®ï¼Œä»¥ä¾¿åœ¨é¼ æ ‡æ‚¬åœåœ¨æ¯ä¸ªæ ‡è®°ä¸Šæ—¶æ˜¾ç¤ºé¢å¤–çš„ä¿¡æ¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ `update_layout` æ¥æŒ‡å®š...\"],[\"```\\n\\næˆ‘ä»¬ä½¿ç”¨ `gr.Column` å’Œ `gr.Row` å¸ƒå±€è¿™äº›ç»„ä»¶ï¼Œå¹¶ä¸ºæ¼”ç¤ºåŠ è½½æ—¶å’Œç‚¹å‡» \\\" æ›´æ–°ç­›é€‰ \\\" æŒ‰é’®æ—¶æ·»åŠ äº†äº‹ä»¶è§¦å‘å™¨ï¼Œä»¥è§¦å‘åœ°å›¾æ›´æ–°æ–°çš„ç­›é€‰æ¡ä»¶ã€‚\\n\\nä»¥ä¸‹æ˜¯å®Œæ•´æ¼”ç¤ºä»£ç ï¼š\\n\\n...\"],[\"Gradio Demo: examples_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: number_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr....\"],[\"Gradio Demo: map_airbnb\\n### Display an interactive map of AirBnB locations with Plotly. Data is host...\"],[\"```\\nimport gradio as gr\\nimport plotly.graph_objects as go\\nfrom datasets import load_dataset\\n\\ndataset...\"],[\"return fig\\n\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_p...\"],[\"Gradio Demo: question-answering\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gra...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Button } from \\\"@gradio\\u002fbutton\\\";\\n\\u003c\\u002fscript\\u003e\\n\\n\\u003cbutton type...\"],[\"Gradio Demo: sales_projections\\n\\n\\n```\\n!pip install -q gradio pandas numpy matplotlib\\n```\\n\\n\\n```\\nimport...\"],[\"Gradio and W&B Integration\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fakhaliq\\u002fJoJoGAN\\nTags: WAND...\"],[\"## Setting up a Gradio Demo for JoJoGAN\\n\\nNow, let's walk you through how to do this on your own. We'...\"],[\"```\\n\\n3. Finetune StyleGAN and W&B experiment tracking\\n\\n   This next step will open a W&B dashboard t...\"],[\"for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0),...\"],[\"```\\n\\nalpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = ...\"],[\"img = generator(in_latent, input_is_latent=True)\\n\\n    with torch.no_grad():\\n        real_feat = disc...\"],[\"```\\n\\n4. Save, Download, and Load Model\\n\\n    Here's how to save and download your model.\\n\\n```python\\n\\n...\"],[\"plt.rcParams['figure.dpi'] = 150\\n\\n\\n\\ntransform = transforms.Compose(\\n    [\\n        transforms.Resize(...\"],[\"```\\n\\n5. Build a Gradio Demo\\n\\n```python\\n\\nimport gradio as gr\\n\\ntitle = \\\"JoJoGAN\\\"\\ndescription = \\\"Gradio...\"],[\"```\\n\\n7. (Optional) Embed W&B plots in your Gradio App\\n\\n   It's also possible to embed W&B plots with...\"],[\"```\\n\\n## Conclusion\\n\\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! ...\"],[\"Gradio Demo: duplicatebutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n...\"],[\"Gradio Demo: upload_button_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    \\n    with g...\"],[\"@gradio\\u002fimageeditor\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6809](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f680...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f6a9151d5c9432c724098...\"],[\"- @gradio\\u002fimage@0.5.0\\n  - @gradio\\u002fupload@0.5.3\\n  - @gradio\\u002fclient@0.9.0\\n  - @gradio\\u002fwasm@0.4.0\\n  - @...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6502](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"Gradio Demo: chatinterface_system_prompt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr...\"],[\"Gradio Demo: streaming_wav2vec\\n\\n\\n```\\n!pip install -q gradio torch transformers \\n```\\n\\n\\n```\\nfrom trans...\"],[\"component-styles\\n\\n## Textbox\\n\\n| name        | type                                 | description    ...\"],[\"## Checkbox\\n\\n| name        | type                                 | description                    |...\"],[\"## Dropdown\\n\\n| name        | type                                 | description                    |...\"],[\"## File\\n\\n| name      | type                                 | description         |\\n| --------- | --...\"],[\"## HighlightedText\\n\\n| name        | type                                 | description              ...\"],[\"## Chatbot\\n\\n| name        | type                                 | description                      ...\"],[\"Gradio Demo: blocks_webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gradio...\"],[\"Gradio Demo: on_listener_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"ä¸»é¢˜ Theming\\n\\nTags: THEMES\\n\\n## ä»‹ç»\\n\\nGradio å…·æœ‰å†…ç½®çš„ä¸»é¢˜å¼•æ“ï¼Œå¯è®©æ‚¨è‡ªå®šä¹‰åº”ç”¨çš„å¤–è§‚å’Œæ„Ÿè§‰ã€‚æ‚¨å¯ä»¥é€‰æ‹©å„ç§ä¸»é¢˜ï¼Œæˆ–è€…åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ã€‚è¦è¿™æ ·åšï¼Œè¯·å°† `theme=...\"],[\"```\\n\\n$demo_theme_builder\\n\\næ‚¨å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„ Spaces ä¸Šè¿è¡Œçš„ Theme Builderï¼Œä½†é€šè¿‡ `gr.themes.builder()` åœ¨æœ¬åœ°å¯åŠ¨æ—¶è¿è¡Œé€Ÿåº¦æ›´å¿«ã€‚...\"],[\"æ‚¨å¯ä»¥ä½¿ç”¨å­—ç¬¦ä¸²å¿«æ·æ–¹å¼ä¿®æ”¹è¿™äº›å€¼ï¼Œä¾‹å¦‚\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=\\\"red\\\", secondary_...\"],[\"```\\n\\næˆ–è€…ç›´æ¥ä½¿ç”¨ `Color` å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-1.hf.space?__theme=light...\"],[\"```\\n\\næˆ–è€…ç›´æ¥ä½¿ç”¨ `Size` å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-2.hf.space?__theme=light...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-3.hf.space?__theme=light...\"],[\"```\\n\\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `loader_color` å’Œ `slider_color` å˜é‡è®¾ç½®ä¸º`#FF0000`ï¼Œå°½ç®¡æ•´ä½“ `primary_color` ä½¿ç”¨è“è‰²è°ƒè‰²æ¿ã€‚æ‚¨å¯ä»¥ä»¥è¿™ç§æ–¹...\"],[\"#### å¼•ç”¨æ ¸å¿ƒå˜é‡\\n\\nè¦å¼•ç”¨å…¶ä¸­ä¸€ä¸ªæ ¸å¿ƒæ„é€ å‡½æ•°å˜é‡ï¼Œè¯·åœ¨å˜é‡åå‰åŠ ä¸Šæ˜Ÿå·ã€‚è¦å¼•ç”¨æ ¸å¿ƒé¢œè‰²ï¼Œè¯·ä½¿ç”¨`*primary_`ã€`*secondary_` æˆ–`*neutral_` å‰ç¼€ï¼Œåè·Ÿäº®åº¦å€¼ã€‚ä¾‹...\"],[\"```\\n\\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `button_primary_background_fill` å’Œ `button_primary_background_fill_hover` å˜é‡åˆ†åˆ«è®¾ç½®ä¸º`*...\"],[\"```\\n\\nç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æ›´æ”¹ `button_primary_background_fill` å˜é‡ï¼Œ`button_primary_background_fill_hover` å’Œ `button_...\"],[\"```\\n\\n`button_primary_border_dark` å°†ä» `button_primary_background_fill_dark` è·å–å…¶å€¼ï¼Œå› ä¸ºæš—æ¨¡å¼æ€»æ˜¯ä½¿ç”¨å˜é‡çš„æš—ç‰ˆæœ¬ã€‚\\n\\n##...\"],[\"\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-new-step-2.hf.space?__theme=light\\\"\\n\\tframebo...\"],[\"- é€šè¿‡ç±»å®ä¾‹\\n\\næ¯ä¸ªä¸»é¢˜å®ä¾‹éƒ½æœ‰ä¸€ä¸ªåä¸ºâ€œpush_to_hubâ€çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥å°†ä¸»é¢˜ä¸Šä¼ åˆ° HuggingFace Hubã€‚\\n\\n```python\\nseafoam.push_to_hub...\"],[\"```\\n\\n- é€šè¿‡å‘½ä»¤è¡Œ\\n\\né¦–å…ˆå°†ä¸»é¢˜ä¿å­˜åˆ°ç£ç›˜\\n\\n```python\\nseafoam.dump(filename=\\\"seafoam.json\\\")\\n```\\n\\nç„¶åä½¿ç”¨â€œupload_themeâ€å‘½ä»¤ï¼š...\"],[\"```\\n\\nè¦ä¸Šä¼ ä¸»é¢˜ï¼Œæ‚¨å¿…é¡»æ‹¥æœ‰ä¸€ä¸ª HuggingFace è´¦æˆ·ï¼Œå¹¶é€šè¿‡ `hf_token` å‚æ•°ä¼ é€’æ‚¨çš„[è®¿é—®ä»¤ç‰Œ](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingfac...\"],[\"### å‘ç°ä¸»é¢˜\\n\\n[ä¸»é¢˜åº“](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002ftheme-gallery)æ˜¾ç¤ºäº†æ‰€æœ‰å…¬å¼€çš„ gradio ä¸»é¢˜ã€‚åœ¨å‘å¸ƒä¸»é¢˜ä¹‹åï¼Œ\\nå®ƒå°†åœ¨å‡ åˆ†...\"],[\"```\\n\\næ‚¨ä¹Ÿå¯ä»¥ç›´æ¥å°†ä¸»é¢˜å­—ç¬¦ä¸²ä¼ é€’ç»™ `Blocks` æˆ– `Interface`ï¼ˆ`gr.Blocks(theme=\\\"gradio\\u002fseafoam\\\")`ï¼‰\\n\\næ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨è¯­ä¹‰ç‰ˆæœ¬è¡¨è¾¾å¼å°†æ‚¨çš„åº”...\"],[\"his demo shows how you can build an interactive dashboard with gradio. Click on a python library on ...\"],[\"gradio\\n\\n## 4.11.0\\n\\n### Features...\"],[\"- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) [`846d52d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6809](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6809) [`1401d99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6833](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6833) [`1b9d423`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes\\n\\n- [#6829](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6829) [`50496f9`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 4.10.0\\n\\n### Features\\n\\n- [#6798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6798) [`245d58e`](https...\"],[\"### Fixes\\n\\n- [#6799](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6799) [`c352811`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 4.9.1\\n\\n### Features\\n\\n- [#6781](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6781) [`a807ede`](https:...\"],[\"### Fixes\\n\\n- [#6525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6525) [`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6745](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6745) [`3240d04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6671](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6671) [`299f5e2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6666](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6666) [`30c9fbb`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6704](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6704) [`24e0481`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6416](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6416) [`5177132`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes...\"],[\"- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6676](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6676) [`fe40308`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6639](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6639) [`9a6ff70`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6694](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6694) [`dfc61ec`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6759](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6759) [`28a7aa9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.8.0\\n\\n### Features...\"],[\"- [#6624](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6624) [`1751f14`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6565](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6565) [`9bf1ad4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6607](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6607) [`13ace03`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6572](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6572) [`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6551](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6551) [`8fc562a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.7.1\\n\\n### Features\\n\\n- [#6537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6537) [`6d3fecfa4`](http...\"],[\"- [#6532](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6532) [`96290d304`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6523](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6523) [`63f466882`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6538](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6538) [`147926196`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6528) [`f53b01cbf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6500](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6500) [`830b6c0e6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.5.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```py\\n\\ndef fn(im):\\n    im[\\\"composite\\\"] # the full canvas\\n    im[\\\"background\\\"] # the background image...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6497](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#6428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6428) [`ac4ca59c9`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6455](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6455) [`179f5bcde`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6423](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6423) [`62d35c3d1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6419](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6419) [`1959471a8`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6441](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6441) [`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6457](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6457) [`d00fcf89d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.3.0\\n\\n### Features...\"],[\"- [#6395](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6395) [`8ef48f852`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6099](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6099) [`d84209703`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6412) [`649f3ceb6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6383](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6383) [`324867f63`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6414](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6414) [`da1e31832`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.2.0\\n\\n### Features...\"],[\"- [#6333](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6333) [`42f76aeeb`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6356](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6356) [`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6368](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6368) [`8a3f45c26`](https:\\u002f\\u002fgithub.co...\"],[\"## 4.1.2\\n\\n### Features\\n\\n- [#6318](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6318) [`d3b53a457`](http...\"],[\"- [#6310](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6310) [`dfdaf1092`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6317](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6317) [`19af2806a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6311](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6311) [`176c4d140`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6309](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6309) [`c56128781`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.1.1\\n\\n### Fixes\\n\\n- [#6288](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6288) [`92278729e`](https:\\u002f...\"],[\"- [#6261](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6261) [`8bbeca0e7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6240) [`dd901c1b0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6232](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6232) [`ac4f2bcde`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6266) [`e32bac894`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6236](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6236) [`6bce259c5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6249](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6249) [`2cffcf3c3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6211](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6211) [`a4a931dd3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.0.2\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"## 4.0.0\\n\\n### Highlights\\n\\n4.0 is a big release, so here are the main highlights:\\n\\n**1. Custom Compon...\"],[\"\\u003cimg style=\\\"width:50%\\\" src=\\\"https:\\u002f\\u002fi.imgur.com\\u002fVFWVsqn.png\\\"\\u003e\\n\\n5. We now support adding arbitrary JS...\"],[\"### Breaking Changes\\n\\nGradio 4.0 is a new major version, and includes breaking changes from 3.x. Her...\"],[\"**Other changes related to the `gradio` library**:\\n\\n* Removes the deprecated `status_tracker` parame...\"],[\"For example, if your code looks like this:\\n\\n```py\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n   ...\"],[\"```\\n\\nIn order for the HTML component to be able to serve `image.png`, you will need to add `image.pn...\"],[\"```\\n\\n\\n#### **Using `concurrency_limit` instead of `concurrency_count`**\\n\\nPreviously, in Gradio 3.x, ...\"],[\"To summarize migration:\\n\\n* For events that execute quickly or don't use much CPU or GPU resources, y...\"],[\"```\\n\\nNow, you should write:\\n\\n```py\\ngr.ImageEditor(sources=(), brush=gr.Brush(colors=[\\\"#000000\\\"]))\\n``...\"],[\"```\\n\\nUnlike the `Image` component, which passes the input image as a single value into the predictio...\"],[\"- [#6184](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6184) [`86edc0199`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6124) [`a7435ba9e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6142](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6142) [`103416d17`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6128](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6128) [`9c3bf3175`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6155](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6155) [`f71ea09ae`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6118](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6118) [`88bccfdba`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6157](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6157) [`db143bdd1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6140](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6140) [`71bf2702c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6071](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6071) [`f08da1a6f`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6093](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6093) [`fadc057bb`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6107](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6107) [`9a40de7bf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6026](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6026) [`338969af2`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6073](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6073) [`abff6fb75`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5968](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5968) [`6b0bb5e6a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5990](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5990) [`85056de5c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6079](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6079) [`3b2d9eaa3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6148](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6148) [`0000a1916`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5984](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5984) [`66549d8d2`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.0-beta.13\\n\\n### Features\\n\\n- [#5964](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5964) [`5fbda0b...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`d2314e53b`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5894](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5894) [`fee3d527e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.48.0\\n\\n### Features...\"],[\"- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5915](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5915) [`e24163e15`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5819](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5819) [`5f1cbc436`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5864](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5864) [`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5904](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5904) [`891d42e9b`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5890](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5890) [`c4ba832b3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5930](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5930) [`361823896`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.47.1\\n\\n### Fixes\\n\\n- [#5816](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5816) [`796145e2c`](https:...\"],[\"Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n### Features\\n\\n- [#5780](https:\\u002f\\u002fgithub.com\\u002fgradi...\"],[\"- [#5798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5798) [`a0d3cc45c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5794](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5794) [`f096c3ae1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.46.1\\n\\n### Features\\n\\n- [#5124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5124) [`6e56a0d9b`](htt...\"],[\"## 3.46.0\\n\\n### Features\\n\\n- [#5699](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5699) [`8f0fed857`](htt...\"],[\"- [#5735](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5735) [`abb5e9df4`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5731](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5731) [`c9af4f794`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.2\\n\\n### Features\\n\\n- [#5722](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5722) [`dba651904`](htt...\"],[\"- [#5714](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5714) [`a0fc5a296`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5705](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5705) [`78e7cf516`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5726) [`96c4b97c7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.1\\n\\n### Fixes\\n\\n- [#5701](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5701) [`ee8eec1e5`](https:...\"],[\"- [#5675](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5675) [`b619e6f6e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5681](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5681) [`40de3d217`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5652](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5652) [`2e25d4305`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5660](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5660) [`d76555a12`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5240) [`da05e59a5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5590](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5590) [`d1ad1f671`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5625](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5625) [`9ccc4794a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5633](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5633) [`341402337`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5593](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5593) [`88d43bd12`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.4\\n\\n### Features\\n\\n- [#5514](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5514) [`52f783175`](htt...\"],[\"### Fixes\\n\\n- [#5587](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5587) [`e0d61b8ba`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5562](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5562) [`50d9747d0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5553](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5553) [`d1bf23cd2`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.2\\n\\n### Fixes\\n\\n- [#5537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5537) [`301c7878`](https:\\u002f...\"],[\"- [#5516](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5516) [`c5fe8eba`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5525) [`21f1db40`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.44.0\\n\\n### Features...\"],[\"- [#5505](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5505) [`9ee20f49`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5488](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5488) [`8909e42a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5474](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5474) [`041560f9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5459) [`bd2fda77`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5496](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5496) [`82ec4d26`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.43.2\\n\\n### Fixes\\n\\n- [#5456](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5456) [`6e381c4f`](https:\\u002f...\"],[\"- [#5165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5165) [`c77f05ab`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5417](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5417) [`d14d63e3`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5412) [`26fef8c7`](https:\\u002f\\u002fgithub.com...\"],[\"Thanks [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82)!\\n\\n#### Added the ability to attach event lis...\"],[\"```\\n\\n Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n### Features...\"],[\"- [#5334](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5334) [`c5bf9138`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5370](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5370) [`61803c65`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5369](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5369) [`b8968898`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5394](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5394) [`4d94ea0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.41.2\\n\\n### Features\\n\\n- [#5284](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5284) [`5f25eb68`](http...\"],[\"## 3.41.1\\n\\n### Fixes\\n\\n- [#5324](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5324) [`31996c99`](https:\\u002f...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"```\\n\\n Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n#### Add `render` function to `\\u003c...\"],[\"```\\n\\n Thanks [@hannahblair](https:\\u002f\\u002fgithub.com\\u002fhannahblair)!\\n\\n### Features...\"],[\"- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5268) [`f49028cf`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5283](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5283) [`a7460557`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5280](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5280) [`a2f42e28`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4943](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4943) [`947d615d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5188](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5188) [`b22e1888`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5305](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5305) [`15075241`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5264](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5264) [`46a2b600`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5179](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5179) [`6fb92b48`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5122](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5122) [`3b805346`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5231](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5231) [`87f1c2b4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5235](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5235) [`1ecf88ac`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.40.0\\n\\n### Highlights\\n\\n#### Client.predict will now return the final output for streaming endpoi...\"],[\"```\\n\\nFrom the backend, streamed outputs are served from the `\\u002fstream\\u002f` endpoint instead of the `\\u002ffil...\"],[\"- [#5081](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5081) [`d7f83823`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5125](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5125) [`80be7a1c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5046) [`5244c587`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5047](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5047) [`883ac364`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5104](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5104) [`34f6b22e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5035](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5035) [`8b4eb8ca`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5080](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5080) [`37caa2e0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5075](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5075) [`67265a58`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5061](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5061) [`136adc9c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5111](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5111) [`b84a35b7`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.39.0\\n\\n### Highlights\\n\\n#### Create Discord Bots from Gradio Apps ğŸ¤– ([#4960](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fgradio-builds.s3.amazonaws.com\\u002fdemo-files\\u002fdiscordbots\\u002fguide\\u002fllama_chat.gif\\\"\\u003e\\n...\"],[\"Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Features...\"],[\"- [#4995](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4995) [`3f8c210b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4985](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4985) [`b74f8453`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#4997](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4997) [`41c83070`](https:\\u002f\\u002fgithub.com...\"],[\"- Provide a parameter `animate` (`False` by default) in `gr.make_waveform()` which animates the over...\"],[\"- Add default sketch color argument `brush_color`. Also, masks drawn on images are now slightly tran...\"],[\"### Bug Fixes:\\n\\n- Fixes `cancels` for generators so that if a generator is canceled before it is com...\"],[\"## 3.37\\n\\n### New Features:\\n\\nIntroducing a new `gr.ChatInterface` abstraction, which allows Gradio us...\"],[\"```\\n\\nWhich produces:\\n\\n\\u003cimg width=\\\"1291\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fassets...\"],[\"- Chatbot messages now show hyperlinks to download files uploaded to `gr.Chatbot()` by [@dawoodkhan8...\"],[\"```\\n\\n```py\\nwith gr.Blocks() as demo:\\n    gr.Markdown(\\\"Ø³Ù„Ø§Ù…\\\", rtl=True)\\ndemo.launch()...\"],[\"```\\n\\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton]...\"],[\"* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@han...\"],[\"### Other Changes:\\n\\n- Warning on mobile that if a user leaves the tab, websocket connection may brea...\"],[\"## 3.36.1\\n\\n### New Features:\\n\\n- Hotfix to support pydantic v1 and v2 by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"### Other Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3....\"],[\"- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg...\"],[\"- Fix `make_waveform` to work with paths that contain spaces [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4...\"],[\"- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](h...\"],[\"- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyabou...\"],[\"- Don't crash when uploaded image has broken EXIF data, by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 476...\"],[\"### Other Changes:...\"],[\"- Add `.git-blame-ignore-revs` by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4586](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"- Better errors when you define two Blocks and reference components in one Blocks from the events in...\"],[\"### Breaking Changes:\\n\\n[PR 4683](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4683) removes the explict...\"],[\"### Other Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3....\"],[\"- Min and max value for gr.Number by [@artegoser](https:\\u002f\\u002fgithub.com\\u002fartegoser) and [@dawoodkhan82](...\"],[\"- Can now issue `gr.Warning` and `gr.Info` modals. Simply put the code `gr.Warning(\\\"Your warning mes...\"],[\"Example:\\n\\n```python\\ndef start_process(name):\\n    gr.Info(\\\"Starting process\\\")\\n    if name is None:\\n  ...\"],[\"```\\n\\n### Bug Fixes:...\"],[\"- Add support for PAUSED state in the JS client by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 4...\"],[\"- Fix new line issue with `gr.Chatbot()` by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR ...\"],[\"### Other Changes:\\n\\n- Change styling of status and toast error components by [@hannahblair](https:\\u002f\\u002f...\"],[\"### Breaking Changes:\\n\\n- The behavior of the `Clear` button has been changed for `Slider`, `Checkbox...\"],[\"- Remove target=\\\"\\\\_blank\\\" override on anchor tags with internal targets by [@hannahblair](https:\\u002f\\u002fgi...\"],[\"- The output directory for files downloaded when calling Blocks as a function is now set to a tempor...\"],[\"### Other Changes:\\n\\n- When running on Spaces, handler functions will be transformed by the [PySpaces...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.33.1\\n\\n### New Features:\\n\\nNo changes to highlig...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.33.0\\n\\n### New Features:\\n\\n- Introduced `gradio ...\"],[\"- Fix bug where Label change event was triggering itself by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffre...\"],[\"- Replace default `markedjs` sanitize function with DOMPurify sanitizer for `gr.Chatbot()` by [@dawo...\"],[\"### Other Changes:\\n\\n- Remove flicker of loading bar by adding opacity transition, by [@aliabid94](ht...\"],[\"### Bug Fixes:\\n\\n- Fixed Gallery\\u002fAnnotatedImage components not respecting GRADIO_DEFAULT_DIR variable...\"],[\"### Other Changes:\\n\\n- Refactor web component `initial_height` attribute by [@whitphx](https:\\u002f\\u002fgithub...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.31.0\\n\\n### New Features:\\n\\n- The reloader comman...\"],[\"- Fix \\\"TypeError: issubclass() arg 1 must be a class\\\" When use Optional[Types] by [@lingfengchencn](...\"],[\"- Fixes JSONDecodeError by [@davidai](https:\\u002f\\u002fgithub.com\\u002fdavidai) in [PR 4241](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"### Other Changes:\\n\\n- Change `gr.Chatbot()` markdown parsing to frontend using `marked` library and ...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.30.0\\n\\n### New Features:\\n\\n- Adds a `root_path` ...\"],[\"### Other Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3....\"],[\"- Allow users to upload audio files in Audio component on iOS by by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- Removes extraneous `State` component info from the `\\u002finfo` route by [@abidlabs](https:\\u002f\\u002fgithub.com...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Bug Fixes:\\n\\n- Fixes issue with `matplotlib` not rendering correctly if the backend was not set t...\"],[\"### Full Changelog:\\n\\n- Safer version of `gr.HuggingFaceDatasetSaver` using HTTP methods instead of g...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\n- CI:...\"],[\"- Fix duplicate play commands in full-screen mode of 'video'. by [@tomchang25](https:\\u002f\\u002fgithub.com\\u002fto...\"],[\"- Fix issue in `gr.Gallery()` where setting height causes aspect ratio of images to collapse by [@da...\"],[\"- Fixes issue where dropdown does not position itself at selected element when opened [@dawoodkhan82...\"],[\"### Documentation Changes:\\n\\n- Make use of `gr` consistent across the docs by [@duerrsimon](https:\\u002f\\u002fg...\"],[\"### Full Changelog:\\n\\n- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https:\\u002f\\u002fgithub.com...\"],[\"Example usage:\\n\\n```python\\nwith gr.Blocks() as demo:\\n    img = gr.Image()\\n    img_section = gr.Annota...\"],[\"```\\n\\nSee the [image_segmentation demo](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002ftree\\u002fmain\\u002fdemo\\u002fimage_seg...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fix code markdown support in `gr.Chatbot()` component by [@dawoodkhan82](http...\"],[\"img.select(select_handler, img, textbox)...\"],[\"```\\n\\n![Recording 2023-04-08 at 17 44 39](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f230748572...\"],[\"- Increase timeout for sending analytics data by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in...\"],[\"- Fix bug where the upload button was not properly handling the `file_count='multiple'` case by [@fr...\"],[\"### Documentation Changes:\\n\\n- Fix invalid argument docstrings, by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in ...\"],[\"No changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\nNo c...\"],[\"```\\n\\n  ![Theme Builder](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f228204929-d71cbba5-69c2-45...\"],[\"- Fixed bug where text for altair plots was not legible in dark mode by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"- Fixed bug where chatbot does not autoscroll inside of a tab, row or column by [@dawoodkhan82](http...\"],[\"- Correct the documentation of `gr.File` component to state that its preprocessing method converts t...\"],[\"### Documentation Changes:\\n\\n- Makes some fixes to the Theme Guide related to naming of variables, by...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Removed heavily-mocked tests related to comet_ml, wandb, ...\"],[\"No changes to highlight.\\n\\n### Full Changelog:\\n\\n- Mobile responsive iframes in themes guide by [@alia...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.23.0\\n\\n### New Features:\\n\\n###### Theme Sha...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.22.1\\n\\n### New Features:\\n\\nNo changes to hi...\"],[\"### Bug Fixes:\\n\\n- Fixes the File.upload() event trigger which broke as part of the change in how we ...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Pinned `pyright==1.1.298` for stability by [@abidlabs](ht...\"],[\"```\\n\\n2. Via the command line\\n\\nFirst save the theme to disk\\n\\n```python\\nmy_theme.dump(filename=\\\"my_the...\"],[\"```\\n\\nby [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3428](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"```\\n\\n\\u003cimg width=\\\"1054\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f1778297\\u002f224116682-...\"],[\"```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gallery = gr.Gallery([\\\"images\\u002f1.jpg\\\", \\\"...\"],[\"```\\n\\nBy [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 3399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio...\"],[\"### Bug Fixes:\\n\\n- Use `huggingface_hub` to send telemetry on `interface` and `blocks`; eventually to...\"],[\"### Documentation Changes:\\n\\n- Added a section on security and access when sharing Gradio apps by [@a...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Fixes tests that were failing locally but passing on CI b...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Prevent in-place updates of ...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.20.1\\n\\n### New Features:\\n\\n- Add `height` k...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Ensure uploaded images are always shown in the sketch tool by [@pngwn](https:...\"],[\"```\\n\\nby [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3211](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- Updated image upload component to accept all image formats, including lossless formats like .webp ...\"],[\"- Allow developers to access the username of a logged-in user from the `gr.Request()` object using t...\"],[\"### Bug Fixes:...\"],[\"- Ensure `mirror_webcam` is always respected by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3245](http...\"],[\"- Remove embed's `initial_height` when loading is complete so the embed finds its natural height onc...\"],[\"- Ensure markdown lists are rendered correctly by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3341](ht...\"],[\"- Support new embeds for huggingface spaces subdomains by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR ...\"],[\"### Documentation Changes:\\n\\n- Added the `types` field to the dependency field in the config by [@fre...\"],[\"### Full Changelog:\\n\\n- Fixed comment typo in components.py by [@eltociear](https:\\u002f\\u002fgithub.com\\u002feltoci...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.19.0\\n\\n### New Features:\\n\\n###### Improved ...\"],[\"Create interactive bar plots from a high-level interface with `gr.BarPlot`.\\nNo need to remember matp...\"],[\"```\\n\\nBy [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3157](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"```\\n\\nBy [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- Fixes `gr.utils.delete_none` to only remove props whose values are `None` from the config by [@abi...\"],[\"- Ensure latext CSS is always applied in light and dark mode by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) i...\"],[\"### Documentation Changes:\\n\\n- Sort components in docs by alphabetic order by [@aliabd](https:\\u002f\\u002fgithu...\"],[\"- Fix demos page css and add close demos button by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3151]...\"],[\"- Fixed gradio share links so that they are persistent and do not reset if network\\n  connection is d...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.18.0\\n\\n### New Features:\\n\\n###### Revamped ...\"],[\"```\\n\\nBy [@maxaudron](https:\\u002f\\u002fgithub.com\\u002fmaxaudron) in [PR 3075](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio...\"],[\"- Fixes URL resolution on Windows by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 3108](https:\\u002f\\u002fg...\"],[\"- A share link will automatically be created when running on Sagemaker notebooks so that the front-e...\"],[\"### Documentation Changes:\\n\\n- Added a guide on the 4 kinds of Gradio Interfaces by [@yvrjsharma](htt...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.17.1\\n\\n### New Features:\\n\\n###### iOS image...\"],[\"### Bug Fixes:\\n\\n- Fix bug where examples were not rendered correctly for demos created with Blocks a...\"],[\"* Fix a broken link in the Quick Start guide, by [@cakiki](https:\\u002f\\u002fgithub.com\\u002fcakiki) in [PR 3109](h...\"],[\"```\\n\\n\\u003cimg width=\\\"1087\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f213260197...\"],[\"```\\n\\n![chatbot_load](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f213260220-3eaa25b7-a38b-48c6...\"],[\"- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fixes issue where markdown support in chatbot breaks older demos [@dawoodkhan82](https:\\u002f\\u002fgithub.co...\"],[\"- Fix several minor frontend bugs (loading animation, examples as gallery) frontend [@aliabid94](htt...\"],[\"- Fixes bug where app would crash if the `file_types` parameter of `gr.File` or `gr.UploadButton` wa...\"],[\"- Fix bug where auth was not respected on HF spaces by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyab...\"],[\"### Documentation Changes:\\n\\n- SEO improvements to guides by[@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [...\"],[\"- Fixed file upload fails for files with zero size by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan8...\"],[\"- Fix bug in `blocks_plug` demo that prevented switching tabs programmatically with python [@TashaSk...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"```\\n\\nProgress indicator bar by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 2750](https:\\u002f\\u002fgithu...\"],[\"```\\n\\n\\u003cimg width=\\\"610\\\" alt=\\\"Screenshot 2023-01-03 at 4 14 36 PM\\\" src=\\\"https:\\u002f\\u002fuser-images.githubuserc...\"],[\"### Bug Fixes:\\n\\n- Fixed bug where an error opening an audio file led to a crash by [@FelixDombek](ht...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using Google Sheets to create a real-time dashboard w...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- The `default_enabled` parame...\"],[\"With this component you can easily create time series visualizations with customizable\\nappearance fo...\"],[\"```\\n\\n![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f208711646-81ae3745-149b-46a3-babd-0...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using BigQuery with Gradio's `DataFrame` and `Scatter...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.14.0\\n\\n### New Features:\\n\\n###### Add Wavef...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixed issue where too many temporary files were created, all with randomly ge...\"],[\"You don't need to do anything differently, but when you set `share=True` in `launch()`,\\nyou'll get t...\"],[\"```\\n\\nThese links are a more secure and scalable way to create shareable demos!\\n\\n### Bug Fixes:\\n\\n- Al...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Fixed typo in parameter `vis...\"],[\"For an example of how to use `gr.ScatterPlot` see below:\\n\\n```python\\nimport gradio as gr\\nfrom vega_da...\"],[\"```\\n\\n\\u003cimg width=\\\"404\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f206737726-...\"],[\"```\\n\\n\\u003cimg width=\\\"1366\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f204660697...\"],[\"```\\n\\n![label_bg_color_update](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f204400372-80e53857-...\"],[\"### Bug Fixes:\\n\\n- Fixed issue where image thumbnails were not showing when an example directory was ...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"Here's a simple example that references a local image `lion.jpg` that is in the same\\nfolder as the P...\"],[\"```\\n\\n![Alt text](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f1778297\\u002f204357455-5c1a4002-eee7-479d-9a1e...\"],[\"```\\n\\n![update_accordion](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f203164176-b102eae3-babe-...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"## 3.11.0\\n\\n### New Features:\\n\\n###### Upload Button\\n\\nThere is now a new component called the `UploadB...\"],[\"```\\n\\n###### Revamped API documentation page\\n\\nNew API Docs page with in-browser playground and update...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixed bug that limited files from being sent over websockets to 16MB. The new...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.10.0\\n\\n- Add support for `'password'` and ...\"],[\"### Documentation Changes:\\n\\n- Fix some typos in the embedded demo names in \\\"05_using_blocks_like_fun...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"```\\n\\nThe `api_name` parameter will take precedence over the `fn_index` parameter.\\n\\n### Bug Fixes:\\n\\n-...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"Here is an example of a live plot that refreshes every half second:\\n\\n```python\\nimport math\\nimport gr...\"],[\"```\\n\\n![live_demo](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f198357377-633ce460-4e31-47bd-82...\"],[\"### Bug Fixes:\\n\\n- Fix whitespace issue when using plotly. [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodk...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.7\\n\\n### New Features:\\n\\n###### Batched Func...\"],[\"```\\n\\nThe advantage of using batched functions is that if you enable queuing, the Gradio\\nserver can a...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixes issue where plotly animations, interactivity, titles, legends, were not...\"],[\"### Documentation Changes:\\n\\n- Added an example interactive dashboard to the \\\"Tabular & Plots\\\" sectio...\"],[\"- Fixes the error message if a user builds Gradio locally and tries to use `share=True` by [@abidlab...\"],[\"- Clearer error message when events are defined outside of a Blocks scope, and a warning if you\\n  tr...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.6\\n\\n### New Features:\\n\\n###### Cancelling R...\"],[\"```\\n\\nFor interfaces, a stop button will be added automatically if the function uses a `yield` statem...\"],[\"```\\n\\n![stop_interface_rl](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f195952883-e7ca4235-aae3...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.5\\n\\n### Bug Fixes:\\n\\n- Ensure that Gradio d...\"],[\"### New Features:\\n\\n- When an `Image` component is set to `source=\\\"upload\\\"`, it is now possible to dr...\"],[\"- Speeds up Gallery component by using temporary files instead of base64 representation in the front...\"],[\"- Automatically restart spaces if they're down by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2405](...\"],[\"- Fix embedded interfaces on touch screen devices by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 245...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4.1\\n\\n### New Features:\\n\\n###### 1. See Pas...\"],[\"1. Fix typo in guide image path by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 2357]...\"],[\"7. Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](...\"],[\"### Documentation Changes:\\n\\n1. New Guide: Connecting to a Database ğŸ—„ï¸\\n\\n   A new guide by [@freddyabo...\"],[\"- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](htt...\"],[\"- Catch the permission exception on the audio component by [@Ian-GL](https:\\u002f\\u002fgithub.com\\u002fIan-GL) in [...\"],[\"- Lets users provide a `gr.update()` dictionary even if post-processing is disabled [@abidlabs](http...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4\\n\\n### New Features:\\n\\n###### 1. Gallery C...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192399521-7360b1a9-7ce0-443e-8e94-8...\"],[\"```\\n\\n![color-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410500-3c8c3e64-a5fd-4df2-...\"],[\"```\\n\\n![webcam-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410820-0ffaf324-776e-4e1f...\"],[\"As well as other fixes\\n\\n### Bug Fixes:\\n\\n1. Fix bug where max concurrency count is not respected in q...\"],[\"### Documentation Changes:\\n\\n1. Adding a Playground Tab to the Website by [@aliabd](https:\\u002f\\u002fgithub.co...\"],[\"- Website fixes and refactoring by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2280](https:\\u002f\\u002fgithub....\"],[\"- Respect Upstream Queue when loading interfaces\\u002fblocks from Spaces by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fix Web Tracker Script by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2308](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"### Contributors Shoutout:\\n\\n- [@SkyTNT](https:\\u002f\\u002fgithub.com\\u002fSkyTNT) made their first contribution in ...\"],[\"```\\n\\n![example](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f189086273-f5e7087d-71fa-4158-90a9-...\"],[\"```\\n\\n![187936493-5c90c01d-a6dd-400f-aa42-833a096156a1](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f902...\"],[\"- safari fixes by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 2138](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- Fixed misleading log when server_name is '0.0.0.0' by [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangt...\"],[\"- Preserve Labels In Interpretation Components by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulto...\"],[\"### Contributors Shoutout:\\n\\n- [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangtung) made their first cont...\"],[\"```\\n\\n- Configure a maximum queue size\\n\\n```python\\ndemo = gr.Interface(...)\\ndemo.queue(max_size=100)\\nd...\"],[\"```\\n\\n- Automatic conversion of videos so they are playable in the browser (thanks to PR #2003). Grad...\"],[\"```\\n\\n###### 5. New Guide ğŸ–Šï¸\\n\\n- [Gradio and W&B Integration](https:\\u002f\\u002fgradio.app\\u002fGradio_and_Wandb_Inte...\"],[\"- Reset components to original state by setting value to None by [@freddyaboulton](https:\\u002f\\u002fgithub.co...\"],[\"- Encourage people to keep trying when queue full by [@apolinario](https:\\u002f\\u002fgithub.com\\u002fapolinario) in...\"],[\"- Allow frontend method execution on Block.load event by [@codedealer](https:\\u002f\\u002fgithub.com\\u002fcodedealer...\"],[\"- feat(samples table\\u002fgallery): Crop thumbs to square by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted)...\"],[\"### Contributors Shoutout:\\n\\n- [@chrisemezue](https:\\u002f\\u002fgithub.com\\u002fchrisemezue) made their first contri...\"],[\"```\\n\\nBut you can also embed demos that are running anywhere, you just need to link the demo to `src`...\"],[\"```\\n\\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ex...\"],[\"We've added the `gr.Examples` component helper to allow you to add examples to any Blocks demo. This...\"],[\"- File component: list multiple files and allow for download #1446 by [@dawoodkhan82](https:\\u002f\\u002fgithub...\"],[\"- Add python-3.7 tests by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 1818](https:\\u002f\\u002f...\"],[\"### Contributors Shoutout:\\n\\n- [@nhankiet](https:\\u002f\\u002fgithub.com\\u002fnhankiet) made their first contribution...\"],[\"```\\n\\n![hello-blocks](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f168684108-78cbd24b-e6bd-4a04-...\"],[\"![Model3d](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f168689062-6ad77151-8cc5-467d-916c-f7c78...\"],[\"- Gradio dash fe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 807](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- 854 textbox by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 859](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio...\"],[\"- fix default_value by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 869](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- rename Model3d to Image3D by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 912](https:\\u002f\\u002f...\"],[\"- fix unit + browser tests by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 926](https:\\u002f\\u002fgithub.com\\u002fgrad...\"],[\"- tabbed-interface-rewritten by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 958](https:\\u002f...\"],[\"- Fix #944 by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 971](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"- indentation fix by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 993](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"- Image3D Examples Fix by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 1001](https:\\u002f\\u002fgith...\"],[\"- [BIG PR] Gradio blocks & redesigned components by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR ...\"],[\"- Model3D Examples fixes by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 1035](https:\\u002f\\u002fgi...\"],[\"- Update text by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted) in [PR 1050](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"- Update PULL_REQUEST_TEMPLATE.md by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 1068](h...\"],[\"- Explicitly list pnpm version 6 in contributing guide by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffredd...\"],[\"- Optional labels fe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1105](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- issue-checkbox-hotfix by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 1127](https:\\u002f\\u002fgit...\"],[\"- highlighted text colors by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1119](https:\\u002f\\u002fgithub.com\\u002fgrad...\"],[\"- html tweaks by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1145](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- Small fixes: queue default fix, ffmpeg installation message by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabid...\"],[\"- 1183 component height by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1185](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"- add copy functionality to json by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1205](https:\\u002f\\u002fgithub.c...\"],[\"- Allow Custom CSS by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 1170](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"- update-shortcut-syntax by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 1234](https:\\u002f\\u002fgi...\"],[\"- Add precision to Number, backend only by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [...\"],[\"- update logo by [@gary149](https:\\u002f\\u002fgithub.com\\u002fgary149) in [PR 1266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"- release 3.0b9 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1283](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"- updated PyPi version to 3.0 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1290](https:\\u002f\\u002fgithu...\"],[\"### Contributors Shoutout:\\n\\n- [@JefferyChiang](https:\\u002f\\u002fgithub.com\\u002fJefferyChiang) made their first co...\"],[\"@gradio\\u002fstatustracker\\n\\n## 0.4.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6...\"],[\"## 0.4.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5342](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5342) [`afac0006`](https...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https...\"],[\"create-svelte\\n\\nEverything you need to build a Svelte project, powered by [`create-svelte`](https:\\u002f\\u002fg...\"],[\"imple image classification in Pytorch with Gradio's Image input and Label output....\"],[\"`@gradio\\u002fatoms`\\n\\n```html\\n\\u003cscript lang=\\\"ts\\\"\\u003e\\n\\timport { Block, BlockTitle, BlockLabel, IconButton, Emp...\"],[\"his text generation demo works like autocomplete. There's only one textbox and it's used for both th...\"],[\"Gradio Demo: sound_alert\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo r...\"],[\"Gradio Demo: theme_extended_step_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"å®æ—¶è¯­éŸ³è¯†åˆ«\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused, https:\\u002f\\u002fhugging...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-streaming-asr-paused.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"350\\\" title=\\\"Gra...\"],[\"ä¸‹é¢æ˜¯æ„å»ºå®æ—¶è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åº”ç”¨ç¨‹åºçš„æ­¥éª¤ï¼š\\n\\n1. [è®¾ç½® Transformers ASR æ¨¡å‹](#1-set-up-the-transformers-asr-model)\\n2. [ä½¿ç”¨ T...\"],[\"```\\n\\nå°±æ˜¯è¿™æ ·ï¼é»˜è®¤æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ç®¡é“ä¼šåŠ è½½ Facebook çš„ `facebook\\u002fwav2vec2-base-960h` æ¨¡å‹ã€‚\\n\\n## 2. ä½¿ç”¨ Transformers åˆ›å»º...\"],[\"```\\n\\né‚£ä¹ˆè¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ`transcribe` å‡½æ•°æ¥å—ä¸€ä¸ªå‚æ•° `audio`ï¼Œå®ƒæ˜¯ç”¨æˆ·å½•åˆ¶çš„éŸ³é¢‘æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ã€‚`pipeline` å¯¹è±¡æœŸæœ›ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åè¿”å›åˆ°å‰ç«¯...\"],[\"å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´åˆšåˆšåˆ›å»ºçš„æ¼”ç¤ºï¼Œä½¿å…¶æˆä¸ºæµå¼çš„ï¼Œä½¿ç”¨ç›¸åŒçš„ `Wav2Vec2` æ¨¡å‹ã€‚\\n\\næœ€å¤§çš„å˜åŒ–æ˜¯æˆ‘ä»¬ç°åœ¨å¿…é¡»å¼•å…¥ä¸€ä¸ª `state` å‚æ•°ï¼Œå®ƒä¿å­˜åˆ°ç›®å‰ä¸ºæ­¢*è½¬å½•çš„éŸ³é¢‘*ã€‚è¿™æ ·ï¼Œ...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å¦ä¸€ä¸ªæ›´æ”¹ï¼Œå³æˆ‘ä»¬è®¾ç½®äº† `live=True`ã€‚è¿™ä½¿å¾— Gradio æ¥å£ä¿æŒæŒç»­è¿è¡Œï¼Œå› æ­¤å®ƒå¯ä»¥è‡ªåŠ¨è½¬å½•éŸ³é¢‘ï¼Œè€Œæ— éœ€ç”¨æˆ·åå¤ç‚¹å‡»æäº¤æŒ‰é’®ã€‚\\n\\nè®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„æ•ˆæœï¼ˆåœ¨ä¸‹...\"],[\"```python\\nfrom transformers import pipeline\\nimport gradio as gr\\nimport time\\n\\np = pipeline(\\\"automatic...\"],[\"```\\n\\nå°è¯•ä¸‹é¢çš„æ¼”ç¤ºï¼ŒæŸ¥çœ‹å·®å¼‚ï¼ˆæˆ–[åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused)ï¼‰ï¼\\n\\n\\u003cifram...\"],[\"ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼ˆåœ¨ Linux ä¸Šï¼‰ï¼š\\n\\né¦–å…ˆé€šè¿‡ç»ˆç«¯å®‰è£… DeepSpeech åº“å¹¶ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š\\n\\n```bash\\nwget https:\\u002f\\u002fgithub.com\\u002fmozilla\\u002fDeep...\"],[\"```\\n\\nç„¶åï¼Œåˆ›å»ºä¸ä¹‹å‰ç›¸ä¼¼çš„ `transcribe()` å‡½æ•°ï¼š\\n\\n```python\\nfrom deepspeech import Model\\nimport numpy as np\\n\\nmode...\"],[\"```\\n\\nè¿è¡Œæ‰€æœ‰è¿™äº›åº”è¯¥å…è®¸æ‚¨ä½¿ç”¨ä¸€ä¸ªæ¼‚äº®çš„ GUI éƒ¨ç½²å®æ—¶ ASR æ¨¡å‹ã€‚å°è¯•ä¸€ä¸‹ï¼Œçœ‹å®ƒåœ¨æ‚¨é‚£é‡Œè¿è¡Œå¾—æœ‰å¤šå¥½ã€‚\\n\\n---\\n\\nä½ å·²ç»å®Œæˆäº†ï¼è¿™å°±æ˜¯æ„å»ºç”¨äº ASR æ¨¡å‹çš„åŸºäº Web çš„ GUI ...\"],[\"Gradio Demo: sine_curve\\n\\n\\n```\\n!pip install -q gradio plotly\\n```\\n\\n\\n```\\nimport math\\nimport gradio as g...\"],[\"his text generation demo takes in input text and returns generated text. It uses the Transformers li...\"],[\"Gradio Demo: color_generator\\n\\n\\n```\\n!pip install -q gradio opencv-python numpy\\n```...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport cv2\\nimport numpy as np\\nimport random\\n\\n\\n# Convert decimal color ...\"],[\"@gradio\\u002fclient\\n\\n## 0.9.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6814) [`...\"],[\"## 0.9.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67ddd40`](https:...\"],[\"## 0.8.1\\n\\n### Fixes\\n\\n- [#6383](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6383) [`324867f63`](https:\\u002f...\"],[\"## 0.7.1\\n\\n### Features\\n\\n- [#6137](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6137) [`2ba14b284`](http...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.7.0-beta.0\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.5.2\\n\\n### Fixes\\n\\n- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f...\"],[\"This component allows you to populate the explorer by passing a glob, but only provides the selected...\"],[\"## 0.4.2\\n\\n### Features\\n\\n- [#5124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5124) [`6e56a0d9b`](http...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5682](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5682) [`c57f1b75e`](http...\"],[\"## 0.3.1\\n\\n### Fixes\\n\\n- [#5412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5412) [`26fef8c7`](https:\\u002f\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5133](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5133) [`61129052`](https...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- [#4717](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4717) [`ab5d1ea0`](...\"],[\"- [#4315](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4315) [`b525b122`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4202](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4202) [`a26e9afd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#3605](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f3605) [`ae4277a9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"@gradio\\u002flabel\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Case Study: A Component to Display PDFs\\n\\nLet's work through an example of building a custom gradio c...\"],[\"```\\n\\n\\nTip: You should change the name of the component.\\nSome of the screenshots assume the component...\"],[\"The complete `package.json` should look like this:\\n\\n```json\\n{\\n  \\\"name\\\": \\\"gradio_pdf\\\",\\n  \\\"version\\\": \\\"...\"],[\"```\\n\\n\\nTip: Running `npm install` will install the latest version of the package available. You can i...\"],[\"```\\n\\n## Step 3: Frontend - Launching the Dev Server\\n\\nRun the `dev` command to launch the development...\"],[\"export let elem_id = \\\"\\\";\\n\\texport let elem_classes: string[] = [];\\n\\texport let visible = true;\\n\\texpor...\"],[\"```\\n\\n\\nTip: The `gradio`` object passed in here contains some metadata about the application as well ...\"],[\"```\\n\\nYou should see the following when you navigate to your app after saving your current changes:\\n\\n...\"],[\"```\\n\\nNow import `PdfUploadText.svelte` in your `\\u003cscript\\u003e` and pass it to the `Upload` component!\\n\\n``...\"],[\"```\\n\\nAlso create the following variables:\\n\\n```ts\\n    let pdfDoc;\\n    let numPages = 1;\\n    let curre...\"],[\"```\\n\\n\\nTip: The `$:` syntax in svelte is how you declare statements to be reactive. Whenever any of t...\"],[\"```\\n\\n\\nTip: The `gradio.dispatch` method is actually what is triggering the `change` or `upload` even...\"],[\"```\\n\\nCongratulations! You have a working pdf uploader!\\n\\n![upload-gif](https:\\u002f\\u002fgradio-builds.s3.amazo...\"],[\"```\\n\\nCongratulations! The frontend is almost complete ğŸ‰\\n\\n![multipage-pdf-gif](https:\\u002f\\u002fgradio-builds....\"],[\"\\u003cdiv\\n\\tclass:table={type === \\\"table\\\"}\\n\\tclass:gallery={type === \\\"gallery\\\"}\\n\\tclass:selected\\n\\tstyle=\\\"jus...\"],[\"```\\n\\n\\nTip: Exercise for the reader - reduce the code duplication between `Index.svelte` and `Example...\"],[\"class PDF(Component):\\n\\n    EVENTS = [\\\"change\\\", \\\"upload\\\"]\\n\\n    data_model = FileData\\n\\n    def __init_...\"],[\"```\\n\\n## Step 10: Add a demo and publish!\\n\\nTo test our backend code, let's add a more complex demo th...\"],[\"```\\n\\nSee our demo in action below!\\n\\n\\u003cvideo autoplay muted loop\\u003e\\n  \\u003csource src=\\\"https:\\u002f\\u002fgradio-builds...\"],[\"Gradio Demo: stream_audio_out\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nfrom pydub import AudioSegment\\nfrom time import sleep\\n\\nwith gr.Blocks(...\"],[\"gradio-ui\\n\\nThis folder contains all of the Gradio UI and component source code.\\n\\n- [set up](#setup)\\n...\"],[\"```\\n\\nIf you have formatting failures then you can run the following command to fix them:\\n\\n```bash\\npn...\"],[\"Gradio Demo: gallery_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"Gradio Demo: blocks_scroll\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndemo = gr.B...\"],[\"website\\n\\n## 0.20.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fcode@0.3.3\\n\\n## 0.20.2\\n...\"],[\"## 0.19.0\\n\\n### Features\\n\\n- [#5885](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5885) [`9919b8a`](https...\"],[\"## 0.17.0\\n\\n### Features\\n\\n- [#6533](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6533) [`e2810fcfc`](htt...\"],[\"## 0.15.0\\n\\n### Features\\n\\n- [#6436](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6436) [`58e3ca826`](htt...\"],[\"## 0.14.0\\n\\n### Features\\n\\n- [#6387](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6387) [`9d6d72f44`](htt...\"],[\"## 0.12.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fcode@0.2.3\\n\\n## 0.12.0\\n\\n### Feat...\"],[\"## 0.11.1\\n\\n### Features\\n\\n- [#6189](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6189) [`345ddd888`](htt...\"],[\"### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c`](https:\\u002f\\u002fgithub...\"],[\"## 0.11.0-beta.0\\n\\n### Features...\"],[\"- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6097](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6097) [`439efa39d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.9.0\\n\\n### Features\\n\\n- [#5386](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5386) [`0312c990f`](http...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5643](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5643) [`f661c0733`](http...\"],[\"### Fixes\\n\\n- [#5608](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5608) [`eebf9d71f`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5423](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5423) [`bb31cd7d`](https...\"],[\"### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.2.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5298](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5076](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5076) [`2745075a`](https...\"],[\"## 0.0.2\\n\\n### Features\\n\\n- [#5009](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5009) [`3e70fc81`](https...\"],[\"Gradio Demo: live_dashboard\\n### This demo shows how you can build a live interactive dashboard with ...\"],[\"```\\n!pip install -q gradio plotly\\n```\\n\\n\\n```\\nimport math\\n\\nimport pandas as pd\\n\\nimport gradio as gr\\nim...\"],[\"Image Classification in TensorFlow and Keras\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs...\"],[\"```\\n\\nThis line automatically downloads the MobileNet model and weights using the Keras library.\\n\\n## ...\"],[\"```\\n\\nLet's break this down. The function takes one parameter:\\n\\n- `inp`: the input image as a `numpy`...\"],[\"Gradio Demo: queue_full_e2e_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"Gradio Demo: blocks_simple_squares\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo...\"],[\"Gradio Demo: blocks_static\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.Bl...\"],[\"Gradio Demo: main_note\\n\\n\\n```\\n!pip install -q gradio scipy numpy matplotlib\\n```\\n\\n\\n```\\n# Downloading f...\"],[\"```\\n\\n\\n```\\nfrom math import log2, pow\\nimport os\\n\\nimport numpy as np\\nfrom scipy.fftpack import fft\\n\\nim...\"],[\"@gradio\\u002fatoms\\n\\n## 0.4.1\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.2\\n\\n### Fixes\\n\\n- [#6254](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.4\\n\\n### Features\\n\\n- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n\\n## 0.1.3\\n\\n### Patch...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"demo for predicting the depth of an image and generating a 3D model of it....\"],[\"Gradio Demo: video_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the de...\"],[\"TensorFlow å’Œ Keras ä¸­çš„å›¾åƒåˆ†ç±»\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fkeras-image-classifier\\næ ‡ç­¾ï¼šVIS...\"],[\"## ç¬¬ä¸€æ­¥ â€”â€” è®¾ç½®å›¾åƒåˆ†ç±»æ¨¡å‹\\n\\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡å‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ Mobile Net æ¨¡å‹ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»[Keras](https:\\u002f\\u002fkeras.io\\u002fa...\"],[\"```\\n\\næ­¤è¡Œä»£ç å°†ä½¿ç”¨ Keras åº“è‡ªåŠ¨ä¸‹è½½ MobileNet æ¨¡å‹å’Œæƒé‡ã€‚\\n\\n## ç¬¬äºŒæ­¥ â€”â€” å®šä¹‰ `predict` å‡½æ•°\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶*ç”¨æˆ·è¾“å…¥*ä½œä¸ºå‚æ•°...\"],[\"```\\n\\nè®©æˆ‘ä»¬æ¥è¯¦ç»†äº†è§£ä¸€ä¸‹ã€‚è¯¥å‡½æ•°æ¥å—ä¸€ä¸ªå‚æ•°ï¼š\\n\\n- `inp`ï¼šè¾“å…¥å›¾åƒçš„ `numpy` æ•°ç»„\\n\\nç„¶åï¼Œå‡½æ•°æ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦ï¼Œé€šè¿‡æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œå¹¶è¿”å›ï¼š\\n\\n- `confidences`ï¼šé¢„...\"],[\"```\\n\\nè¿™å°†ç”Ÿæˆä»¥ä¸‹ç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç«‹å³å°è¯•ï¼ˆå°è¯•ä¸Šä¼ æ‚¨è‡ªå·±çš„ç¤ºä¾‹ï¼ï¼‰ï¼š\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-keras-image-classifier.hf.sp...\"],[\"his demo identifies if two speakers are the same person using Gradio's Audio and HTML components....\"],[\"Gradio Demo: image_classifier_interface_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading f...\"],[\"@gradio\\u002fimage\\n\\n## 0.5.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.5.0\\n\\n### Features\\n\\n- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:...\"],[\"### Fixes\\n\\n- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.3.6\\n\\n### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.3.4\\n\\n### Features\\n\\n- [#6363](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6363) [`4d3aad33a`](http...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.9\\n\\n### Features...\"],[\"- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6146](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6146) [`40a171ea6`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5554](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5554) [`75ddeb390`](http...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: latex\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as ...\"],[\"Gradio Demo: video_identity\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"@gradio\\u002ffallback\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.0-beta.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`14fc612d8`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"# @gradio\\u002fcheckbox\\n\\n## 0.1.1\\n\\n### Fixes\\n\\n- [#5340](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5340) [...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"ğŸš€ Creating Discord Bots from Gradio Apps ğŸš€\\n\\nTags: NLP, TEXT, CHAT\\n\\nWe're excited to announce that Gr...\"],[\"```\\n\\n### Step 2: Deploying our App\\n\\nIn order to create a discord bot for our app, it must be accessi...\"],[\"```\\n\\n### Step 5: Add the bot to your server\\n\\nVisit the space of your discord bot. You should see \\\"Ad...\"],[\"```\\n\\n## ğŸ¦¾ Using State of The Art LLMs ğŸ¦¾\\n\\nWe have created an organization on Hugging Face called [gra...\"],[\"```\\n\\n## ğŸ¦œ Additional LLMs ğŸ¦œ\\n\\nIn addition to Meta's 70 billion Llama 2 model, we have prepared templa...\"],[\"Gradio Demo: blocks_essay\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\ncountries_cities_dict = {\\n    \\\"USA\\\": [\\\"New York\\\", \\\"Los Angeles\\\", \\\"Chicago\\\"]...\"],[\"def reset_bounds(minimum, maximum):\\n        return gr.Number(minimum=minimum, maximum=maximum)\\n\\n    ...\"],[\"Gradio Demo: generate_tone\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport gr...\"],[\"`@gradio\\u002fform`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Form } from \\\"@gradio\\u002fform\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nForm\\n```javasc...\"],[\"Gradio Demo: hello_world_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Components: The Key Concepts\\n\\nIn this section, we discuss a few important concepts when it co...\"],[\"```\\n\\nThe interactive version of the component is much more complex -- you can upload images or snap ...\"],[\"demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \\\"image\\\")\\ndemo.launch()...\"],[\"```\\n\\nThis will create a Gradio app which has an `Image` component as the input and the output. \\nIn t...\"],[\"* As a component author, **YOU** control the format of the data displayed in the frontend as well as...\"],[\"## Conclusion\\n\\nNow that you know the most important pieces to remember about Gradio components, you ...\"],[\"Real Time Speech Recognition\\n\\nTags: ASR, SPEECH, STREAMING\\n\\n## Introduction\\n\\nAutomatic speech recogn...\"],[\"## 1. Set up the Transformers ASR Model\\n\\nFirst, you will need to have an ASR model that you have eit...\"],[\"```\\n\\nThat's it!\\n\\n## 2. Create a Full-Context ASR Demo with Transformers\\n\\nWe will start by creating a...\"],[\"Gradio Demo: unified_demo_text_generation\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\n...\"],[\"Gradio Demo: calculator_blocks_cached\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\n...\"],[\"Gradio Demo: image-simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"`@gradio\\u002fimageeditor`...\"],[\"Gradio Demo: chatbot_multimodal\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"```\\nimport gradio as gr\\nimport os\\nimport time\\n\\n# Chatbot demo with multimodal input (text, markdown,...\"],[\"chatbot.like(print_like_dislike, None, None)\\n\\n\\ndemo.queue()\\nif __name__ == \\\"__main__\\\":\\n    demo.laun...\"],[\"Gradio Demo: dataframe_block-ui-test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwi...\"],[\"Gradio Demo: on_listener_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"The Backend ğŸ\\n\\nThis guide will cover everything you need to know to implement your custom component'...\"],[\"```\\n\\n## The methods you need to implement\\n\\nWhen you inherit from any of these classes, the following...\"],[\"```\\n\\n### `api_info`\\n\\nA JSON-schema representation of the value that the `preprocess` expects. \\nThis ...\"],[\"```\\n\\n## The `data_model`\\n\\nThe `data_model` is how you define the expected data format your component...\"],[\"```\\n\\nBy adding these four lines of code, your component automatically implements the methods needed ...\"],[\"```\\n\\nEven if your component does not expect a \\\"complex\\\" JSON data structure it can be beneficial to ...\"],[\"Gradio Demo: blocks_flag\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport grad...\"],[\"Gradio Demo: concurrency_without_queue\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\ni...\"],[\"Gradio Demo: dashboard\\n### This demo shows how you can build an interactive dashboard with gradio. C...\"],[\"```\\nimport gradio as gr\\nimport pandas as pd\\nimport plotly.express as px\\nfrom helpers import *\\n\\n\\nLIBR...\"],[\"def create_issue_plot(libraries, issue_choices):\\n    if \\\"Issue\\\" not in issue_choices:\\n        return...\"],[\"if __name__ == \\\"__main__\\\":\\n    demo.launch()...\"],[\"@gradio\\u002fslider\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Fixes\\n\\n- [#5984](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5984) [`66549d8d2`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.3\\n\\n### Features\\n\\n- [#5535](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5535) [`d29b1ab74`](http...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"@gradio\\u002fmodel3d\\n\\n## 0.4.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6240) [`dd901c1b0`](http...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.8\\n\\n### Features...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5373](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5373) [`79d8f9d8`](https...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"# @gradio\\u002fmodel3D\\n\\n## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub....\"],[\"mport { Meta } from \\\"@storybook\\u002fblocks\\\";\\n\\n\\u003cMeta title=\\\"Introduction\\\" \\u002f\\u003e\\n\\n\\u003cstyle\\u003e\\n\\t{`\\n    img {\\n     ...\"],[\"\\u003cdiv class=\\\"subheading\\\"\\u003eFeedback\\u003c\\u002fdiv\\u003e\\nIf you have any questions, issues, or feedback on our compone...\"],[\"simple demo showcasing the upload button used with its `upload` event trigger....\"],[\"`@gradio\\u002fhighlightedtext`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseStaticHighlightedText, BaseInteractiveH...\"],[\"Gradio Demo: image_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: reverse_audio\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: blocks_page_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef prin...\"],[\"Gradio & LLM Agents ğŸ¤\\n\\nLarge Language Models (LLMs) are very impressive but they can be made even mo...\"],[\"## gradio_tools - An end-to-end example\\n\\nTo get started with `gradio_tools`, all you need to do is i...\"],[\"```\\n\\nYou'll note that we are using some pre-built tools that come with `gradio_tools`. Please see th...\"],[\"```\\n\\nThe requirements are:\\n\\n1. The name for your tool\\n2. The description for your tool. This is cruc...\"],[\"And that's it!\\n\\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We ...\"],[\"```\\n\\nSome notes on this implementation:\\n\\n1. All instances of `GradioTool` have an attribute called `...\"],[\"his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear...\"],[\"`@gradio\\u002fvideo`\\n\\n```javascript\\n\\u003cscript\\u003e\\n\\timport { BaseInteractiveVideo, BaseStaticVideo, BasePlayer ...\"],[\"utomatic speech recognition English. Record from your microphone and the app will transcribe the aud...\"],[\"Gradio Demo: longest_word\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef longest_...\"],[\"`@gradio\\u002fcolorpicker`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseColorPicker, BaseExample } from \\\"@gradio\\u002fco...\"],[\"Gradio Demo: annotatedimage_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nim...\"],[\"Running a Gradio App on your Web Server with Nginx\\n\\nTags: DEPLOYMENT, WEB SERVER, NGINX\\n\\n## Introduc...\"],[\"```\\n\\n2. Create a new file in the `\\u002fetc\\u002fnginx\\u002fsites-available` directory (create the directory if it ...\"],[\"```\\n\\n2. Start a `tmux` session by typing `tmux` and pressing enter (optional)\\n\\nIt's recommended that...\"],[\"@gradio\\u002fupload\\n\\n## 0.5.6\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`732...\"],[\"## 0.5.3\\n\\n### Fixes\\n\\n- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fg...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.4.2\\n\\n### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6356](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6356) [`854b482f5`](http...\"],[\"## 0.3.2\\n\\n### Fixes\\n\\n- [#6234](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6234) [`aaa55ce85`](https:\\u002f...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.5\\n\\n### Features\\n\\n- [#6044](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6044) [`9053c95a1...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.2\\n\\n## 0.2.0\\n\\n### Featu...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5216](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"def stream_audio(audio_file):\\n    audio = AudioSegment.from_mp3(audio_file)\\n    i = 0\\n    chunk_size...\"],[\"```\\n\\nFrom the backend, streamed outputs are served from the `\\u002fstream\\u002f` endpoint instead of the `\\u002ffil...\"],[\"Getting Started with the Gradio Python client\\n\\nTags: CLIENT, API, SPACES\\n\\nThe Gradio Python client m...\"],[\"```\\n\\n## Connecting to a running Gradio App\\n\\nStart by connecting instantiating a `Client` object and ...\"],[\"```\\n\\nIf you have previously duplicated a Space, re-running `duplicate()` will _not_ create a new Spa...\"],[\"```\\n\\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoi...\"],[\"```\\n\\n## Running jobs asynchronously\\n\\nOe should note that `.predict()` is a _blocking_ operation as i...\"],[\"```\\n\\n## Status\\n\\nThe `Job` object also allows you to get the status of the running job by calling the...\"],[\"```\\n\\nIf the first job has started processing, then it will not be canceled. If the second job\\nhas no...\"],[\"Frequently Asked Questions\\n\\n## What do I need to install before using Custom Components?\\nBefore usin...\"],[\"Utilizing `FileData` is crucial for components that expect file uploads. It ensures secure file hand...\"],[\"Create Your Own Friends with a GAN\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fNimaBoscarino\\u002fcryp...\"],[\"Today we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo ...\"],[\"## Step 1 â€” Create the Generator model\\n\\nTo generate new images with a GAN, you only need the generat...\"],[\"```\\n\\nWe're taking the generator from [this repo by @teddykoker](https:\\u002f\\u002fgithub.com\\u002fteddykoker\\u002fcrypto...\"],[\"```\\n\\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor ge...\"],[\"```\\n\\nThe new input will be passed to our `predict()` function, so we have to make some changes to th...\"],[\"```\\n\\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in t...\"],[\"For reference, here is our full code:\\n\\n```python\\nimport torch\\nfrom torch import nn\\nfrom huggingface_...\"],[\"def predict(seed, num_punks):\\n    torch.manual_seed(seed)\\n    z = torch.randn(num_punks, 100, 1, 1)\\n...\"],[\"```\\n\\n---\\n\\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a ...\"],[\"Gradio Demo: blocks_component_shortcut\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n...\"],[\"Gradio Demo: progress_component\\n\\n\\n```\\n!pip install -q gradio tqdm\\n```\\n\\n\\n```\\nimport gradio as gr\\nimpo...\"],[\"Gradio Demo: calculator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo re...\"],[\"Gradio Demo: cancel_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport time\\nimport gradio as gr\\n\\n\\ndef fake_diffusion(steps):\\n    for i in range(steps):\\n        ...\"],[\"cancel_on_change.change(None, None, None, cancels=[click_event, pred_event])\\n    cancel_on_submit.su...\"],[\"Gradio Demo: live_with_vars\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.I...\"],[\"ååº”å¼ç•Œé¢ (Reactive Interfaces)\\n\\næœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•ä½¿ Gradio ç•Œé¢è‡ªåŠ¨åˆ·æ–°æˆ–è¿ç»­æµå¼ä¼ è¾“æ•°æ®ã€‚\\n\\n## å®æ—¶ç•Œé¢ (Live Interfaces)\\n\\næ‚¨å¯ä»¥é€šè¿‡åœ¨ç•Œé¢ä¸­...\"],[\"Gradio Demo: gpt2_xl\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ntitle = \\\"gpt2-xl\\\"\\n...\"],[\"Gradio Demo: uploadbutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef...\"],[\"Vision Transformers å›¾åƒåˆ†ç±»\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fvision-transformer\\næ ‡ç­¾ï¼šVISION, ...\"],[\"è®©æˆ‘ä»¬å¼€å§‹å§ï¼\\n\\n### å…ˆå†³æ¡ä»¶\\n\\nç¡®ä¿æ‚¨å·²ç»[å®‰è£…](\\u002fgetting_started)äº† `gradio` Python åŒ…ã€‚\\n\\n## æ­¥éª¤ 1 - é€‰æ‹© Vision å›¾åƒåˆ†ç±»æ¨¡å‹\\n\\né¦–å…ˆï¼Œæˆ‘...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª `examples` å‚æ•°ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¸€äº›é¢„å®šä¹‰çš„ç¤ºä¾‹é¢„å¡«å……æˆ‘ä»¬çš„ç•Œé¢ã€‚\\n\\nè¿™å°†ç”Ÿæˆä»¥ä¸‹æ¥å£ï¼Œæ‚¨å¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ä¸­å°è¯•ã€‚å½“æ‚¨è¾“å…¥å›¾åƒæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œé¢„å¤„ç†å¹¶å‘é€åˆ° ...\"],[\"Gradio Demo: blocks_speech_text_sentiment\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\n...\"],[\"Gradio Demo: spectogram\\n\\n\\n```\\n!pip install -q gradio scipy numpy matplotlib\\n```\\n\\n\\n```\\nimport matplot...\"],[\"Gradio Demo: clustering\\n### This demo built with Blocks generates 9 plots based on the input.\\n      ...\"],[\"```\\nimport gradio as gr\\nimport math\\nfrom functools import partial\\nimport matplotlib.pyplot as plt\\nim...\"],[\"def get_moons(n_clusters):\\n    X, labels = make_moons(n_samples=N_SAMPLES, noise=0.05, random_state=...\"],[\"labels = np.zeros(N_SAMPLES, dtype=int)\\n    return normalize(X), labels\\n\\n\\nDATA_MAPPING = {\\n    'regu...\"],[\"def get_spectral(X, labels, n_clusters, **kwargs):\\n    model = SpectralClustering(\\n        n_cluster...\"],[\"# show outliers (if any)\\n    idx = labels == -1\\n    if sum(idx):\\n        ax.scatter(X[idx, 0], X[idx...\"],[\"with gr.Blocks(title=title) as demo:\\n    gr.HTML(f\\\"\\u003cb\\u003e{title}\\u003c\\u002fb\\u003e\\\")\\n    gr.Markdown(description)\\n\\n  ...\"],[\"@gradio\\u002fhtml\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustracker@0.2.1\\n\\n## 0.0.4\\n\\n### Pa...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"`@gradio\\u002fgallery`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseGallery } from \\\"@gradio\\u002fgallery\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nB...\"],[\"å‘½åå®ä½“è¯†åˆ« ï¼ˆNamed-Entity Recognitionï¼‰\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002frajistics\\u002fbiobert_ner_demoï¼Œhtt...\"],[\"### æ–¹æ³•ä¸€ï¼šå®ä½“å­—å…¸åˆ—è¡¨\\n\\nè®¸å¤šå‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¾“å‡ºçš„æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ã€‚æ¯ä¸ªå­—å…¸åŒ…å«ä¸€ä¸ª*å®ä½“*ï¼Œä¸€ä¸ª \\\" èµ·å§‹ \\\" ç´¢å¼•å’Œä¸€ä¸ª \\\" ç»“æŸ \\\" ç´¢å¼•ã€‚è¿™å°±æ˜¯ `transformers` åº“ä¸­çš„ N...\"],[\"```\\n\\nè¾“å‡ºç»“æœï¼š\\n\\n```bash\\n[{'entity': 'I-LOC',\\n  'score': 0.9988978,\\n  'index': 2,\\n  'word': 'Chicago',\\n  ...\"],[\"Blocks and Event Listeners\\n\\nWe briefly descirbed the Blocks class in the [Quickstart](\\u002fmain\\u002fguides\\u002fq...\"],[\"```\\n\\n_Note_: What happens if a Gradio component is neither an input nor an output? If a component is...\"],[\"Both `add()` and `sub()` take `a` and `b` as inputs. However, the syntax is different between these ...\"],[\"```\\n\\nAbove, each return statement returns two values corresponding to `food_box` and `status_box`, r...\"],[\"```\\n\\nNotice how when there is no food, we only update the `status_box` element. We skipped updating ...\"],[\"## Running Events Consecutively\\n\\nYou can also run events consecutively by using the `then` method of...\"],[\"$code_tictactoe\\n$demo_tictactoe\\n\\n## Binding Multiple Triggers to a Function\\n\\nOften times, you may wa...\"],[\"Gradio Demo: no_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport random\\n\\nsen...\"],[\"Gradio Demo: hello_world_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Demo: blocks_plug\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef change_ta...\"],[\"PyTorch å›¾åƒåˆ†ç±»\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fpytorch-image-classifier, https...\"],[\"è®©æˆ‘ä»¬å¼€å§‹å§ï¼\\n\\n### å…ˆå†³æ¡ä»¶\\n\\nç¡®ä¿æ‚¨å·²ç»[å®‰è£…](\\u002fgetting_started)äº† `gradio` Python åŒ…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œæ‰€ä»¥æ‚¨è¿˜åº”è¯¥å®‰è£…äº† `torch...\"],[\"```\\n\\nç”±äºæˆ‘ä»¬å°†ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œæ‰€ä»¥æˆ‘ä»¬è°ƒç”¨äº† `.eval()` æ–¹æ³•ã€‚\\n\\n## ç¬¬äºŒæ­¥ - å®šä¹‰ `predict` å‡½æ•°\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—*ç”¨æˆ·è¾“å…¥*ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­...\"],[\"```\\n\\nè®©æˆ‘ä»¬é€æ­¥æ¥çœ‹ä¸€ä¸‹è¿™æ®µä»£ç ã€‚è¯¥å‡½æ•°æ¥å—ä¸€ä¸ªå‚æ•°ï¼š\\n\\n- `inp`ï¼šè¾“å…¥å›¾ç‰‡ï¼Œç±»å‹ä¸º `PIL` å›¾åƒ\\n\\nç„¶åï¼Œè¯¥å‡½æ•°å°†å›¾åƒè½¬æ¢ä¸º PIL å›¾åƒï¼Œæœ€ç»ˆè½¬æ¢ä¸º PyTorch çš„ `tenso...\"],[\"```\\n\\nè¿™å°†äº§ç”Ÿä»¥ä¸‹ç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç›´æ¥å°è¯•ï¼ˆè¯•è¯•ä¸Šä¼ è‡ªå·±çš„ç¤ºä¾‹å›¾ç‰‡ï¼ï¼‰ï¼š\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-pytorch-image-classifier.hf...\"],[\"Gradio Demo: video_subtitle\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport os\\n\\na = os.path.join(os.path.abspath(''), \\\"files\\u002fa.mp4\\\")  # Vid...\"],[\"@gradio\\u002fstate\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"## 0.1.0-beta.1\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"åˆ†äº«æ‚¨çš„åº”ç”¨\\n\\nå¦‚ä½•åˆ†äº«æ‚¨çš„ Gradio åº”ç”¨ï¼š\\n\\n1. [ä½¿ç”¨ share å‚æ•°åˆ†äº«æ¼”ç¤º](#sharing-demos)\\n2. [åœ¨ HF Spaces ä¸Šæ‰˜ç®¡](#hosting-on-hf-...\"],[\"```\\n\\nè¿™å°†ç”Ÿæˆä¸€ä¸ªå…¬å¼€çš„å¯åˆ†äº«é“¾æ¥ï¼Œæ‚¨å¯ä»¥å°†å…¶å‘é€ç»™ä»»ä½•äººï¼å½“æ‚¨å‘é€æ­¤é“¾æ¥æ—¶ï¼Œå¯¹æ–¹ç”¨æˆ·å¯ä»¥åœ¨å…¶æµè§ˆå™¨ä¸­å°è¯•æ¨¡å‹ã€‚å› ä¸ºå¤„ç†è¿‡ç¨‹å‘ç”Ÿåœ¨æ‚¨çš„è®¾å¤‡ä¸Šï¼ˆåªè¦æ‚¨çš„è®¾å¤‡ä¿æŒå¼€å¯ï¼ï¼‰ï¼Œæ‚¨ä¸å¿…æ‹…å¿ƒä»»ä½•æ‰“åŒ…ä¾èµ–é¡¹çš„é—®...\"],[\"åœ¨æ‚¨åˆ›å»ºäº†ä¸€ä¸ªå…è´¹çš„ Hugging Face è´¦æˆ·åï¼Œæœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å°†æ‚¨çš„ Gradio åº”ç”¨éƒ¨ç½²åˆ° Hugging Face Spacesï¼š\\n\\n1. ä»ç»ˆç«¯ï¼šåœ¨åº”ç”¨ç›®å½•ä¸­è¿è¡Œ `gradio de...\"],[\"![åµŒå…¥æ­¤ç©ºé—´ä¸‹æ‹‰é€‰é¡¹](\\u002fassets\\u002fguides\\u002fembed_this_space.png)\\n\\n### ä½¿ç”¨ Web ç»„ä»¶åµŒå…¥\\n\\nä¸ IFrames ç›¸æ¯”ï¼ŒWeb ç»„ä»¶é€šå¸¸ä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„ä½“éªŒã€‚...\"],[\"```\\n\\n2.  åœ¨æ‚¨æƒ³æ”¾ç½®åº”ç”¨çš„ä½ç½®æ·»åŠ \\n    `html\\n&lt;gradio-app src=\\\"https:\\u002f\\u002f$your_space_host.hf.space\\\"\\u003e&lt;\\u002fgradio-a...\"],[\"```\\n\\n\\u003cscript\\u003e\\nfetch(\\\"https:\\u002f\\u002fpypi.org\\u002fpypi\\u002fgradio\\u002fjson\\\"\\n).then(r =\\u003e r.json()\\n).then(obj =\\u003e {\\n    let...\"],[\"æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¼ é€’ç»™ `\\u003cgradio-app\\u003e` æ ‡ç­¾çš„å±æ€§æ¥è‡ªå®šä¹‰ Web ç»„ä»¶çš„å¤–è§‚å’Œè¡Œä¸ºï¼š\\n\\n- `src`ï¼šå¦‚å‰æ‰€è¿°ï¼Œ`src` å±æ€§é“¾æ¥åˆ°æ‚¨æƒ³è¦åµŒå…¥çš„æ‰˜ç®¡ Gradio æ¼”ç¤ºçš„ URL\\n- ...\"],[\"ä»¥ä¸‹æ˜¯ä½¿ç”¨è¿™äº›å±æ€§åˆ›å»ºä¸€ä¸ªæ‡’åŠ è½½ä¸”åˆå§‹é«˜åº¦ä¸º 0px çš„ Gradio åº”ç”¨çš„ç¤ºä¾‹ã€‚\\n\\n```html\\n&lt;gradio-app space=\\\"gradio\\u002fEchocardiogram-Segm...\"],[\"```\\n\\n_ æ³¨æ„ï¼šGradio çš„ CSS æ°¸è¿œä¸ä¼šå½±å“åµŒå…¥é¡µé¢ï¼Œä½†åµŒå…¥é¡µé¢å¯ä»¥å½±å“åµŒå…¥çš„ Gradio åº”ç”¨çš„æ ·å¼ã€‚è¯·ç¡®ä¿çˆ¶é¡µé¢ä¸­çš„ä»»ä½• CSS ä¸æ˜¯å¦‚æ­¤é€šç”¨ï¼Œä»¥è‡³äºå®ƒä¹Ÿå¯èƒ½é€‚ç”¨äºåµŒå…¥çš„ Grad...\"],[\"```\\n\\nåŒæ ·ï¼Œæ‚¨å¯ä»¥åœ¨â€œåµŒå…¥æ­¤ç©ºé—´â€æŒ‰é’®ä¸­æ‰¾åˆ°æ‚¨çš„ Space çš„åµŒå…¥ URL çš„ `src=` å±æ€§ã€‚\\n\\næ³¨æ„ï¼šå¦‚æœæ‚¨ä½¿ç”¨ IFramesï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ·»åŠ ä¸€ä¸ªå›ºå®šçš„ `height` å±æ€§ï¼Œå¹¶è®¾ç½®...\"],[\"```\\n\\nè¿™å°†è®°å½•è‡ªåŠ¨ç”Ÿæˆçš„ API é¡µé¢çš„ç«¯ç‚¹ `\\u002fapi\\u002faddition\\u002f`ã€‚\\n\\n_æ³¨æ„_ï¼šå¯¹äºå¯ç”¨äº†[é˜Ÿåˆ—åŠŸèƒ½](https:\\u002f\\u002fgradio.app\\u002fkey-features#queuing...\"],[\"```\\n\\nä¸ºäº†ä½¿èº«ä»½éªŒè¯æ­£å¸¸å·¥ä½œï¼Œå¿…é¡»åœ¨æµè§ˆå™¨ä¸­å¯ç”¨ç¬¬ä¸‰æ–¹ Cookieã€‚\\né»˜è®¤æƒ…å†µä¸‹ï¼ŒSafariã€Chrome éšç§æ¨¡å¼ä¸ä¼šå¯ç”¨æ­¤åŠŸèƒ½ã€‚\\n\\n## ç›´æ¥è®¿é—®ç½‘ç»œè¯·æ±‚\\n\\nå½“ç”¨æˆ·å‘æ‚¨çš„åº”ç”¨ç¨‹åºè¿›è¡Œé¢„æµ‹æ—¶...\"],[\"```\\n\\næ³¨æ„ï¼šå¦‚æœç›´æ¥è°ƒç”¨å‡½æ•°è€Œä¸æ˜¯é€šè¿‡ UIï¼ˆä¾‹å¦‚åœ¨ç¼“å­˜ç¤ºä¾‹æ—¶ï¼‰ï¼Œåˆ™ `request` å°†ä¸º `None`ã€‚æ‚¨åº”è¯¥æ˜ç¡®å¤„ç†æ­¤æƒ…å†µï¼Œä»¥ç¡®ä¿æ‚¨çš„åº”ç”¨ç¨‹åºä¸ä¼šæŠ›å‡ºä»»ä½•é”™è¯¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ‰æ˜¾å¼æ£€æŸ¥ ...\"],[\"- **Gradio åˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶ã€‚** è¿™äº›æ˜¯ç”± Gradio ä½œä¸ºè¿è¡Œæ‚¨çš„é¢„æµ‹å‡½æ•°çš„ä¸€éƒ¨åˆ†åˆ›å»ºçš„æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„é¢„æµ‹å‡½æ•°è¿”å›ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œåˆ™ Gradio å°†è¯¥è§†é¢‘ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ä¸­ï¼Œç„¶åå°†ä¸´...\"],[\"Gradio Demo: english_translator\\n\\n\\n```\\n!pip install -q gradio transformers torch\\n```\\n\\n\\n```\\nimport gra...\"],[\"@gradio\\u002frow\\n\\n## 0.1.1\\n\\n### Features\\n\\n- [#6399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6399) [`053...\"],[\"## 0.1.0-beta.1\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"Gradio Demo: image_editor\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"åˆ†å—çŠ¶æ€ (State in Blocks)\\n\\næˆ‘ä»¬å·²ç»ä»‹ç»äº†[æ¥å£çŠ¶æ€](https:\\u002f\\u002fgradio.app\\u002finterface-state)ï¼Œè¿™ç¯‡æŒ‡å—å°†ä»‹ç»åˆ†å—çŠ¶æ€ï¼Œå®ƒçš„å·¥ä½œåŸç†å¤§è‡´ç›¸åŒã€‚\\n\\n#...\"],[\"å¯¹äºæ›´å¤æ‚çš„åº”ç”¨ç¨‹åºï¼Œæ‚¨å¯èƒ½ä¼šåœ¨ä¸€ä¸ªå•ç‹¬çš„åˆ†å—åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨è®¸å¤šå­˜å‚¨ä¼šè¯çŠ¶æ€çš„ `State` å˜é‡ã€‚\\n\\nåœ¨[æ–‡æ¡£](https:\\u002f\\u002fgradio.app\\u002fdocs#state)ä¸­äº†è§£æ›´å¤šå…³äº `St...\"],[\"ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full c...\"],[\"Gradio Demo: matrix_transpose\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gra...\"],[\"Gradio Demo: blocks_textbox_max_lines\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\n...\"],[\"ecreate the viral AnimeGAN image transformation demo....\"],[\"Gradio Demo: calculator_list_and_dict\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nw...\"],[\"åŒºå—å’Œäº‹ä»¶ç›‘å¬å™¨ (Blocks and Event Listeners)\\n\\næˆ‘ä»¬åœ¨[å¿«é€Ÿå…¥é—¨](https:\\u002f\\u002fgradio.app\\u002fquickstart\\u002f#blocks-more-flexibil...\"],[\"```python\\noutput = gr.Textbox(label=\\\"è¾“å‡º\\\", interactive=True)...\"],[\"```\\n\\n## äº‹ä»¶ç›‘å¬å™¨çš„ç±»å‹ (Types of Event Listeners)\\n\\nè¯·æŸ¥çœ‹ä¸‹é¢çš„æ¼”ç¤ºï¼š\\n\\n$code_blocks_hello\\n$demo_blocks_hello\\n\\n`welc...\"],[\"1. ä½œä¸ºå‚æ•°åˆ—è¡¨ï¼Œæˆ–\\n2. ä½œä¸ºä»¥ç»„ä»¶ä¸ºé”®çš„å•ä¸ªå€¼å­—å…¸\\n\\nè®©æˆ‘ä»¬åˆ†åˆ«çœ‹ä¸€ä¸ªä¾‹å­ï¼š\\n$code_calculator_list_and_dict\\n\\n`add()` å’Œ `sub()` éƒ½å°† `a` å’Œ...\"],[\"```\\n\\nä¸Šé¢çš„æ¯ä¸ªè¿”å›è¯­å¥åˆ†åˆ«è¿”å›ä¸ `food_box` å’Œ `status_box` ç›¸å¯¹åº”çš„ä¸¤ä¸ªå€¼ã€‚\\n\\né™¤äº†è¿”å›ä¸æ¯ä¸ªè¾“å‡ºç»„ä»¶é¡ºåºç›¸å¯¹åº”çš„å€¼åˆ—è¡¨å¤–ï¼Œæ‚¨è¿˜å¯ä»¥è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®å¯¹åº”äºè¾“å‡ºç»„ä»¶ï¼Œ...\"],[\"```\\n\\næ³¨æ„ï¼Œåœ¨æ²¡æœ‰é£Ÿç‰©çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæ›´æ–° `status_box` å…ƒç´ ã€‚æˆ‘ä»¬è·³è¿‡æ›´æ–° `food_box` ç»„ä»¶ã€‚\\n\\nå­—å…¸è¿”å›åœ¨äº‹ä»¶ç›‘å¬å™¨å½±å“å¤šä¸ªç»„ä»¶çš„è¿”å›å€¼æˆ–æœ‰æ¡ä»¶åœ°å½±å“è¾“å‡ºæ—¶éå¸¸æœ‰ç”¨ã€‚\\n\\n...\"],[\"$code_chatbot_simple\\n$demo_chatbot_simple\\n\\näº‹ä»¶ç›‘å¬å™¨çš„ `.then()` æ–¹æ³•ä¼šæ‰§è¡Œåç»­äº‹ä»¶ï¼Œæ— è®ºå‰ä¸€ä¸ªäº‹ä»¶æ˜¯å¦å¼•å‘ä»»ä½•é”™è¯¯ã€‚å¦‚æœåªæƒ³åœ¨å‰ä¸€ä¸ªäº‹ä»¶æˆåŠŸæ‰§è¡Œ...\"],[\"åœ¨ä¸‹é¢çš„åŒäººäº•å­—æ¸¸æˆæ¼”ç¤ºä¸­ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹© `DataFrame` ä¸­çš„ä¸€ä¸ªå•å…ƒæ ¼è¿›è¡Œç§»åŠ¨ã€‚äº‹ä»¶æ•°æ®å‚æ•°åŒ…å«æœ‰å…³æ‰€é€‰å•å…ƒæ ¼çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥é¦–å…ˆæ£€æŸ¥å•å…ƒæ ¼æ˜¯å¦ä¸ºç©ºï¼Œç„¶åç”¨ç”¨æˆ·çš„ç§»åŠ¨æ›´æ–°å•å…ƒæ ¼ã€‚\\n\\n$cod...\"],[\"Gradio Demo: checkbox_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith g...\"],[\"@gradio\\u002ficons\\n\\n## 0.3.2\\n\\n### Features\\n\\n- [#6399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6399) [`0...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.1\\n\\n### Fixes\\n\\n- [#6254](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.2.0-beta.3\\n\\n### Features\\n\\n- [#6094](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6094) [`c476bd5a5...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5699](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5699) [`8f0fed857`](http...\"],[\"Gradio Demo: audio_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        with gr.Column():\\n    ...\"],[\"output_video.play(lambda n: n + 1, output_num_play, output_num_play)\\n            output_video.pause(...\"],[\"Gradio Demo: chatinterface_random_response\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport random\\nimp...\"],[\"Gradio Demo: colorpicker_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwit...\"],[\"Gradio Demo: blocks_update\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Bloc...\"],[\"Getting Started with the Gradio JavaScript client\\n\\nTags: CLIENT, API, SPACES\\n\\nThe Gradio JavaScript ...\"],[\"```\\n\\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text su...\"],[\"```\\n\\n## Duplicating a Space for private use\\n\\nWhile you can use any public Space as an API, you may g...\"],[\"```\\n\\n## Connecting a general Gradio app\\n\\nIf your app is running somewhere else, just provide the ful...\"],[\"```\\n\\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoi...\"],[\"```\\n\\nFor certain inputs, such as images, you should pass in a `Buffer`, `Blob` or `File` depending o...\"],[\"```\\n\\n## Status\\n\\nThe event interface also allows you to get the status of the running job by listenin...\"],[\"```\\n\\nIf the first job has started processing, then it will not be canceled but the client will no lo...\"],[\"æ›´å¤šç¤ºä¾‹ (More on Examples)\\n\\næœ¬æŒ‡å—ä»‹ç»äº†æœ‰å…³ç¤ºä¾‹çš„æ›´å¤šå†…å®¹ï¼šä»ç›®å½•ä¸­åŠ è½½ç¤ºä¾‹ï¼Œæä¾›éƒ¨åˆ†ç¤ºä¾‹å’Œç¼“å­˜ã€‚å¦‚æœä½ å¯¹ç¤ºä¾‹è¿˜ä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹ [å…³é”®ç‰¹æ€§](..\\u002fkey-features\\u002f#e...\"],[\"```csv\\nnum,operation,num2\\n5,\\\"add\\\",3\\n4,\\\"divide\\\",2\\n5,\\\"multiply\\\",3...\"],[\"```\\n\\nå½“æµè§ˆæ ‡è®°æ•°æ®æ—¶ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚åªéœ€æŒ‡å‘æ ‡è®°ç›®å½•ï¼Œ`Interface` å°†ä»æ ‡è®°æ•°æ®åŠ è½½ç¤ºä¾‹ã€‚\\n\\n### æä¾›éƒ¨åˆ†ç¤ºä¾‹\\n\\næœ‰æ—¶ä½ çš„åº”ç”¨ç¨‹åºæœ‰è®¸å¤šè¾“å…¥ç»„ä»¶ï¼Œä½†ä½ åªæƒ³ä¸ºå…¶ä¸­çš„ä¸€éƒ¨åˆ†æä¾›ç¤ºä¾‹ã€‚ä¸º...\"],[\"ä» Supabase æ•°æ®åˆ›å»ºä»ªè¡¨ç›˜\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Supabase](https:\\u002f\\u002fsupabase.com\\u002f) æ˜¯ä¸€ä¸ªåŸºäºäº‘çš„å¼€æºåç«¯ï¼Œæ...\"],[\"2\\\\. ç»™æ‚¨çš„é¡¹ç›®å‘½åå¹¶è®¾ç½®æ•°æ®åº“å¯†ç ã€‚æ‚¨è¿˜å¯ä»¥é€‰æ‹©å®šä»·è®¡åˆ’ï¼ˆå¯¹äºæˆ‘ä»¬æ¥è¯´ï¼Œå…è´¹è®¡åˆ’å·²è¶³å¤Ÿï¼ï¼‰\\n\\n3\\\\. åœ¨æ•°æ®åº“å¯åŠ¨æ—¶ï¼ˆå¯èƒ½éœ€è¦å¤šè¾¾ 2 åˆ†é’Ÿï¼‰ï¼Œæ‚¨å°†çœ‹åˆ°æ‚¨çš„ API å¯†é’¥ã€‚\\n\\n4\\\\. åœ¨å·¦ä¾§çª—æ ¼ä¸­...\"],[\"```\\n\\n7\\\\. è·å–é¡¹ç›® URL å’Œ API å¯†é’¥ã€‚ç‚¹å‡»å·¦ä¾§çª—æ ¼ä¸Šçš„è®¾ç½®ï¼ˆé½¿è½®å›¾æ ‡ï¼‰ï¼Œç„¶åç‚¹å‡» 'API'ã€‚URL åˆ—åœ¨é¡¹ç›® URL æ¡†ä¸­ï¼ŒAPI å¯†é’¥åˆ—åœ¨é¡¹ç›® API å¯†é’¥ï¼ˆå¸¦æœ‰ `service...\"],[\"```\\n\\nè¿”å› Supabase ä»ªè¡¨æ¿å¹¶åˆ·æ–°é¡µé¢ï¼Œæ‚¨å°†çœ‹åˆ° 10 è¡Œæ•°æ®å¡«å……åˆ° `Product` è¡¨ä¸­ï¼\\n\\n## åœ¨å®æ—¶ Gradio ä»ªè¡¨ç›˜ä¸­å¯è§†åŒ–æ•°æ®\\n\\næœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ `supaba...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼Œé€šè¿‡å°†å‡½æ•°ä¼ é€’ç»™ `gr.BarPlot()`ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç½‘ç»œåº”ç”¨åŠ è½½æ—¶æŸ¥è¯¢æ•°æ®åº“ï¼ˆç„¶åæ¯ 60 ç§’æŸ¥è¯¢ä¸€æ¬¡ï¼Œå› ä¸ºæœ‰ `every` å‚æ•°ï¼‰ã€‚æ‚¨çš„æœ€ç»ˆä»ªè¡¨ç›˜åº”å¦‚ä¸‹æ‰€ç¤ºï¼š\\n\\n\\u003cgrad...\"],[\"Gradio Demo: model3d_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"@gradio\\u002fmarkdown\\n\\n## 0.6.0\\n\\n### Features\\n\\n- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) ...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.3.1\\n  - @gradio\\u002fstatustracker@0.4....\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6071](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6071) [`f08da1a6f...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.3.2\\n\\n### Fixes\\n\\n- [#5897](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5897) [`0592c301d`](https:\\u002f...\"],[\"## 0.2.2\\n\\n### Fixes\\n\\n- [#5701](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5701) [`ee8eec1e5`](https:\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5342](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5342) [`afac0006`](https...\"],[\"- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5368](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5368) [`b27f7583`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 0.1.1\\n\\n### Fixes\\n\\n- [#5324](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5324) [`31996c99`](https:\\u002f\\u002f...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: video_identity_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef video...\"],[\"`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\\n\\nThis directory contains the sou...\"],[\"```\\n\\nYou can also connect to private Spaces by passing in your HF token with the `hf_token` paramete...\"],[\"```\\n\\n### Inspecting the API endpoints\\n\\nOnce you have connected to a Gradio app, you can view the API...\"],[\"```\\n\\nFor certain inputs, such as images, you should pass in the filepath or URL to the file. Likewis...\"],[\"Gradio Demo: image_classification\\n### Simple image classification in Pytorch with Gradio's Image inp...\"],[\"Gradio Demo: label_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"è‡ªå®šä¹‰çš„ JS å’Œ CSS\\n\\næœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•æ›´çµæ´»åœ°ä¸º Blocks æ·»åŠ æ ·å¼ï¼Œå¹¶æ·»åŠ  JavaScript ä»£ç åˆ°äº‹ä»¶ç›‘å¬å™¨ä¸­ã€‚\\n\\n**è­¦å‘Š**ï¼šåœ¨è‡ªå®šä¹‰çš„ JS å’Œ CSS ä¸­ä½¿ç”¨æŸ¥è¯¢é€‰æ‹©å™¨ä¸èƒ½...\"],[\"```\\n\\nå¦‚æœæ‚¨æƒ³åœ¨æ‚¨çš„ CSS ä¸­å¼•ç”¨å¤–éƒ¨æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ `\\\"file=\\\"` ä½œä¸ºæ–‡ä»¶è·¯å¾„çš„å‰ç¼€ï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰ï¼Œä¾‹å¦‚ï¼š\\n\\n```python\\nwith gr.Blocks(css=\\\".g...\"],[\"```\\n\\nCSS `#warning` è§„åˆ™é›†ä»…é’ˆå¯¹ç¬¬äºŒä¸ªæ–‡æœ¬æ¡†ï¼Œè€Œ `.feedback` è§„åˆ™é›†å°†åŒæ—¶ä½œç”¨äºä¸¤ä¸ªæ–‡æœ¬æ¡†ã€‚è¯·æ³¨æ„ï¼Œåœ¨é’ˆå¯¹ç±»æ—¶ï¼Œæ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨ `!important` é€‰æ‹©å™¨æ¥è¦†ç›–é»˜...\"],[\"Gradio Demo: bokeh_plot\\n\\n\\n```\\n!pip install -q gradio bokeh\\u003e=3.0 xyzservices\\n```...\"],[\"```\\nimport gradio as gr\\nimport xyzservices.providers as xyz\\nfrom bokeh.models import ColumnDataSourc...\"],[\"SPECIES = sorted(data.species.unique())\\n        MARKERS = [\\\"hex\\\", \\\"circle_x\\\", \\\"triangle\\\"]\\n\\n        p...\"],[\"# JavaScript Client Library\\n\\nA javascript (and typescript) client to call Gradio APIs.\\n\\n## Installat...\"],[\"```\\n\\n##### `status_callback`\\n\\nThis should be a function which will notify your of the status of a sp...\"],[\"```\\n\\nThe gradio client returns an object with a number of methods and properties:\\n\\n#### `predict`\\n\\nT...\"],[\"```\\n\\nThe `submit` method accepts the same [`endpoint`](#endpoint) and [`payload`](#payload) argument...\"],[\"```\\n\\n##### `off`\\n\\nThe `off` method unsubscribes from a specific event of the submitted job and works...\"],[\"```\\n\\n#### `view_api`\\n\\nThe `view_api` method provides details about the API you are connected to. It ...\"],[\"```\\n\\nThis function accepts two arguments: `source` and `options`:\\n\\n#### `source`\\n\\nThe space to dupli...\"],[\"```\\n\\n##### `hardware`\\n\\nThis is an optional property specific to `duplicate`'s options object and wil...\"],[\"iles in this directory are used in:\\n\\n- tests for the gradio library\\n- example inputs in the view API...\"],[\"How to Use the 3D Model Component\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fModel3D, htt...\"],[\"```python\\nimport gradio as gr\\nimport os\\n\\n\\ndef load_mesh(mesh_file_name):\\n    return mesh_file_name\\n\\n...\"],[\"```\\n\\nLet's break down the code above:\\n\\n`load_mesh`: This is our 'prediction' function and for simpli...\"],[\"@gradio\\u002fdataset\\n\\n## 0.1.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.1.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.2.0-beta.3\\n\\n## 0.0.5-beta.2\\n\\n### P...\"],[\"Gradio Demo: blocks_kinematics\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport pandas as pd\\nimport nu...\"],[\"Gradio Demo: blocks_neural_instrument_coding\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading f...\"],[\"```\\n# A Blocks implementation of https:\\u002f\\u002ferlj.notion.site\\u002fNeural-Instrument-Cloning-from-very-few-sa...\"],[\"io4 = gr.Interface(\\n    lambda x, y, z: os.path.join(os.path.abspath(''),\\\"sax2.wav\\\"),\\n    [\\n        ...\"],[\"m(\\n        \\\"\\\"\\\"\\\\n\\n        Here is a **generated** saxophone recordings:\\\"\\\"\\\"\\n    )\\n    a = gr.Audio(os....\"],[\"Gradio Demo: image_classifier\\n\\n\\n```\\n!pip install -q gradio numpy tensorflow\\n```\\n\\n\\n```\\n# Downloading ...\"],[\"```\\n\\n\\n```\\nimport os\\nimport requests\\nimport tensorflow as tf\\n\\nimport gradio as gr\\n\\ninception_net = tf...\"],[\"Using Gradio Blocks Like Functions\\n\\nTags: TRANSLATION, HUB, SPACES\\n\\n**Prerequisite**: This Guide bui...\"],[\"Note that the variable `english_translator` is my english to german app, but its used in `generate_t...\"],[\"```\\n\\nThe `api_name` gives this function a unique name in our app. You can use this name to tell grad...\"],[\"Gradio Demo: animeganv2\\n### Recreate the viral AnimeGAN image transformation demo.\\n        \\n\\n\\n```\\n!p...\"],[\"```\\nimport gradio as gr\\nimport torch\\n\\nmodel2 = torch.hub.load(\\n    \\\"AK391\\u002fanimegan2-pytorch:main\\\",\\n ...\"],[\"demo = gr.Interface(\\n    fn=inference, \\n    inputs=[gr.Image(type=\\\"pil\\\"),gr.Radio(['version 1 (ğŸ”º sty...\"],[\"Gradio Demo: image_segmentation\\n### Simple image segmentation using gradio's AnnotatedImage componen...\"],[\"```\\nimport gradio as gr\\nimport numpy as np\\nimport random\\n\\nwith gr.Blocks() as demo:\\n    section_labe...\"],[\"section_btn = gr.Button(\\\"Identify Sections\\\")\\n    selected_section = gr.Textbox(label=\\\"Selected Secti...\"],[\"Gradio Demo: stable-diffusion\\n### Note: This is a simplified version of the code needed to create th...\"],[\"```\\nimport gradio as gr\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\nfrom PIL import I...\"],[\"advanced_button = gr.Button(\\\"Advanced options\\\", elem_id=\\\"advanced-btn\\\")\\n\\n        with gr.Row(elem_id...\"],[\"`@gradio\\u002fstatustracker`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {StatusTracker, Toast, Loader} from `@gradio\\u002fst...\"],[\"@gradio\\u002fsimpletextbox\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.1\\n\\n### Features\\n\\n- [#5990](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5990) [`85056de5c...\"],[\"Gradio Demo: chatbot_consecutive\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"Gradio Demo: file_explorer\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\nfrom pathlib import Path\\n\\ncurrent_file_path = Path(__file__).resolve()\\nrelat...\"],[\"code = gr.Code(lines=30, scale=2, language=\\\"python\\\")\\n\\n    file_3.change(get_file_content, file_3, co...\"],[\"è¿è¡Œåå°ä»»åŠ¡\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002ffreddyaboulton\\u002fgradio-google-forms\\nTags: TASKS...\"],[\"æˆ‘ä»¬å°†ä½¿ç”¨ `sqlite3` åº“æ¥è¿æ¥æˆ‘ä»¬çš„ sqlite æ•°æ®åº“ï¼Œä½† gradio å¯ä»¥ä¸ä»»ä½•åº“ä¸€èµ·ä½¿ç”¨ã€‚\\n\\nä»£ç å¦‚ä¸‹ :\\n\\n```python\\nDB_FILE = \\\".\\u002freviews.db\\\"\\n...\"],[\"```\\n\\nè®©æˆ‘ä»¬è¿˜å†™ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨ gradio åº”ç”¨ç¨‹åºåŠ è½½æ—¶åŠ è½½æœ€æ–°çš„è¯„è®º :\\n\\n```python\\ndef load_data():\\n    db = sqlite3.connect(DB_FIL...\"],[\"```\\n\\n## ç¬¬ä¸‰æ­¥ - ä¸ HuggingFace æ•°æ®é›†åŒæ­¥ ğŸ¤—\\n\\nåœ¨ç¬¬ 2 æ­¥åæˆ‘ä»¬å¯ä»¥è°ƒç”¨ `demo.launch()` æ¥è¿è¡Œä¸€ä¸ªå®Œæ•´åŠŸèƒ½çš„åº”ç”¨ç¨‹åºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ•°æ®å°†å­˜å‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šã€‚...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦ä» HuggingFace çš„â€œè®¾ç½®â€é€‰é¡¹å¡ä¸­è·å–è®¿é—®ä»¤ç‰Œï¼Œä»¥ä¸Šä»£ç æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚åœ¨è„šæœ¬ä¸­ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡å®‰å…¨è®¿é—®ä»¤ç‰Œã€‚\\n\\n![access_token](\\u002fassets\\u002fgui...\"],[\"```\\n\\n## ç¬¬å››æ­¥ï¼ˆé™„åŠ ï¼‰- éƒ¨ç½²åˆ° HuggingFace Spaces\\n\\næ‚¨å¯ä»¥ä½¿ç”¨ HuggingFace [Spaces](https:\\u002f\\u002fhuggingface.co\\u002fspaces) å¹³...\"],[\"Gradio Demo: file_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"],[\"Gradio Demo: blocks_group\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\ndef greet(name):\\n    return \\\"Hello \\\" + name + \\\"!\\\"\\n\\nwith gr.Blocks() as demo...\"],[\"gr.Markdown(\\\"### container=False removes label, padding, and block border, placing elements 'directl...\"],[\"é€šè¿‡è‡ªåŠ¨é‡è½½å®ç°æ›´å¿«çš„å¼€å‘\\n\\n**å…ˆå†³æ¡ä»¶**ï¼šæœ¬æŒ‡å—è¦æ±‚æ‚¨äº†è§£å—çš„çŸ¥è¯†ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»å—æŒ‡å—](https:\\u002f\\u002fgradio.app\\u002fquickstart\\u002f#blocks-more-flexibil...\"],[\"inp.change(fn=lambda x: f\\\"æ¬¢è¿ï¼Œ{x}ï¼\\\",\\n               inputs=inp,\\n               outputs=out)\\n\\nif __nam...\"],[\"```\\n\\né—®é¢˜åœ¨äºï¼Œæ¯å½“æ‚¨æƒ³è¦æ›´æ”¹å¸ƒå±€ã€äº‹ä»¶æˆ–ç»„ä»¶æ—¶ï¼Œéƒ½å¿…é¡»é€šè¿‡ç¼–å†™ `python run.py` æ¥å…³é—­å’Œé‡æ–°è¿è¡Œåº”ç”¨ç¨‹åºã€‚\\n\\nè€Œä¸æ˜¯è¿™æ ·åšï¼Œæ‚¨å¯ä»¥é€šè¿‡æ›´æ”¹ 1 ä¸ªå•è¯æ¥ä»¥**é‡æ–°åŠ è½½æ¨¡å¼**è¿è¡Œ...\"],[\"```\\n\\nè¿™é‡Œæœ€é‡è¦çš„ä¸€è¡Œæ˜¯ `æ­£åœ¨è§‚å¯Ÿ ...`ã€‚è¿™é‡Œå‘ç”Ÿçš„æƒ…å†µæ˜¯ Gradio å°†è§‚å¯Ÿ `run.py` æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œå¦‚æœæ–‡ä»¶å‘ç”Ÿæ›´æ”¹ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨é‡æ–°è¿è¡Œæ–‡ä»¶ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€ä¸“æ³¨äºç¼–å†™ä»£ç ï¼ŒG...\"],[\"```\\n\\né‚£ä¹ˆæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨å®ƒï¼š`gradio run.py my_demo.app`ã€‚\\n\\nGradioé»˜è®¤ä½¿ç”¨UTF-8ç¼–ç æ ¼å¼ã€‚å¯¹äº**é‡æ–°åŠ è½½æ¨¡å¼**ï¼Œå¦‚æœä½ çš„è„šæœ¬ä½¿ç”¨çš„æ˜¯é™¤UTF-8ä»¥å¤–çš„ç¼–ç ...\"],[\"```\\n\\næ‚¨å¯ä»¥åƒè¿™æ ·è¿è¡Œå®ƒï¼š`gradio run.py --name Gretel`\\n\\nä½œä¸ºä¸€ä¸ªå°æç¤ºï¼Œåªè¦æ›´æ”¹äº† `run.py` æºä»£ç æˆ– Gradio æºä»£ç ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½å°±ä¼šå‘ç”Ÿã€‚è¿™æ„å‘³ç€...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼š\\n\\n- æ‚¨ä¸éœ€è¦æ”¾ç½®æ ·æ¿ä»£ç  `with gr.Blocks() as demo:` å’Œ `demo.launch()` â€” Gradio ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®Œæˆï¼\\n\\n- æ¯æ¬¡é‡æ–°è¿è¡Œå•å…ƒæ ¼...\"],[\"Gradio Demo: sentiment_analysis\\n### This sentiment analaysis demo takes in input text and returns it...\"],[\"Gradio Demo: image_mod\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo rep...\"],[\"Gradio Demo: hello_blocks\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(nam...\"],[\"Installing Gradio in a Virtual Environment\\n\\nTags: INSTALLATION\\n\\nIn this guide, we will describe step...\"],[\"```\\n\\n5. **Verification**:\\n   To verify the installation, run `python` and then type:\\n\\n   ```python\\n ...\"],[\"Gradio Demo: score_tracker\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nscores = []\\n...\"],[\"`@gradio\\u002ffile`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseFile, BaseFileUpload, FilePreview, BaseExample } from...\"],[\"Gradio Demo: scatterplot_component\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\nimport gradi...\"],[\"Gradio Demo: xgboost-income-prediction-with-explainability\\n### This demo takes in 12 inputs from the...\"],[\"```\\nimport gradio as gr\\nimport random\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport sha...\"],[\"def interpret(*args):\\n    df = pd.DataFrame([args], columns=X_train.columns)\\n    df = df.astype({col...\"],[\"with gr.Blocks() as demo:\\n    gr.Markdown(\\\"\\\"\\\"\\n    **Income Classification with XGBoost ğŸ’°**:  This de...\"],[\")\\n            hours_per_week = gr.Slider(\\n                label=\\\"Hours Per Week Worked\\\", minimum=1, ...\"],[\"demo.launch()...\"],[\"his demo built with Blocks generates 9 plots based on the input....\"],[\"Gradio & LLM Agents ğŸ¤\\n\\néå¸¸å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚æœæˆ‘ä»¬èƒ½èµ‹äºˆå®ƒä»¬å®Œæˆä¸“é—¨ä»»åŠ¡çš„æŠ€èƒ½ï¼Œå®ƒä»¬å°†å˜å¾—æ›´åŠ å¼ºå¤§ã€‚\\n\\n[gradio_tools](https:\\u002f\\u002fgithub...\"],[\"### Gradioæ˜¯ä»€ä¹ˆï¼Ÿ\\n\\n[Gradio](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio)æ˜¯ç”¨äºæ„å»ºæœºå™¨å­¦ä¹ Webåº”ç”¨ç¨‹åºå¹¶ä¸å…¨çƒå…±äº«çš„äº‹å®ä¸Šçš„æ ‡å‡†æ¡†æ¶-å®Œå…¨ç”±Pyt...\"],[\"from langchain.memory import ConversationBufferMemory\\n\\nllm = OpenAI(temperature=0)\\nmemory = Conversa...\"],[\"```\\n\\næ‚¨ä¼šæ³¨æ„åˆ°æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ä¸€äº›ä¸`gradio_tools`ä¸€èµ·æä¾›çš„é¢„æ„å»ºå·¥å…·ã€‚è¯·å‚é˜…æ­¤[æ–‡æ¡£](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton\\u002fgradio-tools#...\"],[\"```\\n\\néœ€è¦æ»¡è¶³çš„è¦æ±‚æ˜¯ï¼š...\"],[\"1. å·¥å…·çš„åç§°\\n2. å·¥å…·çš„æè¿°ã€‚è¿™éå¸¸å…³é”®ï¼ä»£ç†æ ¹æ®å…¶æè¿°å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚è¯·ç¡®åˆ‡æè¿°è¾“å…¥å’Œè¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæœ€å¥½åŒ…æ‹¬ç¤ºä¾‹ã€‚\\n3. Gradioåº”ç”¨ç¨‹åºçš„urlæˆ–space idï¼Œä¾‹å¦‚`fred...\"],[\"5. postprocess - ç»™å®šä½œä¸šçš„ç»“æœï¼Œå°†å…¶è½¬æ¢ä¸ºLLMå¯ä»¥å‘ç”¨æˆ·æ˜¾ç¤ºçš„å­—ç¬¦ä¸²ã€‚\\n6. _Optionalå¯é€‰_ - æŸäº›åº“ï¼Œä¾‹å¦‚[MiniChain](https:\\u002f\\u002fgithub.com...\"],[\"å°±æ˜¯è¿™æ ·ï¼\\n\\nä¸€æ—¦æ‚¨åˆ›å»ºäº†è‡ªå·±çš„å·¥å…·ï¼Œè¯·åœ¨`gradio_tools`å­˜å‚¨åº“ä¸Šå‘èµ·æ‹‰å–è¯·æ±‚ï¼æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰è´¡çŒ®ã€‚\\n\\n## ç¤ºä¾‹å·¥å…· - ç¨³å®šæ‰©æ•£\\n\\nä»¥ä¸‹æ˜¯ä½œä¸ºç¤ºä¾‹çš„ç¨³å®šæ‰©æ•£å·¥å…·ä»£ç ï¼š\\n\\nfrom gra...\"],[\"```\\nå…³äºæ­¤å®ç°çš„ä¸€äº›æ³¨æ„äº‹é¡¹ï¼š\\n1. æ‰€æœ‰çš„ `GradioTool` å®ä¾‹éƒ½æœ‰ä¸€ä¸ªåä¸º `client` çš„å±æ€§ï¼Œå®ƒæŒ‡å‘åº•å±‚çš„ [gradio å®¢æˆ·ç«¯](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"Gradio Demo: barplot_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport pa...\"],[\"Gradio Demo: blocks_hello\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef welcome(n...\"],[\"his is a fake GAN that shows how to create a text-to-image interface for image generation. Check out...\"],[\"Gradio Demo: loginbutton_component\\n\\n\\n```\\n!pip install -q gradio gradio[oauth]\\n```\\n\\n\\n```\\nimport gradi...\"],[\"@gradio\\u002fgallery\\n\\n## 0.4.14\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.12\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.4.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.4.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6204ccac5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.2\\n\\n### Fixes\\n\\n- [#6277](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6277) [`5fe091367`](https:\\u002f...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.9\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fimage@0.3.0-beta.7\\n\\n## 0.4.0-beta.6\\n\\n### F...\"],[\"## 0.5.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.5.0\\n\\n### Features\\n\\n- [#5780](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5780) [`ed0f9a21b`](http...\"],[\"## 0.4.1\\n\\n### Fixes\\n\\n- [#5735](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5735) [`abb5e9df4`](https:\\u002f...\"],[\"## 0.3.3\\n\\n### Fixes\\n\\n- [#5528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5528) [`dc86e4a7`](https:\\u002f\\u002f...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5025](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5025) [`6693660a`](https...\"],[\"åœ¨ Web æœåŠ¡å™¨ä¸Šä½¿ç”¨ Nginx è¿è¡Œ Gradio åº”ç”¨\\n\\næ ‡ç­¾ï¼šéƒ¨ç½²ï¼ŒWeb æœåŠ¡å™¨ï¼ŒNginx\\n\\n## ä»‹ç»\\n\\nGradio æ˜¯ä¸€ä¸ª Python åº“ï¼Œå…è®¸æ‚¨å¿«é€Ÿåˆ›å»ºå¯å®šåˆ¶çš„ Web åº”ç”¨ç¨‹...\"],[\"```\\n\\n2. åœ¨ `\\u002fetc\\u002fnginx\\u002fsites-available` ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰ï¼Œæ–‡ä»¶åè¡¨ç¤ºæ‚¨çš„åº”ç”¨ï¼Œä¾‹å¦‚ï¼š`sudo nano \\u002fetc\\u002fnginx\\u002fsit...\"],[\"```\\n\\n2. é€šè¿‡é”®å…¥ `tmux` å¹¶æŒ‰å›è½¦é”®ï¼ˆå¯é€‰ï¼‰å¯åŠ¨ `tmux` ä¼šè¯\\n\\næ¨èåœ¨ `tmux` ä¼šè¯ä¸­è¿è¡Œ Gradio åº”ç”¨ï¼Œä»¥ä¾¿å¯ä»¥è½»æ¾åœ°åœ¨åå°è¿è¡Œå®ƒ\\n\\n3. ç„¶åï¼Œå¯åŠ¨æ‚¨çš„ Grad...\"],[\"@gradio\\u002fhighlightedtext\\n\\n## 0.4.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgi...\"],[\"## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.4.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Fixes\\n\\n- [#5602](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5602) [`54d21d3f1`](https:\\u002f...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5046) [`5244c587`](https...\"],[\"Gradio Demo: stream_frames\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport numpy ...\"],[\"@gradio\\u002futils\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"## 0.2.0-beta.5\\n\\n### Features\\n\\n- [#5966](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5966) [`9cad2127b...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"ä» Google Sheets åˆ›å»ºå®æ—¶ä»ªè¡¨ç›˜\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n[Google Sheets](https:\\u002f\\u002fwww.google.com\\u002fshee...\"],[\"```\\n\\n2. ç°åœ¨ï¼Œä¿®æ”¹æ­¤ç½‘å€å¹¶ä½¿ç”¨å®ƒä» Google Sheets è¯»å–æ•°æ®åˆ° Pandas DataFrame ä¸­ã€‚ (åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œç”¨æ‚¨çš„å…¬å¼€ Google Sheet çš„ç½‘å€æ›¿æ¢ `URL...\"],[\"```\\n\\nåˆ°æ­¤ä¸ºæ­¢ï¼æ‚¨ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªä»ªè¡¨ç›˜ï¼Œæ¯ 5 ç§’åˆ·æ–°ä¸€æ¬¡ï¼Œä» Google Sheets ä¸­è·å–æ•°æ®ã€‚\\n\\n## ç§æœ‰ Google Sheets\\n\\nå¯¹äºç§æœ‰ Google Sheetsï¼Œæµç¨‹éœ€è¦æ›´...\"],[\"```json\\n{\\n\\t\\\"type\\\": \\\"service_account\\\",\\n\\t\\\"project_id\\\": \\\"your project\\\",\\n\\t\\\"private_key_id\\\": \\\"your privat...\"],[\"```\\n\\n### æŸ¥è¯¢\\n\\nåœ¨è·å¾—å‡­æ®çš„ `.json` æ–‡ä»¶åï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æŸ¥è¯¢æ‚¨çš„ Google Sheetï¼š\\n\\n1. å•å‡» Google Sheet å³ä¸Šè§’çš„â€œå…±äº«â€æŒ‰é’®ã€‚ä½¿ç”¨èº«ä»½éªŒè¯å­éƒ¨åˆ†ç¬¬...\"],[\"```\\n\\n4\\\\. æ•°æ®æŸ¥è¯¢æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¿™æ„å‘³ç€å¯ä»¥ä½¿ç”¨ `gr.DataFrame` ç»„ä»¶å®æ—¶æ˜¾ç¤ºæ•°æ®ï¼Œæˆ–ä½¿ç”¨ `gr.LinePlot` ç»„ä»¶å®æ—¶ç»˜åˆ¶æ•°æ®ï¼ˆå½“ç„¶ï¼Œæ ¹æ®æ•°æ®çš„ä¸åŒï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒçš„å›¾...\"],[\"@gradio\\u002fradio\\n\\n## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Fixes\\n\\n- [#6262](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6262) [`afb72bd19`](https:\\u002f...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustracker@0.2....\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: markdown_example\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\ncss = (\\n    \\\"footer {display: none !important;} .gradio-container {min-heig...\"],[\"## Tech\\n\\nDillinger uses a number of open source projects to work properly:\\n\\n- [AngularJS] - HTML enh...\"],[\"```\\n\\nFor production environments...\\n\\n```bash\\nnpm install --production\\nNODE_ENV=production node app\\n`...\"],[\"```\\n\\nThis will create the dillinger image and pull in the necessary dependencies.\\nBe sure to swap ou...\"],[\"```\\n\\n## License\\n\\nMIT\\n\\n**Free Software, Hell Yeah!**\\n\\n[\\u002f\\u002f]: # (These are reference links used in the ...\"],[\"[PlDb]: \\u003chttps:\\u002f\\u002fgithub.com\\u002fjoemccann\\u002fdillinger\\u002ftree\\u002fmaster\\u002fplugins\\u002fdropbox\\u002fREADME.md\\u003e\\n   [PlGh]: \\u003ch...\"],[\"gradio_client\\n\\n## 0.7.3\\n\\n### Fixes\\n\\n- [#6693](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6693) [`34f9...\"],[\"## 0.7.2\\n\\n### Features\\n\\n- [#6598](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6598) [`7cbf96e`](https:...\"],[\"### Fixes\\n\\n- [#6556](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6556) [`d76bcaa`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.7.0-beta.2\\n\\n### Features\\n\\n- [#6094](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6094) [`c476bd5a5...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33...\"],[\"## 0.7.0-beta.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13...\"],[\"## 0.6.0\\n\\n### Highlights\\n\\n#### new `FileExplorer` component ([#5672](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"## 0.5.2\\n\\n### Features\\n\\n- [#5653](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5653) [`ea0e00b20`](http...\"],[\"```python\\nimport gradio_client as grc\\nclient = grc.Client(\\\"gradio\\u002fstream_audio_out\\\")\\n\\n# Get the enti...\"],[\"```\\n\\n Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Fixes\\n\\n- [#5295](https:\\u002f\\u002fgit...\"],[\"Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Features\\n\\n- [#5076](https:\\u002f\\u002fgithub...\"],[\"It's as easy as importing `gradio_client`, connecting to the app, and calling `deploy_discord`!\\n\\n_ğŸ¦™ ...\"],[\"Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### New Features:\\n\\n- Endpoints that re...\"],[\"### Bug Fixes:\\n\\n- Fix bug where space duplication would error if the demo has cpu-basic hardware by ...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.\\n\\n# 0....\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nSepar...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"```\\n\\nRead more about how to use the `gradio_client` library here: https:\\u002f\\u002fgradio.app\\u002fgetting-started...\"],[\"Gradio Demo: blocks_form\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks...\"],[\"Gradio Demo: tictactoe\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks()...\"],[\"Gradio Demo: fake_gan_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo re...\"],[\"simple dashboard ranking spaces by number of likes....\"],[\"@gradio\\u002fjson\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Fixes\\n\\n- [#5944](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5944) [`465f58957`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5554](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5554) [`75ddeb390`](http...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"`gradio\\u002fmodel3d`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {BaseModel3D, BaseModel3DUpload, BaseExample } from `@...\"],[\"@gradio\\u002ftheme\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"This component allows you to populate the explorer by passing a glob, but only provides the selected...\"],[\"We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli...\"],[\"Gradio Demo: automatic-speech-recognition\\n### Automatic speech recognition English. Record from your...\"],[\"Gradio Demo: event_trigger\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"```\\n# %%\\nimport gradio as gr\\n\\n\\nTEST_VIDEO_A = \\\"mp4\\u002fa.mp4\\\"\\nTEST_VIDEO_B = \\\"mp4\\u002fb.mp4\\\"\\n\\nTEST_IMAGE_A =...\"],[\"def video_pause():\\n            print(\\\"video_pause\\\")\\n\\n        def video_stop():\\n            print(\\\"vi...\"],[\"def change_file(index):\\n            if index == 0:\\n                return [TEST_IMAGE_A]\\n           ...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseChatBot } from \\\"@gradio\\u002fchatbot\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\n\\nB...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n[\\u003cimg src=\\\"readme_files\\u002fgradio.svg\\\" alt=\\\"gradio\\\" width=400\\u003e](https:\\u002f\\u002fgradio.app...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n# Gradio: Build Machine Learning Web Apps â€” in Python\\n\\n$getting_started\\n\\n## Questions?\\n\\nIf y...\"],[\"If you like Gradio, please leave us a â­ on GitHub!\\n\\n## Open Source Stack\\n\\nGradio is built on top of ...\"],[\"## License\\n\\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in ...\"],[\"```\\n@article{abid2019gradio,\\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the ...\"],[\"Gradio Demo: theme_new_step_3\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nfrom __future__ import annotations\\nfrom typing import Iterable\\nimport gradio as gr\\nfrom gradio.t...\"],[\"class Seafoam(Base):\\n    def __init__(\\n        self,\\n        *,\\n        primary_hue: colors.Color | ...\"],[\"button_primary_background_fill_hover=\\\"linear-gradient(90deg, *primary_200, *secondary_300)\\\",\\n       ...\"],[\"seafoam = Seafoam()\\n\\nwith gr.Blocks(theme=seafoam) as demo:\\n    textbox = gr.Textbox(label=\\\"Name\\\")\\n ...\"],[\"Gradio Demo: dataframe_datatype\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport p...\"],[\"Gradio Demo: Echocardiogram-Segmentation\\n\\n\\n```\\n!pip install -q gradio -f https:\\u002f\\u002fdownload.pytorch.or...\"],[\"```\\nimport os\\nimport numpy as np\\nimport torch\\nimport torchvision\\nimport wget \\n\\n\\ndestination_folder =...\"],[\"print(\\\"loading weights from \\\", os.path.join(destination_for_weights, \\\"deeplabv3_resnet50_random\\\"))\\n\\n...\"],[\"import gradio as gr\\n\\ni = gr.Image(label=\\\"Echocardiogram\\\")\\no = gr.Image(label=\\\"Segmentation Mask\\\")\\n\\ne...\"],[\"`@gradio\\u002fbutton`\\n\\n```javascript\\n\\u003cscript\\u003e\\n\\timport { BaseButton } from \\\"@gradio\\u002fbutton\\\";\\n\\timport { cre...\"],[\"Gradio Demo: reverse_audio_2\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport gradio as gr\\nimport...\"],[\"Gradio Demo: model3d_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nw...\"],[\"Gradio Demo: sentence_builder\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\n\\ndef sentence_builder(quantity, animal, countries, place, activity_list, mo...\"],[\"@gradio\\u002faudio\\n\\n## 0.6.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.6.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.5.5\\n\\n### Fixes\\n\\n- [#6551](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6551) [`8fc562a`](https:\\u002f\\u002fg...\"],[\"## 0.5.2\\n\\n### Features\\n\\n- [#6419](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6419) [`1959471a8`](http...\"],[\"## 0.5.1\\n\\n### Fixes\\n\\n- [#6382](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6382) [`2090aad73`](https:\\u002f...\"],[\"## 0.4.3\\n\\n### Fixes\\n\\n- [#6317](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6317) [`19af2806a`](https:\\u002f...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.9\\n\\n### Features...\"],[\"- [#6153](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6153) [`1162ed621`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.4.0-beta.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`174b73619`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.4.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.3.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.3\\n\\n### Fixes\\n\\n- [#5459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5459) [`bd2fda77`](https:\\u002f\\u002f...\"],[\"## 0.3.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5149) [`144df459`](https...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#4993](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4993) [`dc07a9f9`](https...\"],[\"Gradio Demo: waveform\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport random\\n\\n\\nCO...\"],[\"Gradio Demo: hello_login\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport argparse...\"],[\"@gradio\\u002ffileexplorer\\n\\n## 0.3.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.3.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.3.9\\n\\n### Fixes\\n\\n- [#6550](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6550) [`3156598`](https:\\u002f\\u002fg...\"],[\"## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`324867f63`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0-beta.2\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.0\\n\\n### Highlights\\n\\n#### new `FileExplorer` component ([#5672](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"Using Gradio and Comet\\n\\nTags: COMET, SPACES\\nContributed by the Comet team\\n\\n## Introduction\\n\\nIn this ...\"],[\"```\\n\\nNext, you will need to [sign up for a Comet Account](https:\\u002f\\u002fwww.comet.com\\u002fsignup?utm_source=gr...\"],[\"```\\n\\n## 1. Logging Gradio UI's to your Comet Experiments\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.research.g...\"],[\"inputs = gr.Image()\\noutputs = gr.Label(num_top_classes=3)\\n\\nio = gr.Interface(\\n    fn=predict, inputs...\"],[\"```\\n\\nThe last line in this snippet will log the URL of the Gradio Application to your Comet Experime...\"],[\"Next, search for Gradio Panel Extended in the Public Panels section and click `Add`.\\n\\n\\u003cimg width=\\\"56...\"],[\"Go to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up...\"],[\"In the previous examples, we demonstrated the various ways in which you can interact with a Gradio a...\"],[\"experiment = comet_ml.APIExperiment(\\n            workspace=workspace, project_name=project_name\\n    ...\"],[\"```\\n\\nInferences from this snippet will be saved in the HTML tab of your experiment.\\n\\n\\u003cvideo width=\\\"5...\"],[\"`@gradio\\u002fmarkdown`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseMarkdown, MarkdownCode, BaseExample } from `@g...\"],[\"Gradio Demo: calculator_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef calcul...\"],[\"Named-Entity Recognition\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002frajistics\\u002fbiobert_ner_demo, ...\"],[\"```\\n\\nOutput:\\n\\n```bash\\n[{'entity': 'I-LOC',\\n  'score': 0.9988978,\\n  'index': 2,\\n  'word': 'Chicago',\\n...\"],[\"Gradio Demo: fraud_detector\\n\\n\\n```\\n!pip install -q gradio pandas\\n```\\n\\n\\n```\\n# Downloading files from t...\"],[\"!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides\\u002f1)getting_start...\"],[\"[Website](https:\\u002f\\u002fgradio.app)\\n| [Documentation](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f)\\n| [Guides](https:\\u002f\\u002fgradio....\"],[\"```\\npip install gradio\\n```\\n\\n\\n\\u003e [!TIP]\\n \\u003e it is best to install Gradio in a virtual environment. Deta...\"],[\"```\\n\\n\\n\\n\\u003e [!TIP]\\n \\u003e We shorten the imported name from \\u003ccode\\u003egradio\\u003c\\u002fcode\\u003e to \\u003ccode\\u003egr\\u003c\\u002fcode\\u003e for bett...\"],[\"The `Interface` class has three core arguments:\\n\\n- `fn`: the function to wrap a user interface (UI) ...\"],[\"We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https:\\u002f\\u002fwww.gradio....\"],[\"```\\n\\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, so...\"],[\"#### The Gradio Python & JavaScript Ecosystem\\n\\nThat's the gist of the core `gradio` Python library, ...\"],[\"Or, if you already know the basics and are looking for something specific, you can search the more [...\"],[\"Building a FastAPI App with the Gradio Python Client\\n\\nTags: CLIENT, API, WEB APP\\n\\nIn this blog post,...\"],[\"```\\n\\nYou will also need to have ffmpeg installed. You can check to see if you already have ffmpeg by...\"],[\"```\\n\\nEverything else remains the same!\\n\\n---\\n\\nNow, of course, we are working with video files, so we ...\"],[\"```\\n\\nYou can read up on [ffmpeg documentation](https:\\u002f\\u002fffmpeg.org\\u002fffmpeg.html) if you'd like to unde...\"],[\"```\\n\\nIn this example, the FastAPI app has two routes: `\\u002f` and `\\u002fuploadvideo\\u002f`.\\n\\nThe `\\u002f` route return...\"],[\"```\\n\\nWrite the following as the contents of `home.html`:...\"],[\"```html\\n&lt;!DOCTYPE html\\u003e &lt;html\\u003e &lt;head\\u003e &lt;title\\u003eVideo Gallery&lt;\\u002ftitle\\u003e\\n&lt;style\\u003e body { ...\"],[\"type=\\\"video\\u002fmp4\\\"\\u003e Your browser does not support the video tag. &lt;\\u002fvideo\\u003e\\n&lt;p\\u003e{{ video }}&lt;\\u002fp\\u003e ...\"],[\"```\\n\\n## Step 4: Run your FastAPI app\\n\\nFinally, we are ready to run our FastAPI app, powered by the G...\"],[\"Developing Faster with Auto-Reloading\\n\\n**Prerequisite**: This Guide requires you to know about Block...\"],[\"```\\n\\nThe problem is that anytime that you want to make a change to your layout, events, or component...\"],[\"```\\n\\nThe important part here is the line that says `Watching...` What's happening here is that Gradi...\"],[\"```\\n\\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\\n\\nBy default, the Gr...\"],[\"```\\n\\nWhich you could run like this: `gradio run.py --name Gretel`\\n\\nAs a small aside, this auto-reloa...\"],[\"```\\n\\nNotice that:\\n\\n- You do not need to launch your demo â€” Gradio does that for you automatically!\\n\\n...\"],[\"Running Background Tasks\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002ffreddyaboulton\\u002fgradio-google...\"],[\"The code will look like this:\\n\\n```python\\nDB_FILE = \\\".\\u002freviews.db\\\"\\ndb = sqlite3.connect(DB_FILE)\\n\\n# C...\"],[\"```\\n\\nLet's also write a function to load the latest reviews when the gradio application loads:\\n\\n```p...\"],[\"```\\n\\n## Step 3 - Synchronize with HuggingFace Datasets ğŸ¤—\\n\\nWe could call `demo.launch()` after step 2...\"],[\"```\\n\\nNote that you'll have to get an access token from the \\\"Settings\\\" tab of your HuggingFace for th...\"],[\"```\\n\\n## Step 4 (Bonus) - Deployment to HuggingFace Spaces\\n\\nYou can use the HuggingFace [Spaces](http...\"],[\"How to Use the Plot Component for Maps\\n\\nTags: PLOTS, MAPS\\n\\n## Introduction\\n\\nThis guide explains how ...\"],[\"```\\n\\nIn the code above, we first load the csv data into a pandas dataframe. Let's begin by defining ...\"],[\"```\\n\\nAbove, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to...\"],[\"```\\n\\nWe layout these components using the `gr.Column` and `gr.Row` and we'll also add event triggers...\"],[\"Gradio Demo: blocks_style\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks(title=\\\"Styling Examples\\\") as demo:\\n    with gr.Column(...\"],[\"Using Flagging\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fcalculator-flagging-crowdsource...\"],[\"There are [four parameters](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f#interface-header) in `gradio.Interface` that co...\"],[\"Here's an example: The code below creates the calculator interface embedded below it:\\n\\n```python\\nimp...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fcalculator-flag-basic\\u002f\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nWhen you click the flag button...\"],[\"```\\n\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to th...\"],[\"```\\n\\n## The HuggingFaceDatasetSaver Callback\\n\\nSometimes, saving the data to a local CSV file doesn't...\"],[\"```\\n\\nNotice that we define our own\\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Fa...\"],[\"Here is an example with an image sepia filter Blocks demo that lets you flag\\ndata using the default ...\"],[\"his demo shows how you can build a live interactive dashboard with gradio.\\nThe current time is refre...\"],[\"Gradio Demo: plot_component\\n\\n\\n```\\n!pip install -q gradio matplotlib numpy\\n```\\n\\n\\n```\\nimport gradio as...\"],[\"Gradio Demo: request_ip_headers\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef pr...\"],[\"è¿æ¥åˆ°æ•°æ®åº“\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchicago-bike-share-dashboard\\næ ‡ç­¾ï¼šTABULAR, PLOTS\\n\\n##...\"],[\"**é‡è¦æç¤º**ï¼šå¦‚æœæ‚¨è®¡åˆ’åœ¨ HuggingFace Spaces ä¸Šæ‰˜ç®¡æ­¤æ¼”ç¤ºï¼Œè¯·ç¡®ä¿æ•°æ®åº“åœ¨ **8080** ç«¯å£ä¸Šã€‚Spaces\\nå°†é˜»æ­¢é™¤ç«¯å£ 80ã€443 æˆ– 8080 ä¹‹å¤–çš„æ‰€æœ‰å¤–éƒ¨è¿æ¥...\"],[\"connection_string = f\\\"postgresql:\\u002f\\u002f{DB_USER}:{DB_PASSWORD}@{DB_HOST}?port={PORT}&dbname={DB_NAME}\\\"\\n\\n...\"],[\"```\\n\\nå¦‚æœæ‚¨åœ¨æœ¬åœ°è¿è¡Œæˆ‘ä»¬çš„è„šæœ¬ï¼Œå¯ä»¥åƒä¸‹é¢è¿™æ ·å°†å‡­æ®ä½œä¸ºç¯å¢ƒå˜é‡ä¼ é€’ï¼š\\n\\n```bash\\nDB_USER='username' DB_PASSWORD='password' DB_HOST='h...\"],[\"```\\n\\n## æ­¥éª¤ 3 - éƒ¨ç½²\\n\\nå¦‚æœæ‚¨è¿è¡Œä¸Šè¿°ä»£ç ï¼Œæ‚¨çš„åº”ç”¨ç¨‹åºå°†åœ¨æœ¬åœ°è¿è¡Œã€‚\\næ‚¨ç”šè‡³å¯ä»¥é€šè¿‡å°† `share=True` å‚æ•°ä¼ é€’ç»™ `launch` æ¥è·å¾—ä¸€ä¸ªä¸´æ—¶å…±äº«é“¾æ¥ã€‚\\n\\nä½†æ˜¯å¦‚æœæ‚¨æƒ³...\"],[\"Gradio Demo: chatinterface_streaming_echo\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport time\\nimport...\"],[\"Gradio Demo: hangman\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nsecret_word = \\\"gra...\"],[\"The 4 Kinds of Gradio Interfaces\\n\\nSo far, we've always assumed that in order to build an Gradio demo...\"],[\"$code_fake_gan_no_input\\n$demo_fake_gan_no_input\\n\\n## Input-only demos\\n\\nSimilarly, to create a demo th...\"],[\"`@gradio\\u002fdataframe`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseDataFrame, BaseExample } from \\\"@gradio\\u002fdatafr...\"],[\"Gradio Demo: line_plot\\n\\n\\n```\\n!pip install -q gradio vega_datasets pandas\\n```...\"],[\"```\\nimport gradio as gr\\nfrom vega_datasets import data\\n\\nstocks = data.stocks()\\ngapminder = data.gapm...\"],[\"def line_plot_fn(dataset):\\n    if dataset == \\\"stocks\\\":\\n        return gr.LinePlot(\\n            stock...\"],[\"Contributing to Gradio\\n\\n![GitHub issues by-label](https:\\u002f\\u002fimg.shields.io\\u002fgithub\\u002fissues\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\n### ğŸ“¦ Using dev containers\\n\\nYou can alternatively use dev containers. This is supported on all ...\"],[\"## ğŸ§± Structure of the Repository\\n\\nIf you're a newcomer to Gradio, we recommend getting familiar with...\"],[\"```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n   gr.Button()\\n    \\nif __name__ == \\\"__main__\\\":\\n  ...\"],[\"```\\npnpm test:browser:full\\n```\\n\\nYou can also run browser tests in the UI mode by adding the `--ui` f...\"],[\"```\\npnpm storybook\\n```\\n\\n\\n## ğŸ“® Submitting PRs\\n\\nAll PRs should be against `main`, and ideally should a...\"],[\"```\\nbash scripts\\u002fformat_backend.sh\\n```\\n\\n```\\nbash scripts\\u002fformat_frontend.sh\\n```\\n\\nThank you for takin...\"],[\"Gradio Demo: leaderboard\\n### A simple dashboard ranking spaces by number of likes.\\n        \\n\\n\\n```\\n!p...\"],[\"```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport requests\\nimport pandas as pd\\nfrom h...\"],[\"Gradio ç•Œé¢çš„ 4 ç§ç±»å‹\\n\\nåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´å‡è®¾æ„å»º Gradio æ¼”ç¤ºéœ€è¦åŒæ—¶å…·å¤‡è¾“å…¥å’Œè¾“å‡ºã€‚ä½†å¯¹äºæœºå™¨å­¦ä¹ æ¼”ç¤ºæ¥è¯´ï¼Œå¹¶ä¸æ€»æ˜¯å¦‚æ­¤ï¼šä¾‹å¦‚ï¼Œ*æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹*ä¸éœ€è¦ä»»ä½•è¾“å…¥ï¼Œä½†ä¼šç”Ÿæˆä¸€...\"],[\"$code_sepia_filter\\n$demo_sepia_filter\\n\\n## ä»…è¾“å‡ºæ¼”ç¤º (Output-only demos)\\n\\né‚£ä¹ˆä»…åŒ…å«è¾“å‡ºçš„æ¼”ç¤ºå‘¢ï¼Ÿä¸ºäº†æ„å»ºè¿™æ ·çš„æ¼”ç¤ºï¼Œåªéœ€å°† `Inte...\"],[\"Gradio Demo: calculator_blocks\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef cal...\"],[\"@gradio\\u002fdataframe\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`846d52d`](https:\\u002f\\u002fgithub.c...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6603](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6603) [`6b1401c`](https:...\"],[\"## 0.3.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fbutton@0.2.5\\n  - @gradio\\u002fupload@0.4.1\\n\\n## ...\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`aaa55ce85`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5877](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5877) [`a55b80942`](http...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5569](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5569) [`2a5b9e03b`](http...\"],[\"## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`ee8eec1e5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.1\\n\\n### Fixes\\n\\n- [#5445](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5445) [`67bb7bcb`](https:\\u002f\\u002f...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`31996c99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5268) [`f49028cf`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5283](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5283) [`a7460557`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com...\"],[\"Gradio Demo: rows_and_columns\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"Gradio Demo: lineplot_component\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\nimport gradio a...\"],[\"@gradio\\u002fbox\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`73268ee`](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.0-beta.5\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.6\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.4\\n\\n## 0.0.5\\n\\n### Patch...\"],[\"Gradio Demo: webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef snap(image, v...\"],[\"Gradio Demo: theme_new_step_2\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nfrom __future__ import annotations\\nfrom typing import Iterable\\nimport gradio as gr\\nfrom gradio.t...\"],[\"def repeat(name, count):\\n        time.sleep(3)\\n        return name * count\\n\\n    button.click(repeat,...\"],[\"Gradio å’Œ ONNX åœ¨ Hugging Face ä¸Š\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fonnx\\u002fEfficientNet-Lite...\"],[\"## ONNX æ¨¡å‹ä»“åº“æ˜¯ä»€ä¹ˆï¼Ÿ\\n\\nOpen Neural Network Exchangeï¼ˆ[ONNX](https:\\u002f\\u002fonnx.ai\\u002f)ï¼‰æ˜¯ä¸€ç§è¡¨ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„å¼€æ”¾æ ‡å‡†æ ¼å¼ã€‚ONNX ç”±ä¸€ä¸ªå®...\"],[\"åœ¨æ­¤å¤„å¼€å§‹[https:\\u002f\\u002fgradio.app\\u002fgetting_started](https:\\u002f\\u002fgradio.app\\u002fgetting_started)\\n\\n### Hugging Face Spac...\"],[\"## ONNX Runtime çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\\n\\nONNX Runtime æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æ¨ç†å’Œè®­ç»ƒæœºå™¨å­¦ä¹ åŠ é€Ÿå™¨ã€‚å®ƒä½¿å¾—åœ¨ Hugging Face ä¸Šä½¿ç”¨ ONNX æ¨¡å‹ä»“åº“ä¸­çš„æ¨¡å‹è¿›è¡Œå®æ—¶ Gr...\"],[\"åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Gradio ä¸º EfficientNet-Lite4 è®¾ç½®ç¤ºä¾‹æ¼”ç¤º\\n\\né¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„ä¾èµ–é¡¹å¹¶ä¸‹è½½å’Œè½½å…¥æ¥è‡ª ONNX æ¨¡å‹ä»“åº“çš„ efficientnet-lite...\"],[\"# ä½¿ç”¨ç­‰æ¯”ä¾‹ç¼©æ”¾è°ƒæ•´å›¾åƒå°ºå¯¸\\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.IN...\"],[\"title = \\\"EfficientNet-Lite4\\\"\\ndescription = \\\"EfficientNet-Lite 4æ˜¯æœ€å¤§çš„å˜ä½“ï¼Œä¹Ÿæ˜¯EfficientNet-Liteæ¨¡å‹é›†åˆä¸­æœ€å‡†ç¡®çš„ã€‚å®ƒ...\"],[\"```\\n\\n## å¦‚ä½•ä½¿ç”¨ ONNX æ¨¡å‹åœ¨ HF Spaces ä¸Šè´¡çŒ® Gradio æ¼”ç¤º\\n\\n- å°†æ¨¡å‹æ·»åŠ åˆ°[onnx model zoo](https:\\u002f\\u002fgithub.com\\u002fonnx\\u002fmode...\"],[\"Gradio Demo: slider_release\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef identi...\"],[\"How to add support for more languages\\n\\nWe would love to support more languages for Gradio ğŸŒ\\n\\nTo add ...\"],[\"Setting Up a Demo for Maximum Performance\\n\\nTags: CONCURRENCY, LATENCY, PERFORMANCE\\n\\nLet's say that y...\"],[\"(2) They allow the server to send multiple updates to the frontend. This means, for example, that th...\"],[\"```\\n\\n**How Requests are Processed from the Queue**\\n\\nWhen a Gradio server is launched, a pool of thre...\"],[\"```\\n\\nInitially, 3 workers will get dispatched to handle requests 1, 2, and 5 (corresponding to funct...\"],[\"### The `concurrency_limit` parameter in events\\n\\nYou can also set the number of requests that can be...\"],[\"### The `max_batch_size` parameter in events\\n\\nAnother way to increase the parallelism of your Gradio...\"],[\"```\\n\\nHere's the same function rewritten to take in a batch of samples:\\n\\n```py\\nimport time\\n\\ndef trim_...\"],[\"Gradio Demo: blocks_outputs\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\n\\ndef make_markdown():\\n    return [\\n        [\\n            \\\"# hello again\\\",\\n ...\"],[\"with gr.Blocks() as demo:\\n    with gr.Column():\\n        txt = gr.Textbox(label=\\\"Small Textbox\\\", line...\"],[\"interactive=True,\\n            headers=[\\\"One\\\", \\\"Two\\\", \\\"Three\\\", \\\"Four\\\"],\\n            col_count=(4, \\\"fi...\"],[\"Gradio Demo: slider_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr....\"],[\"`@gradio\\u002fradio`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseRadio, BaseExample } from \\\"@gradio\\u002fradio\\\"; \\n\\u003c\\u002fscr...\"],[\"Gradio Demo: clearbutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nwit...\"],[\"`@gradio\\u002fhtml`\\n\\n```javascript\\nimport { BaseHTML } from \\\"@gradio\\u002fhtml\\\";\\n```\\n\\nBaseHTML\\n```javascript\\n\\t...\"],[\"Gradio Demo: hello_world\\n### The simplest possible Gradio demo. It wraps a 'Hello {name}!' function ...\"],[\"Gradio Demo: ner_pipeline\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nfrom transformer...\"],[\"æ¥å£çŠ¶æ€ (Interface State)\\n\\næœ¬æŒ‡å—ä»‹ç»äº† Gradio ä¸­å¦‚ä½•å¤„ç†çŠ¶æ€ã€‚äº†è§£å…¨å±€çŠ¶æ€å’Œä¼šè¯çŠ¶æ€çš„åŒºåˆ«ï¼Œä»¥åŠå¦‚ä½•åŒæ—¶ä½¿ç”¨å®ƒä»¬ã€‚\\n\\n## å…¨å±€çŠ¶æ€ (Global State)\\n\\næ‚¨çš„...\"],[\"$code_chatbot_dialogpt\\n$demo_chatbot_dialogpt\\n\\nè¯·æ³¨æ„ï¼Œåœ¨æ¯ä¸ªé¡µé¢ä¸­ï¼ŒçŠ¶æ€åœ¨æäº¤ä¹‹é—´ä¿æŒä¸å˜ï¼Œä½†æ˜¯å¦‚æœåœ¨å¦ä¸€ä¸ªæ ‡ç­¾ä¸­åŠ è½½æ­¤æ¼”ç¤ºï¼ˆæˆ–åˆ·æ–°é¡µé¢ï¼‰ï¼Œæ¼”ç¤ºå°†ä¸å…±äº«...\"],[\"ä½¿ç”¨ Hugging Face é›†æˆ\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fhelsinki_translation_en_es\\næ ‡ç­¾ï¼šHUBï¼ŒSPAC...\"],[\"```python\\nimport gradio as gr\\n\\nfrom transformers import pipeline\\n\\npipe = pipeline(\\\"translation\\\", mod...\"],[\"```\\n\\nä½†æ˜¯ï¼Œ`gradio` å®é™…ä¸Šä½¿å°† `pipeline` è½¬æ¢ä¸ºæ¼”ç¤ºæ›´åŠ å®¹æ˜“ï¼Œåªéœ€ä½¿ç”¨ `gradio.Interface.from_pipeline` æ–¹æ³•ï¼Œæ— éœ€æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºç»„ä»¶ï¼š\\n\\n`...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€æŒ‡å®šæ¨¡å‹åç§°å¹¶è¯´æ˜ `src` åº”ä¸º `models`ï¼ˆHugging Face çš„ Model Hubï¼‰ã€‚ç”±äºæ‚¨ä¸ä¼šåœ¨è®¡ç®—æœºä¸ŠåŠ è½½æ¨¡å‹ï¼Œå› æ­¤æ— éœ€å®‰è£…ä»»ä½•ä¾èµ–é¡¹ï¼ˆé™¤äº† `gr...\"],[\"```python\\nfrom huggingface_hub import (\\n    create_repo,\\n    get_full_repo_name,\\n    upload_file,\\n)\\n...\"],[\"```\\n\\nåœ¨è¿™é‡Œï¼Œ`create_repo` ä½¿ç”¨ç‰¹å®šå¸æˆ·çš„ Write Token åœ¨ç‰¹å®šå¸æˆ·ä¸‹åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç›®æ ‡åç§°çš„ gradio repoã€‚`repo_name` è·å–ç›¸å…³å­˜å‚¨åº“çš„å®Œæ•´å­˜å‚¨åº“åç§°...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `gr.load()`ï¼Œè¿™ä¸ä½¿ç”¨æ¨ç† API åŠ è½½æ¨¡å‹æ‰€ä½¿ç”¨çš„æ–¹æ³•ç›¸åŒã€‚ä½†æ˜¯ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‡å®š `src` ä¸º `spaces`ï¼ˆHugging Face Spacesï¼‰...\"],[\"Gradio Demo: chatbot_dialogpt\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gradi...\"],[\"Gradio Demo: blocks_joined\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: zip_to_json\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom zipfile import ZipFile\\n\\nimport...\"],[\"@gradio\\u002fgroup\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"## 0.0.2-beta.0\\n\\n### Features\\n\\n- [#5648](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5648) [`c573e2339...\"],[\"Gradio Demo: titanic_survival\\n\\n\\n```\\n!pip install -q gradio scikit-learn numpy pandas\\n```\\n\\n\\n```\\n# Dow...\"],[\"```\\nimport os\\n\\nimport pandas as pd\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn....\"],[\"def encode_df(df):\\n    df = encode_age(df)\\n    df = encode_fare(df)\\n    sex_mapping = {\\\"male\\\": 0, \\\"f...\"],[\"clf = RandomForestClassifier()\\nclf.fit(X_train, y_train)\\npredictions = clf.predict(X_test)\\n\\n\\ndef pre...\"],[\"`@gradio\\u002fuploadbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseUploadButton } from \\\"@gradio\\u002fuploadbutton\\\"...\"],[\"Gradio Demo: translation\\n### This translation demo takes in the text, source and target languages, a...\"],[\"```\\n!pip install -q gradio git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers gradio torch\\n```\\n\\n\\n```\\nim...\"],[\"Using Hugging Face Integrations\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fen2es\\nTags: HU...\"],[\"```\\n\\nFor any Hugging Face model supported in the Inference API, Gradio automatically infers the expe...\"],[\"Alternatively, you can create a Space programmatically, making use of the [huggingface_hub client li...\"],[\"```\\n\\nHere, `create_repo` creates a gradio repo with the target name under a specific account using t...\"],[\"```\\n\\nNotice that we use `gr.load()`, the same method we used to load models using the Inference API....\"],[\"```\\n\\nThe previous code produces the following interface, which you can try right here in your browse...\"],[\"ä½¿ç”¨ Gradio å—åƒå‡½æ•°ä¸€æ ·\\n\\nTags: TRANSLATION, HUB, SPACES\\n\\n**å…ˆå†³æ¡ä»¶**: æœ¬æŒ‡å—æ˜¯åœ¨å—ä»‹ç»çš„åŸºç¡€ä¸Šæ„å»ºçš„ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»è¯¥æŒ‡å—](https:\\u002f\\u002fgrad...\"],[\"2. åœ¨ä½ çš„åº”ç”¨ç¨‹åºä¸­åŠ è½½æˆ‘çš„è‹±å¾·ç¿»è¯‘ï¼Œå¹¶å°†å…¶å½“ä½œæ™®é€šçš„ Python å‡½æ•°å¤„ç†ã€‚\\n\\né€‰é¡¹ 1 ä»æŠ€æœ¯ä¸Šè®²æ€»æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒç»å¸¸å¼•å…¥ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚\\n\\né€‰é¡¹ 2 å…è®¸ä½ å€Ÿç”¨æ‰€éœ€çš„åŠŸèƒ½ï¼Œè€Œä¸ä¼šè¿‡äºç´§å¯†åœ°è€¦...\"],[\"english_generator(text, fn_index=1)[0][\\\"generated_text\\\"]\\n\\nGradio ç©ºé—´ä¸­çš„å‡½æ•°æ˜¯ä»é›¶å¼€å§‹ç´¢å¼•çš„ï¼Œæ‰€ä»¥è¥¿ç­ç‰™è¯­ç¿»è¯‘å™¨å°†æ˜¯æˆ‘çš„ç©ºé—´ä¸­çš„ç¬¬äºŒä¸ª...\"],[\"Gradio Demo: unispeech-speaker-verification\\n\\n\\n```\\n!pip install -q gradio git+https:\\u002f\\u002fgithub.com\\u002fhugg...\"],[\"```\\nimport gradio as gr\\nimport torch\\nfrom torchaudio.sox_effects import apply_effects_file\\nfrom tran...\"],[\"STYLE = \\\"\\\"\\\"\\n\\u003clink rel=\\\"stylesheet\\\" href=\\\"https:\\u002f\\u002fcdn.jsdelivr.net\\u002fnpm\\u002fbootstrap@5.1.3\\u002fdist\\u002fcss\\u002fboots...\"],[\"\\u003cdiv class=\\\"row\\\"\\u003e\\u003ch1 style=\\\"text-align: center\\\"\\u003esimilar\\u003c\\u002fh1\\u003e\\u003c\\u002fdiv\\u003e\\n        \\u003cdiv class=\\\"row\\\"\\u003e\\u003ch1 clas...\"],[\"EFFECTS = [\\n    [\\\"remix\\\", \\\"-\\\"],\\n    [\\\"channels\\\", \\\"1\\\"],\\n    [\\\"rate\\\", \\\"16000\\\"],\\n    [\\\"gain\\\", \\\"-1.0\\\"],\\n...\"],[\"if similarity \\u003e= THRESHOLD:\\n        output = OUTPUT_OK.format(similarity * 100)\\n    else:\\n        ou...\"],[\"demo = gr.Interface(\\n    fn=similarity_fn,\\n    inputs=inputs,\\n    outputs=output,\\n    title=\\\"Voice A...\"],[\"Gradio Demo: white_noise_vid_not_playable\\n\\n\\n```\\n!pip install -q gradio opencv-python\\n```\\n\\n\\n```\\nimpor...\"],[\"Gradio Demo: logoutbutton_component\\n\\n\\n```\\n!pip install -q gradio gradio[oauth]\\n```\\n\\n\\n```\\nimport grad...\"],[\"Gradio Demo: chicago-bikeshare-dashboard\\n\\n\\n```\\n!pip install -q gradio psycopg2 matplotlib SQLAlchemy...\"],[\"```\\nimport os\\nimport gradio as gr\\nimport pandas as pd\\n\\nDB_USER = os.getenv(\\\"DB_USER\\\")\\nDB_PASSWORD = ...\"],[\"If data were added to the database, the plots in this demo would update\\n    whenever the webpage is ...\"],[\"Gradio Demo: textbox_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"ä½¿ç”¨æ ‡è®°\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fcalculator-flagging-crowdsourced, https:\\u002f\\u002fhuggingfac...\"],[\"## åœ¨ `gradio.Interface` ä¸­ä½¿ç”¨**æ ‡è®°**æŒ‰é’®\\n\\nä½¿ç”¨ Gradio çš„ `Interface` è¿›è¡Œæ ‡è®°ç‰¹åˆ«ç®€å•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨è¾“å‡ºç»„ä»¶ä¸‹æ–¹æœ‰ä¸€ä¸ªæ ‡è®°ä¸º**æ ‡è®°**çš„æŒ‰é’®ã€‚å½“...\"],[\"- `allow_flagging`ï¼šæ­¤å‚æ•°å¯ä»¥è®¾ç½®ä¸º `\\\"manual\\\"`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œ`\\\"auto\\\"` æˆ– `\\\"never\\\"`ã€‚\\n  - `manual`ï¼šç”¨æˆ·å°†çœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œåªæœ‰åœ¨ç‚¹å‡»æŒ‰é’®æ—¶æ ·...\"],[\"## æ ‡è®°çš„æ•°æ®ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\\n\\nåœ¨ `flagging_dir` å‚æ•°æä¾›çš„ç›®å½•ä¸­ï¼Œå°†è®°å½•æ ‡è®°çš„æ•°æ®çš„ CSV æ–‡ä»¶ã€‚\\n\\nä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼šä¸‹é¢çš„ä»£ç åˆ›å»ºäº†åµŒå…¥å…¶ä¸­çš„è®¡ç®—å™¨ç•Œé¢ï¼š\\n\\n```python\\n...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fcalculator-flag-basic\\u002f\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nå½“æ‚¨ç‚¹å‡»ä¸Šé¢çš„æ ‡è®°æŒ‰é’®æ—¶ï¼Œå¯åŠ¨ç•Œé¢çš„ç›®å½•å°†åŒ…æ‹¬ä¸€ä¸ªæ–°çš„æ ‡è®°å­...\"],[\"```\\n\\nå¦‚æœæ‚¨å¸Œæœ›ç”¨æˆ·ä¸ºæ ‡è®°æä¾›ä¸€ä¸ªåŸå› ï¼Œæ‚¨å¯ä»¥å°†å­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™ Interface çš„ `flagging_options` å‚æ•°ã€‚ç”¨æˆ·åœ¨æ ‡è®°æ—¶å¿…é¡»é€‰æ‹©å…¶ä¸­ä¸€é¡¹ï¼Œé€‰é¡¹å°†ä½œä¸ºé™„åŠ åˆ—ä¿å­˜åœ¨ CSV ...\"],[\"```\\n\\n## HuggingFaceDatasetSaver å›è°ƒ\\n\\næœ‰æ—¶ï¼Œå°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ° CSV æ–‡ä»¶æ˜¯ä¸åˆç†çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ Hugging Face Spaces ä¸Š\\nï¼Œå¼€å‘è€…é€šå¸¸æ— æ³•è®¿é—®æ‰˜ç®¡ ...\"],[\"```\\n\\næ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„ Hugging Face ä»¤ç‰Œå’Œ\\nè¦ä¿å­˜æ ·æœ¬çš„æ•°æ®é›†çš„åç§°ï¼Œå®šä¹‰äº†æˆ‘ä»¬è‡ªå·±çš„\\n`gradio.HuggingFaceDatasetSaver` çš„å®ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°† ...\"],[\"åŒæ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨ç°æœ‰çš„ `FlaggingCallback` æ¥é¿å…ç¼–å†™é¢å¤–çš„ä»£ç ã€‚\\nè¿™éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š\\n\\n1. æ‚¨å¿…é¡»åœ¨ä»£ç ä¸­çš„æŸä¸ªä½ç½®è¿è¡Œæ‚¨çš„å›è°ƒçš„ `.setup()` æ–¹æ³•\\n   åœ¨ç¬¬ä¸€æ¬¡æ ‡...\"],[\"Image Classification in PyTorch\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fpytorch-imag...\"],[\"```\\n\\nBecause we will be using the model for inference, we have called the `.eval()` method.\\n\\n## Step...\"],[\"```\\n\\nLet's break this down. The function takes one parameter:\\n\\n- `inp`: the input image as a `PIL` i...\"],[\"Theming\\n\\nTags: THEMES\\n\\n## Introduction\\n\\nGradio features a built-in theming engine that lets you cust...\"],[\"```\\n\\n$demo_theme_builder\\n\\nYou can use the Theme Builder running on Spaces above, though it runs much...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-1.hf.space?__theme=light...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-2.hf.space?__theme=light...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-3.hf.space?__theme=light...\"],[\"```\\n\\nIn the example above, we've set the `loader_color` and `slider_color` variables to `#FF0000`, d...\"],[\"```python\\ntheme = gr.themes.Default(primary_hue=\\\"blue\\\").set(\\n    button_primary_background_fill=\\\"*pr...\"],[\"```\\n\\nIn the example above, we've set the `button_primary_background_fill` and `button_primary_backgr...\"],[\"```\\n\\nNow, if we change the `button_primary_background_fill` variable, the `button_primary_background...\"],[\"```\\n\\n`button_primary_border_dark` will draw its value from `button_primary_background_fill_dark`, be...\"],[\"See how the primary button and the loading animation are now green? These CSS variables are tied to ...\"],[\"```\\n\\n- Via the command line\\n\\nFirst save the theme to disk\\n\\n```python\\nseafoam.dump(filename=\\\"seafoam....\"],[\"```\\n\\nIn order to upload a theme, you must have a HuggingFace account and pass your [Access Token](ht...\"],[\"You can sort the themes by the number of likes on the space and from most to least recently created ...\"],[\"```\\n\\nYou can also pass the theme string directly to `Blocks` or `Interface` (`gr.Blocks(theme=\\\"gradi...\"],[\"@gradio\\u002ftabs\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`28...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.0.7\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n\\n## 0.0.6\\n\\n### Featu...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"å¦‚ä½•åˆ›å»ºä¸€ä¸ªæ–°ç»„ä»¶\\n\\n## ç®€ä»‹\\n\\næœ¬æŒ‡å—æ—¨åœ¨è¯´æ˜å¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°ç»„ä»¶ï¼Œä½ å¯ä»¥åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨è¯¥ç»„ä»¶ã€‚è¯¥æŒ‡å—å°†é€šè¿‡ä»£ç ç‰‡æ®µé€æ­¥å±•ç¤ºå¦‚ä½•æ·»åŠ [ColorPicker](https:\\u002f\\u002fgr...\"],[\"è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ·»åŠ åˆ°[components.py](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fgradio\\u002fcomponents.py)æ–‡ä»¶ä¸­çš„ C...\"],[\"def get_config(self):\\n        return {\\n            \\\"value\\\": self.value,\\n            **IOComponent.ge...\"],[\"```\\n\\nä¸€æ—¦å®šä¹‰å®Œï¼Œå°±éœ€è¦åœ¨[\\\\_\\\\_init\\\\_\\\\_](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fgradio\\u002f__init__.py)æ¨¡å—ç±»ä¸­...\"],[\"```\\n\\n### 1.1 ä¸º Python ç±»ç¼–å†™å•å…ƒæµ‹è¯•\\n\\nåœ¨å¼€å‘æ–°ç»„ä»¶æ—¶ï¼Œè¿˜åº”ä¸ºå…¶ç¼–å†™ä¸€å¥—å•å…ƒæµ‹è¯•ã€‚è¿™äº›æµ‹è¯•åº”è¯¥æ”¾åœ¨[gradio\\u002ftest\\u002ftest_components.py](https:\\u002f...\"],[\"color_picker_input.interpretation_replacement = \\\"unknown\\\"\\n\\n        self.assertEqual(\\n            col...\"],[\"```\\n\\n## 2. åˆ›å»ºä¸€ä¸ªæ–°çš„ Svelte ç»„ä»¶\\n\\nè®©æˆ‘ä»¬æ¥çœ‹çœ‹åˆ›å»ºæ–°ç»„ä»¶çš„å‰ç«¯å¹¶å°†å…¶ä¸å…¶ Python ä»£ç æ˜ å°„èµ·æ¥çš„æ­¥éª¤ï¼š\\n\\n- åœ¨ [js æ–‡ä»¶å¤¹](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"$: value;\\n\\t$: handle_change(value);\\n\\n\\tconst dispatch = createEventDispatcher\\u003c{\\n\\t\\tchange: string;\\n\\t\\ts...\"],[\"```\\n\\n- é€šè¿‡æ‰§è¡Œ `export { default as FileName } from \\\".\\u002fFileName.svelte\\\"`ï¼Œåœ¨æ‚¨å°† Svelte ç»„ä»¶æ”¾ç½®çš„åŒ…çš„ index.ts æ–‡ä»¶...\"],[\"export let style: Styles = {};\\n\\n\\texport let loading_status: LoadingStatus;\\n\\n\\texport let interactive:...\"],[\"```\\n\\nç¬¬äºŒä¸ªæ–‡ä»¶åŒ…å«äº†å‰ç«¯çš„æµ‹è¯•ï¼Œä¾‹å¦‚ ColorPicker ç»„ä»¶çš„æµ‹è¯•ï¼š\\n\\n```typescript\\nimport { test, describe, assert, afterEach }...\"],[\"```\\n\\n- `directory.ts` æ–‡ä»¶ä¸­æ·»åŠ ç»„ä»¶çš„æ˜ å°„ã€‚å¤åˆ¶å¹¶ç²˜è´´ä»»ä½•ç»„ä»¶çš„æ˜ å°„è¡Œï¼Œå¹¶ç¼–è¾‘å…¶æ–‡æœ¬ã€‚é”®åå¿…é¡»æ˜¯ Python åº“ä¸­å®é™…ç»„ä»¶åç§°çš„å°å†™ç‰ˆæœ¬ã€‚ä¾‹å¦‚ï¼Œå¯¹äº ColorPicker ç»„...\"],[\"```\\n\\n### 2.1 ä¸º Svelte ç»„ä»¶ç¼–å†™å•å…ƒæµ‹è¯•\\n\\nåœ¨å¼€å‘æ–°ç»„ä»¶æ—¶ï¼Œæ‚¨è¿˜åº”è¯¥ä¸ºå…¶ç¼–å†™ä¸€å¥—å•å…ƒæµ‹è¯•ã€‚æµ‹è¯•åº”è¯¥æ”¾ç½®åœ¨æ–°ç»„ä»¶çš„æ–‡ä»¶å¤¹ä¸­ï¼Œæ–‡ä»¶åä¸º MyAwesomeComponent.test....\"],[\"è¦æµ‹è¯•åº”ç”¨ç¨‹åºï¼š\\n\\n- åœ¨ç»ˆç«¯ä¸Šè¿è¡Œ `python path\\u002fdemo\\u002frun.py`ï¼Œå®ƒä¼šåœ¨åœ°å€ [http:\\u002f\\u002flocalhost:7860](http:\\u002f\\u002flocalhost:7860) å¯åŠ¨...\"],[\"Gradio Demo: concurrency_with_queue\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpo...\"],[\"Gradio Demo: blocks_essay_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef c...\"],[\"@gradio\\u002fform\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`73268ee`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.7\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.1.0-beta.5\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n  - @gradio\\u002fatoms@0.1.4\\n\\n## 0....\"],[\"## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.0.2\\n  - @gradio\\u002fatoms@0....\"],[\"Gradio Demo: blocks_chained_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"Gradio Demo: dataframe_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith g...\"],[\"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\\n\\nTags: SERVERLESS, BROWSER, PYODIDE\\n...\"],[\"```\\n\\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio ...\"],[\"```\\n\\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https:\\u002f\\u002fhu...\"],[\"æ§åˆ¶å¸ƒå±€ (Controlling Layout)\\n\\né»˜è®¤æƒ…å†µä¸‹ï¼Œå—ä¸­çš„ç»„ä»¶æ˜¯å‚ç›´æ’åˆ—çš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é‡æ–°æ’åˆ—ç»„ä»¶ã€‚åœ¨å¹•åï¼Œè¿™ç§å¸ƒå±€ç»“æ„ä½¿ç”¨äº†[Web å¼€å‘çš„ flexbox æ¨¡å‹](https:\\u002f...\"],[\"```\\n\\n- `min_width` å°†è®¾ç½®å…ƒç´ çš„æœ€å°å®½åº¦ã€‚å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´æ»¡è¶³æ‰€æœ‰çš„ `min_width` å€¼ï¼Œè¡Œå°†æ¢è¡Œã€‚\\n\\nåœ¨[æ–‡æ¡£](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f#row...\"],[\"åœ¨æ–‡æ¡£ä¸­äº†è§£æœ‰å…³[Tabs](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f#tab)å’Œ[Accordions](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f#accordion)çš„æ›´å¤šä¿¡æ¯ã€‚\\n...\"],[\"Gradio Demo: blocks_kitchen_sink\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\nimport time\\nfrom os.path import abspath, join, pardir\\n\\nKS_FILES = abspath(jo...\"],[\"var link_elem = document.createElement('link');\\n                link_elem.classList.add('link-css');...\"],[\"with gr.Row():\\n        slider1 = gr.Slider(label=\\\"Slider 1\\\")\\n        slider2 = gr.Slider(label=\\\"Slid...\"],[\"go_btn.click(go, [radio, drop, drop_2, check, name], img, api_name=\\\"go\\\")\\n\\n                def clear(...\"],[\"gr.Markdown(\\\"## Media Files\\\")\\n\\n    with gr.Tabs() as tabs:\\n        with gr.Tab(\\\"Audio\\\"):\\n           ...\"],[\"with gr.Row():\\n        with gr.Column(scale=2):\\n            highlight = gr.HighlightedText(\\n        ...\"],[\"for selectable in selectables:\\n                    selectable.select(\\n                        select...\"],[\"isplay an interactive map of AirBnB locations with Plotly. Data is hosted on HuggingFace Datasets....\"],[\"ä½¿ç”¨ Gradio å’Œ Comet\\n\\nTags: COMET, SPACES\\nç”± Comet å›¢é˜Ÿè´¡çŒ®\\n\\n## ä»‹ç»\\n\\nåœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºæ‚¨å¯ä»¥å¦‚ä½•ä½¿ç”¨ Gradio å’Œ Cometã€‚æˆ‘ä»¬å°†ä»‹ç»...\"],[\"```\\n\\næ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦[æ³¨å†Œä¸€ä¸ª Comet è´¦æˆ·](https:\\u002f\\u002fwww.comet.com\\u002fsignup?utm_source=gradio&utm_medium=referral&utm_c...\"],[\"```\\n\\n## 1. å°† Gradio UI è®°å½•åˆ°æ‚¨çš„ Comet å®éªŒä¸­\\n\\n[![åœ¨ Colab ä¸­æ‰“å¼€](https:\\u002f\\u002fcolab.research.google.com\\u002fassets\\u002fcol...\"],[\"```\\n\\næ­¤ç‰‡æ®µä¸­çš„æœ€åä¸€è¡Œå°†å°† Gradio åº”ç”¨ç¨‹åºçš„ URL è®°å½•åˆ°æ‚¨çš„ Comet å®éªŒä¸­ã€‚æ‚¨å¯ä»¥åœ¨å®éªŒçš„æ–‡æœ¬é€‰é¡¹å¡ä¸­æ‰¾åˆ°è¯¥ URLã€‚\\n\\n\\u003cvideo width=\\\"560\\\" height=\\\"...\"],[\"è½¬åˆ°æ‚¨çš„ Comet é¡¹ç›®é¡µé¢ï¼Œè½¬åˆ°é¢æ¿é€‰é¡¹å¡ã€‚å•å‡»â€œ+ æ·»åŠ â€æŒ‰é’®ä»¥æ‰“å¼€é¢æ¿æœç´¢é¡µé¢ã€‚\\n\\n\\u003cimg width=\\\"560\\\" alt=\\\"adding-panels\\\" src=\\\"https:\\u002f\\u002fuser...\"],[\"## 3. ç›´æ¥å°† Hugging Face Spaces åµŒå…¥åˆ°æ‚¨çš„ Comet é¡¹ç›®ä¸­\\n\\n\\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.you...\"],[\"## 4. è®°å½•æ¨¡å‹æ¨æ–­ç»“æœåˆ° Comet\\n\\n\\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembed\\u002fKZnpH7msP...\"],[\"MODEL_NAME = \\\"gpt2\\\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\\n\\n# set model decoder t...\"],[\"# Log a message to the Experiment to provide more context\\n    experiment_message = gr.Textbox(label=...\"],[\"```\\n\\nè¯¥ä»£ç æ®µä¸­çš„æ¨æ–­ç»“æœå°†ä¿å­˜åœ¨å®éªŒçš„ HTML é€‰é¡¹å¡ä¸­ã€‚\\n\\n\\u003cvideo width=\\\"560\\\" height=\\\"315\\\" controls\\u003e\\n    \\u003csource src=\\\"https:...\"],[\"@gradio\\u002fcolumn\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`...\"],[\"## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"Gradio Demo: asr\\n\\n\\n```\\n!pip install -q gradio torch torchaudio transformers\\n```\\n\\n\\n```\\nimport gradio ...\"],[\"he simplest possible Gradio demo. It wraps a 'Hello {name}!' function in an Interface that accepts a...\"],[\"his  demo converts text to speech in 14 languages....\"],[\"Contributing a Guide\\n\\nWant to help teach Gradio? Consider contributing a Guide! ğŸ¤—\\n\\nBroadly speaking,...\"],[\"## How to Contribute a Guide\\n\\n1. Clone or fork this `gradio` repo\\n2. Add a new markdown document wit...\"],[\"Gradio Demo: blocks_xray\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\ndi...\"],[\"Gradio Demo: fake_gan\\n### This is a fake GAN that shows how to create a text-to-image interface for ...\"],[\"```\\n\\n\\n```\\n# This demo needs to be run from the repo folder.\\n# python demo\\u002ffake_gan\\u002frun.py\\nimport ran...\"],[\"å¦‚ä½•ä½¿ç”¨ 3D æ¨¡å‹ç»„ä»¶\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdawood\\u002fModel3D, https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fradam...\"],[\"```python\\nimport gradio as gr\\n\\ndef load_mesh(mesh_file_name):\\n    return mesh_file_name\\n\\ndemo = gr.I...\"],[\"```\\n\\nè®©æˆ‘ä»¬æ¥è§£æä¸Šé¢çš„ä»£ç ï¼š\\n\\n`load_mesh`ï¼šè¿™æ˜¯æˆ‘ä»¬çš„â€œé¢„æµ‹â€å‡½æ•°ï¼Œä¸ºç®€å•èµ·è§ï¼Œè¯¥å‡½æ•°å°†æ¥æ”¶ 3D æ¨¡å‹ç½‘æ ¼å¹¶è¿”å›å®ƒã€‚\\n\\nåˆ›å»ºç•Œé¢ï¼š\\n\\n- `fn`ï¼šå½“ç”¨æˆ·ç‚¹å‡»æäº¤æ—¶ä½¿ç”¨çš„é¢„æµ‹å‡½æ•°ã€‚...\"],[\"ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ PIFu æ¨¡å‹å°†ç©¿ç€è¡£ç‰©çš„äººçš„å›¾åƒè½¬æ¢ä¸º 3D æ•°å­—åŒ–æ¨¡å‹çš„æ¼”ç¤ºã€‚æŸ¥çœ‹[spaces.py](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fradames\\u002fPIFu-Cl...\"],[\"@gradio\\u002faccordion\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.c...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.1\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustracker@0.2....\"],[\"Gradio Demo: highlightedtext_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n...\"],[\"Gradio Demo: on_listener_decorator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith...\"],[\"Gradio Demo: zip_files\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo rep...\"],[\"his demo uses a fake model to showcase iterative output. The Image output will update every time a g...\"],[\"Gradio Demo: color_picker\\n\\n\\n```\\n!pip install -q gradio Pillow\\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: tax_calculator\\n### Calculate taxes using Textbox, Radio, and Dataframe components\\n     ...\"],[\"Customizing your demo with CSS and Javascript\\n\\nGradio allows you to customize your demo in several w...\"],[\"```\\n\\nNote: By default, files in the host machine are not accessible to users running the Gradio app....\"],[\"```\\n\\nThe CSS `#warning` ruleset will only target the second Textbox, while the `.feedback` ruleset w...\"],[\"```python\\nhead = f\\\"\\\"\\\"\\n\\u003cscript async src=\\\"https:\\u002f\\u002fwww.googletagmanager.com\\u002fgtag\\u002fjs?id={google_analyti...\"],[\"```\\n\\nNote: The `head` parameter accepts any HTML tags you would normally insert into the `\\u003chead\\u003e` of...\"],[\"The `Interface` class\\n\\nAs mentioned in the [Quickstart](\\u002fmain\\u002fguides\\u002fquickstart), the `gr.Interface`...\"],[\"Gradio supports many types of components, such as `Image`, `DataFrame`, `Video`, or `Label`. Let's t...\"],[\"```\\n\\nAlso note that our input `Image` component comes with an edit button ğŸ–‰, which allows for croppi...\"],[\"![annotated](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fguides\\u002fassets\\u002fannotated.png?raw=true)\\n\\nI...\"],[\"```\\n\\n## Flagging\\n\\nBy default, an `Interface` will have \\\"Flag\\\" button. When a user testing your `Inte...\"],[\"Gradio Demo: variable_outputs\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nmax_textb...\"],[\"Security Policy\\n\\n## Reporting a Vulnerability\\n\\nIf you discover a security vulnerability, we would be...\"],[\"Configuring Your Custom Component\\n\\nThe custom components workflow focuses on [convention over config...\"],[\"```\\n\\n## Python Dependencies\\n\\nYou can add python dependencies by modifying the `dependencies` key in ...\"],[\"Gradio Demo: blocks_gpt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\napi = gr.load(\\\"...\"],[\"imple image segmentation using gradio's AnnotatedImage component....\"],[\"Gradio Demo: save_file_no_output\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport random\\nimport string...\"],[\"@gradio\\u002fcolorpicker\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustracker@0.2.1\\n\\n## 0.1.2\\n\\n### Pa...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.0.2\\n\\n### Fixes\\n\\n- [#5118](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5118) [`1b017e68`](https:\\u002f\\u002f...\"],[\"Gradio Demo: code_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blo...\"],[\"Gradio Demo: fake_diffusion\\n### This demo uses a fake model to showcase iterative output. The Image ...\"],[\"Gradio Demo: altair_plot\\n\\n\\n```\\n!pip install -q gradio altair vega_datasets\\n```...\"],[\"```\\nimport altair as alt\\nimport gradio as gr\\nimport numpy as np\\nimport pandas as pd\\nfrom vega_datase...\"],[\"pts = alt.selection(type=\\\"single\\\", encodings=['x'])\\n\\n        rect = alt.Chart(data.movies.url).mark_...\"],[\"c2 = base.mark_text(radiusOffset=10).encode(text=\\\"values:Q\\\")\\n\\n        return c1 + c2\\n    elif plot_t...\"],[\"Gradio Demo: native_plots\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\n# Downloading files f...\"],[\"Gradio Demo: same-person-or-different\\n### This demo identifies if two speakers are the same person u...\"],[\"```\\nimport gradio as gr\\nimport torch\\nfrom torchaudio.sox_effects import apply_effects_file\\nfrom tran...\"],[\"OUTPUT_OK = (\\n    \\\"\\\"\\\"\\n    \\u003cdiv class=\\\"container\\\"\\u003e\\n        \\u003cdiv class=\\\"row\\\"\\u003e\\u003ch1 style=\\\"text-align: ce...\"],[\"EFFECTS = [\\n    [\\\"remix\\\", \\\"-\\\"],\\n    [\\\"channels\\\", \\\"1\\\"],\\n    [\\\"rate\\\", \\\"16000\\\"],\\n    [\\\"gain\\\", \\\"-1.0\\\"],\\n...\"],[\"if similarity \\u003e= THRESHOLD:\\n        output = OUTPUT_OK.format(similarity * 100)\\n    else:\\n        ou...\"],[\"How to Create a Custom Chatbot with Gradio Blocks\\n\\nTags: NLP, TEXT, CHAT\\nRelated spaces: https:\\u002f\\u002fhug...\"],[\"We have a single function, `respond()`, which takes in the entire history of the chatbot, appends a ...\"],[\"Of course, in practice, you would replace `bot()` with your own more complex function, which might c...\"],[\"```\\n\\n## Adding Markdown, Images, Audio, or Videos\\n\\nThe `gr.Chatbot` component supports a subset of m...\"],[\"Gradio Demo: video_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: bar_plot\\n\\n\\n```\\n!pip install -q gradio pandas\\n```...\"],[\"```\\nimport gradio as gr\\nimport pandas as pd\\nimport random\\n\\nsimple = pd.DataFrame(\\n    {\\n        \\\"a\\\":...\"],[\"def bar_plot_fn(display):\\n    if display == \\\"simple\\\":\\n        return gr.BarPlot(\\n            simple,...\"],[\"with gr.Blocks() as bar_plot:\\n    with gr.Row():\\n        with gr.Column():\\n            display = gr....\"],[\"Gradio Demo: progress_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time...\"],[\"@gradio\\u002fuploadbutton\\n\\n## 0.3.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithu...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.1\\n\\n### Features\\n\\n- [#6181](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6181) [`62ec2075c`](http...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.7\\n\\n### Features...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.0-beta.5\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e4a307ed6`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.0.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`c57f1b75e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.0.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`119c8343`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6...\"],[\"The Frontend ğŸŒâ­ï¸\\n\\nThis guide will cover everything you need to know to implement your custom compone...\"],[\"```\\n\\n* `elem_id` and `elem_classes` allow Gradio app developers to target your component with custom...\"],[\"```\\n\\n## The Example.svelte file\\n\\nThe `Example.svelte` file should expose the following props:\\n\\n```ty...\"],[\"```\\n\\n## Handling Files\\n\\nIf your component deals with files, these files **should** be uploaded to th...\"],[\"```\\n\\nThe component exposes a prop named `root`. \\nThis is passed down by the parent gradio app and it...\"],[\"```\\n\\n## Leveraging Existing Gradio Components\\n\\nMost of Gradio's frontend components are published on...\"],[\"```\\n\\nYou can also combine existing Gradio components to create entirely unique experiences.\\nLike ren...\"],[\"@gradio\\u002fsimpledropdown\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgit...\"],[\"## 0.1.0\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#5996](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5996) [`9cf40f76f...\"],[\"Test Coverage\\n\\nJust a little reference docs to understand what is tested\\u002f needs testing. Perhaps tem...\"],[\"| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show...\"],[\"| Dataframe       | `âŒ`    | `âœ…`      | `âœ…`      | `âœ…`           | `âŒ`        | `âœ…`    | `âŒ`        ...\"],[\"| ScatterPlot     | `âŒ`    | `âŒ`      | `âŒ`      | `âŒ`           | `âŒ`        | `âŒ`    | `âŒ`        ...\"],[\"### Events...\"],[\"| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show...\"],[\"| Dataframe       | `âŒ`    | `âŒ`      | `âŒ`      | `âŒ`           | `âŒ`        | `âŒ`    | `âŒ`        ...\"],[\"| Plot            | `âŒ`    | `âŒ`      | `âŒ`      | `âŒ`           | `âŒ`        | `âŒ`    | `âŒ`        ...\"],[\"### `AnnotatedImage`\\n\\n### `Audio`\\n\\n### `BarPlot`\\n\\n### `Button`\\n\\n### `Chatbot`\\n\\n### `Checkbox`\\n\\n### `...\"],[\"@gradio\\u002fcode\\n\\n## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.2.9\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.2.5\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fupload@0.4.1\\n\\n## 0.2.4\\n\\n### Fixe...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fupload@0.3.1\\n\\n## 0.2.0\\n\\n### Feat...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: text_analysis\\n### This simple demo takes advantage of Gradio's HighlightedText, JSON an...\"],[\"Gradio Demo: diff_texts\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom difflib import Differ\\n\\nimport g...\"],[\"Gradio Demo: code\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\nimp...\"],[\"Reactive Interfaces\\n\\nFinally, we cover how to get Gradio demos to refresh automatically or continuou...\"],[\"`@gradio\\u002ftheme`\\n\\ncss for gradio...\"],[\"Gradio Demo: chatbot_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport rando...\"],[\"his sentiment analaysis demo takes in input text and returns its classification for either positive,...\"],[\"Gradio Demo: count_generator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time...\"],[\"Creating a Real-Time Dashboard from Google Sheets\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Google Sheets](...\"],[\"```\\n\\n2\\\\. Now, let's modify this URL and then use it to read the data from the Google Sheets into a P...\"],[\"```\\n\\nAnd that's it! You have a dashboard that refreshes every 5 seconds, pulling the data from your ...\"],[\"6\\\\. After selecting the service account, select the \\\"JSON\\\" key type and then click on the \\\"Create\\\" b...\"],[\"```\\n\\n### Querying\\n\\nOnce you have the credentials `.json` file, you can use the following steps to qu...\"],[\"```\\n\\n4\\\\. The data query is a function, which means that it's easy to display it real-time using the ...\"],[\"@gradio\\u002fnumber\\n\\n## 0.3.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustracker@0.2....\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5047](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5047) [`883ac364`](https...\"],[\"@gradio\\u002fplot\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5642](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5642) [`21c7225bd`](http...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: chatbot_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: interface_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nd...\"],[\"his demo identifies musical instruments from an audio file. It uses Gradio's Audio and Label compone...\"],[\"Gradio Demo: blocks_js_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef welcome...\"],[\"Gradio Demo: image_mod_default_image\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files fro...\"],[\"Gradio Demo: on_listener_basic\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: reversible_flow\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef increa...\"],[\"@gradio\\u002fcheckboxgroup\\n\\n## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Gradio Demo: digit_classifier\\n\\n\\n```\\n!pip install -q gradio tensorflow\\n```\\n\\n\\n```\\nfrom urllib.request ...\"],[\"@gradio\\u002fwasm\\n\\n## 0.4.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#6099](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6099) [`d84209703`](http...\"],[\"## 0.2.0-beta.1\\n\\n### Features\\n\\n- [#5963](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5963) [`174b73619...\"],[\"## 0.2.0-beta.0\\n\\n### Features\\n\\n- [#5956](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5956) [`f769876e0...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5868](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5868) [`4e0d87e9c`](http...\"],[\"### Fixes\\n\\n- [#5919](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5919) [`1724918f0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.0.2\\n\\n### Fixes\\n\\n- [#5538](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5538) [`b5c6f7b08`](https:\\u002f...\"],[\"@gradio\\u002fcheckbox\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustracker@0.2....\"],[\"## 0.1.1\\n\\n### Fixes\\n\\n- [#5340](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5340) [`df090e89`](https:\\u002f\\u002f...\"],[\"Gradio Demo: input_output\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef image_mo...\"],[\"Key Features\\n\\nLet's go through some of the key features of Gradio. This guide is intended to be a hi...\"],[\"**Preprocessing and Postprocessing**\\n\\nWhen a component is used as an input, Gradio automatically han...\"],[\"```\\n\\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data...\"],[\"```\\n\\nThis limits the number of requests processed for this event listener at a single time to 5. By ...\"],[\"```\\n\\nYou supply a generator into Gradio the same way as you would a regular function. For example, h...\"],[\"```\\n\\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can exten...\"],[\"```\\n\\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can a...\"],[\"Gradio Demo: file_explorer_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nw...\"],[\"Gradio Demo: timeseries-forecasting-with-prophet\\n### A simple dashboard showing pypi stats for pytho...\"],[\"```\\nimport gradio as gr\\nimport pypistats\\nfrom datetime import date\\nfrom dateutil.relativedelta impor...\"],[\"plt = gr.Plot()\\n\\n    lib.change(get_forecast, [lib, time], plt, queue=False)\\n    time.change(get_for...\"],[\"Custom Components in 5 minutes\\n\\nGradio 4.0 introduces Custom Components -- the ability for developer...\"],[\"Each of these steps is done via the Custom Component CLI. You can invoke it with `gradio cc` or `gra...\"],[\"```\\n\\nInstead of `MyComponent`, give your component any name.\\n\\nInstead of `SimpleTextbox`, you can us...\"],[\"```\\n\\nThis will create a `tar.gz` and `.whl` file in a `dist\\u002f` subdirectory.\\nIf you or anyone install...\"],[\"Test Strategy\\n\\nVery brief, mildly aspirational test strategy document. This isn't where we are but i...\"],[\"## Types of testing\\n\\nOur tests will broadly fall into one of three categories:\\n\\n- Static Quality che...\"],[\"### Dynamic code tests\\n\\n- pytest (python unit and integration tests)\\n- vitest (node-based unit and i...\"],[\"For instructions on how to write and run tests see the [contributing guide](https:\\u002f\\u002fgithub.com\\u002fgradi...\"],[\"Gradio Demo: audio_debugger\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"`@gradio\\u002fupload`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { Upload, ModifyUpload, normalise_file, get_fetchable_...\"],[\"```\\n\\nModifyUpload\\n```javascript\\n    export let editable = false;\\n\\texport let undoable = false;\\n\\texpo...\"],[\"Gradio Demo: hello_blocks_decorator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nwi...\"],[\"Connecting to a Database\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchicago-bikeshare-das...\"],[\"Once your database is created, download the dataset from Kaggle and upload it to your database.\\nFor ...\"],[\"def get_most_popular_stations():\\n\\n    df = pd.read_sql(\\n        \\\"\\\"\\\"\\n    SELECT COUNT(ride_id) as n, ...\"],[\"```\\n\\nIf you were to run our script locally, you could pass in your credentials as environment variab...\"],[\"```\\n\\n## Step 3 - Deployment\\n\\nIf you run the code above, your app will start running locally.\\nYou can...\"],[\"Gradio Demo: image_classifier_2\\n\\n\\n```\\n!pip install -q gradio pillow torch torchvision\\n```\\n\\n\\n```\\n# Do...\"],[\"Gradio Demo: scatter_plot\\n\\n\\n```\\n!pip install -q gradio vega_datasets pandas\\n```...\"],[\"```\\nimport gradio as gr\\nfrom vega_datasets import data\\n\\ncars = data.cars()\\niris = data.iris()\\n\\n# # O...\"],[\"# cars = pd.DataFrame(cars_data)\\n# iris = pd.DataFrame(iris_data)\\n\\n\\ndef scatter_plot_fn(dataset):\\n  ...\"],[\"ä½¿ç”¨Gradio Pythonå®¢æˆ·ç«¯æ„å»ºFastAPIåº”ç”¨\\n\\nTags: CLIENT, API, WEB APP\\n\\nåœ¨æœ¬åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ `gradio_client` [Python...\"],[\"```\\n\\næ‚¨è¿˜éœ€è¦å®‰è£…ffmpegã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ£€æŸ¥æ‚¨æ˜¯å¦å·²å®‰è£…ffmpegï¼š\\n\\n```bash\\n$ ffmpeg version\\n```\\n\\nå¦åˆ™ï¼Œé€šè¿‡æŒ‰ç…§è¿™äº›è¯´æ˜å®‰è£…ffmpeg...\"],[\"```\\n\\næ‰€éœ€çš„ä»£ç ä»…å¦‚ä¸Šæ‰€ç¤º--è¯·æ³¨æ„ï¼ŒAPIç«¯ç‚¹è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆä¸€ä¸ªæ²¡æœ‰éŸ³ä¹ï¼Œä¸€ä¸ªåªæœ‰éŸ³ä¹ï¼‰çš„åˆ—è¡¨ï¼Œå› æ­¤æˆ‘ä»¬åªè¿”å›åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚\\n\\n---\\n\\n**æ³¨æ„**ï¼šç”±äºè¿™æ˜¯ä¸€ä¸ªå…¬å…±Space...\"],[\"```\\n\\nå…¶ä»–çš„ä»£ç ä¿æŒä¸å˜ï¼\\n\\n---\\n\\nç°åœ¨ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆéœ€è¦ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`ffmpeg`åº“ï¼Œå®ƒåœ¨å¤„ç†éŸ³é¢‘å’Œè§†é¢‘æ–‡ä»¶æ—¶åšäº†å¾ˆå¤šè‰°å·¨çš„å·¥ä½œã€‚ä½¿ç”¨...\"],[\"```\\n\\nå¦‚æœæ‚¨æƒ³äº†è§£æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·é˜…è¯»[ffmpegæ–‡æ¡£](https:\\u002f\\u002fffmpeg.org\\u002fffmpeg.html)ï¼Œå› ä¸ºå®ƒä»¬è¶…å‡ºäº†æœ¬æ•™ç¨‹çš„èŒƒå›´ã€‚\\n\\n## æ­¥éª¤2: åˆ›å»ºä¸€ä¸ªFa...\"],[\"```\\n\\nåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒFastAPIåº”ç”¨ç¨‹åºæœ‰ä¸¤ä¸ªè·¯ç”±ï¼š`\\u002f` å’Œ `\\u002fuploadvideo\\u002f`ã€‚\\n\\n`\\u002f` è·¯ç”±è¿”å›ä¸€ä¸ªæ˜¾ç¤ºæ‰€æœ‰ä¸Šä¼ è§†é¢‘çš„ç”»å»Šçš„HTMLæ¨¡æ¿ã€‚\\n\\n`\\u002fuploadvideo\\u002f` ...\"],[\"```\\n\\nå°†ä»¥ä¸‹å†…å®¹å†™å…¥`home.html`æ–‡ä»¶ä¸­ï¼š...\"],[\"```html\\n&lt;!DOCTYPE html\\u003e &lt;html\\u003e &lt;head\\u003e &lt;title\\u003e è§†é¢‘åº“ &lt;\\u002ftitle\\u003e &lt;style\\u003e\\nbody { font-fam...\"],[\"type=\\\"video\\u002fmp4\\\"\\u003e æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè§†é¢‘æ ‡ç­¾ã€‚ &lt;\\u002fvideo\\u003e &lt;p\\u003e{{ video\\n}}&lt;\\u002fp\\u003e &lt;\\u002fdiv\\u003e {% endfor %} &lt;\\u002fdiv...\"],[\"```\\n\\n## ç¬¬4æ­¥ï¼šè¿è¡Œ FastAPI åº”ç”¨\\n\\næœ€åï¼Œæˆ‘ä»¬å‡†å¤‡å¥½è¿è¡Œç”± Gradio Python å®¢æˆ·ç«¯æä¾›æ”¯æŒçš„ FastAPI åº”ç”¨ç¨‹åºã€‚\\n\\næ‰“å¼€ç»ˆç«¯å¹¶å¯¼èˆªåˆ°åŒ…å« `main.py` æ–‡ä»¶...\"],[\"Gradio Demo: theme_extended_step_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"@gradio\\u002ftabitem\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.4\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.1\\n  - @gradio\\u002ftabs@0.0...\"],[\"ä» BigQuery æ•°æ®åˆ›å»ºå®æ—¶ä»ªè¡¨ç›˜\\n\\nTags: è¡¨æ ¼ , ä»ªè¡¨ç›˜ , ç»˜å›¾\\n\\n[Google BigQuery](https:\\u002f\\u002fcloud.google.com\\u002fbigquery) æ˜¯ä¸€ä¸ªåŸº...\"],[\"## è®¾ç½® BigQuery å‡­æ®\\n\\nè¦ä½¿ç”¨ Gradio å’Œ BigQueryï¼Œæ‚¨éœ€è¦è·å–æ‚¨çš„ BigQuery å‡­æ®ï¼Œå¹¶å°†å…¶ä¸ [BigQuery Python å®¢æˆ·ç«¯](https:\\u002f\\u002fpypi...\"],[\"```\\n\\n## ä½¿ç”¨ BigQuery å®¢æˆ·ç«¯\\n\\nè·å¾—å‡­æ®åï¼Œæ‚¨éœ€è¦ä½¿ç”¨ BigQuery Python å®¢æˆ·ç«¯ä½¿ç”¨æ‚¨çš„å‡­æ®è¿›è¡Œèº«ä»½éªŒè¯ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… BigQuery Pyt...\"],[\"```\\n\\n## æ„å»ºå®æ—¶ä»ªè¡¨ç›˜\\n\\nä¸€æ—¦æ‚¨æœ‰äº†æŸ¥è¯¢æ•°æ®çš„å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Gradio åº“çš„ `gr.DataFrame` ç»„ä»¶ä»¥è¡¨æ ¼å½¢å¼æ˜¾ç¤ºç»“æœã€‚è¿™æ˜¯ä¸€ç§æ£€æŸ¥æ•°æ®å¹¶ç¡®ä¿æŸ¥è¯¢æ­£ç¡®çš„æœ‰ç”¨æ–¹å¼ã€‚\\n\\nä»¥ä¸‹æ˜¯å¦‚...\"],[\"```\\n\\nä¹Ÿè®¸æ‚¨æƒ³åœ¨æˆ‘ä»¬çš„ä»ªè¡¨ç›˜ä¸­æ·»åŠ ä¸€ä¸ªå¯è§†åŒ–æ•ˆæœã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `gr.ScatterPlot()` ç»„ä»¶å°†æ•°æ®å¯è§†åŒ–ä¸ºæ•£ç‚¹å›¾ã€‚è¿™å¯ä»¥è®©æ‚¨æŸ¥çœ‹æ•°æ®ä¸­ä¸åŒå˜é‡ï¼ˆä¾‹å¦‚ç—…ä¾‹æ•°å’Œæ­»äº¡æ•°ï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¯ç”¨äº...\"],[\"Gradio Demo: progress\\n\\n\\n```\\n!pip install -q gradio tqdm datasets\\n```...\"],[\"```\\nimport gradio as gr\\nimport random\\nimport time\\nimport tqdm\\nfrom datasets import load_dataset\\nimpo...\"],[\"# track iterable of unknown length\\n    def load_random(data, progress=gr.Progress()):\\n        def yi...\"],[\"def bind_internal_tqdm(data, progress=gr.Progress(track_tqdm=True)):\\n        outdir = \\\"__tmp\\u002f\\\" + str...\"],[\"Gradio Demo: theme_new_step_1\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nfrom gradi...\"],[\"Interface State\\n\\nSo far, we've assumed that your demos are *stateless*: that they do not persist inf...\"],[\"Gradio Demo: tabbed_interface_lite\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nhell...\"],[\"Gradio Demo: theme_extended_step_1\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"ä½¿ç”¨ GAN åˆ›å»ºæ‚¨è‡ªå·±çš„æœ‹å‹\\n\\nspaces\\u002fNimaBoscarino\\u002fcryptopunks, https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fnateraw\\u002fcryptopunks...\"],[\"ä»Šå¤©æˆ‘ä»¬å°†ç®€è¦ä»‹ç» GAN çš„é«˜çº§ç›´è§‰ï¼Œç„¶åæˆ‘ä»¬å°†å›´ç»•ä¸€ä¸ªé¢„è®­ç»ƒçš„ GAN æ„å»ºä¸€ä¸ªå°å‹æ¼”ç¤ºï¼Œçœ‹çœ‹è¿™ä¸€åˆ‡éƒ½æ˜¯æ€ä¹ˆå›äº‹ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬å°†è¦ç»„åˆçš„ä¸œè¥¿çš„ä¸€ç¥ï¼š\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fni...\"],[\"ç”Ÿæˆå™¨ä¸æ–­è®­ç»ƒä»¥åˆ›å»ºå¯¹é‰´åˆ«å™¨æ›´éš¾ä»¥è¯†åˆ«çš„å›¾åƒï¼Œè€Œé‰´åˆ«å™¨æ¯æ¬¡æ­£ç¡®æ£€æµ‹åˆ°ä¼ªé€ å›¾åƒæ—¶ï¼Œéƒ½ä¼šä¸ºç”Ÿæˆå™¨è®¾ç½®æ›´é«˜çš„é—¨æ§›ã€‚éšç€ç½‘ç»œä¹‹é—´çš„è¿™ç§ç«äº‰ï¼ˆ**adversarial å¯¹æŠ—æ€§ï¼**ï¼‰ï¼Œç”Ÿæˆçš„å›¾åƒæ”¹å–„åˆ°äº†å¯¹äººçœ¼...\"],[\"```python\\nfrom torch import nn\\n\\nclass Generator(nn.Module):\\n    # æœ‰å…³ncï¼Œnzå’Œngfçš„è§£é‡Šï¼Œè¯·å‚è§ä¸‹é¢çš„é“¾æ¥\\n    # http...\"],[\"```\\n\\næˆ‘ä»¬æ­£åœ¨ä½¿ç”¨æ¥è‡ª[æ­¤ repo çš„ @teddykoker](https:\\u002f\\u002fgithub.com\\u002fteddykoker\\u002fcryptopunks-gan\\u002fblob\\u002fmain\\u002ftrain.py...\"],[\"```\\n\\n## æ­¥éª¤ 2 - å®šä¹‰â€œpredictâ€å‡½æ•°\\n\\n`predict` å‡½æ•°æ˜¯ä½¿ Gradio å·¥ä½œçš„å…³é”®ï¼æˆ‘ä»¬é€šè¿‡ Gradio ç•Œé¢é€‰æ‹©çš„ä»»ä½•è¾“å…¥éƒ½å°†é€šè¿‡æˆ‘ä»¬çš„ `predict` å‡½æ•°ä¼ ...\"],[\"```\\n\\næˆ‘ä»¬ç»™ `predict` å‡½æ•°ä¸€ä¸ª `seed` å‚æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç§å­å›ºå®šéšæœºå¼ é‡ç”Ÿæˆã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ å…¥ç›¸åŒçš„ç§å­å†æ¬¡æŸ¥çœ‹ç”Ÿæˆçš„ punksã€‚\\n\\n_æ³¨æ„ï¼_ æˆ‘ä»¬çš„æ¨¡å‹éœ€...\"],[\"```\\n\\nå¯åŠ¨ç•Œé¢åï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°åƒè¿™æ ·çš„ä¸œè¥¿ :\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fnimaboscarino-cryptopunks-1.hf.space\\\" frameBorder=\\\"0...\"],[\"```\\n\\næ–°çš„è¾“å…¥å°†ä¼ é€’ç»™æˆ‘ä»¬çš„ `predict()` å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å¯¹è¯¥å‡½æ•°è¿›è¡Œä¸€äº›æ›´æ”¹ï¼Œä»¥æ¥å—ä¸€ä¸ªæ–°çš„å‚æ•° :\\n\\n```python\\ndef predict(seed, num_punks)...\"],[\"```\\n\\n`examples` å‚æ•°æ¥å—ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œå…¶ä¸­å­åˆ—è¡¨ä¸­çš„æ¯ä¸ªé¡¹ç›®çš„é¡ºåºä¸æˆ‘ä»¬åˆ—å‡ºçš„ `inputs` çš„é¡ºåºç›¸åŒã€‚æ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ`[seed, num_punks]`ã€‚è¯•ä¸€è¯•å§ï¼\\n...\"],[\"```python\\nimport torch\\nfrom torch import nn\\nfrom huggingface_hub import hf_hub_download\\nfrom torchvi...\"],[\"```\\n\\n---\\n\\næ­å–œï¼ä½ å·²ç»æˆåŠŸæ„å»ºäº†è‡ªå·±çš„åŸºäº GAN çš„ CryptoPunks ç”Ÿæˆå™¨ï¼Œé…å¤‡äº†ä¸€ä¸ªæ—¶å°šçš„ Gradio ç•Œé¢ï¼Œä½¿ä»»ä½•äººéƒ½èƒ½è½»æ¾ä½¿ç”¨ã€‚ç°åœ¨ä½ å¯ä»¥åœ¨ Hub ä¸Š[å¯»æ‰¾æ›´å¤šçš„ GA...\"],[\"Gradio Demo: blocks_flipper\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\nimport gradio...\"],[\"@gradio\\u002fannotatedimage\\n\\n## 0.3.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgi...\"],[\"## 0.3.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.3.9\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.2\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5554](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5554) [`75ddeb390`](http...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: stt_or_tts\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ntts_examples = ...\"],[\"Gradio and W&B Integration\\n\\nç›¸å…³ç©ºé—´ï¼šhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fakhaliq\\u002fJoJoGAN\\næ ‡ç­¾ï¼šWANDB, SPACES\\nç”± Gr...\"],[\"\\u003cimg alt=\\\"Screen Shot 2022-08-01 at 5 54 59 PM\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f81195...\"],[\"è®©æˆ‘ä»¬å¼€å§‹å§ï¼\\n\\n1. åˆ›å»º W&B è´¦å·\\n\\n   å¦‚æœæ‚¨è¿˜æ²¡æœ‰ W&B è´¦å·ï¼Œè¯·æŒ‰ç…§[è¿™äº›å¿«é€Ÿè¯´æ˜](https:\\u002f\\u002fapp.wandb.ai\\u002flogin)åˆ›å»ºå…è´¹è´¦å·ã€‚è¿™ä¸åº”è¯¥è¶…è¿‡å‡ åˆ†é’Ÿçš„æ—¶é—´ã€‚ä¸€...\"],[\"```\\n\\n3. å¾®è°ƒ StyleGAN å’Œ W&B å®éªŒè·Ÿè¸ª\\n\\n   ä¸‹ä¸€æ­¥å°†æ‰“å¼€ä¸€ä¸ª W&B ä»ªè¡¨æ¿ï¼Œä»¥è·Ÿè¸ªå®éªŒï¼Œå¹¶æ˜¾ç¤ºä¸€ä¸ª Gradio æ¼”ç¤ºæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä»ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©ã€‚è¿™æ˜¯æ‚¨éœ€è¦çš„...\"],[\"for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0),...\"],[\"```\\n\\n4. ä¿å­˜ã€ä¸‹è½½å’ŒåŠ è½½æ¨¡å‹\\n\\n   ä»¥ä¸‹æ˜¯å¦‚ä½•ä¿å­˜å’Œä¸‹è½½æ‚¨çš„æ¨¡å‹ã€‚\\n\\n   ```python\\n\\n   from PIL import Image\\n   import torch\\n   to...\"],[\"plt.rcParams['figure.dpi'] = 150\\n\\n   transform = transforms.Compose(\\n       [\\n           transforms....\"],[\"```\\n\\n5. æ„å»º Gradio æ¼”ç¤º\\n\\n   ```python\\n\\n   import gradio as gr\\n\\n   title = \\\"JoJoGAN\\\"\\n   description = \\\"J...\"],[\"```\\n\\n7.ï¼ˆå¯é€‰ï¼‰åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­åµŒå…¥ W&B å›¾\\n\\n    ä¹Ÿå¯ä»¥åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­åµŒå…¥ W&B å›¾ã€‚ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ª W&B æŠ¥å‘Šï¼Œå¹¶åœ¨ä¸€ä¸ª `gr.HTML` å—ä¸­...\"],[\"```\\n\\n## ç»“è®º\\n\\nå¸Œæœ›æ‚¨å–œæ¬¢æ­¤åµŒå…¥ Gradio æ¼”ç¤ºåˆ° W&B æŠ¥å‘Šçš„ç®€çŸ­æ¼”ç¤ºï¼æ„Ÿè°¢æ‚¨ä¸€ç›´é˜…è¯»åˆ°æœ€åã€‚å›é¡¾ä¸€ä¸‹ :\\n\\n- ä»…éœ€è¦ä¸€ä¸ªå•ä¸€å‚è€ƒå›¾åƒå³å¯å¯¹ JoJoGAN è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸åœ¨ GPU...\"],[\"Gradio Demo: dataset\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\n...\"],[\"```\\nimport gradio as gr\\nimport os\\nimport numpy as np\\n\\n\\ntxt = \\\"the quick brown fox\\\"\\nnum = 10\\n\\nimg = o...\"],[\"c_2 = gr.CheckboxGroup(visible=False, choices=['a', 'b', 'c'])\\n    gr.Dataset(\\n        label=\\\"Checkb...\"],[\"gr.Dataset(\\n        components=[f],\\n        label=\\\"File\\\",\\n        samples=[\\n            [csv],\\n     ...\"],[\")\\n    r = gr.Radio(visible=False, choices=[\\\"one\\\", \\\"two\\\", \\\"three\\\"])\\n    gr.Dataset(\\n        component...\"],[\"`@gradio\\u002fdropdown`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {BaseDropdown, BaseMultiselect, BaseExample } from \\\"...\"],[\"Gradio Demo: fake_diffusion_with_gif\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files fro...\"],[\"Gradio Demo: theme_soft\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\nwit...\"],[\"Gradio Demo: radio_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"Gradio Demo: image_selections\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\nimport numpy as np\\n\\nwith gr.Blocks() as demo:\\n    tolerance = gr.Slider(labe...\"],[\"out = img.copy() * 0.2\\n        out = out.astype(np.uint8)\\n        for pixel in pixels_in_segment:\\n  ...\"],[\"ä½¿ç”¨ Gradio Python å®¢æˆ·ç«¯å…¥é—¨\\n\\nTags: CLIENT, API, SPACES\\n\\nGradio Python å®¢æˆ·ç«¯ä½¿å¾—å°†ä»»ä½• Gradio åº”ç”¨ç¨‹åºä½œä¸º API ä½¿ç”¨å˜å¾—éå¸¸å®¹æ˜“...\"],[\"```\\n\\nGradio å®¢æˆ·ç«¯é€‚ç”¨äºä»»ä½•æ‰˜ç®¡åœ¨ Hugging Face Spaces ä¸Šçš„ Gradio åº”ç”¨ç¨‹åºï¼Œæ— è®ºæ˜¯å›¾åƒç”Ÿæˆå™¨ã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ã€æœ‰çŠ¶æ€èŠå¤©æœºå™¨äººã€ç¨é‡‘è®¡ç®—å™¨è¿˜æ˜¯å…¶ä»–ä»»ä½•åº”ç”¨ç¨‹åº...\"],[\"```\\n\\n## å¤åˆ¶ç©ºé—´ä»¥ä¾›ç§äººä½¿ç”¨\\n\\nè™½ç„¶ä½ å¯ä»¥å°†ä»»ä½•å…¬å…±ç©ºé—´ç”¨ä½œ APIï¼Œä½†å¦‚æœä½ å‘å‡ºå¤ªå¤šè¯·æ±‚ï¼Œä½ å¯èƒ½ä¼šå—åˆ° Hugging Face çš„é¢‘ç‡é™åˆ¶ã€‚è¦æ— é™åˆ¶åœ°ä½¿ç”¨ä¸€ä¸ªç©ºé—´ï¼Œåªéœ€å°†å…¶å¤åˆ¶ä»¥åˆ›å»ºä¸€ä¸ªç§...\"],[\"```\\n\\n\\u003e \\u003e \\\" è¿™æ˜¯ Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æµ‹è¯•ã€‚\\\"\\n\\nå¦‚æœä¹‹å‰å·²å¤åˆ¶äº†ä¸€ä¸ªç©ºé—´ï¼Œé‡æ–°è¿è¡Œ `duplicate()` å°†*ä¸ä¼š*åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºé—´ã€‚ç›¸åï¼Œå®¢æˆ·ç«¯å°†è¿æ¥åˆ°ä¹‹å‰åˆ›å»ºçš„ç©ºé—´ã€‚å› ...\"],[\"```\\n\\nè¿™æ˜¾ç¤ºäº†åœ¨æ­¤ç©ºé—´ä¸­æœ‰ 1 ä¸ª API ç«¯ç‚¹ï¼Œå¹¶æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ API ç«¯ç‚¹è¿›è¡Œé¢„æµ‹ï¼šæˆ‘ä»¬åº”è¯¥è°ƒç”¨ `.predict()` æ–¹æ³•ï¼ˆæˆ‘ä»¬å°†åœ¨ä¸‹é¢æ¢è®¨ï¼‰ï¼Œæä¾›ç±»å‹ä¸º `str` çš„å‚æ•° `inp...\"],[\"```\\n\\nå¦‚æœæœ‰å¤šä¸ªå‚æ•°ï¼Œé‚£ä¹ˆä½ åº”è¯¥å°†å®ƒä»¬ä½œä¸ºå•ç‹¬çš„å‚æ•°ä¼ é€’ç»™ `.predict()`ï¼Œå°±åƒè¿™æ ·ï¼š\\n\\n````python\\nfrom gradio_client import Client\\n\\ncli...\"],[\"```\\n\\n## æ·»åŠ å›è°ƒ ï¼ˆAdding callbacksï¼‰\\n\\næˆ–è€…ï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªå›è°ƒæ¥åœ¨ä½œä¸šå®Œæˆåæ‰§è¡Œæ“ä½œï¼Œåƒè¿™æ ·ï¼š\\n\\n```python\\nfrom gradio_client import...\"],[\"```\\n\\n_æ³¨æ„_ï¼š`Job`ç±»è¿˜æœ‰ä¸€ä¸ª`.done()`å®ä¾‹æ–¹æ³•ï¼Œè¿”å›ä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºä½œä¸šæ˜¯å¦å·²å®Œæˆã€‚\\n\\n## å–æ¶ˆä½œä¸š ï¼ˆCancelling Jobsï¼‰\\n\\n`Job`ç±»è¿˜æœ‰ä¸€ä¸ª`.cancel(...\"],[\"```\\n\\nè¯·æ³¨æ„ï¼Œåœ¨ç”Ÿæˆå™¨ç«¯ç‚¹ä¸Šè¿è¡Œ`job.result()`åªä¼šè·å¾—ç«¯ç‚¹è¿”å›çš„*ç¬¬ä¸€ä¸ª*å€¼ã€‚\\n\\n`Job`å¯¹è±¡è¿˜æ˜¯å¯è¿­ä»£çš„ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæŒ‰ç…§ä»ç«¯ç‚¹è¿”å›çš„ç»“æœé€ä¸ªæ˜¾ç¤ºç”Ÿæˆå™¨å‡½æ•°çš„ç»“æœã€‚ä»¥ä¸‹æ˜¯...\"],[\"Gradio Demo: markdown_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr...\"],[\"Gradio Demo: gpt2_xl_unified\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ncomponent ...\"],[\"How to Create a Chatbot with Gradio\\n\\nTags: NLP, TEXT, CHAT\\n\\n## Introduction\\n\\nChatbots are a popular ...\"],[\"```\\n\\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the w...\"],[\"```\\n\\nNotice that we've [enabled queuing](\\u002fguides\\u002fkey-features#queuing), which is required to use gen...\"],[\"```\\n\\n## Additional Inputs\\n\\nYou may want to add additional parameters to your chatbot and expose them...\"],[\"```\\n\\nIf you need to create something even more custom, then its best to construct the chatbot UI usi...\"],[\"llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\\n\\ndef predict(message, history):\\n    hi...\"],[\"```\\n\\n## A streaming example using `openai`\\n\\nOf course, we could also use the `openai` library direct...\"],[\"```\\n\\n## Example using a local, open-source LLM with Hugging Face\\n\\nOf course, in many cases you want ...\"],[\"model_inputs = tokenizer([messages], return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n    streamer = TextIteratorStre...\"],[\"```\\n\\nWith those examples, you should be all set to create your own Gradio Chatbot demos soon! For bu...\"],[\"Create a Dashboard from Supabase Data\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Supabase](https:\\u002f\\u002fsupabase....\"],[\"4\\\\. Click on \\\"Table Editor\\\" (the table icon) in the left pane to create a new table. We'll create a ...\"],[\"```\\n\\n7\\\\. Get your project URL and API key. Click the Settings (gear icon) on the left pane and click...\"],[\"```\\n\\nReturn to your Supabase dashboard and refresh the page, you should now see 10 rows populated in...\"],[\"```\\n\\nNotice that by passing in a function to `gr.BarPlot()`, we have the BarPlot query the database ...\"],[\"Gradio Demo: hello_world_4\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Demo: fake_gan_no_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport time\\n\\nimport gradio as...\"],[\"gradio_test\\n\\n## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"@gradio\\u002fpreview\\n\\n## 0.6.0\\n\\n### Features\\n\\n- [#6738](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6738) [...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6532](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6532) [`96290d304`](http...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.2\\n\\n### Features\\n\\n- [#6467](https:\\u002f\\u002fgithub.c...\"],[\"## 0.1.1\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.8\\n\\n### Features\\n\\n- [#6094](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6094) [`c476bd5a5...\"],[\"## 0.1.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5962](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5962) [`d298e7695`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.0-beta.3\\n\\n### Features\\n\\n- [#5648](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5648) [`c573e2339...\"],[\"## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5507](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5507) [`1385dc688...\"],[\"Gradio Demo: blocks_js_methods\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nblocks =...\"],[\"Gradio Demo: theme_extended_step_4\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"@gradio\\u002ftextbox\\n\\n## 0.4.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5652](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5652) [`2e25d4305`](http...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5417](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5417) [`d14d63e3`](https...\"],[\"## 0.1.1\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"### Fixes\\n\\n- [#5114](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5114) [`56d2609d`](https:\\u002f\\u002fgithub.com...\"],[\"Gradio Demo: dataset_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: blocks_layout\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndemo = gr.B...\"],[\"@gradio\\u002fapp\\n\\n## 1.17.0\\n\\n### Features\\n\\n- [#6831](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6831) [`f3...\"],[\"## 1.16.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`245d58e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.16.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.16.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67ddd40`](https...\"],[\"## 1.15.0\\n\\n### Features\\n\\n- [#6512](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6512) [`4f040c7`](https...\"],[\"## 1.13.1\\n\\n### Fixes\\n\\n- [#6536](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6536) [`1bbd6cab3`](https:...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 1.12.0\\n\\n### Features\\n\\n- [#6427](https:\\u002f\\u002fgithub.c...\"],[\"## 1.11.0\\n\\n### Features\\n\\n- [#6099](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6099) [`d84209703`](htt...\"],[\"- Updated dependencies [[`6204ccac5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f6204ccac5967763e0e...\"],[\"- @gradio\\u002fgallery@0.4.5\\n  - @gradio\\u002frow@0.1.0\\n  - @gradio\\u002fvideo@0.1.4\\n  - @gradio\\u002fannotatedimage@0.3...\"],[\"## 1.10.2\\n\\n### Patch Changes...\"],[\"- Updated dependencies [[`4b1011bab`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f4b1011bab03c0b6a09...\"],[\"- @gradio\\u002fupload@0.3.3\\n  - @gradio\\u002fvideo@0.1.3\\n  - @gradio\\u002fannotatedimage@0.3.3\\n  - @gradio\\u002fbutton@0...\"],[\"## 1.10.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`92278729e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"- [#6266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6266) [`e32bac894`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6236](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6236) [`6bce259c5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 1.9.2\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.co...\"],[\"- [#6124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6124) [`a7435ba9e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6118](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6118) [`88bccfdba`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6069](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6069) [`bf127e124`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 1.9.0-beta.2\\n\\n### Features...\"],[\"- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6107](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6107) [`9a40de7bf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5990](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5990) [`85056de5c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6065](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6065) [`7d07001e8`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 1.9.0-beta.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`174b73619`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 1.9.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.co...\"],[\"## 1.7.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`796145e2c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n### Fixes\\n\\n- [#5794](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"- Updated dependencies [[`abb5e9df4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002fabb5e9df47989b2c56...\"],[\"- @gradio\\u002ffile@0.1.5\\n  - @gradio\\u002fform@0.0.6\\n  - @gradio\\u002fhighlightedtext@0.3.2\\n  - @gradio\\u002fimage@0.3....\"],[\"## 1.6.2\\n\\n### Features\\n\\n- [#5721](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5721) [`84e03fe50`](http...\"],[\"## 1.6.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`ee8eec1e5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 1.5.4\\n\\n### Features\\n\\n- [#5514](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5514) [`52f783175`](http...\"],[\"## 1.5.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`a0cc9ac9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.5.0\\n\\n### Features\\n\\n- [#5505](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5505) [`9ee20f49`](https...\"],[\"## 1.4.2\\n\\n### Fixes\\n\\n- [#5447](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5447) [`7a4a89e5`](https:\\u002f\\u002f...\"],[\"- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002fafac0006337ce2840cf...\"],[\"- @gradio\\u002fimage@0.2.2\\n  - @gradio\\u002fjson@0.0.5\\n  - @gradio\\u002flabel@0.1.2\\n  - @gradio\\u002fmodel3d@0.2.1\\n  - @...\"],[\"## 1.4.0\\n\\n### Features\\n\\n- [#5267](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5267) [`119c8343`](https...\"],[\"## 1.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5f25eb68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"```\\n\\nThanks [@hannahblair](https:\\u002f\\u002fgithub.com\\u002fhannahblair)!\\n\\n### Features...\"],[\"- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5264](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5264) [`46a2b600`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5285](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5285) [`cdfd4217`](https:\\u002f\\u002fgithub.com...\"],[\"## 1.2.0\\n\\n### Highlights\\n\\n#### Client.predict will now return the final output for streaming endpoin...\"],[\"- [#5025](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5025) [`6693660a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5005](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5005) [`f5539c76`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 1.1.0\\n\\n### Features\\n\\n- [#4995](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4995) [`3f8c210b`](https...\"],[\"Gradio Demo: kitchen_sink\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"```\\nimport os\\nimport json\\n\\nimport numpy as np\\n\\nimport gradio as gr\\n\\nCHOICES = [\\\"foo\\\", \\\"bar\\\", \\\"baz\\\"]\\n...\"],[\"def fn(\\n    text1,\\n    text2,\\n    num,\\n    slider1,\\n    slider2,\\n    single_checkbox,\\n    checkboxes...\"],[\"(\\\".\\\", \\\"punc\\\"),\\n        ]\\n        + [(f\\\"test {x}\\\", f\\\"test {x}\\\") for x in range(10)],  # HighlightedTe...\"],[\"demo = gr.Interface(\\n    fn,\\n    inputs=[\\n        gr.Textbox(value=\\\"Lorem ipsum\\\", label=\\\"Textbox\\\"),\\n...\"],[\"gr.Video(label=\\\"Video\\\"),\\n        gr.HighlightedText(\\n            label=\\\"HighlightedText\\\", color_map=...\"],[\"]\\n    ]\\n    * 3,\\n    title=\\\"Kitchen Sink\\\",\\n    description=\\\"Try out all the components!\\\",\\n    articl...\"],[\"`@gradio\\u002futils`\\n\\nGeneral functions for handling events in Gradio Svelte components\\n\\n\\n```javascript\\ne...\"],[\"Gradio Demo: audio_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Bl...\"],[\"Gradio Demo: musical_instrument_identification\\n### This demo identifies musical instruments from an ...\"],[\"```\\nimport gradio as gr\\nimport torch\\nimport torchaudio\\nfrom timeit import default_timer as timer\\nfro...\"],[\"def predict(audio_path):\\n    start_time = timer()\\n    wavform, sample_rate = torchaudio.load(audio_p...\"],[\"Gradio Demo: change_vs_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the de...\"],[\"```\\nimport os\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    set_button = gr.Button(\\\"Set Values\\\"...\"],[\"with gr.Column(min_width=200):\\n            gr.Markdown(\\\"# ON:CHANGE\\\")\\n            text_ch = gr.Textb...\"],[\"counter = gr.Number(label=\\\"Change counter\\\")\\n\\n    lion = os.path.join(os.path.abspath(''), \\\"files\\u002flio...\"],[\"text.change(lambda x,y:(x,y+1), [text, counter], [text_ch, counter])\\n    num.change(lambda x,y:(x, y...\"],[\"text_ch.change(lambda x:x, text_ch, text_ch2)\\n    num_ch.change(lambda x:x, num_ch, num_ch2)\\n    sli...\"],[\"@gradio\\u002fchatbot\\n\\n## 0.5.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`846d52d`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.5.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.5.1\\n\\n### Fixes\\n\\n- [#6574](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6574) [`2b625ad`](https:\\u002f\\u002fg...\"],[\"## 0.4.8\\n\\n### Features\\n\\n- [#6296](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6296) [`46f13f496`](http...\"],[\"## 0.4.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.4.0-beta.7\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.5.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e4a307ed6`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Fixes\\n\\n- [#5755](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5755) [`e842a561a`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#5604](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5604) [`faad01f8e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`31996c99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"### Fixes\\n\\n- [#5242](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5242) [`2b397791`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5125](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5125) [`80be7a1c`](https...\"],[\"@gradio\\u002fbutton\\n\\n## 0.2.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.2.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.2.0-beta.6\\n\\n### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n  - @gradio\\u002fupload@0.3.2\\n\\n## 0...\"],[\"## 0.1.1\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5080](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5080) [`37caa2e0`](https...\"],[\"Gradio Demo: dataframe_colorful\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport pandas as pd \\nimport ...\"],[\"How to Style the Gradio Dataframe\\n\\nTags: DATAFRAME, STYLE, COLOR\\n\\n## Introduction\\n\\nData visualizatio...\"],[\"# Applying style to highlight the maximum value in each row\\nstyler = df.style.highlight_max(color = ...\"],[\"```\\n\\nThe Styler class can be used to apply conditional formatting and styling to dataframes, making ...\"],[\"```\\n\\nHere's how it looks:\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"```\\n\\nIn this script, we define a custom function highlight_cols that changes the text color to purpl...\"],[\"```\\n\\nIn this script, the format method of the Styler object is used to set the precision of numbers ...\"],[\"Image Classification with Vision Transformers\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlab...\"],[\"Besides the import statement, it only takes a single line of Python to load and launch the demo.\\n\\nWe...\"],[\"```\\n\\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate ou...\"],[\"Gradio Demo: text_generation\\n### This text generation demo takes in input text and returns generated...\"],[\"Gradio Demo: diffusers_with_batching\\n\\n\\n```\\n!pip install -q gradio torch transformers diffusers\\n```\\n\\n...\"],[\"Gradio Demo: depth_estimation\\n### A demo for predicting the depth of an image and generating a 3D mo...\"],[\"```\\nimport gradio as gr\\nfrom transformers import DPTFeatureExtractor, DPTForDepthEstimation\\nimport t...\"],[\"def create_3d_obj(rgb_image, depth_image, image_path, depth=10):\\n    depth_o3d = o3d.geometry.Image(...\"],[\"voxel_size = max(mesh_raw.get_max_bound() - mesh_raw.get_min_bound()) \\u002f 256\\n    print(f'voxel_size =...\"],[\"Using Gradio for Tabular Data Science Workflows\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fsciki...\"],[\"# we will give our dataframe as example\\ndf = datasets.load_dataset(\\\"merve\\u002fsupersoaker-failures\\\")\\ndf ...\"],[\"```\\n\\nLet's break down above code.\\n\\n- `fn`: the inference function that takes input dataframe and ret...\"],[\"def plot(df):\\n  plt.scatter(df.measurement_13, df.measurement_15, c = df.loading,alpha=0.5)\\n  plt.sa...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fgradio-analysis-dashboard-minimal\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nWe will use the sam...\"],[\"Controlling Layout\\n\\nBy default, Components in Blocks are arranged vertically. Let's take a look at h...\"],[\"```\\n\\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn'...\"],[\"```\\n\\nIn this example, the Column layout component is given a height of 100% of the viewport height (...\"],[\"$code_variable_outputs\\n$demo_variable_outputs\\n\\n## Defining and Rendering Components Separately\\n\\nIn s...\"],[\"Build a Custom Multimodal Chatbot - Part 1\\n\\nThis is the first in a two part series where we build a ...\"],[\"```\\n\\nAnd we're ready to go!\\n\\nTip: Make sure to modify the `Author` key in the `pyproject.toml` file....\"],[\"```\\n\\n\\nTip: The `data_model`s are implemented using `Pydantic V2`. Read the documentation [here](http...\"],[\"```\\n\\nBefore we wrap up with the backend code, let's modify the `example_inputs` method to return a v...\"],[\"```\\n\\nWe need to normalize each message to make sure each file has a proper URL to fetch its contents...\"],[\"```\\n\\nNow for the fun part, actually rendering the text and files in the same message!\\n\\nYou should se...\"],[\"```\\n\\nWe will modify this code to always display the text message and then loop through the files and...\"],[\"```\\n\\nWe did it! ğŸ‰\\n\\n## Part 4 - The demo\\n\\nFor this tutorial, let's keep the demo simple and just disp...\"],[\"```\\n\\n\\nTip: Change the filepaths so that they correspond to files on your machine. Also, if you are r...\"],[\"Creating a Real-Time Dashboard from BigQuery Data\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Google BigQuery...\"],[\"1. First, log in to your Google Cloud account and go to the Google Cloud Console (https:\\u002f\\u002fconsole.cl...\"],[\"```\\n\\n## Using the BigQuery Client\\n\\nOnce you have the credentials, you will need to use the BigQuery ...\"],[\"```\\n\\n## Building the Real-Time Dashboard\\n\\nOnce you have a function to query the data, you can use th...\"],[\"Gradio Demo: sepia_filter\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\nimport gradio a...\"],[\"`@gradio\\u002fimage`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseImageUploader, BaseStaticImage, Webcam, BaseExample ...\"],[\"`@gradio\\u002faudio`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseStaticAudio, BaseInteractiveAudio, BasePlayer, BaseE...\"],[\"```\\n\\nBasePlayer:\\n```javascript\\n\\texport let value: null | { name: string; data: string } = null;\\n\\texp...\"],[\"Gradio Demo: blocks_multiple_event_triggers\\n\\n\\n```\\n!pip install -q gradio plotly pypistats\\n```\\n\\n\\n```\\n...\"],[\"!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides\\u002f1)getting_start...\"],[\"[å®˜ç½‘](https:\\u002f\\u002fgradio.app)\\n| [æ–‡æ¡£](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f)\\n| [æŒ‡å—](https:\\u002f\\u002fgradio.app\\u002fguides\\u002f)\\n| [å¼€å§‹](...\"],[\"Gradioè®©ä½ å¯ä»¥**ç”¨Pythonæ„å»ºæ¼”ç¤ºå¹¶åˆ†äº«å®ƒä»¬**ï¼Œè€Œä¸”é€šå¸¸åªéœ€å‡ è¡Œä»£ç ï¼ä¸‹é¢è®©æˆ‘ä»¬å¼€å§‹å§ã€‚\\n\\n#### Hello, World\\n\\nè¦ç”¨Gradioè¿è¡Œ\\\"Hello World\\\"ç¤ºä¾‹ï¼Œéœ€è¦...\"],[\"```\\n\\n2\\\\. ç”¨Pythonè„šæœ¬æˆ–åœ¨Jupyter Notebookä¸­è¿è¡Œä¸‹é¢çš„ä»£ç  ï¼ˆæˆ–è€…ä½¿ç”¨ [Google Colab](https:\\u002f\\u002fcolab.research.google.com\\u002f...\"],[\"```\\n\\n3\\\\. ä¸‹é¢çš„æ¼”ç¤ºä¼šè‡ªåŠ¨å‡ºç°åœ¨Jupyter Notebookä¸­ï¼Œå¦‚æœä½¿ç”¨è„šæœ¬è¿è¡Œåˆ™ä¼šåœ¨æµè§ˆå™¨[http:\\u002f\\u002flocalhost:7860](http:\\u002f\\u002flocalhost:7860)å¼¹å‡º...\"],[\"```python\\nimport gradio as gr\\n\\ndef greet(name):\\n    return \\\"Hello \\\" + name + \\\"!\\\"\\n\\ndemo = gr.Interfac...\"],[\"```\\n\\n![`hello_world_2` demo](..\\u002f..\\u002fdemo\\u002fhello_world_2\\u002fscreenshot.gif)\\n\\n#### å¤šè¾“å…¥å’Œè¾“å‡ºç»„ä»¶\\n\\nå‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„å‡½æ•°ï¼Œæœ‰...\"],[\"```\\n\\n![`hello_world_3` demo](..\\u002f..\\u002fdemo\\u002fhello_world_3\\u002fscreenshot.gif)\\n\\næ‚¨åªéœ€å°†ç»„ä»¶åŒ…è£…åœ¨åˆ—è¡¨ä¸­ã€‚è¾“å…¥åˆ—è¡¨`inputs`ä¸­çš„æ¯ä¸ª...\"],[\"```\\n\\n![`sepia_filter` demo](..\\u002f..\\u002fdemo\\u002fsepia_filter\\u002fscreenshot.gif)\\n\\nå½“ä½¿ç”¨`Image`ç»„ä»¶ä½œä¸ºè¾“å…¥æ—¶ï¼Œæ‚¨çš„å‡½æ•°å°†æ¥æ”¶ä¸€ä¸ªå½¢çŠ¶ä¸º ...\"],[\"```\\n\\nè¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬çš„è¾“å…¥ `Image` ç»„ä»¶å¸¦æœ‰ä¸€ä¸ªç¼–è¾‘æŒ‰é’® ğŸ–‰ï¼Œå®ƒå…è®¸è£å‰ªå’Œæ”¾å¤§å›¾åƒã€‚ä»¥è¿™ç§æ–¹å¼æ“ä½œå›¾åƒå¯ä»¥å¸®åŠ©æ­ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„åè§æˆ–éšè—çš„ç¼ºé™·ï¼\\n\\næ‚¨å¯ä»¥åœ¨[Gradioæ–‡æ¡£](htt...\"],[\"```\\n\\n![`hello_blocks` demo](..\\u002f..\\u002fdemo\\u002fhello_blocks\\u002fscreenshot.gif)\\n\\næ³¨æ„äº‹é¡¹ï¼š\\n\\n- `Blocks` ç”± `with` å­å¥ç»„æˆ...\"],[\"```\\n\\n![`blocks_flipper` demo](..\\u002f..\\u002fdemo\\u002fblocks_flipper\\u002fscreenshot.gif)\\n\\nè¿˜æœ‰å¾ˆå¤šäº‹æƒ…å¯ä»¥åšï¼æˆ‘ä»¬å°†åœ¨[ä½¿ç”¨blocksæ„å»º](...\"],[\"## åè®®\\n\\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the r...\"],[\"More on Examples\\n\\nIn the [previous Guide](\\u002fmain\\u002fguides\\u002fthe-interface-class), we discussed how to pro...\"],[\"```\\n\\nThis can be helpful when browsing flagged data. Simply point to the flagged directory and the `...\"],[\"Backend Testing Guidelines\\n\\n- All the tests should test Backend functionalities. Frontend functional...\"],[\"Gradio and ONNX on Hugging Face\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fonnx\\u002fEfficientNet-Lit...\"],[\"Get started [here](https:\\u002f\\u002fgradio.app\\u002fgetting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces...\"],[\"## Setting up a Gradio Demo for EfficientNet-Lite4\\n\\nEfficientNet-Lite 4 is the largest variant and m...\"],[\"# sets image file dimensions to 224x224 by resizing and cropping image from center\\ndef pre_process_e...\"],[\"def inference(img):\\n  img = cv2.imread(img)\\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\\n  img = pr...\"],[\"```\\n\\n## How to contribute Gradio demos on HF spaces using ONNX models\\n\\n- Add model to the [onnx mode...\"],[\"# ä½¿ç”¨ Gradio è¿›è¡Œè¡¨æ ¼æ•°æ®ç§‘å­¦å·¥ä½œæµ\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fscikit-learn\\u002fgradio-skops-int...\"],[\"inputs = [gr.Dataframe(row_count = (2, \\\"dynamic\\\"), col_count=(4,\\\"dynamic\\\"), label=\\\"Input Data\\\", inte...\"],[\"```\\n\\nè®©æˆ‘ä»¬æ¥è§£æä¸Šè¿°ä»£ç ã€‚\\n\\n- `fn`ï¼šæ¨ç†å‡½æ•°ï¼Œæ¥å—è¾“å…¥æ•°æ®å¸§å¹¶è¿”å›é¢„æµ‹ç»“æœã€‚\\n- `inputs`ï¼šæˆ‘ä»¬ä½¿ç”¨ `Dataframe` ç»„ä»¶ä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬å°†è¾“å…¥å®šä¹‰ä¸ºå…·æœ‰ 2 è¡Œ 4 åˆ—çš„...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fgradio-analysis-dashboard-minimal\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\næˆ‘ä»¬å°†ä½¿ç”¨ä¸è®­ç»ƒæ¨¡å‹ç›¸åŒçš„æ•°æ®é›†ï¼Œä½†è¿™...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fgradio-skops-integration\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nä½¿ç”¨ `skops` å°† `sklearn` æ¨¡å‹æ¨é€åˆ°...\"],[\"his translation demo takes in the text, source and target languages, and returns the translation. It...\"],[\"Gradio Demo: model3D\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\n...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport os\\n\\n\\ndef load_mesh(mesh_file_name):\\n    return mesh_file_name\\n\\n...\"],[\"@gradio\\u002flite\\n\\n## 0.4.4\\n\\n## 0.4.4-beta.0\\n\\n### Features\\n\\n- [#6147](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.1\\n\\n### Fixes\\n\\n- [#5988](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5988) [`bea931c31`](https:\\u002f...\"],[\"## 0.3.1\\n\\n### Features\\n\\n- [#5226](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5226) [`64039707`](https...\"],[\"- [#4826](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4826) [`f0150c62`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4785](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4785) [`da0e9447`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- [#4731](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4731) [`f9171288`](...\"],[\"@gradio\\u002fvideo\\n\\n## 0.2.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:...\"],[\"## 0.1.9\\n\\n### Fixes\\n\\n- [#6566](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6566) [`d548202`](https:\\u002f\\u002fg...\"],[\"## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6204ccac5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.2\\n\\n### Fixes\\n\\n- [#6234](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6234) [`aaa55ce85`](https:\\u002f...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.9\\n\\n### Features...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.0-beta.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`174b73619`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.0.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.0.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`44ac8ad0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: outbreak_forecast\\n### Generate a plot based on 5 inputs.\\n        \\n\\n\\n```\\n!pip install -q...\"],[\"```\\nimport altair\\n\\nimport gradio as gr\\nfrom math import sqrt\\nimport matplotlib.pyplot as plt\\nimport ...\"],[\"inputs = [\\n    gr.Dropdown([\\\"Matplotlib\\\", \\\"Plotly\\\", \\\"Altair\\\"], label=\\\"Plot Type\\\"),\\n    gr.Slider(1, ...\"],[\"@gradio\\u002ffile\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67ddd40`](https:...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#6511](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6511) [`71f1a1f99`](http...\"],[\"## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.2.0-beta.8\\n\\n### Features...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`796145e2c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependenc...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`c57f1b75e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5265](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5265) [`06982212`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5253](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5253) [`ddac7e4d`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.0.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`61129052`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: blocks_flashcards\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport random\\n\\nimport gradio as gr\\n\\ndemo = gr.Blocks()\\n\\nwith demo:\\n    gr.Markdown(\\n        \\\"Loa...\"],[\"def flip_card(card):\\n        return card[1], gr.Column(visible=True)\\n\\n    flip_btn.click(flip_card, ...\"],[\"`@gradio\\u002fjson`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseJSON } from \\\"@gradio\\u002fjson\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nBaseJSON\\n`...\"],[\"ä½¿ç”¨Gradio JavaScriptå®¢æˆ·ç«¯å¿«é€Ÿå…¥é—¨\\n\\nTags: CLIENT, API, SPACES\\n\\nGradio JavaScriptå®¢æˆ·ç«¯ä½¿å¾—ä½¿ç”¨ä»»ä½•Gradioåº”ç”¨ä½œä¸ºAPIéå¸¸ç®€å•ã€‚ä¾‹...\"],[\"```\\n\\nGradioå®¢æˆ·ç«¯é€‚ç”¨äºä»»ä½•æ‰˜ç®¡çš„Gradioåº”ç”¨ï¼Œæ— è®ºæ˜¯å›¾åƒç”Ÿæˆå™¨ã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ã€æœ‰çŠ¶æ€çš„èŠå¤©æœºå™¨äººã€ç¨æ”¶è®¡ç®—å™¨è¿˜æ˜¯å…¶ä»–ä»»ä½•åº”ç”¨ï¼Gradioå®¢æˆ·ç«¯é€šå¸¸ä¸æ‰˜ç®¡åœ¨[Hugging Face...\"],[\"```\\n\\n## ä¸ºç§äººä½¿ç”¨å¤åˆ¶ä¸€ä¸ªSpace\\n\\nè™½ç„¶æ‚¨å¯ä»¥å°†ä»»ä½•å…¬å…±Spaceç”¨ä½œAPIï¼Œä½†æ˜¯å¦‚æœæ‚¨å‘å‡ºçš„è¯·æ±‚è¿‡å¤šï¼ŒHugging Faceå¯èƒ½ä¼šå¯¹æ‚¨è¿›è¡Œé€Ÿç‡é™åˆ¶ã€‚ä¸ºäº†æ— é™åˆ¶ä½¿ç”¨Spaceï¼Œåªéœ€å¤åˆ¶S...\"],[\"```\\n\\nå¦‚æœæ‚¨ä¹‹å‰å¤åˆ¶è¿‡ä¸€ä¸ªSpaceï¼Œåˆ™é‡æ–°è¿è¡Œ`duplicate`ä¸ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„Spaceã€‚è€Œæ˜¯å®¢æˆ·ç«¯å°†è¿æ¥åˆ°å…ˆå‰åˆ›å»ºçš„Spaceã€‚å› æ­¤ï¼Œå¯ä»¥å®‰å…¨åœ°å¤šæ¬¡ä½¿ç”¨ç›¸åŒçš„Spaceé‡æ–°è¿è¡Œ`dupl...\"],[\"```\\n\\n## æ£€æŸ¥APIç«¯ç‚¹\\n\\nä¸€æ—¦è¿æ¥åˆ°Gradioåº”ç”¨ç¨‹åºï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨`client`çš„`view_api`æ–¹æ³•æ¥æŸ¥çœ‹å¯ç”¨çš„APIç«¯ç‚¹ã€‚\\n\\nå¯¹äºWhisper Spaceï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š\\n\\n...\"],[\"```\\n\\nè¿™å‘Šè¯‰æˆ‘ä»¬è¯¥Spaceä¸­æœ‰1ä¸ªAPIç«¯ç‚¹ï¼Œå¹¶æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨APIç«¯ç‚¹è¿›è¡Œé¢„æµ‹ï¼šæˆ‘ä»¬åº”è¯¥è°ƒç”¨`.predict()`æ–¹æ³•ï¼ˆä¸‹é¢å°†è¿›è¡Œæ›´å¤šæ¢ç´¢ï¼‰ï¼Œå¹¶æä¾›ç±»å‹ä¸º`string`çš„å‚æ•°`input_...\"],[\"```\\n\\nå¯¹äºæŸäº›è¾“å…¥ï¼Œä¾‹å¦‚å›¾åƒï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‰€éœ€è¦çš„æ–¹ä¾¿ç¨‹åº¦ä¼ å…¥`Buffer`ã€`Blob`æˆ–`File`ã€‚åœ¨Node.jsä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Buffer`æˆ–`Blob`ï¼›åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Bl...\"],[\"```\\n\\n## çŠ¶æ€\\n\\näº‹ä»¶æ¥å£è¿˜å¯ä»¥é€šè¿‡ç›‘å¬`\\\"status\\\"`äº‹ä»¶æ¥è·å–è¿è¡Œä½œä¸šçš„çŠ¶æ€ã€‚è¿™å°†è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹å±æ€§ï¼š`status`ï¼ˆå½“å‰ä½œä¸šçš„äººç±»å¯è¯»çŠ¶æ€ï¼Œ`\\\"pending\\\" | \\\"g...\"],[\"```\\n\\nå¦‚æœç¬¬ä¸€ä¸ªä½œä¸šå·²ç»å¼€å§‹å¤„ç†ï¼Œé‚£ä¹ˆå®ƒå°†ä¸ä¼šè¢«å–æ¶ˆï¼Œä½†å®¢æˆ·ç«¯å°†ä¸å†ç›‘å¬æ›´æ–°ï¼ˆä¸¢å¼ƒè¯¥ä½œä¸šï¼‰ã€‚å¦‚æœç¬¬äºŒä¸ªä½œä¸šå°šæœªå¯åŠ¨ï¼Œå®ƒå°†è¢«æˆåŠŸå–æ¶ˆå¹¶ä»é˜Ÿåˆ—ä¸­ç§»é™¤ã€‚\\n\\n## ç”Ÿæˆå™¨ç«¯ç‚¹\\n\\næŸäº›Gradio APIç«¯...\"],[\"@gradio\\u002ftootils\\n\\n## 0.1.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fstatustracker@0.3.1\\n\\n## 0.1.1\\n\\n#...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.0-beta.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`14fc612d8`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Quickstart\\n\\nGradio is an open-source Python package that allows you to quickly **build** a demo or w...\"],[\"```\\n\\n\\nTip: it is best to install Gradio in a virtual environment. Detailed installation instructions...\"],[\"**Understanding the `Interface` Class**\\n\\nYou'll notice that in order to make your first demo, you cr...\"],[\"We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https:\\u002f\\u002fwww.gradio....\"],[\"```\\n\\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, so...\"],[\"### The Gradio Python & JavaScript Ecosystem\\n\\nThat's the gist of the core `gradio` Python library, b...\"],[\"å¦‚ä½•åˆ›å»ºä¸€ä¸ªèŠå¤©æœºå™¨äºº\\n\\nTags: NLP, TEXT, CHAT\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchatbot_stre...\"],[\"## ç®€å•èŠå¤©æœºå™¨äººæ¼”ç¤º\\n\\nè®©æˆ‘ä»¬ä»é‡æ–°åˆ›å»ºä¸Šé¢çš„ç®€å•æ¼”ç¤ºå¼€å§‹ã€‚æ­£å¦‚æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°çš„ï¼Œæˆ‘ä»¬çš„æœºå™¨äººåªæ˜¯éšæœºå¯¹ä»»ä½•è¾“å…¥å›å¤ \\\" ä½ å¥½å—ï¼Ÿ\\\"ã€\\\" æˆ‘çˆ±ä½  \\\" æˆ– \\\" æˆ‘éå¸¸é¥¿ \\\"ã€‚è¿™æ˜¯ä½¿ç”¨ Gradio...\"],[\"$code_chatbot_streaming\\n\\nå½“ç”¨æˆ·æäº¤ä»–ä»¬çš„æ¶ˆæ¯æ—¶ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°æˆ‘ä»¬ç°åœ¨ä½¿ç”¨ `.then()` ä¸ä¸‰ä¸ªäº‹ä»¶äº‹ä»¶ _é“¾_ èµ·æ¥ï¼š\\n\\n1. ç¬¬ä¸€ä¸ªæ–¹æ³• `user()` ç”¨ç”¨æˆ·æ¶ˆæ¯æ›´...\"],[\"## æ·»åŠ  Markdownã€å›¾ç‰‡ã€éŸ³é¢‘æˆ–è§†é¢‘\\n\\n`gr.Chatbot` ç»„ä»¶æ”¯æŒåŒ…å«åŠ ç²—ã€æ–œä½“å’Œä»£ç ç­‰ä¸€éƒ¨åˆ† Markdown åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œä»¥ç²—ä½“å›å¤ç”¨æˆ·çš„æ¶ˆæ¯ï¼Œç±»ä¼¼äº **...\"],[\"```\\n\\næ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥å¤„ç†å›¾ç‰‡ã€éŸ³é¢‘å’Œè§†é¢‘ç­‰åª’ä½“æ–‡ä»¶ã€‚è¦ä¼ é€’åª’ä½“æ–‡ä»¶ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ–‡ä»¶ä½œä¸ºä¸¤ä¸ªå­—ç¬¦ä¸²çš„å…ƒç»„ä¼ é€’ï¼Œå¦‚`(filepath, alt_text)` æ‰€ç¤ºã€‚`alt_text` æ˜¯å¯é€‰çš„ï¼Œå› ...\"],[\"Gradio Demo: chatbot_streaming\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ra...\"],[\"Gradio Demo: upload_button\\n### A simple demo showcasing the upload button used with its `upload` eve...\"],[\"Gradio Demo: button_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"Gradio Demo: image_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"`@gradio\\u002fcheckbox`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseCheckbox } from \\\"@gradio\\u002fcheckbox\\\";\\n\\u003c\\u002fscript\\u003e\\n...\"],[\"Gradio Demo: dropdown_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith g...\"],[\"Gradio Demo: filter_records\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef filter...\"],[\"@gradio\\u002fdropdown\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.2\\n\\n### Fixes\\n\\n- [#6425](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6425) [`b3ba17dd1`](https:\\u002f...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.1\\n\\n### Fixes\\n\\n- [#5525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5525) [`21f1db40`](https:\\u002f\\u002f...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.0.2\\n\\n### Fixes\\n\\n- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002f...\"],[\"Gradio Demo: autocomplete\\n### This text generation demo works like autocomplete. There's only one te...\"],[\"Gradio Demo: generate_english_german\\n\\n\\n```\\n!pip install -q gradio transformers torch\\n```\\n\\n\\n```\\nimpor...\"],[\"Gradio Demo: gallery_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\n...\"],[\"Gradio Demo: theme_builder\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.th...\"],[\"Gradio Demo: blocks_inputs\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: html_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"],[\"Gradio Demo: checkboxgroup_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nw...\"],[\"Gradio Demo: state_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"`@gradio\\u002ftextbox`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseTextbox, BaseExample } from \\\"@gradio\\u002ftextbox\\\";\\n...\"],[\"Gradio Demo: clear_components\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"```\\nimport gradio as gr\\nfrom datetime import datetime\\nimport os\\nimport random\\nimport string\\nimport p...\"],[\"images = [\\n    \\\"https:\\u002f\\u002fimages.unsplash.com\\u002fphoto-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=Mnw...\"],[\"\\\"index\\\": 5,\\n        \\\"word\\\": \\\"Pakistani\\\",\\n        \\\"start\\\": 22,\\n        \\\"end\\\": 31,\\n    },\\n]\\nhighlighte...\"],[\"highlighted_text = \\\"Does Chicago have any Pakistani restaurants\\\"\\n\\n\\ndef random_model3d():\\n    model_3...\"],[\"components = [\\n    gr.Textbox(value=lambda: datetime.now(), label=\\\"Current Time\\\"),\\n    gr.Number(val...\"],[\"),\\n    gr.ColorPicker(value=lambda: random.choice([\\\"#000000\\\", \\\"#ff0000\\\", \\\"#0000FF\\\"])),\\n    gr.Label(...\"],[\"def evaluate_values(*args):\\n    are_false = []\\n    for a in args:\\n        if isinstance(a, (pd.DataF...\"],[\"å¿«é€Ÿå¼€å§‹\\n\\n**å…ˆå†³æ¡ä»¶**ï¼šGradio éœ€è¦ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå°±æ˜¯è¿™æ ·ï¼\\n\\n## Gradio æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ\\n\\nä¸ä»–äººåˆ†äº«æ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€API æˆ–æ•°æ®ç§‘å­¦æµç¨‹çš„*æœ€ä½³æ–¹å¼ä¹‹ä¸€...\"],[\"```\\n\\n2. å°†ä¸‹é¢çš„ä»£ç ä½œä¸º Python è„šæœ¬è¿è¡Œæˆ–åœ¨ Jupyter Notebook ä¸­è¿è¡Œï¼ˆæˆ–è€… [Google Colab](https:\\u002f\\u002fcolab.research.google....\"],[\"```\\n\\næ³¨æ„ï¼šæ‚¨ä¹Ÿå¯ä»¥è¿è¡Œ `python app.py`ï¼Œä½†å®ƒä¸ä¼šæä¾›è‡ªåŠ¨é‡æ–°åŠ è½½æœºåˆ¶ã€‚\\n\\n## `Interface` ç±»\\n\\næ‚¨ä¼šæ³¨æ„åˆ°ä¸ºäº†åˆ›å»ºæ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª `gr.Interface`...\"],[\"$code_hello_world_2\\n$demo_hello_world_2\\n\\n## å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºç»„ä»¶\\n\\nå‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„å‡½æ•°ï¼Œå…·æœ‰å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ¥å—å­—ç¬¦ä¸²ã€å¸ƒ...\"],[\"```\\n\\nè¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬çš„è¾“å…¥ `Image` ç»„ä»¶é™„å¸¦æœ‰ä¸€ä¸ªç¼–è¾‘æŒ‰é’®ğŸ–‰ï¼Œå…è®¸è£å‰ªå’Œç¼©æ”¾å›¾åƒã€‚é€šè¿‡è¿™ç§æ–¹å¼æ“ä½œå›¾åƒå¯ä»¥å¸®åŠ©æ­ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„åè§æˆ–éšè—çš„ç¼ºé™·ï¼\\n\\næ‚¨å¯ä»¥åœ¨[Gradio æ–‡æ¡£](ht...\"],[\"è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œæ­¤å¤„çš„ API ä¸ `Interface` ä¸åŒã€‚\\n\\n$code_hello_blocks\\n$demo_hello_blocks\\n\\néœ€è¦æ³¨æ„çš„äº‹é¡¹ï¼š\\n\\n- `Block...\"],[\"@gradio\\u002fstorybook\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6451](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6451)...\"],[\"## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5966](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5966) [`9cad2127b...\"],[\"### Fixes\\n\\n- [#6065](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6065) [`7d07001e8`](https:\\u002f\\u002fgithub.co...\"],[\"Sharing Your App\\n\\nHow to share your Gradio app:\\n\\n1. [Sharing demos with the share parameter](#sharin...\"],[\"```\\n\\nThis generates a public, shareable link that you can send to anybody! When you send this link, ...\"],[\"After you have [created a free Hugging Face account](https:\\u002f\\u002fhuggingface.co\\u002fjoin), you have two meth...\"],[\"To embed with Web Components:\\n\\n1. Import the gradio JS library into into your site by adding the scr...\"],[\"```\\n\\n2. Add\\n\\n```html\\n\\u003cgradio-app src=\\\"https:\\u002f\\u002f$your_space_host.hf.space\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n```\\n\\nelement ...\"],[\"```\\n\\n\\u003cscript\\u003e\\nfetch(\\\"https:\\u002f\\u002fpypi.org\\u002fpypi\\u002fgradio\\u002fjson\\\"\\n).then(r =\\u003e r.json()\\n).then(obj =\\u003e {\\n    let...\"],[\"You can also customize the appearance and behavior of your web component with attributes that you pa...\"],[\"```\\n\\nHere's another example of how to use the `render` event. An event listener is used to capture t...\"],[\"```\\n\\nAgain, you can find the `src=` attribute to your Space's embed URL, which you can find in the \\\"...\"],[\"```\\n\\nThis will add and document the endpoint `\\u002fapi\\u002faddition\\u002f` to the automatically generated API pag...\"],[\"```\\n\\nFor authentication to work properly, third party cookies must be enabled in your browser.\\nThis ...\"],[\"def hello(profile: gr.OAuthProfile | None) -\\u003e str:\\n    if profile is None:\\n        return \\\"I don't k...\"],[\"```\\n\\nWhen the user clicks on the login button, they get redirected in a new page to authorize your S...\"],[\"```\\n\\nNote: if your function is called directly instead of through the UI (this happens, for\\nexample,...\"],[\"- **Cached examples created by Gradio.** These are files that are created by Gradio as part of cachi...\"],[\"`@gradio\\u002ftooltip`\\n\\n```javascript\\nimport { Tooltip } from \\\"@gradio\\u002ftooltip\\\";\\n```\\n\\n```javascript\\n\\texpo...\"],[\"enerate a plot based on 5 inputs....\"],[\"Gradio Demo: stock_forecast\\n\\n\\n```\\n!pip install -q gradio numpy matplotlib\\n```\\n\\n\\n```\\nimport matplotli...\"],[\"simple dashboard showing pypi stats for python libraries. Updates on load, and has no buttons!...\"],[\"Gradio Demo: neon-tts-plugin-coqui\\n### This  demo converts text to speech in 14 languages.\\n        \\n...\"],[\"@gradio\\u002ftooltip\\n\\n## 0.1.0\\n\\n## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"`@gradio\\u002fcode`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseCode, BaseCopy, BaseDownload, BaseWidget, BaseExam...\"],[\"Gradio Demo: gif_maker\\n\\n\\n```\\n!pip install -q gradio opencv-python\\n```\\n\\n\\n```\\nimport cv2\\nimport gradio...\"],[\"Gradio Demo: file_explorer_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading fi...\"],[\"```\\nimport gradio as gr\\nfrom pathlib import Path\\n\\nbase_root = Path(__file__).parent.resolve()\\n\\nwith ...\"],[\"with gr.Row():\\n        a = gr.Textbox(elem_id=\\\"input-box\\\")\\n        a.change(lambda x: x, inputs=[a])...\"],[\"Gradio Demo: stream_audio\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport numpy a...\"],[\"Gradio Demo: gallery_selections\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport n...\"],[\"Gradio Demo: stream_asr\\n\\n\\n```\\n!pip install -q gradio torch torchaudio transformers\\n```\\n\\n\\n```\\nimport ...\"],[\"his demo takes in 12 inputs from the user in dropdowns and sliders and predicts income. It also has ...\"],[\"`@gradio\\u002flabel`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseLabel } from \\\"@gradio\\u002flabel\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nBaseLab...\"],[\"alculate taxes using Textbox, Radio, and Dataframe components...\"],[\"Gradio Demo: json_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"]],\"hovertemplate\":\"source=gradio\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"gradio, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"gradio, circle\",\"showlegend\":true,\"x\":[11.8679495,11.948249,11.623731,11.571332,11.494446,11.750573,11.403248,12.414541,11.95763,11.958902,12.060448,11.922917,13.360957,12.839029,11.262093,11.651214,11.286339,11.5617285,11.67409,13.074307,13.403886,12.63869,11.619504,11.381268,12.4932165,14.3581,12.25395,12.83552,13.38952,12.689229,-0.7876146,-0.9024993,-0.81746054,0.049005747,-0.8072339,12.971325,13.230887,12.956096,13.383704,13.323272,13.325318,16.370241,19.464066,18.661364,17.149738,19.400124,10.370392,16.248808,13.55658,12.306545,9.546481,9.448111,9.489815,9.806009,9.890993,10.509328,13.1287155,13.371873,11.85416,11.580508,11.692198,11.609742,11.977686,11.419139,12.01526,11.876307,11.099516,11.298815,11.25406,11.320847,11.629155,11.76432,5.8151617,10.534813,10.988932,11.783435,11.671245,12.249972,17.530418,16.149755,16.164106,16.087463,16.369974,16.78618,16.467676,17.397243,16.039827,16.352386,16.233885,16.414003,16.26545,14.85595,16.190687,8.874159,16.142235,16.028818,16.095049,16.263393,15.708535,8.966607,16.352032,16.289955,16.048212,16.260946,15.56291,18.059929,16.147377,16.41731,15.403199,16.479624,14.855235,16.951574,-0.69615,16.695461,16.02338,15.939294,16.036043,15.980947,16.54716,16.86144,9.053769,15.989084,16.230125,15.949557,16.325712,16.027983,8.97518,16.248177,16.381628,16.682457,16.674332,16.033573,16.258757,16.19329,16.486889,17.048182,16.395414,16.244165,16.129982,16.55234,16.232113,16.430092,16.162859,18.006008,16.860483,16.841112,16.805992,16.687551,12.965303,13.019248,13.533577,13.074039,11.9953,12.823596,16.429945,16.010353,16.199993,16.651913,16.483406,16.022358,16.60011,16.34809,15.975544,16.455511,17.172804,16.306135,16.169115,16.079145,16.382486,16.384888,16.319864,17.00712,16.625437,16.160933,15.258096,16.825102,16.325645,15.763914,16.22509,16.428328,16.500723,16.516016,16.630922,18.0568,16.303583,15.767302,16.224388,16.289797,16.299074,16.289515,15.169496,16.462906,16.32659,8.890257,15.996974,16.249079,16.322437,16.13203,16.39557,16.041046,16.392141,16.066246,15.919342,16.104994,16.209318,16.33825,16.661343,16.792192,16.180368,16.369604,16.453928,16.308483,15.796412,16.328196,17.035202,16.62444,15.974434,16.312746,16.528173,16.439589,15.068061,16.175924,16.34369,16.260399,16.942898,16.482727,16.545582,15.747096,16.231064,15.971883,16.243773,8.922144,15.985753,16.192566,15.635112,16.263937,15.120912,16.813957,16.288967,15.932121,16.334583,13.621056,8.210616,15.550146,16.059372,16.226118,16.22918,16.424297,16.547176,17.591827,16.270084,13.561782,7.826908,17.222824,15.893792,16.322716,16.255182,16.452581,14.71014,16.32402,16.264395,16.447477,16.156986,16.36936,16.30919,13.955355,9.740449,15.991603,15.750097,16.121681,15.81024,16.332598,16.240244,15.532634,16.22053,16.16069,16.16086,16.090368,14.200945,13.745122,8.021752,15.817853,14.508038,16.219862,14.553915,13.526738,16.421274,13.782116,13.869089,14.042328,12.166058,16.532454,16.581627,17.18425,16.989927,16.825352,16.518955,16.378716,16.543427,16.298824,16.472075,8.823187,15.875802,16.364151,16.719528,16.607054,16.169935,15.643858,1.1628885,9.123659,16.44319,16.282494,16.904097,16.76616,16.46705,15.403811,16.786152,16.90391,16.887417,16.280619,15.792451,17.037508,16.78701,16.882633,16.819597,15.792155,15.671001,16.827946,16.882057,16.872274,16.580982,15.914657,16.834188,16.899904,16.41793,17.01048,16.879377,16.40768,16.54872,16.168356,16.685661,16.707476,12.607695,13.314245,13.548222,10.716543,8.844391,16.563555,16.24799,16.877886,16.860563,16.528297,16.389511,16.19901,16.050644,16.89133,16.871576,16.872179,16.937748,16.870865,16.676193,16.933872,5.6346593,13.943667,13.221989,12.898727,16.259884,16.424803,16.78712,16.896023,16.903107,16.479671,13.806789,15.003947,16.597618,14.887086,8.982696,16.44819,16.498596,16.417007,6.506817,16.767895,16.901789,16.779438,11.696883,15.552966,15.782405,16.497648,13.741282,16.888193,15.98604,16.296726,16.83238,16.409359,16.462236,16.454021,16.839787,16.886885,16.415539,16.674778,12.384211,15.526855,16.541464,16.489182,16.440731,16.599976,16.432262,16.678751,16.540632,15.38734,16.889639,14.305305,8.902532,16.459593,16.513815,16.411694,11.619144,16.134758,16.643456,16.587328,15.958394,14.313381,14.754966,16.239323,11.716688,12.430946,13.161454,15.954372,16.809847,16.640158,13.021889,13.495104,16.858458,16.906677,14.14528,14.203847,16.413223,16.794888,16.898962,16.852457,16.821896,15.935797,16.553953,12.300304,16.74437,16.536522,13.410307,13.107634,16.38375,16.758625,16.444843,15.695698,16.379625,12.670053,16.296349,16.857714,16.791157,16.795229,16.612179,16.350485,16.021526,16.904997,16.578054,16.496162,16.826584,16.339611,16.519398,16.612679,16.708254,12.921003,13.0177145,8.824458,16.24255,16.491026,16.262003,16.34499,16.332375,13.469137,13.408445,9.116542,16.179535,16.241922,15.305464,15.681347,12.697007,12.477593,16.471342,16.139305,16.368708,16.379889,15.430483,14.57746,13.059898,12.747516,12.906427,15.087228,14.712437,16.895798,16.461405,8.750844,15.393835,14.536907,16.452719,16.107645,16.175844,16.234173,16.312935,16.149693,15.997809,16.4279,16.144367,15.989927,16.355436,16.368713,16.189203,15.739885,15.766566,16.4189,16.45161,16.13527,16.457296,15.696401,16.171064,16.210012,16.541363,16.346172,16.179993,15.375573,19.378315,16.92237,19.227667,19.414194,19.386404,17.631233,17.56881,4.124878,12.457575,14.464993,10.105742,12.849315,13.413468,11.421818,11.851632,8.521964,11.679107,11.116429,11.219435,11.5613165,11.973897,11.430454,10.05878,11.382488,10.772312,12.334417,-4.156167,13.01191,12.415272,15.986781,16.766962,16.972023,17.821472,18.225863,18.263802,17.95539,15.879163,15.178055,17.287222,17.327528,16.990313,18.189001,19.176584,16.330217,16.43864,16.183046,19.385687,20.063942,17.960384,17.927534,19.495722,19.938648,18.158834,17.639498,14.172717,14.18913,14.347085,14.367988,14.089489,14.276445,13.829691,14.008271,13.7254,12.053558,12.848895,13.861178,13.733147,13.6507,9.436507,13.934319,13.589162,12.607148,14.113217,12.878597,13.11284,14.19718,2.0230467,13.096061,13.352215,18.942228,16.895622,18.122766,16.619164,17.84508,18.855492,18.966427,17.915089,9.068724,16.407501,15.686438,17.167507,17.346085,17.776407,16.90225,18.352177,16.410286,18.206186,16.409084,17.948105,16.856836,12.318758,12.147794,12.067718,-0.73048705,12.370025,13.337122,13.176048,13.307264,12.991802,12.644399,17.698782,16.527248,17.964127,17.979319,18.195765,18.528185,17.3577,-0.1206122,13.16934,11.539382,8.937757,9.088799,11.839354,11.272251,12.647962,12.764097,18.941116,17.129005,18.11037,16.786104,16.786299,19.60142,19.328415,16.85683,16.506336,9.077615,17.164305,16.863457,16.938044,18.819908,18.612383,17.114902,17.81081,19.27294,18.10064,17.531986,13.024974,13.248637,19.529152,19.787748,18.856411,17.953762,19.131048,19.236246,17.566385,17.576508,13.86582,13.739502,13.865452,13.706522,13.77295,13.253014,12.384062,10.553523,12.930653,14.47938,13.681496,13.501177,12.899231,11.238483,13.056511,13.431774,14.691131,11.787101,-4.0365224,11.033491,12.838529,13.183304,13.097787,15.83666,13.637921,13.08711,12.047653,12.685822,13.503288,14.152411,12.131963,13.119607,12.94166,13.137887,13.875145,13.148603,13.45102,12.340947,11.741543,11.148166,11.195047,19.472504,18.466492,17.93603,18.009287,17.146742,19.352531,18.391142,18.128492,19.128672,19.69854,20.021276,19.285355,19.760353,17.557444,16.957293,9.084757,15.956234,17.85248,19.429607,19.95694,17.13806,17.301924,19.286076,14.232448,9.202282,10.647641,14.467616,13.1900625,12.91308,13.394313,13.151068,13.173921,13.014276,13.380106,12.833576,13.029689,13.107006,14.462794,10.804007,13.096624,14.597644,12.995675,13.479152,13.820179,13.558937,18.988104,18.858694,16.618267,17.77547,18.038881,18.140537,17.984726,18.506666,19.814775,17.939041,15.6222315,10.259658,13.515825,13.15061,12.522828,6.086742,13.268981,12.7552185,13.061084,13.159267,14.102783,14.171122,-5.530636,-9.425663,-1.2500498,0.087684125,11.989358,12.565894,10.455753,-0.06413082,-0.5586067,12.92935,13.409316,13.382269,13.260901,13.365764,12.600767,10.745931,13.475692,11.622226,12.967345,13.340262,11.666693,11.877361,11.472594,12.43921,12.715394,12.563098,11.622901,0.25216857,0.11659973,0.14135899,0.7497428,11.522394,19.580757,19.866364,18.436522,17.877642,19.448847,18.536772,17.303047,14.545224,10.602529,0.9958257,11.156607,12.775962,13.121632,12.445045,0.16169712,12.684023,12.0128,12.316598,13.410703,13.638815,13.356551,11.199479,11.695791,9.353694,11.714072,11.513497,13.200047,13.069473,17.97604,18.520819,11.862084,11.718342,11.733908,11.9429455,12.046789,12.235669,11.879361,11.915946,12.014517,10.684823,12.071354,12.19002,12.442115,12.578006,12.728556,16.56442,18.529058,12.998189,11.85674,11.980392,-3.8238451,12.793847,13.406094,-4.3137,13.255527,11.626407,11.665497,11.458658,11.50981,11.286995,11.32175,11.192392,10.665942,13.423074,16.52525,16.981827,17.567522,17.125805,13.049755,12.905518,10.768219,13.572808,13.341307,13.269466,13.090058,12.938942,11.993868,13.275561,13.572111,13.24511,13.423241,13.775948,10.6921835,0.07913084,11.601781,11.443531,10.816177,11.065122,11.750422,11.821699,13.371251,17.821497,19.44452,19.849953,18.465046,17.708134,17.659216,18.68134,16.385365,17.01843,17.236881,16.285637,16.23159,18.038322,17.069048,13.225426,13.182544,12.2765665,13.257625,13.328782,12.353283,13.323842,11.931401,11.831089,12.062296,12.377148,11.591106,11.327429,13.265512,12.914596,13.417586,13.486822,13.937535,12.602963,11.730647,6.039114,15.363931,12.920532,12.998211,12.827049,19.117968,19.663067,19.711529,19.681643,19.352741,16.886168,15.932899,17.982754,18.48342,19.54516,12.468684,12.970674,12.580267,-6.3846345,-6.3390746,12.499615,-0.5003673,13.03102,-3.1306276,12.956378,12.80432,12.31328,11.919759,12.820556,12.496744,11.190082,12.649201,-1.7571725,11.451745,14.443616,19.177149,19.997854,19.374544,17.244852,13.610587,13.189271,13.603211,11.389345,11.753501,11.511214,11.893687,5.967713,10.390213,11.572704,13.304127,13.269782,13.029061,11.267073,12.147687,10.89615,12.548324,12.427067,12.412006,12.289833,11.748183,12.761057,13.028197,13.621794,13.397048,13.715222,13.053818,14.521963,12.216209,12.473768,11.707156,0.39940643,12.268552,11.454734,10.955433,9.98525,12.0902,12.139492,-2.5484362,12.492853,7.570239,11.947752,12.113396,11.883142,12.237435,12.18853,13.660226,-4.7702093,13.532051,19.522413,19.186983,19.688297,19.743023,19.551231,19.548433,17.641733,16.21741,17.892408,17.971262,19.18149,19.70402,16.729332,16.928328,19.65088,18.43465,17.364742,16.492533,12.190889,12.744135,12.606713,19.454107,19.998943,18.06118,17.970036,18.00523,19.897253,19.045996,18.825779,18.104517,16.696611,13.137424,17.038054,17.598316,18.347143,17.398674,11.688156,11.688258,11.0665655,3.7645488,11.451367,11.850432,19.42537,19.766989,16.46614,17.925037,17.933514,19.55665,19.198303,18.282001,17.296661,13.280016,6.674454,6.3647356,4.2394896,3.727879,6.985898,6.771432,15.746473,17.425276,16.367842,17.95788,17.942963,17.95267,17.97657,15.815865,14.98382,13.102184,14.341418,14.429074,13.751862,16.723867,16.776594,16.914911,16.883066,16.880737,16.135021,13.325876,13.305546,13.202664,12.9476185,-5.7972627,19.459187,20.033314,18.489597,17.977467,19.165781,18.885359,18.191658,14.574461,15.712556,15.387696,18.06559,12.47368,13.24461,12.945569,10.639622,10.5756,14.459069,14.310849,13.380711,13.79488,13.680266,-6.8597145,13.354145,12.686792,11.301645,10.458804,11.384189,12.231646,12.738697,-0.24491154,-0.48113754,12.881514,14.072155,12.828671,13.142302,13.28364,12.5219965,16.611816,18.504042,17.97722,17.334738,16.967701,17.91923,19.43284,16.805054,16.794794,9.0554285,16.45743,17.646263,17.70464,19.057522,18.907513,17.280241,19.574854,18.234243,18.141956,17.313799,17.640184,16.911543,12.927988,13.650477,19.549448,19.284266,17.545013,19.954529,19.615307,19.586134,19.116056,18.044205,19.277254,15.8509035,12.945282,13.164846,12.614409,11.946996,13.533906,12.802018,5.9997425,12.783548,12.145955,12.593151,14.52383,13.181071,-4.4356527,-0.8874573,12.532943,14.173038,13.319837,13.528927,13.218094,12.834396,13.090358,13.591571,13.389853,14.207274,13.21518,11.53404,10.512107,10.518614,9.969793,7.573868,9.377679,10.195292,12.934717,13.315154,13.420507,13.323535,13.012297,13.013586,13.305883,12.866585,2.1037104,12.249483,3.3209822,12.60566,12.709117,11.995241,11.719203,11.614893,12.691294,13.276049,12.7025585,12.721653,12.926905,12.925618,14.197585,12.112863,12.228425,12.5905905,11.057201,12.450346,12.326278,13.31463,11.775468,7.0493083,11.403849,12.118711,11.557844,13.637288,13.444667,12.923647,10.429286,14.46481,12.194789,11.81022,11.168719,14.15256,13.970907,14.115531,14.135281,14.1483555,3.376889,14.681547,12.502087,12.711262,11.818913,10.657268,13.125774,18.456667,19.386068,19.758179,18.682625,19.07922,17.896973,17.890259,17.318707,17.216522,16.956068,17.778004,17.683273,19.751387,18.272367,17.218744,15.096945,17.228704,13.044287,12.366629,19.5258,19.52242,17.933456,19.392303,19.545916,13.095276,13.3607435,12.375242,11.284651,11.588313,11.685082,11.797389,-3.22136,11.789752,10.431512,10.642563,11.680036,13.371939,14.696893,13.366187,13.052349,13.606248,13.306389,12.670227,13.108071,-1.6497744,13.258919,13.477476,11.625569,11.065393,13.3889885,14.58084,13.526577,14.487381,13.702983,12.751792,11.8564205,11.138391,7.707345,12.808104,12.602744,11.619104,4.321868,12.135201,11.83985,13.692239,13.085328,13.276969,18.004004,18.495722,13.125365,11.946344,-0.15838446,-0.26267344,14.466597,12.813682,12.852744,11.716493,12.503936,5.5010333,13.114962,-2.095616,12.850284,11.608247,10.932065,11.111752,12.531347,12.178753,8.590201,8.780347,-6.333706,-6.0727677,11.499675,13.011621,13.443878,12.455906,11.938244,11.50515,13.409604,11.786984,12.045064,11.220873,11.914688,12.027013,11.8501,11.746343,11.645452,11.180253,12.295896,-0.9662563,12.439745,13.172886,12.6745615,12.6012,12.467318,12.881954,12.052371,11.877464,12.053742,11.6810465,12.975329,12.958716,5.4700747,6.025662,12.999373,12.899726,18.499756,18.284067,17.477352,17.371597,11.757811,11.621304,10.363596,11.724852,11.667116,11.02562,11.77647,10.553596,11.613898,9.883287,11.156888,10.390426,11.756949,11.931476,13.437429,13.248817,19.471287,19.61062,17.926558,19.293795,19.701273,19.628633,13.655084,12.5264015,13.535995,13.948955,13.3643465,11.167119,11.788152,11.849008,13.338488,13.60096,9.50322,11.311426,11.235647,11.669264,12.017911,11.27561,6.4749293,11.74166,11.882388,11.744206,11.660543,11.797988,11.632509,11.58077,-1.2643317,11.280339,11.642728,18.192627,18.561972,12.346389,13.676185,-6.527346,13.534499,13.856059,12.927003,12.892886,12.894956,11.665609,13.028145,11.666192,11.66033,19.497375,18.854885,18.004543,19.218515,13.438186,13.697639,13.270193,-1.1267185,13.272805,13.065684,13.402112,14.022552,13.461616,12.402586,7.105246,12.94078,12.887107,12.918257,13.493202,11.588168,13.314262,15.7363,13.976388,14.24464,13.251995,13.0045595,13.082723,19.446041,18.549303,17.968962,19.54029,18.499012,17.217709,16.410496,13.452176,12.68163,12.428792,11.5062275,11.272264,11.29157,12.26249,12.672431,12.173528,8.006899,-6.312768,-6.1183743,13.687522,13.296405,13.034502,12.620053,13.225206,12.179601,12.093959,11.132943,11.43726,13.30472,19.307344,19.390244,19.357876,19.545288,16.947756,18.306711,9.03284,17.885508,19.0762,19.577866,19.397429,18.39761,17.363771,19.029604,14.06908,13.744773,10.563641,14.1288595,14.066211,14.094081,14.089697,19.295908,19.401947,17.878311,5.929737,10.112061,9.811412,9.955575,8.501129,10.030355,9.966409,9.869613,10.874364,19.50547,19.932417,18.642069,18.926126,19.507002,19.838993,18.365456,17.511602,12.837358,13.142831,13.372887,12.9084835,14.487494,13.624539,12.544899,13.236582,12.094596,11.774899,4.051227,3.8125544,3.7919521,11.995045,19.544254,18.60641,19.175615,17.579596,16.874987,19.035439,18.78271,17.977152,19.625332,17.915075,18.323498,17.517881,13.766109,13.356824,12.840377,13.517459,13.023335,13.561414,13.337708,19.346989,19.633366,12.3349,16.29876,16.913925,18.127745,16.902313,17.628714,16.98056,18.12917,19.443117,17.888746,17.890251,19.524372,19.336517,18.123808,13.194492,13.204066,13.101563,13.032043,13.233646,12.910408,13.147813,13.058716,13.292812,12.312378,12.129138,11.655197,14.383199,14.315196,14.245417,14.203081,14.342466,1.5028911,14.271,14.670152,12.932013,14.325267,10.684747,13.639259,11.945271,11.546806,11.107966,12.42873,12.783768,12.348987,12.157178,11.965692,11.399462,12.400339,11.928336,11.886266,10.934679,10.52878,10.540233,10.216081,10.139421,10.531047,12.374142,13.466891,18.40838,17.616608,18.466784,11.85096,11.792811,11.523836,11.958002,11.844006,13.093477,12.334535,11.93137,2.155023,13.426617,12.866301,13.476747,13.380844,9.860767,11.370351,9.20054,-0.7055604,6.106229,11.944255,11.946117,11.535556,11.974926,11.636128,0.27609515,11.041072,13.047881,19.26145,19.767263,19.948715,19.741394,19.739336,18.222109,18.065197,19.25312,19.312763,17.580751,13.083187,11.786016,11.812311,11.5792675,11.522394,-0.7907293,0.60560656,-0.78382176,12.038718,11.916903,11.813867,13.152578,12.529471,11.328844,11.339373,11.270728,14.499288,12.756348,13.470464,13.286081,13.045689,12.596194,11.497383,12.166447,12.109687,12.160805,12.299239,12.402694,12.345422,12.123121,12.026745,12.109083,13.365544,13.180022,13.742886,13.31167,13.501532,12.940106,13.574714,-3.7726285,12.975662,-3.9810703,-2.761678,13.838195,11.580251,11.165965,3.4746172,11.7085,12.111986,13.61829,12.839472,19.222925,19.830208,17.889835,17.89499,17.271444,17.82422,17.592014,17.977755,16.658178,16.248796,18.066347,18.01219,17.86789,16.15716,15.657856,15.979008,17.70811,18.605837,18.127264,13.397002,13.383027,18.871286,19.290798,19.255379,17.953,18.421524,19.952982,16.494392,18.796402,18.196592,16.213402,12.837411,13.324568,17.242577,18.908674,18.730215,17.05839,18.253418,16.830128,16.672485,17.27457,17.359583,15.183375,9.088666,18.25141,15.049875,18.244656,16.687891,16.30092,17.971878,16.592236,16.507816,16.389181,17.080406,16.27034,16.387463,9.105464,17.175245,16.369843,15.812087,16.418154,18.449915,18.154814,17.14272,16.611626,16.396162,18.490662,11.366989,17.332159,18.507145,18.36244,18.911705,17.263905,17.036236,17.347935,9.881562,17.307425,19.169674,18.095684,7.953123,17.43445,16.336971,16.799284,13.822957,15.53276,16.049593,16.860798,13.236901,12.545533,10.377428,-0.02303187,11.382022,12.542489,11.843444,13.8243475,12.979908,12.657958,12.162015,-6.255373,13.225857,12.555447,11.366565,12.340265,11.05587,10.713684,19.052652,18.785692,17.117043,19.564787,19.67179,18.854862,19.26995,16.982807,17.204298,17.97015,18.817804,18.111485,18.656288,16.485458,18.320896,16.286383,18.245504,17.355425,16.375711,17.9341,19.438726,19.61048,19.669344,19.753239,18.129854,18.050966,18.62521,19.22225,18.187487,19.110674,12.36037,12.323788,12.152958,12.468563,11.97331,11.997221,11.931556,-9.324858,12.798531,12.76733,12.843509,12.913083,12.833872,-0.5759084,6.824524,6.8836765,12.064077,0.006659322,11.920387,10.857802,12.238474,10.40347,12.56167,10.986137,11.976312,13.728315,13.012756,12.526444,11.719073,10.511065,9.893253,10.342086,13.183179,14.088203,11.831864,3.458671,11.438561,12.003015,12.793176,14.503235,14.281108,10.325213,12.480304,12.828574,12.064634,12.607531,11.928057,11.713861,13.378004,11.871727,11.859618,11.519951,11.807213,11.748246,11.649237,13.17633,11.042619,12.262758,14.303075,12.23019,12.170234,12.2202635,-0.54174805,-11.252332,12.509729,11.89682,0.16475967,11.8981905,11.997685,11.790725,-5.0774946,13.192256,13.021015,17.880575,18.291945,17.257738,15.642612,16.538734,16.918901,18.002943,17.234655,18.329754,18.800488,19.26347,18.139978,16.958649,9.066964,16.37861,17.956568,19.174652,17.423483,19.951658,19.123697,18.057032,19.583652,12.180877,11.485237,11.380678,19.63709,19.39664,17.287466,19.78292,19.80361,16.482574,9.061074,17.947378,19.52932,16.52908,19.299746,19.618887,19.827549,17.600578,16.317245,16.447786,19.707224,13.201716,12.349528,0.49671182,14.361475,12.058198,12.256356,12.094807,11.939295,12.183151,12.153601,12.2052,12.141598,12.015913,19.454493,19.002256,18.566488,18.759071,18.127375,17.311771,13.313987,13.501915,12.978711,13.086388,13.596149,13.38926,11.551558,11.144927,11.396137,10.996993,10.393373,13.597074,13.283647,13.410823,13.2012005,14.452523,13.403067,12.758381,18.595161,19.850782,19.09687,18.63937,18.00318,17.953283,18.582367,19.305267,16.531149,19.426638,17.517235,16.38807,13.133536,12.805493,13.167491,13.413201,13.188412,13.591545,13.409673,13.4274,14.52493,13.448293,12.029387,8.603647,-0.52811337,-1.9386117,11.336343,11.007613,0.62550753,12.570369,12.264591,11.630822,11.786248,11.841355,11.740805,16.744305,17.441952,16.617655,12.949465,12.8714485,12.654715,13.95586,12.561732,14.099159,12.900249,12.189787,12.676039,4.4568257,5.154385,4.4870057,5.4507446,13.66541,14.930891,14.530169,11.043263,12.29377,1.7710006,12.595088,17.91366,14.51996,12.993869,13.270895,13.116794,11.616436,12.809823,12.948294,12.184204,10.292358,14.465143,9.634121,13.351302],\"xaxis\":\"x\",\"y\":[2.2380998,2.2134945,3.9860666,2.688587,2.690416,2.230454,2.3260658,3.203431,2.2825758,2.4618354,2.395909,2.3243594,5.417745,3.853605,2.3219662,4.3402085,2.5596108,2.318508,2.1204884,5.448902,5.676264,4.002934,4.473885,4.208681,4.2758403,4.684276,4.8411922,3.6808178,3.9962633,3.9176087,-5.490215,-6.283597,-6.102198,0.72489333,-6.430989,3.856481,3.8004727,3.6777837,5.6338887,5.4832153,5.382554,3.0886416,2.3461127,2.7024994,2.807632,2.4001138,3.2086093,3.0291677,4.2947907,4.758046,2.8659296,2.678198,2.7715137,3.0215216,2.7892962,2.8273757,5.2751617,5.4138794,2.1474853,2.2201135,2.5136566,2.4941719,2.373186,2.4337413,2.360856,2.2520933,2.209741,2.3745112,2.3268201,2.3407283,2.1287565,2.1376498,0.37189037,2.4000268,1.9761517,2.0357304,2.1463933,4.053486,3.0462682,3.252739,3.3839195,3.4461486,3.4032862,3.3239865,3.1330574,3.0495648,3.3074656,3.0613294,3.2039719,3.4218633,3.34504,3.4694865,3.391571,2.1327915,3.6295803,3.2618876,3.4513912,3.3664436,3.481044,2.03788,3.2965896,3.5022814,3.1943903,3.3634508,3.6357272,3.1392856,3.4022026,3.313496,3.2557888,3.3551776,3.4257193,2.8513927,-6.6197,3.132454,3.4478962,3.6767225,3.1109169,3.4406168,3.4487758,3.2767766,2.0155559,3.5619414,3.393791,3.5140553,3.3137615,3.1658845,2.0635247,3.351268,3.5612266,3.0898173,2.8857186,3.0389078,3.5257132,3.4329126,3.299947,2.8277617,3.3035908,3.2045271,3.1711504,3.25638,3.24083,3.4286478,3.4526029,3.314094,2.8107247,2.8491492,2.7934885,2.7809427,3.9909236,3.928223,3.3114698,3.8436592,3.755131,4.0930314,3.580604,3.6081405,3.5571897,3.6329381,4.0950627,3.2816901,3.9813728,3.6008067,3.6931295,3.4267051,3.8337915,3.5973144,3.5111258,3.571964,3.4725945,3.5150764,3.5534124,3.6707025,3.5789046,3.4461293,3.8282297,3.290317,3.4564807,3.608841,3.5279768,3.2557077,3.0677795,3.3461802,3.0915945,3.490069,3.4991963,3.6029992,3.907308,3.4253526,3.5322294,3.5264528,3.2866058,3.299187,3.4427314,2.1377113,3.5238862,3.535102,3.306882,3.3067672,3.4373286,3.483835,3.0085802,3.0075269,3.2376235,3.1945448,3.2860487,3.5531266,2.6945431,2.5550203,2.9805193,3.4319344,3.0550013,2.9380996,3.3725169,3.5318544,2.9770133,3.2489922,3.3405998,2.8392892,3.168019,3.7960865,3.2431705,3.4577103,3.0380678,2.9794862,3.107211,3.6381335,3.3747368,3.428135,3.1733425,3.2446089,3.1837823,2.1038656,3.4030483,3.2845564,3.2675366,3.3530457,3.591942,2.8514884,2.9915416,3.1403399,3.1608925,4.4413657,2.7029612,3.2905214,3.249876,3.5064902,3.1379058,3.0367515,3.1976666,2.4923704,2.7620497,3.7987902,2.7193942,3.393924,3.14207,3.3238447,3.3612304,3.1114135,3.273587,3.571256,2.9455411,3.5489147,3.1555476,3.710626,3.5143359,3.3724937,3.4983006,3.5349212,3.2181394,3.226915,3.3616943,3.5094166,3.5180714,3.4510484,3.2613127,3.2676702,3.3779554,3.107832,3.5215445,3.287498,2.35631,3.2539132,3.422279,2.7713318,3.4227357,3.6197226,2.8763752,3.7323365,3.555666,3.543658,3.8838403,2.8189964,2.7761164,2.8685055,2.099974,2.0711577,2.967324,3.169769,2.959044,3.319121,2.9012144,2.1286798,3.5104153,3.076381,2.469299,2.3559415,3.1799507,3.6044924,1.8812057,2.1293344,2.9164865,3.004619,2.4995031,2.361121,2.8994846,3.4396098,2.5772684,1.9443929,2.012627,3.119044,3.1446116,2.5995982,2.864351,2.4200683,1.920801,3.4782531,3.1706383,2.2940328,1.8577068,1.941271,3.2783957,3.372203,2.2039785,1.9087197,2.5356393,2.523671,1.9077603,2.9209733,2.8754036,3.2165704,2.535912,2.83499,4.0433636,4.4445667,3.8920538,3.707501,2.2214053,2.909707,3.2236266,2.1195462,1.9581407,2.9147532,2.8687432,2.9124599,3.4678538,2.840763,2.6033318,1.9572365,1.9018519,1.8641319,2.9417896,1.9703882,1.6462736,3.494416,3.9460218,4.342336,3.4260447,2.9507241,2.7094646,2.21054,1.8349522,2.5911424,3.6665723,3.4292307,2.9186149,3.4909587,2.153517,2.842247,2.9038317,2.9231408,1.9083849,2.3652883,2.361566,2.6357706,4.6355195,3.0229523,3.5363572,2.9244168,3.023048,2.120962,3.1226938,3.3939972,1.9788159,3.4815297,2.9869847,2.888308,2.7347577,1.9201051,2.9118521,2.3973656,3.1186733,3.2598395,2.9111447,2.7676227,2.8691428,2.8942366,2.945882,2.5715542,2.9453669,3.6053185,1.9057438,3.6936433,2.1992233,2.9264035,2.4198475,2.4145336,4.3971763,3.126145,2.3682232,2.6928058,3.3037791,3.5521538,3.3219893,2.654428,4.6423807,4.1416035,3.6368954,3.148628,2.200809,2.2784097,3.9193356,3.8626058,3.1639917,1.7863705,4.0544424,3.705216,3.0024247,1.9741149,1.914496,2.296868,2.0902977,2.9931061,2.329691,4.3294144,2.3837483,2.4017477,3.4240186,3.6142492,2.971806,2.1393967,3.0507028,3.536174,2.3718204,3.821044,3.237421,1.946333,1.9545959,2.6433442,2.8184605,3.0317984,3.0627463,1.8513389,3.4942126,3.1556964,2.263407,3.114621,3.4182172,2.9625437,2.2383413,3.7701414,4.086708,2.2499778,3.1177816,2.7373261,3.0606282,3.0001853,2.9888527,3.6619704,3.8061538,2.0861242,3.1136699,3.0640502,3.6510148,3.157748,3.6635323,3.8528016,3.1131477,3.139531,2.9699845,3.0561936,3.2899764,3.461266,3.6217818,4.0425224,4.121904,3.6665525,3.7780337,2.760665,2.9292643,2.337413,3.4674263,3.5602274,2.971384,3.130576,3.2157495,3.031072,3.0637655,3.0780067,3.1449363,3.077975,3.0923083,3.024901,3.120273,3.1588268,3.131637,3.0185173,2.9612646,2.9987159,3.038983,3.0274863,3.1446087,3.0268917,3.0509279,3.0771706,2.8772197,2.9083998,3.0770752,3.3153687,2.4185946,2.8468497,2.5119627,2.5697556,2.6359122,2.531147,3.7956402,1.5498677,4.6980257,5.11066,3.4519584,5.1573877,5.575166,2.1500008,2.135406,0.4494207,2.9311442,2.3257966,2.6939833,2.2124271,4.7489166,2.071161,2.4989023,2.334403,2.6992502,4.8176503,3.3970633,4.9103546,3.824334,3.3369503,3.1449232,3.2240744,3.9407046,3.6709566,3.7053223,4.2435865,3.3362434,3.489465,3.179284,3.2377274,3.2609174,3.1687887,2.4891791,3.4769995,3.327079,3.4464965,2.3326972,2.3163476,3.7595277,4.252444,3.3012455,2.3690386,2.0294876,3.8293684,3.7659266,3.8474529,3.9707673,3.8786292,3.8239746,4.9494,3.8192022,3.4317052,3.9565012,3.7317674,3.794307,4.0257373,3.686626,3.8427153,2.9708192,3.894422,4.0315375,4.399075,3.8689766,5.27741,4.3787503,3.9143429,1.8638958,5.38063,5.562493,2.5727904,2.9215777,3.1866212,2.9027503,3.2940092,2.7880337,2.903614,4.163555,1.9440814,3.3359172,3.4354901,3.2411406,2.5615585,3.1827865,2.9335346,3.1457582,3.0161328,2.0673444,3.350909,3.1707633,2.8417866,4.476124,4.7760634,4.5580573,2.0649865,4.245496,5.306053,5.3384733,5.5807843,5.3447433,4.8845444,2.6898181,3.4438996,3.5609205,4.016129,3.2271833,2.2381098,2.347099,-6.369731,5.4241567,2.0481174,0.0015422002,0.55868316,2.329777,1.9554386,4.503199,5.1824937,2.4876091,2.9579973,2.6511738,3.166816,3.0141177,2.3700316,2.4239814,4.0650234,3.6818187,1.9892474,4.0075474,3.7477875,3.1781652,2.791361,3.4269354,3.0348992,3.0724764,2.4050698,1.9200413,2.7537935,5.3305655,5.347008,2.3220327,2.3471665,3.5379164,4.1997933,3.2810197,2.7294216,2.4335318,3.771935,3.6943588,3.4425385,3.673438,3.3953831,3.451394,5.579557,4.2535763,3.7308958,5.092808,5.066307,5.2347894,3.8103464,3.9060957,3.7988555,3.887615,3.8106217,3.748954,4.833912,1.1454676,4.473337,4.65086,5.3165665,5.455254,3.1751602,3.9291482,3.3760316,3.2988586,5.191455,5.2410994,3.7750096,3.7116346,3.954498,4.0011735,3.8728194,4.087922,5.2037277,5.1155424,4.231246,4.768863,4.550345,3.650835,2.3377042,3.8124425,3.5193548,4.298466,3.02746,2.4427226,2.7357526,2.0876045,2.2879937,2.3484442,2.2730918,2.4221509,2.3501344,2.7547905,3.8762758,1.9238694,3.588841,4.263639,3.2403977,2.3461785,2.7361765,2.323789,2.3906367,3.6130145,2.7559283,3.2135804,5.154902,5.3244243,5.264365,5.40181,3.735702,3.715799,3.7216077,3.689892,3.917385,3.5552287,4.6343307,4.80559,4.5026565,5.0230045,5.168561,5.0760937,3.623888,3.6445007,3.5388372,2.4102168,2.480084,3.1162212,3.1841416,3.0302281,3.902273,3.9505968,3.3197472,2.3649004,2.4115329,3.509078,4.0043674,3.4860456,3.409431,2.9270663,1.5862304,3.1270258,3.6614883,3.4685335,3.4656377,3.7820153,3.999235,-5.3431463,0.016455451,-6.449228,0.6810975,4.0169444,3.8804314,3.3811383,1.0452039,-6.124631,4.1483374,5.342949,5.5203686,5.3438797,5.4309425,3.924121,3.5504417,5.3080807,2.176714,5.200377,5.3605213,2.1318839,2.2752364,2.0768259,4.6593366,5.0520267,4.7782297,4.4573607,5.1129103,5.014822,4.991909,4.8980446,4.2582707,2.308593,2.3601182,3.7425737,4.3294716,3.252986,2.3575926,2.3889024,5.1491103,2.1808376,1.4998716,2.2853897,3.93888,3.8717408,3.7264934,3.5673215,3.926335,3.5577161,3.8207655,5.0077415,5.2497206,5.415617,2.2271986,2.2885642,0.37219492,2.4670568,2.028064,5.504186,4.7837863,4.0003157,3.753648,2.1609118,2.059741,2.0428777,2.082134,2.2402098,2.381198,2.112994,2.1247861,2.137701,2.417454,2.3133411,2.3381565,2.4621649,2.559411,4.347306,3.0949738,3.811282,5.288957,2.1810338,2.2023027,-6.934511,5.1268735,5.3309464,-6.3765917,5.365871,2.0831246,3.2196746,2.2568438,2.308731,2.4054704,2.2087286,2.4151113,2.72977,5.6404595,2.7564917,3.439713,3.2858515,2.628414,5.356386,4.3588314,3.6007621,4.0687594,5.455005,5.3551044,3.1483374,3.008654,2.2646446,3.2212117,3.2954016,3.552347,3.6181097,3.3811533,2.5737636,4.558542,2.2957811,2.8029053,2.3654904,2.4395683,2.6202142,2.4009037,5.4255085,2.6270068,2.3873332,2.342177,3.731431,3.9483938,3.9115148,3.411387,2.783692,3.0970433,2.74022,3.0244362,3.243826,2.1014016,2.965582,5.3171034,3.468583,2.8893466,3.1642387,3.6031234,4.5629606,5.5382633,2.2177627,2.1657143,2.2847135,5.047172,4.5818667,4.464074,3.0619233,3.3556256,3.1947238,3.3679142,3.4529452,2.8605757,2.4649448,1.6086287,3.4875998,4.0761986,4.528286,4.0340104,2.293664,2.34598,2.3518996,2.3429253,2.463239,3.8680873,4.137578,4.1975937,3.729542,2.433139,4.9375257,5.343827,4.2408633,7.404055,7.456242,4.804829,1.5773523,3.9760525,4.3472023,3.8269382,4.768911,4.6342864,4.0079627,4.8151913,4.2428365,4.0933604,4.206024,-7.5913467,4.1015,5.099061,2.4223835,2.3008697,2.6584399,3.10704,4.132754,5.621889,4.61016,3.7443027,2.330747,2.341617,2.483099,0.4013682,2.0048218,2.1085143,5.6502757,5.599197,4.179811,3.9226768,2.4352052,3.4171078,2.6874151,2.5895052,2.7079182,2.6139975,2.1112022,4.9180346,5.42138,5.348574,4.234783,3.8861938,5.1152043,5.134109,5.1229315,4.813551,4.652544,4.7842727,4.411041,4.468582,3.4809291,3.5242445,2.285553,2.4581761,3.045658,2.8966136,2.9833543,2.2760444,2.4597504,2.2515004,2.4709158,5.1196976,5.4072804,-6.4531536,5.6719303,2.3307993,2.4700625,2.331032,2.3092194,2.4117105,2.3254752,2.6603384,3.8640656,3.9672844,4.292964,2.562339,2.3900967,2.9752917,2.8591824,2.3129635,2.2053776,2.3647816,2.9832776,2.29172,2.6355543,2.5418575,2.3408065,2.301411,3.7187407,4.1126065,4.240637,2.3181827,2.4665515,2.50333,2.0884495,2.9692988,5.0896616,3.6393666,3.221726,2.267622,2.3474283,2.4228144,2.7528894,2.3826835,4.1051054,2.6187148,2.5838325,2.3542168,2.3163702,3.4299161,4.1333394,4.338487,3.2218015,2.387007,2.172902,3.6268427,5.585734,2.5018415,2.2970407,1.5744643,1.2026801,2.634723,2.5851848,3.0219896,3.0979521,3.1531951,3.4612982,3.5587432,3.4601305,3.3018167,3.2976947,3.2761502,3.7971976,3.2167625,3.4627452,3.4645498,2.3517535,2.6406875,1.8803844,1.8728981,1.8453933,3.0132554,3.7244432,5.3882456,5.3412657,5.02545,3.4378014,2.2966607,2.319838,3.8562918,4.2466745,2.5107741,2.4423168,2.0808837,5.128566,3.4191475,3.432811,2.1710215,4.3938193,5.4960375,4.3005366,3.5474782,3.5364544,5.070123,3.3513703,3.6840656,3.5176568,3.3695621,0.21512575,5.5523896,3.7809374,3.5336435,2.8784738,3.8796675,4.809231,4.8404393,1.0611613,1.2615824,4.605799,4.4512606,5.076985,5.0247416,5.362762,4.0697794,3.3968515,2.8057315,3.1083648,3.2895439,3.5565653,3.1051645,2.4002259,3.6302388,4.002519,2.0223382,3.4333844,4.0650167,4.0469413,2.7608814,3.5362222,3.136534,2.388915,2.9610975,2.061805,3.9052858,3.1848981,3.1758046,5.090469,5.397773,2.2952306,2.4127066,2.8453784,2.2954767,2.3624914,2.2850964,2.715794,4.164758,3.276017,3.3163075,3.707662,3.7242625,4.1462655,4.085732,3.5831876,3.3366485,1.8230304,3.769476,3.9338021,3.3748074,5.178274,5.353215,3.8327992,4.242476,5.2609076,3.5914273,3.533912,5.0807056,3.7674642,3.8786058,3.7646673,3.7100687,3.6872807,3.6296852,3.2948768,3.6570637,3.489368,3.3703723,3.340853,2.7125416,2.6490438,3.2397523,3.8865175,3.5775578,3.633734,3.5614398,3.8213398,3.662614,3.7931192,3.6452,3.9980443,3.5773299,2.4127123,3.5389304,3.221997,4.377507,4.545404,4.4387393,3.4479222,5.643149,4.071323,3.6889203,3.5934846,4.1378694,3.6231403,3.2322426,3.2616396,3.5742764,2.815972,4.1534033,5.1337576,4.1517916,2.370595,0.82097393,4.1279955,2.708362,2.048626,4.090816,5.36228,3.938334,3.3763928,5.1665573,5.116352,4.5966587,4.479828,3.878301,3.621029,3.9295025,3.5864632,3.7917588,1.5567503,3.8193958,4.379846,4.6547465,2.1499083,2.2345684,5.246286,2.380115,2.4582057,2.3027055,2.4698222,3.1186225,4.2075806,3.3485184,2.7073302,2.691245,2.6167204,2.5569642,2.576662,2.3416548,2.1064034,3.2597709,3.2984526,2.7037454,5.347609,5.1604743,2.3137174,2.5527408,4.342587,3.2671583,2.38807,5.050766,5.5445895,3.6307023,3.76275,2.041794,2.336337,2.1288695,-0.28886002,2.2494223,2.7754102,2.479801,2.1329548,5.423387,3.4284217,3.477687,3.6331513,3.423854,3.5244513,3.541629,3.5136762,-1.0875403,5.613128,3.9117825,4.067699,4.0628123,5.6189675,5.20699,5.606008,5.135389,5.208598,4.5770855,2.1240575,2.475095,0.3474588,4.2064085,3.5723653,2.1013925,0.89193106,2.25962,2.1615877,3.853725,5.344193,5.617524,3.9639022,3.859548,5.438394,4.4890294,4.664239,4.49978,5.024113,4.3403893,4.4643526,3.4027207,3.5489144,1.5286864,3.4864705,1.725903,3.7868912,2.126715,2.0446894,1.8540652,4.858832,5.051918,2.8352706,2.7194023,7.489557,7.2161403,3.6955316,5.205164,5.563499,4.73304,3.957603,4.048601,5.5590057,2.4134054,2.3335073,2.5367475,2.6081803,2.5317383,2.5939338,2.4357457,2.1553965,2.5424085,4.664844,2.6926768,4.214631,3.269075,3.1924691,3.013852,3.0416784,3.1276717,3.1558366,2.985076,3.0764198,2.9313662,3.117027,3.1618025,1.7792042,1.6259675,3.0950205,3.3449097,3.457262,3.3258655,2.8684516,2.3779013,2.132672,2.2689776,2.5274565,2.227326,2.3260312,2.5920389,2.150623,3.3502438,2.0867648,3.2469141,2.314606,2.9598846,2.251825,2.3957422,5.1026664,5.1126447,2.3275027,2.5786226,4.405394,3.2466378,2.33219,2.3769457,5.0760875,5.308807,3.6904376,4.052065,3.732112,2.4771986,2.5075147,2.189606,5.5858684,3.829955,2.970026,3.8526185,3.8551092,3.9466996,3.840138,3.9271355,2.0642133,1.9802836,2.2821155,2.2047663,2.0386393,2.0993981,2.052936,2.1495705,1.8769177,3.6472766,1.9824294,3.848968,3.680786,4.8548408,4.5072436,6.3246846,3.6193304,3.6037617,5.187106,4.6268764,4.3315454,2.16678,4.4757123,2.1359606,2.0656762,2.2775538,3.0428536,4.226276,2.481094,5.2991667,5.2892737,5.626751,-6.2938666,4.02003,5.25626,3.4141593,3.4400551,3.8337839,3.768042,2.71064,3.9538627,4.0471563,3.7457755,3.8666925,3.2725894,5.445117,3.387632,3.689922,3.8762336,5.3738027,4.9485273,5.350988,2.3568795,3.869427,4.3098025,3.2278821,2.3183475,2.3735552,2.8840308,5.4903197,4.37558,5.0956216,4.5163717,4.4178095,4.3887777,5.1213474,4.729231,4.790774,3.2962525,7.715421,7.2487664,3.6371765,3.5971158,3.441592,3.3101492,5.251413,5.111397,4.578992,4.442227,4.3085685,5.3903704,2.415228,2.4528258,2.4558008,2.3658051,2.953657,3.773856,1.9738163,4.2653613,3.3396728,2.3742206,2.4326344,2.2208717,2.397542,2.5960295,3.8406103,3.738528,3.3536234,4.1953063,4.4279723,3.988039,3.7174866,2.3501503,2.662069,3.3098397,1.5028182,3.0131001,2.9843538,2.9070494,2.1551676,2.9368231,2.9214573,3.0969543,3.4219096,2.3095546,2.3066206,2.4377065,3.3590107,3.2511506,2.4086962,2.1721878,3.7683704,4.63005,5.210006,4.945871,3.9496462,3.1749473,3.8335886,4.58103,5.316865,4.075053,3.7758522,0.5803597,0.84569865,3.1491024,4.116898,2.3067915,3.7731562,2.4399583,3.7284284,3.251121,2.233524,3.5726795,4.256584,2.4251652,2.6130319,2.1851711,3.716633,3.9062834,5.167293,4.9540296,5.2225237,5.3946857,5.2726307,5.3902597,2.389876,2.4199486,4.53326,3.5028787,3.3425636,3.4779425,3.306212,3.277246,3.1987224,3.1619203,2.3558042,4.11352,4.2838645,3.2871258,2.4351456,2.1080637,5.499416,3.911584,3.8108513,3.706566,3.5821962,3.9038491,3.4569333,3.5810401,5.722277,4.668611,4.758968,4.239612,3.852852,3.726553,3.7703145,3.8369837,3.6718907,1.8210167,3.68142,3.5869946,5.238583,4.7799463,3.3502948,5.361523,3.6801114,3.9111483,3.8672082,3.856989,3.3893132,4.5874696,5.1040297,4.643501,4.472614,2.4056242,2.3829858,2.2422462,2.5934289,2.5310116,2.7753541,2.4440897,2.517304,2.7299564,2.5590706,5.6463966,3.815713,3.2195594,2.2294123,2.260386,2.307387,3.183051,2.8243458,2.7609222,5.4295783,4.0701165,3.828843,2.3492417,5.601399,3.7203438,5.2497654,5.525849,2.0890048,2.0313141,0.5067871,1.1656655,0.19846201,2.9242976,2.4489224,2.0753977,2.6239252,2.1816764,0.9266842,1.9488956,5.017,2.3693414,2.3402014,2.3104153,2.3317156,2.3522627,3.7424407,4.1018333,3.0993464,2.5573957,3.752202,5.1659474,2.1539307,2.1527011,2.1565855,2.4154167,0.31151062,0.72649765,-6.415437,2.3851357,2.1419942,2.0959804,5.6137896,4.098348,4.096094,4.164796,4.086771,5.156326,4.574013,5.5494313,5.6355896,5.266572,4.4377036,3.9364507,2.326749,2.3376389,2.3907897,2.31952,2.3519695,2.4302394,2.420779,2.5255084,2.5232902,5.4635305,5.245613,3.7606657,3.5403316,3.4592774,3.3842125,3.5175016,1.816668,3.7356281,1.8818129,3.2160172,3.5211349,3.6696181,3.0794175,2.3340592,3.6980703,4.018787,5.3311405,4.9976416,2.5554886,2.3396409,3.4802172,4.1322455,2.8462737,3.3169923,3.3384924,3.4177058,4.0712523,3.6725628,3.4559462,4.335537,3.373592,3.6646755,3.949623,3.632054,3.1785252,3.788183,3.6573021,5.126394,5.5085607,2.4909933,2.4215217,2.8550575,4.178245,3.2507155,2.3321085,3.0149636,2.5698702,2.0375385,3.3256562,5.3135448,5.621028,2.8086932,2.4400623,2.4950638,2.9330857,3.2224517,2.8554718,3.162287,3.1354332,3.2944086,3.0528798,2.093757,2.5368154,3.2388783,2.5522478,3.2700536,3.38192,3.4495504,3.62713,3.598157,3.323637,3.7786636,3.6065862,3.4652224,1.9780455,3.9026237,3.3489106,3.3876085,3.2942867,2.9318392,3.4487443,3.2010171,3.1256676,3.4671955,2.5192537,3.271566,3.0086827,2.4685395,2.9230006,2.5349212,2.780228,2.8439276,2.6931646,2.9974837,2.6714573,2.4774532,2.0978115,2.6954489,3.6128035,3.6364634,2.9010477,3.1064317,3.3445606,3.3746455,3.0173275,5.548711,4.2836823,3.606938,4.4102063,4.033603,4.2659025,2.3628383,4.1762156,5.4049277,5.0041323,4.812867,7.2296686,5.498381,4.1782236,4.009825,4.217787,4.059786,3.6898773,2.4338608,2.524494,2.7957973,2.5314744,2.3589077,2.4968626,2.4745405,3.7713625,3.973371,4.311074,3.3752723,2.549503,2.526502,3.0052001,2.5069745,3.1299634,2.160785,3.442878,2.9727247,2.7559917,2.385845,2.3724182,2.3480682,2.3436282,3.8225405,3.9634016,3.4697468,2.5002356,1.9669924,2.6361551,4.9286666,4.5452676,4.569823,4.541851,4.5968924,4.5834684,4.358006,-0.7661285,4.164364,3.8263748,4.800327,3.9993522,4.567006,1.2888275,-9.393369,-9.405157,4.0513387,4.4917016,4.460768,4.2737527,3.797742,3.0817509,3.3528552,3.1410942,3.681594,3.462851,3.305943,3.351288,3.4287143,3.3658433,3.2469373,3.414246,3.3322835,3.632094,3.734938,0.65099764,3.7377474,3.8700228,4.817037,5.111243,4.9240313,3.5631623,4.832124,2.6313553,2.3222208,2.8890972,2.62056,2.2905362,4.6876755,2.5732036,2.4057405,2.8413436,2.2119312,2.2277474,2.1127775,3.0020483,3.2866383,3.6961198,3.6522617,3.393773,3.3953166,4.565721,-6.5481176,-0.86286044,3.2470164,2.6783283,4.532799,2.5521827,2.5760052,2.4885957,2.0592873,5.082353,4.4992776,3.4560177,3.297885,2.7527394,3.648104,3.116358,3.2723176,2.6337144,2.8530655,2.5031202,2.4526336,2.475455,3.046361,3.7350118,2.1256971,3.5108767,4.276438,2.9132423,3.0483835,2.2696993,2.35592,2.1146765,2.3225071,4.8826356,4.6218076,4.4034233,2.2980967,2.5436597,2.8511362,2.3300457,2.3373587,4.1091356,2.0437946,4.2817445,3.2689664,3.0995297,2.4253192,2.31777,2.3314128,3.6265867,3.4742057,3.3774111,2.359246,5.7079854,4.3162875,3.7466183,4.851093,2.2777483,2.35982,2.3038926,2.324606,2.3051927,2.237759,2.3448343,2.3115015,2.2657244,2.3405082,2.6365824,3.7494397,2.8328009,2.2558665,2.3645697,3.80974,3.9045472,3.867911,3.7066796,3.6397367,3.6367567,2.073215,2.1342704,2.2700307,2.548688,2.377509,3.893835,5.367348,5.6577697,5.4831095,5.0768003,5.5851307,5.1068606,2.5702372,2.3575342,2.4771378,3.8016293,4.0624285,4.2303743,3.3272371,2.4750853,3.2061908,2.3898432,3.7255075,3.3200114,4.69892,4.4758935,5.298847,5.484867,5.453954,5.0776896,5.582118,5.644883,5.174482,5.582237,4.7556596,2.7998593,5.5743494,3.4966104,3.9952493,3.6886616,4.029555,2.5892959,2.4625492,2.4226925,2.4023054,2.243832,2.196173,3.3821914,3.2252038,3.294184,3.4812384,3.495193,3.4375098,3.8143592,3.337991,4.0041294,3.4561977,3.4139266,3.3407154,1.7770997,0.750221,1.5922115,1.0559691,3.357773,3.3056605,5.173906,4.423921,4.818215,2.0263803,4.724041,3.7323275,5.178154,5.0231557,5.6334887,4.2913084,3.844608,5.0674343,4.7885427,4.829984,3.6727087,5.189933,3.7238955,5.6003284],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"New users are often very confused by the range of TPUs, and the different ways to access them. The f...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directl...\"],[\"When you access a free TPU via Colab, you generally get a single v2-8 TPU.\\n\\n### I keep hearing about...\"],[\"\\u003cTip\\u003e\\n\\n**ğŸ¤—Specific HuggingFace TipğŸ¤—:** Weâ€™ve put a lot of effort into rewriting our TensorFlow model...\"],[\"```\\n\\nThis might seem very restrictive at first, but most neural net code doesnâ€™t need to do this. Yo...\"],[\"```\\n\\nThis code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of ...\"],[\"```\\n\\nHere, we avoid data-dependent shapes by computing the loss for every position, but zeroing out ...\"],[\"\\u003cTip\\u003e\\n\\n**ğŸ¤—Specific HuggingFace TipğŸ¤—:** Our tokenizers and data collators have methods that can help ...\"],[\"### Summary\\n\\nThere was a lot in here, so letâ€™s summarize with a quick checklist you can follow when ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nğŸ‘€ The resulting model can be found here: https:\\u002f\\u002fhuggingface.co\\u002fnielsr\\u002flayoutlmv3-finetuned-fun...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderPro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Zero-shot object detection pipeline\\n\\nThe simplest way to try out inference with OWL-ViT is t...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```py\\n\\u003e\\u003e\\u003e predictions = detector(\\n...     image,\\n...     candidate_labels=[\\\"human face\\\", \\\"rocket\\\", \\\"...\"],[\"```\\n\\nLet's visualize the predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e from PIL import ImageDraw\\n\\n\\u003e\\u003e\\u003e draw = ImageDraw.Dra...\"],[\"```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n\\u003e\\u003e\\u003e import requests\\n\\n\\u003e\\u003e\\u003e url = \\\"https:...\"],[\"```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image pro...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nPreviously for post-processing you passed the single image's size as a tensor, but you can also...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fcircleci.com\\u002fgh\\u002fhuggingface\\u002ftransformers\\\"\\u003e\\n        \\u003cimg alt=...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fR...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fhf.co\\u002fcourse\\\"\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhug...\"],[\"ĞœĞ¾Ğ´ĞµĞ»Ğ¸ transformers Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ€Ğ°...\"],[\"ğŸ¤— Transformers Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ - [Jax](https:\\u002f\\u002fjax.r...\"],[\"Ğ’ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ NLP ( ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ ):\\n- [ĞœĞ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ...\"],[\"- [ĞĞ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ BART](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+me...\"],[\"- [ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ...\"],[\"DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+u...\"],[\"+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+...\"],[\"amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+depart...\"],[\"- [ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berli...\"],[\"Ğ’ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ:\\n- [ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ViT](https:\\u002f\\u002fhuggingface.co\\u002fg...\"],[\"Ğ’ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ²ÑƒĞºĞ°:\\n- [ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002ffac...\"],[\"Ğ’ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…:\\n- [ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fg...\"],[\"## 100 Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Transformers\\n\\nTransformers - ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾...\"],[\"Ğ•ÑĞ»Ğ¸ Ğ²Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ĞµÑÑŒ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†ĞµĞ¼ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹, Ğ¿Ğ¾ Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ²ĞºĞ»ÑÑ‡...\"],[\"## Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ³Ğ°Ğ¹Ğ´\\n\\nĞ”Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğµ (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ·Ğ²ÑƒĞº, ...) Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾...\"],[\"```\\n\\nĞ’Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° ĞºĞ¾Ğ´Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ¸ ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ÑƒÑ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾...\"],[\"# Ğ’Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²\\n\\u003e\\u003e\\u003e object_detector = pipeline('object-detection')\\n\\u003e\\u003e...\"],[\"```\\n\\nĞ—Ğ´ĞµÑÑŒ Ğ¼Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸, Ñ Ñ€Ğ°Ğ¼ĞºĞ¾Ğ¹ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½Ğº...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = AutoModel.from_pretra...\"],[\"```\\n\\nĞ Ğ²Ğ¾Ñ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ TensorFlow:\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, T...\"],[\"```\\n\\nĞ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° Ğ²ÑÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ...\"],[\"Ğ¡Ğ°Ğ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ [Pytorch `nn.Module`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fnn.html...\"],[\"## ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ transformers?\\n\\n1. ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:\\n    ...\"],[\"1. Ğ‘Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ \\\"ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ»ĞµĞ´\\\":\\n    - Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²...\"],[\"1. Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¶Ğ¸Ğ·Ğ½Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:\\n    - ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´...\"],[\"1. Ğ›ĞµĞ³ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¿Ğ¾Ğ´ ÑĞ²Ğ¾Ğ¸ Ğ½ÑƒĞ¶Ğ´Ñ‹:\\n    - ĞœÑ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚...\"],[\"- Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞšĞ¾Ğ´ Ğ² Ñ„Ğ°Ğ¹...\"],[\"- ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ñ‹ ÑÑ‚Ñ€ĞµĞ¼Ğ¸Ğ¼ÑÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ² Ğ½Ğ°...\"],[\"## Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°\\n\\n### Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ pip\\n\\nĞ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Python 3.8+, Flax 0.4.1+, PyTor...\"],[\"Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ¾Ğ´Ğ¸Ğ½ Ğ±ĞµĞºĞµĞ½Ğ´ Ğ¸Ğ· Flax, PyTorch Ğ¸Ğ»Ğ¸ TensorFlow.\\nĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚...\"],[\"```\\n\\nĞ•ÑĞ»Ğ¸ Ğ²Ñ‹ Ñ…Ğ¾Ñ‚Ğ¸Ñ‚Ğµ Ğ¿Ğ¾Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ²Ğ°Ğ¼ Ğ½ÑƒĞ¶ĞµĞ½ ÑĞ°Ğ¼Ñ‹Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²Ñ‹ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ¾...\"],[\"```\\n\\nĞ Ñ‚Ğ¾Ğ¼, ĞºĞ°Ğº ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Flax, PyTorch Ğ¸Ğ»Ğ¸ TensorFlow Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ conda, Ñ‡Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ñ…, Ğ¿Ğ¾ÑĞ²Ñ...\"],[\"Ğ¢ĞµĞºÑƒÑ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº: ![](https:\\u002f\\u002fimg.shields.io\\u002fendpoint?url=https:\\u002f\\u002fhuggingface.co...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BEiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbeit)** (from Microsoft) released wit...\"],[\"1. **[BigBird-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbig_bird)** (from Google R...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BLOOM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbloom)** (from BigScience workshop) ...\"],[\"1. **[CamemBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcamembert)** (from Inria\\u002fFaceboo...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (from OpenAI) released with t...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[ConvNeXT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnext)** (from Facebook AI) re...\"],[\"1. **[CTRL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fctrl)** (from Salesforce) released wi...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DePlot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeplot)** (from Google AI) released...\"],[\"1. **[DiNAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinat)** (from SHI Labs) released wi...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[DPR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdpr)** (from Facebook) released with t...\"],[\"1. **[EnCodec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fencodec)** (from Meta AI) released...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (from Meta AI) are transformer ...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (from CNRS) released ...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (from CMU\\u002fGoo...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (from OpenAI) released w...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (from OpenAI) released with ...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (from BigCode) r...\"],[\"1. **[GroupViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgroupvit)** (from UCSD, NVIDIA) r...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[Jukebox](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fjukebox)** (from OpenAI) released ...\"],[\"1. **[LayoutXLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutxlm)** (from Microsoft Res...\"],[\"1. **[LLaMA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama)** (from The FAIR team of Meta...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[Longformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongformer)** (from AllenAI) re...\"],[\"1. **[M2M100](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fm2m_100)** (from Facebook) released...\"],[\"1. **[Mask2Former](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmask2former)** (from FAIR and ...\"],[\"1. **[mBART-50](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released...\"],[\"1. **[MGP-STR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmgp-str)** (from Alibaba Research)...\"],[\"1. **[MobileNetV1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v1)** (from Google I...\"],[\"1. **[MPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmpt)** (from MosaiML) released with th...\"],[\"1. **[NAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnat)** (from SHI Labs) released with t...\"],[\"1. **[OneFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002foneformer)** (from SHI Labs) rel...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fmain\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft Research) ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (from UCLA NLP) released ...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (from Nanjing University, The U...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[SegFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsegformer)** (from NVIDIA) relea...\"],[\"1. **[SpeechT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeecht5)** (from Microsoft Resea...\"],[\"1. **[SqueezeBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsqueezebert)** (from Berkeley)...\"],[\"1. **[Swin2SR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin2sr)** (from University of WÃ¼r...\"],[\"1. **[Table Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftable-transformer)** (fr...\"],[\"1. **[Trajectory Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrajectory_transfor...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (from Goog...\"],[\"1. **[ViTMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_mae)** (from Meta AI) released ...\"],[\"1. **[ViViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvivit)** (from Google Research) rele...\"],[\"1. **[WavLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwavlm)** (from Microsoft Research) r...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta)** (from Facebook ...\"],[\"1. **[XLS-R](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxls_r)** (from Facebook AI) released...\"],[\"1. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in ...\"],[\"Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ñƒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Flax, PyTorch Ğ¸Ğ»Ğ¸ TensorFlow, Ğ¸Ğ»Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ...\"],[\"| Ğ¡ĞµĞºÑ†Ğ¸Ñ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |\\n|-|-|\\n| [Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002f) | ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ´Ğ¾Ğº...\"],[\"| [Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_shari...\"],[\"## Ğ¦Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ\\n\\nĞ¢ĞµĞ¿ĞµÑ€ÑŒ Ñƒ Ğ½Ğ°Ñ ĞµÑÑ‚ÑŒ [ÑÑ‚Ğ°Ñ‚ÑŒÑ](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f), ĞºĞ¾...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Repository features\\n\\nEach repository on the Model Hub behaves like a typical GitHub repos...\"],[\"```\\n\\nFiles are also easily edited in a repository, and you can view the commit history as well as th...\"],[\"```\\n\\n## Convert a model for all frameworks\\n\\nTo ensure your model can be used by someone working with...\"],[\"```\\n\\u003c\\u002fjax\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Push a model during training\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cYoutube id...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] fun...\"],[\"```\\n\\nThe `push_to_hub` function can also be used to add other files to a model repository. For examp...\"],[\"```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model rep...\"],[\"* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button i...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [fcakyon](https:\\u002f\\u002fhuggingface.co\\u002ffcakyon).\\nThe original code can be fo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale NLP models have been shown to significan...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\nFor fine-tuning, RemBERT can be thought of as a bigger version of mBERT with an ALBER...\"],[\"## RemBertForMultipleChoice\\n\\n[[autodoc]] RemBertForMultipleChoice\\n    - forward\\n\\n## RemBertForTokenC...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model augments the Transformer as a deep decomposition architecture, which can progressively de...\"],[\"- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-and-language reasoning requires an understand...\"],[\"## Usage tips\\n\\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any k...\"],[\"## LxmertForPreTraining\\n\\n[[autodoc]] LxmertForPreTraining\\n    - forward\\n\\n## LxmertForQuestionAnsweri...\"],[\"Author: [@vasudevgupta7](https:\\u002f\\u002fgithub.com\\u002fthevasudevgupta\\u002f)\\n\\n## Intro\\n\\nIn this project, we fine-tu...\"],[\"```\\n\\n## Evaluation\\n\\nOur evaluation script is different from the original script and we are evaluatin...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [Preprocessing data](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_doc\\u002fen\\u002fprepro...\"],[\"| [Summary of the tokenizers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_doc\\u002fen...\"],[\"### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"| [How to fine-tune a model on token classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to fine-tune a model on summarization](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexa...\"],[\"| [How to generate text](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f02_how_to_generate....\"],[\"| [Reformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f03_reformer.ipynb)| How Reforme...\"],[\"#### Computer Vision[[pytorch-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"| [How to fine-tune a model on image classification (Torchvision)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fno...\"],[\"| [How to fine-tune a model on image classification (Kornia)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"| [How to fine-tune an image captioning model](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fex...\"],[\"| [How to fine-tune a SegFormer model on semantic segmentation](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"#### Audio[[pytorch-audio]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a speech recognition model in any language](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"#### Biological Sequences[[pytorch-bio]]...\"],[\"| Notebook     | Description                                                                        ...\"],[\"| [How to generate protein folds](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexamples\\u002fprotei...\"],[\"| [Fine-tune a Nucleotide Transformer model with LoRA](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob...\"],[\"#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                ...\"],[\"#### Utility notebooks[[pytorch-utility]]\\n\\n| Notebook     |      Description      |   |   |\\n|:------...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"| [How to fine-tune a model on token classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to fine-tune a model on summarization](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexa...\"],[\"#### Computer Vision[[tensorflow-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to fine-tune a model on image classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----...\"],[\"#### Utility notebooks[[tensorflow-utility]]\\n\\n| Notebook     |      Description      |   |          ...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"| [How to fine-tune a model on text classification with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https:\\u002f\\u002fhf.c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nThe [`Trainer`] class is optimized for ğŸ¤— Transformers models and can have surp...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been o...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003cjax\\u003e\\n\\n## FlaxBigBirdModel\\n\\n[[autodoc]] FlaxBigBirdModel\\n    - __call__\\n\\n## FlaxBigBirdForPreT...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In terms of model details, the work outlines the architecture and training methodology of Persimmon-...\"],[\"Tips:\\n\\n- To convert the model, you need to clone the original repository using `git clone https:\\u002f\\u002fgi...\"],[\"```\\n\\nFor the chat model:\\n```bash\\nwget https:\\u002f\\u002faxtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-o...\"],[\"*TEMPLATE**\\n=====================================\\n\\n*search & replace the following keywords, e.g.:*\\n...\"],[\"General overview of ğŸ¤— Transformers\\n----------------------------------\\n\\nFirst, you should get a gener...\"],[\"Let's take a look:\\n\\n![image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolv...\"],[\"```\\n\\nSimilar to the model, the configuration inherits basic serialization and\\ndeserialization functi...\"],[\"From experience, we can tell you that the most important things to keep\\nin mind when adding a model ...\"],[\"10. [ ] Run end-to-end integration tests\\n\\n11. [ ] Finished docs\\n\\n12. [ ] Uploaded model weights to t...\"],[\"After you feel like you have gotten a good overview of the architecture\\nof the model, you might want...\"],[\"```\\n\\n3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    `...\"],[\"```\\n\\nNow you have set up a development environment to port *[camelcase name of model]*\\nto ğŸ¤— Transfor...\"],[\"It is very important that before you start the porting process, that you\\ncan **efficiently** debug c...\"],[\"```python\\nmodel = [camelcase name of model]Model.load_pretrained_checkpoint(\\\"\\u002fpath\\u002fto\\u002fcheckpoint\\u002f\\\")\\n...\"],[\"```\\n\\nNext, regarding the debugging strategy, there are generally a few from\\nwhich to choose from:\\n\\n-...\"],[\"No matter which strategy you choose, the recommended procedure is often\\nthe same in that you should ...\"],[\"```\\n\\nWe expect that every model added to ğŸ¤— Transformers passes a couple of\\nintegration tests, meanin...\"],[\"-   Find the best way of debugging intermediate results. Is the original\\n    repository written in P...\"],[\"you can directly input the ids instead of an input string.\\n-   Make sure that the model in your debu...\"],[\"#### More details on how to create a debugging environment for [camelcase name of model] \\n\\n[TODO FIL...\"],[\"```\\n    git checkout -b add_[lowercase name of model]\\n```\\n\\n2.  Commit the automatically generated co...\"],[\"```\\n\\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\\n    Click on \\\"Pull reques...\"],[\"Now you can finally start coding :). The generated code in\\n`src\\u002ftransformers\\u002fmodels\\u002f[lowercase name ...\"],[\"```\\n\\nThe above command will create a model according to the default\\nparameters as defined in `[camel...\"],[\"```python\\nfrom torch import nn\\n\\nclass SimpleModel(nn.Module):\\n    def __init__(self):\\n            su...\"],[\"```\\n\\nNow we can create an instance of this model definition which will fill\\nall weights: `dense`, `i...\"],[\"```\\n\\nto see that the weights were randomly initialized...\"],[\"```bash\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0...\"],[\"[-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,\\n          0.0333, -0.0536],...\"],[\"```\\n\\nIn the conversion script, you should fill those randomly initialized\\nweights with the exact wei...\"],[\"```\\n\\nIf either the shape or the name doesn't match, you probably assigned\\nthe wrong checkpoint weigh...\"],[\"```\\n\\n[TODO FILL: Here the mentor should add very specific information on what exactly has to be done...\"],[\"```\\n\\nIt is very likely that the ğŸ¤— Transformers implementation and the\\noriginal model implementation ...\"],[\"The best way to fix the problem is usually to look at the forward pass\\nof the original implementatio...\"],[\"```\\n\\n[TODO FILL: Here the mentor should add very specific information on what tests are likely to fa...\"],[\"```\\n\\n**Note:** In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n...\"],[\"```\\n\\nYou might have to take a deeper look again into the original repository\\nto find the correct tok...\"],[\"```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer\\ntest file should also ...\"],[\"**11. Add Docstring**\\n\\nNow, all the necessary functionality for *[camelcase name of model]* is added...\"],[\"```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality...\"],[\"```\\n\\nThere are a couple of other very strict design tests in ğŸ¤— Transformers\\nthat might still be fail...\"],[\"### Share your work!!\\n\\nNow, it's time to get some credit from the community for your work!\\nHaving co...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nFor Chinese models, we need to generate a reference files (which requires the ltp library), bec...\"],[\"```\\n\\nThen you can run the script like this: \\n\\n\\n```bash\\nexport TRAIN_FILE=\\u002fpath\\u002fto\\u002ftrain\\u002ffile\\nexport ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Implementation Notes\\n\\n- Each model is about 298 MB on disk, there are more than 1,000 models.\\n- T...\"],[\"## Examples\\n\\n- Since Marian models are smaller than many other translation models available in the l...\"],[\"\\u003e\\u003e\\u003e model_name = \\\"Helsinki-NLP\\u002fopus-mt-en-roa\\\"\\n\\u003e\\u003e\\u003e tokenizer = MarianTokenizer.from_pretrained(model...\"],[\"```\\n\\nHere is the code to see all available pretrained models on the hub:\\n\\n```python\\nfrom huggingface...\"],[\"```\\n\\n## Old Style Multi-Lingual Models\\n\\nThese are the old style multi-lingual models ported from the...\"],[\"```python no-style\\n['Helsinki-NLP\\u002fopus-mt-NORTH_EU-NORTH_EU',\\n 'Helsinki-NLP\\u002fopus-mt-ROMANCE-en',\\n '...\"],[\"'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lm...\"],[\"```\\n\\nExample of translating english to many romance languages, using old-style 2 character language ...\"],[\"```\\n\\n## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Att...\"],[\"[[autodoc]] Wav2Vec2ConformerForAudioFrameClassification\\n    - forward\\n\\n## Wav2Vec2ConformerForXVect...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"TAPEX has been fine-tuned on several datasets: \\n- [SQA](https:\\u002f\\u002fwww.microsoft.com\\u002fen-us\\u002fdownload\\u002fdet...\"],[\"## Usage tips\\n\\n- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPE...\"],[\"\\u003e\\u003e\\u003e encoding = tokenizer(table, question, return_tensors=\\\"pt\\\")\\n\\n\\u003e\\u003e\\u003e # let the model generate an answ...\"],[\"```\\n\\nNote that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of...\"],[\"```\\n\\nIn case one wants to do table verification (i.e. the task of determining whether a given senten...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nYou will use [PyTorchVideo](https:\\u002f\\u002fpytorchvideo.org\\u002f) (dubbed `pytorchvideo`) to process and p...\"],[\"```\\n\\nThe (`sorted`) video paths appear like so:\\n\\n```bash\\n...\\n'UCF101_subset\\u002ftrain\\u002fApplyEyeMakeup\\u002fv_A...\"],[\"```\\n\\nYou will notice that there are video clips belonging to the same group \\u002f scene where group is d...\"],[\"```\\n\\nThere are 10 unique classes. For each class, there are 30 videos in the training set.\\n\\n## Load ...\"],[\"```\\n\\nWhile the model is loading, you might notice the following warning:\\n\\n```bash\\nSome weights of th...\"],[\"```\\n\\nThe warning is telling us we are throwing away some weights (e.g. the weights and bias of the `...\"],[\"```\\n\\nFor the training dataset transformations, use a combination of uniform temporal subsampling, pi...\"],[\"```\\n\\nNow, define the dataset-specific transformations and the datasets respectively. Starting with t...\"],[\"```\\n\\nThe same sequence of workflow can be applied to the validation and evaluation sets: \\n\\n```py \\n\\u003e\\u003e...\"],[\"```\\n\\n**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https:...\"],[\"```\\n\\n## Visualize the preprocessed video for better debugging \\n\\n```py \\n\\u003e\\u003e\\u003e import imageio\\n\\u003e\\u003e\\u003e import...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"\\u003e\\u003e\\u003e args = TrainingArguments(\\n...     new_model_name,\\n...     remove_unused_columns=False,\\n...     e...\"],[\"```\\n\\nThe dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As...\"],[\"```\\n\\nThen you just pass all of this along with the datasets to `Trainer`:\\n\\n```py \\n\\u003e\\u003e\\u003e trainer = Trai...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nYou can also manually replicate the results of the `pipeline` if you'd like.\\n\\n\\n```py\\n\\u003e\\u003e\\u003e def ru...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Natural language understanding comprises a wide rang...\"],[\"```\\n\\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\\nus...\"],[\"- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https:\\u002f\\u002fwww.philschmid.de\\u002f...\"],[\"- [`OpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgi...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- A course material on [Byte-Pair Encoding tokenizat...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fzenodo.org\\u002fbadge\\u002flatestdoi\\u002f155220641\\\"\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fzenodo.org\\u002fbadge\\u002f1...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ğŸ¤— Transformers æä¾›äº†æ•°ä»¥åƒè®¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒ 100 å¤šç§è¯­è¨€çš„æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æŠ½å–ã€é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®©æœ€å…ˆè¿›çš„ NLP æŠ€æœ¯äººäººæ˜“ç”¨ã€‚\\n\\nğŸ¤— Transform...\"],[\"è¿™é‡Œæ˜¯ä¸€äº›ä¾‹å­ï¼š\\n- [ç”¨ BERT åšæ©ç å¡«è¯](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F...\"],[\"- [ç”¨ RoBERTa åšè‡ªç„¶è¯­è¨€æ¨ç†](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+a...\"],[\"- [ç”¨ DistilBERT...\"],[\"åšé—®ç­”](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+...\"],[\"+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+...\"],[\"+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+i...\"],[\"- [ç”¨ T5 åšç¿»è¯‘](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**ï¼Œç”±æŠ±æŠ±è„¸å›¢é˜Ÿæ‰“é€ ï¼Œæ˜¯ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ demoã€‚\\n\\n## å¦‚æœä½ åœ¨å¯»...\"],[\"```\\n\\nç¬¬äºŒè¡Œä»£ç ä¸‹è½½å¹¶ç¼“å­˜äº†æµæ°´çº¿ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè€Œç¬¬ä¸‰è¡Œä»£ç åˆ™åœ¨ç»™å®šçš„æ–‡æœ¬ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è¿™é‡Œçš„ç­”æ¡ˆâ€œæ­£é¢â€ (positive) å…·æœ‰ 99 çš„ç½®ä¿¡åº¦ã€‚\\n\\nè®¸å¤šçš„ NLP ä»»åŠ¡éƒ½æœ‰å¼€ç®±å³ç”¨çš„é¢„...\"],[\"```\\nè¿™é‡Œæ˜¯ç­‰æ•ˆçš„ TensorFlow ä»£ç ï¼š\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, TFAutoModel\\n\\n\\u003e\\u003e\\u003e tok...\"],[\"```\\n\\nè¯ç¬¦åŒ–å™¨ (tokenizer) ä¸ºæ‰€æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æä¾›äº†é¢„å¤„ç†ï¼Œå¹¶å¯ä»¥ç›´æ¥å¯¹å•ä¸ªå­—ç¬¦ä¸²è¿›è¡Œè°ƒç”¨ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–å¯¹åˆ—è¡¨ (list) è°ƒç”¨ã€‚å®ƒä¼šè¾“å‡ºä¸€ä¸ªä½ å¯ä»¥åœ¨ä¸‹æ¸¸ä»£ç é‡Œä½¿ç”¨æˆ–ç›´æ¥é€šè¿‡ ...\"],[\"1. å¯¹äºæ¨¡å‹ç”Ÿå‘½å‘¨æœŸçš„æ¯ä¸€ä¸ªéƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š\\n    - è®­ç»ƒå…ˆè¿›çš„æ¨¡å‹ï¼Œåªéœ€ 3 è¡Œä»£ç \\n    - æ¨¡å‹åœ¨ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æ¶é—´ä»»æ„è½¬ç§»ï¼Œéšä½ å¿ƒæ„\\n    - ä¸ºè®­ç»ƒã€è¯„ä¼°å’Œç”Ÿäº§é€‰æ‹©æœ€é€‚åˆçš„æ¡†æ¶ï¼Œè¡”...\"],[\"### ä½¿ç”¨ pip\\n\\nè¿™ä¸ªä»“åº“å·²åœ¨ Python 3.8+ã€Flax 0.4.1+ã€PyTorch 1.10+ å’Œ TensorFlow 2.6+ ä¸‹ç»è¿‡æµ‹è¯•ã€‚\\n\\nä½ å¯ä»¥åœ¨[è™šæ‹Ÿç¯å¢ƒ](https:...\"],[\"```\\n\\nå¦‚æœä½ æƒ³è¦è¯•è¯•ç”¨ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼å‘å¸ƒå‰ä½¿ç”¨æœ€æ–°çš„å¼€å‘ä¸­ä»£ç ï¼Œä½ å¾—[ä»æºä»£ç å®‰è£…](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002finstallation#i...\"],[\"```\\n\\nè¦é€šè¿‡ conda å®‰è£… Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè¯·å‚é˜…å®ƒä»¬å„è‡ªå®‰è£…é¡µçš„è¯´æ˜ã€‚\\n\\n## æ¨¡å‹æ¶æ„\\n\\nğŸ¤— Transformers æ”¯æŒçš„[**æ‰€æœ‰çš„æ¨¡å‹...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (æ¥è‡ª Google Research and t...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (æ¥è‡ª VinAI Research) ä¼´éšè®º...\"],[\"1. **[BigBird-Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbigbird_pegasus)** (æ¥è‡ª Goo...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [Big Transf...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (æ¥è‡ª Salesforce) ä¼´éšè®ºæ–‡ [BLI...\"],[\"1. **[ByT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbyt5)** (æ¥è‡ª Google Research) ä¼´éšè®ºæ–‡ [By...\"],[\"1. **[CLAP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclap)** (æ¥è‡ª LAION-AI) ä¼´éšè®ºæ–‡ [Large-sca...\"],[\"1. **[CodeGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcodegen)** (æ¥è‡ª Salesforce) ä¼´éšè®ºæ–‡ [A...\"],[\"1. **[ConvBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvbert)** (æ¥è‡ª YituTech) ä¼´éšè®ºæ–‡ [C...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (æ¥è‡ª Tsinghua University) ä¼´éšè®ºæ–‡ [...\"],[\"1. **[Data2Vec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdata2vec)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [D...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (æ¥è‡ª Sen...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [End-to-En...\"],[\"1. **[DINOv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinov2)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ [DINOv2...\"],[\"1. **[DiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdit)** (æ¥è‡ª Microsoft Research) ä¼´éšè®ºæ–‡ [D...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (æ¥è‡ª Sna...\"],[\"1. **[ERNIE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie)** (æ¥è‡ª Baidu) ä¼´éšè®ºæ–‡ [ERNIE: Enh...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (from Meta AI) are transformer ...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (æ¥è‡ª CNRS) ä¼´éšè®ºæ–‡ [FlauB...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (æ¥è‡ª CMU\\u002fGoogl...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (æ¥è‡ª OpenAI) ä¼´éšè®ºæ–‡ [Improv...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (æ¥è‡ª OpenAI) ä¼´éšè®ºæ–‡ [Language M...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (æ¥è‡ª BigCode) ä¼´éšè®º...\"],[\"1. **[GroupViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgroupvit)** (æ¥è‡ª UCSD, NVIDIA) ä¼´éšè®º...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[Jukebox](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fjukebox)** (from OpenAI) released ...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (æ¥è‡ª Microsoft Res...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (æ¥è‡ª South China University of...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (æ¥è‡ª The FAIR team of Meta...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (æ¥è‡ª Microsoft Research & Un...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (æ¥è‡ª UNC Chapel Hill) ä¼´éšè®ºæ–‡...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (from Meta and UI...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [Mega: Mov...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [Scaling Spe...\"],[\"1. **[MobileViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevit)** (æ¥è‡ª Apple) ä¼´éšè®ºæ–‡ [Mo...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [mT5: A mas...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (æ¥è‡ªåä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤) ä¼´éšè®ºæ–‡ [NEZHA: ...\"],[\"1. **[NystrÃ¶mformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (æ¥è‡ª the Uni...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [Sim...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (æ¥è‡ª Google) ä¼´éšè®ºæ–‡ [PEGAS...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (æ¥è‡ª UCLA NLP) ä¼´éšè®ºæ–‡ [Unifi...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (æ¥è‡ª Nanjing University, The Uni...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (æ¥è‡ª Google Research) ...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[SeamlessM4T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t)** (from Meta AI)...\"],[\"1. **[SEW](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew)** (æ¥è‡ª ASAPP) ä¼´éšè®ºæ–‡ [Performance-Ef...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin)** (æ¥è‡ª Microsoft) ä¼´éš...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [Exploring th...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (æ¥è‡ª Microsoft Research) ä¼´éšè®º...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (æ¥è‡ª Microsoft) ä¼´éšè®ºæ–‡ [TrOCR:...\"],[\"1. **[UMT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fumt5)** (æ¥è‡ª Google Research) ä¼´éšè®ºæ–‡ [Un...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (æ¥è‡ª Peking University) ...\"],[\"1. **[VipLlava](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvipllava)** (æ¥è‡ª University of Wis...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (æ¥è‡ª Google AI) ä¼´éš...\"],[\"1. **[ViTMSN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_msn)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ [Maske...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (æ¥è‡ª Microsoft Research) ä¼´éš...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [Cross-lingu...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ [XLM-V: O...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (æ¥è‡ª Faceboo...\"],[\"è¦æ£€æŸ¥æŸä¸ªæ¨¡å‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å®ç°ï¼Œæˆ–å…¶æ˜¯å¦åœ¨ ğŸ¤— Tokenizers åº“ä¸­æœ‰å¯¹åº”è¯ç¬¦åŒ–å™¨ï¼ˆtokenizerï¼‰ï¼Œæ•¬è¯·å‚é˜…[æ­¤è¡¨](https:\\u002f\\u002fh...\"],[\"## å¼•ç”¨\\n\\næˆ‘ä»¬å·²å°†æ­¤åº“çš„[è®ºæ–‡](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)æ­£å¼å‘è¡¨ï¼Œå¦‚æœä½ ä½¿ç”¨äº† ğŸ¤— Transformers åº“...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"There are several libraries for quantizing models with the AWQ algorithm, such as [llm-awq](https:\\u002f\\u002f...\"],[\"```\\n\\nAWQ-quantized models can be identified by checking the `quantization_config` attribute in the m...\"],[\"```\\n\\nAWQ quantization can also be combined with [FlashAttention-2](perf_infer_gpu_one#flashattention...\"],[\"```\\n\\n### Fused modules\\n\\nFused modules offers improved accuracy and performance and it is supported o...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"unsupported architectures\\\"\\u003e\\n\\nFor architectures that don't support fus...\"],[\"```\\n\\nThe parameter `modules_to_fuse` should include:\\n\\n- `\\\"attention\\\"`: The names of the attention la...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe [AutoGPTQ](https:\\u002f\\u002fgithub.com\\u002fPanQiWei\\u002fAutoGPTQ) library implements the GPTQ algorithm, ...\"],[\"```\\n\\nTo quantize a model (currently only supported for text models), you need to create a [`GPTQConf...\"],[\"```\\n\\nIf you're running out of memory because a dataset is too large, disk offloading is not supporte...\"],[\"```\\n\\nYou could also save your quantized model locally with the [`~PreTrainedModel.save_pretrained`] ...\"],[\"```\\n\\n### ExLlama\\n\\n[ExLlama](https:\\u002f\\u002fgithub.com\\u002fturboderp\\u002fexllama) is a Python\\u002fC++\\u002fCUDA implementatio...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nOnly 4-bit models are supported, and we recommend deactivating the ExLlam...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"4-bit\\\"\\u003e\\n\\n```bash\\npip install bitsandbytes\\u003e=0.39.0\\npip install --upgra...\"],[\"```\\n\\nOnce a model is quantized to 8-bit, you can't push the quantized weights to the Hub unless you'...\"],[\"```\\n\\nOnce a model is quantized to 4-bit, you can't push the quantized weights to the Hub.\\n\\n\\u003c\\u002fhfoptio...\"],[\"```\\n\\nDesign a custom device map to fit everything on your GPU except for the `lm_head`, which you'll...\"],[\"```\\n\\n#### Skip module conversion\\n\\nFor some models, like [Jukebox](model_doc\\u002fjukebox), you don't need...\"],[\"```\\n\\n#### Finetuning\\n\\nWith the [PEFT](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft) library, you can finetune...\"],[\"```\\n\\n#### Normal Float 4 (NF4)\\n\\nNF4 is a 4-bit data type from the [QLoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2305....\"],[\"```\\n\\nFor inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, t...\"],[\"```\\n\\n## Optimum\\n\\nThe [Optimum](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002findex) library supports quantizat...\"],[\"\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the ...\"],[\"\\u003cfigcaption class=\\\"text-center text-gray-500 text-lg\\\"\\u003eUnfused module\\u003c\\u002ffigcaption\\u003e\\n\\n|   Batch Size | ...\"],[\"\\u003cfigcaption class=\\\"text-center text-gray-500 text-lg\\\"\\u003eFused module\\u003c\\u002ffigcaption\\u003e\\n\\n|   Batch Size |   ...\"],[\"The speed and throughput of fused and unfused modules were also tested with the [optimum-benchmark](...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The model is trained end-to-end with a combination of losses derived from variational lower bound an...\"],[\"## Usage examples\\n\\nBoth the VITS and MMS-TTS checkpoints can be used with the same API. Since the fl...\"],[\"```\\n\\nThe resulting waveform can be saved as a `.wav` file:\\n\\n```python\\nimport scipy\\n\\nscipy.io.wavfile...\"],[\"```\\n\\nYou can then pre-process the text input using the following code snippet. You can either rely o...\"],[\"```\\n\\n## VitsConfig\\n\\n[[autodoc]] VitsConfig\\n\\n## VitsTokenizer\\n\\n[[autodoc]] VitsTokenizer\\n    - __call...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper shows that pretraining multilingual langu...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForMaskedLM`] is supported by this [example scrip...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForQuestionAnswering`] is supported by t...\"],[\"**Multiple choice**\\n\\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https:\\u002f...\"],[\"## XLMRobertaForCausalLM\\n\\n[[autodoc]] XLMRobertaForCausalLM\\n    - forward\\n\\n## XLMRobertaForMaskedLM\\n...\"],[\"## FlaxXLMRobertaModel\\n\\n[[autodoc]] FlaxXLMRobertaModel\\n    - __call__\\n\\n## FlaxXLMRobertaForCausalLM...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fimagegpt_arc...\"],[\"- ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that a different activati...\"],[\"use [`ImageGPTForImageClassification`].\\n- ImageGPT comes in different sizes: there's ImageGPT-small,...\"],[\"| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size** | **Params (M)** | **I...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Piano covers of pop music are enjoyed by many people...\"],[\"```\\npip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\\n```\\nPlease note that you ma...\"],[\"```\\n\\n- Example using your own audio file:\\n\\n```python\\n\\u003e\\u003e\\u003e import librosa\\n\\u003e\\u003e\\u003e from transformers import...\"],[\"```\\n\\n- Example of processing multiple audio files in batch:\\n\\n```python\\n\\u003e\\u003e\\u003e import librosa\\n\\u003e\\u003e\\u003e from t...\"],[\"```\\n\\n\\n- Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor` and `...\"],[\"\\u003e\\u003e\\u003e # Since we now have 2 generated MIDI files\\n\\u003e\\u003e\\u003e tokenizer_output[0].write(\\\".\\u002fOutputs\\u002fmidi_output1...\"],[\"```\\n\\n\\n## Pop2PianoConfig\\n\\n[[autodoc]] Pop2PianoConfig\\n\\n## Pop2PianoFeatureExtractor\\n\\n[[autodoc]] Pop...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003csmall\\u003e Deformable DETR architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2010.04159\\\"\\u003eorig...\"],[\"### Fine-tuning BERT on SQuAD1.0 with relative position embeddings\\n\\nThe following examples show how ...\"],[\"##### Base models fine-tuning\\n\\n```bash\\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\\ntorchrun --nproc_...\"],[\"```\\nTraining with the above command leads to the following results. It boosts the BERT default from ...\"],[\"```\\nTraining with the above command leads to the f1 score of 93.52, which is slightly better than th...\"],[\"```\\n\\n##### Results for SQuAD2.0 with the previously defined hyper-parameters:\\n\\n```python\\n{\\n\\\"exact\\\": ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Detection Transformer (DETR) directly transforms que...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large Transformer models routinely achieve state-of-...\"],[\"### Axial Positional Encodings\\n\\nAxial Positional Encodings were first implemented in Google's [trax ...\"],[\"with:\\n\\n$$d = d^1 + d^2 \\\\text{ and } n_s = n_s^1 \\\\times n_s^2 .$$\\n\\nTherefore the following holds:\\n\\n$$...\"],[\"In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple \\\\\\\\((d^1, d^2)\\\\\\\\) which sum...\"],[\"For more information, see the [original Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2001.04451) or this great [blog...\"],[\"### Local Self Attention\\n\\nLocal self attention is essentially a \\\"normal\\\" self attention layer with k...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Question ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] {{cookiecutter.camelcase_modelname}}Model\\n    - forward\\n\\n{% if cookiecutter.is_encoder_d...\"],[\"[[autodoc]] {{cookiecutter.camelcase_modelname}}ForQuestionAnswering\\n    - forward\\n\\n\\n## {{cookiecutt...\"],[\"[[autodoc]] TF{{cookiecutter.camelcase_modelname}}ForQuestionAnswering\\n    - call\\n\\n\\n{%- else %}\\n## T...\"],[\"[[autodoc]] Flax{{cookiecutter.camelcase_modelname}}ForQuestionAnswering\\n    - call\\n\\n\\n{%- else %}\\n##...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained language models have attracted increasin...\"],[\"## Usage tips\\n\\n- BioGPT is a model with absolute position embeddings so it's usually advised to pad ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, plain vision Transformers (ViTs) have show...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, Speec...\"],[\"```\\n\\n## Initialising `SpeechEncoderDecoderModel` from a pretrained encoder and a pretrained decoder....\"],[\"```\\n\\n## Loading an existing `SpeechEncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load f...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"\\u003e\\u003e\\u003e # load its corresponding transcription and tokenize to generate labels\\n\\u003e\\u003e\\u003e labels = tokenizer(ds...\"],[\"```\\n\\n## SpeechEncoderDecoderConfig\\n\\n[[autodoc]] SpeechEncoderDecoderConfig\\n\\n## SpeechEncoderDecoderM...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Backward\\n\\nThe last addition is to replace the typical `loss.backward()` in your training loo...\"],[\"```\\n\\nAs you can see in the following code, you only need to add four additional lines of code to you...\"],[\"```\\n\\nThen launch your training with:\\n\\n```bash\\naccelerate launch train.py\\n```\\n\\n### Train with a noteb...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose focal modulation networks (FocalNets in s...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Pipeline\\n\\n\\u003cYoutube id=\\\"tiZFewofSLM\\\"\\u002f\\u003e\\n\\nThe [`pipeline`] is the eas...\"],[\"| **Task**                     | **Description**                                                    ...\"],[\"| Visual question answering    | answer a question about the image, given an image and a question   ...\"],[\"Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this ...\"],[\"```\\n\\nThe [`pipeline`] downloads and caches a default [pretrained model](https:\\u002f\\u002fhuggingface.co\\u002fdisti...\"],[\"```\\n\\nYou need to make sure the sampling rate of the dataset matches the sampling \\nrate [`facebook\\u002fwa...\"],[\"```\\n\\nFor larger datasets where the inputs are big (like in speech or vision), you'll want to pass a ...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nSpecify the model and tokenizer in the [`pipeline`], and now you can ...\"],[\"```\\n\\nIf you can't find a model for your use-case, you'll need to finetune a pretrained model on your...\"],[\"```\\n\\nPass your text to the tokenizer:\\n\\n```py\\n\\u003e\\u003e\\u003e encoding = tokenizer(\\\"We are very happy to show you...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [preprocess](.\\u002fpreprocessing) tutorial for more ...\"],[\"```\\n\\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t...\"],[\"```\\n\\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nOnce your model is fine-tuned, you can save it with its tokenizer using [`TFPreTraine...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Custom model builds\\n\\nYou can modify the model's configuration clas...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nTake a look at the [Create a custom architecture](.\\u002fcreate_a_model) g...\"],[\"```\\n\\n3. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor tasks - like translation or summarization - that use a sequence-to-sequence model, u...\"],[\"```py\\n   \\u003e\\u003e\\u003e from transformers import TFAutoModelForSequenceClassification\\n\\n   \\u003e\\u003e\\u003e model = TFAutoMod...\"],[\"```\\n\\n2. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Randomly initializing `EncoderDecoderModel` from model configurations.\\n\\n[`EncoderDecoderModel`] c...\"],[\"```\\n\\n## Initialising `EncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\\n\\n[`En...\"],[\"```\\n\\n## Loading an existing `EncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load fine-tu...\"],[\"```\\n\\n## Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.\\n\\n[`TFEncoderDecoderModel.from_pre...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"\\u003e\\u003e\\u003e labels = tokenizer(\\n...     \\\"the eiffel tower surpassed the washington monument to become the ta...\"],[\"```\\n\\nDetailed [colab](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-attention has become a defacto choice for captu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"âš ï¸ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to M...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we develop and release Llama 2, a coll...\"],[\"The `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\\\"auto\\\"` whe...\"],[\"Training the model in `float16` is not recommended and is known to produce `nan`; as such, the model...\"],[\"```bash\\npython src\\u002ftransformers\\u002fmodels\\u002fllama\\u002fconvert_llama_weights_to_hf.py \\\\\\n    --input_dir \\u002fpath\\u002f...\"],[\"```\\n\\n- After conversion, the model and tokenizer can be loaded via:\\n\\n```python\\nfrom transformers imp...\"],[\"```\\n\\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\"\\u002f\\u003e\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1P...\"],[\"âš¡ï¸ Inference\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1TC56ArKerXUpbgRy5vM3woRsbTEVNq7...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Francesco](https:\\u002f\\u002fhuggingface.co\\u002fFrancesco). The TensorFlow version ...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFRegNetModel\\n\\n[[autodoc]] TFRegNetModel\\n    - call\\n\\n## TFRegNetForImageClassificatio...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised learning (SSL) achieves great succes...\"],[\"## WavLMConfig\\n\\n[[autodoc]] WavLMConfig\\n\\n## WavLMModel\\n\\n[[autodoc]] WavLMModel\\n    - forward\\n\\n## Wav...\"],[\"Performer fine-tuning\\n\\nExample authors: @TevenLeScao, @Patrickvonplaten\\n\\nPaper authors: Krzysztof Ch...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Deeper neural networks are more difficult to train. ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"End-to-End finetuning of RAG (including DPR retriever) for Question Answering.\\n\\nThis finetuning scri...\"],[\"To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script als...\"],[\"# Results\\n\\n- We train both models for 10 epochs.\\n\\n| Model Type          | EM-Score|\\n| --------------...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"This model was contributed by [shangz](https:\\u002f\\u002fhuggingface.co\\u002fshangz).\\n\\n## Usage tips\\n\\n- QDQBERT mod...\"],[\"Before creating QDQBERT model, one has to set the default `QuantDescriptor` defining default tensor ...\"],[\"```\\n\\n### Calibration\\n\\nCalibration is the terminology of passing data samples to the quantizer and de...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten). The Autho...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Depth estimation pipeline\\n\\nThe simplest way to try out inference with a model supporting dep...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nVisualize the results:\\n\\n```py\\n\\u003e\\u003e\\u003e import numpy as np\\n\\n\\u003e\\u003e\\u003e # interpolate to original size\\n\\u003e\\u003e\\u003e pr...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Most neural vocoders employ band-limited mel-spectro...\"],[\"Tips:\\n\\n- The `noise_sequence` argument for [`UnivNetModel.forward`] should be standard Gaussian nois...\"],[\"ds = load_dataset(\\\"hf-internal-testing\\u002flibrispeech_asr_dummy\\\", \\\"clean\\\", split=\\\"validation\\\")\\n# Resamp...\"],[\"```\\n\\nThis model was contributed by [dg845](https:\\u002f\\u002fhuggingface.co\\u002fdg845).\\nTo the best of my knowledg...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Understanding document images (e.g., invoices) is a ...\"],[\"## Usage tips\\n\\n- The quickest way to get started with Donut is by checking the [tutorial\\n  notebooks...\"],[\"\\u003e\\u003e\\u003e # prepare decoder inputs\\n\\u003e\\u003e\\u003e task_prompt = \\\"\\u003cs_rvlcdip\\u003e\\\"\\n\\u003e\\u003e\\u003e decoder_input_ids = processor.token...\"],[\"```\\n\\n- Step-by-step Document Parsing\\n\\n```py\\n\\u003e\\u003e\\u003e import re\\n\\n\\u003e\\u003e\\u003e from transformers import DonutProcess...\"],[\"\\u003e\\u003e\\u003e sequence = processor.batch_decode(outputs.sequences)[0]\\n\\u003e\\u003e\\u003e sequence = sequence.replace(processo...\"],[\"```\\n\\n- Step-by-step Document Visual Question Answering (DocVQA)\\n\\n```py\\n\\u003e\\u003e\\u003e import re\\n\\n\\u003e\\u003e\\u003e from trans...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\n\\u003e\\u003e\\u003e outputs = model.generate(...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=donut) to look for Donut checkpoints.\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer based models, like BERT and RoBERTa, hav...\"],[\"## IBertConfig\\n\\n[[autodoc]] IBertConfig\\n\\n## IBertModel\\n\\n[[autodoc]] IBertModel\\n    - forward\\n\\n## IBe...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr). The original code (written in...\"],[\"- To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-o...\"],[\"2D interpolation of the pre-trained position embeddings, according to their location in the original...\"],[\"## Resources\\n\\nDemo notebooks regarding inference as well as fine-tuning ViT on custom data can be fo...\"],[\"âš—ï¸ Optimization\\n\\n- A blog post on how to [Accelerate Vision Transformer (ViT) with Quantization usin...\"],[\"\\u003c\\u002ftf\\u003e\\n\\u003cjax\\u003e\\n\\n## FlaxVitModel\\n\\n[[autodoc]] FlaxViTModel\\n    - __call__\\n\\n## FlaxViTForImageClassificat...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- Speech2Text2 achieves state-of-the-art results on the CoVoST Speech Translation dat...\"],[\"\\u003e\\u003e\\u003e def map_to_array(batch):\\n...     speech, _ = sf.read(batch[\\\"file\\\"])\\n...     batch[\\\"speech\\\"] = sp...\"],[\"```\\n\\n- Speech Translation via Pipelines\\n\\n  The automatic speech recognition pipeline can also be use...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The `truncation` argument controls truncation. It can be a boolean or a string:\\n\\n  - `True` or `'lon...\"],[\"| Truncation                           | Padding                           | Instruction            ...\"],[\"|                                      | padding to max model input length | `tokenizer(batch_senten...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nwhere task name can be one of cola, sst2, mrpc, stsb, qqp, mnli, qnli, rte, wnli.\\n\\nWe get the f...\"],[\"The following example fine-tunes BERT on the `imdb` dataset hosted on our [hub](https:\\u002f\\u002fhuggingface....\"],[\"```\\n\\n\\u003e If your model classification head dimensions do not fit the number of labels in the dataset, ...\"],[\"```\\nTraining for 1 epoch results in acc of around 0.5958 for review_body only and 0.659 for title+bo...\"],[\"```\\n It results in a Micro F1 score of around 0.82 without any text and label filtering. Note that y...\"],[\"Using mixed precision training usually results in 2x-speedup for training with the same final result...\"],[\"Like `run_glue.py`, this script allows you to fine-tune any of the models on the [hub](https:\\u002f\\u002fhuggi...\"],[\"```\\n\\nthen\\n\\n```bash\\nexport TASK_NAME=mrpc\\n\\npython run_glue_no_trainer.py \\\\\\n  --model_name_or_path ber...\"],[\"```\\n\\nThis command is the same and will work for:\\n\\n- a CPU-only setup\\n- a setup with one GPU\\n- a dist...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision Transformers (ViT) have shown rapid progress ...\"],[\"## Documentation resources\\n\\n- [Image classification task guide](..\\u002ftasks\\u002fimage_classification)\\n\\n## E...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Fine-tuning ViLT\\n\\nViLT model incorporates text embeddings into a Vision Transformer (ViT), allowi...\"],[\"```\\n\\nWe encourage you to share your model with the community. Log in to your Hugging Face account to...\"],[\"```\\n\\nLet's take a look at an example to understand the dataset's features:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset[0]\\n{'q...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nNow that we have the mappings, we can replace the string answers with their ids, and flatten th...\"],[\"```\\n\\nTo preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. ...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets [`~datasets.map`] f...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nThe model in this guide has only been trained on 200 examples, so don't expect a lot from it. L...\"],[\"```\\n\\n## Zero-shot VQA\\n\\nThe previous model treated VQA as a classification task. Some recent models, ...\"],[\"```\\n\\nNow we need to preprocess the image\\u002fprompt with the model's processor, pass the processed input...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```\\n\\nwhere task name can be one of cola, mnli, mnli_mismatched, mnli_matched, mrpc, qnli, qqp, rte, ...\"],[\"```\\n\\nor directly on the hub under *Training metrics*.\\n\\n### Accuracy Evaluation\\n\\nWe train five replic...\"],[\"| Task  | Metric                       | Acc (best run) | Acc (avg\\u002f5runs) | Stdev     | Metrics     ...\"],[\"| QQP   | Accuracy\\u002fF1                  | 90.81\\u002f87.58    | 90.76\\u002f87.51     | 0.05\\u002f0.06 | [tfhub.dev](...\"],[\"Some of these results are significantly different from the ones reported on the test set of GLUE ben...\"],[\"| Task  | TPU v3-8  | 8 GPU      | [1 GPU](https:\\u002f\\u002ftensorboard.dev\\u002fexperiment\\u002fmkPS4Zh8TnGe1HB6Yzwj4Q...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose an efficient design of Transformer-based ...\"],[\"## PatchTSTConfig\\n\\n[[autodoc]] PatchTSTConfig\\n\\n## PatchTSTModel\\n\\n[[autodoc]] PatchTSTModel\\n    - for...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ğŸ¤— TransformersëŠ” ë¶„ë¥˜, ì •ë³´ ì¶”ì¶œ, ì§ˆë¬¸ ë‹µë³€, ìš”ì•½, ë²ˆì—­, ë¬¸ì¥ ìƒì„± ë“±ì„ 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì²œê°œì˜ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª©...\"],[\"ğŸ¤— TransformersëŠ” ê°€ì¥ ìœ ëª…í•œ 3ê°œì˜ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ë“¤ì€ ì„œë¡œ ì™„ë²½íˆ ì—°ë™ë©ë‹ˆë‹¤ â€” [Jax](https:\\u002f\\u002fjax.readthedocs.io\\u002fen\\u002f...\"],[\"ì˜ˆì‹œ:\\n- [BERTë¡œ ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ ì™„ì„±í•˜ê¸°](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+...\"],[\"- [BARTë¥¼ ì´ìš©í•œ ìš”ì•½](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+metres+%281%2C...\"],[\"- [DistilBERTë¥¼ ì´ìš©í•œ ì§ˆë¬¸...\"],[\"ë‹µë³€](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+d...\"],[\"est+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+squa...\"],[\"nts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+department...\"],[\"- [T5ë¡œ ë²ˆì—­í•˜ê¸°](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[Transformerì™€ ê¸€ì“°ê¸°](https:\\u002f\\u002ftransformer.huggingface.co)** ëŠ” ì´ ì €ì¥ì†Œì˜ í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ì— ê´€í•œ Hugging Face íŒ€ì˜ ê³µì‹...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n# Allocate a pipeline for sentiment-analysis\\n\\u003e\\u003e\\u003e cl...\"],[\"```\\n\\nì½”ë“œì˜ ë‘ë²ˆì§¸ ì¤„ì€ pipelineì´ ì‚¬ìš©í•˜ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œë¡œ ì €ì¥í•©ë‹ˆë‹¤. ì„¸ë²ˆì§¸ ì¤„ì—ì„  ê·¸ ëª¨ë¸ì´ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë¸ì€ 99.9...\"],[\"```\\n\\në‹µë³€ë¿ë§Œ ì•„ë‹ˆë¼, ì—¬ê¸°ì— ì‚¬ìš©ëœ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ í™•ì‹ ë„ì™€ í† í¬ë‚˜ì´ì¦ˆëœ ë¬¸ì¥ ì† ë‹µë³€ì˜ ì‹œì‘ì , ëì ê¹Œì§€ ë°˜í™˜í•©ë‹ˆë‹¤. [ì´ íŠœí† ë¦¬ì–¼](https:\\u002f\\u002fhuggingface.c...\"],[\"```\\n\\ní† í¬ë‚˜ì´ì €ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ì˜ ëª¨ë“  ì „ì²˜ë¦¬ë¥¼ ì±…ì„ì§‘ë‹ˆë‹¤. ê·¸ë¦¬ê³  (ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼) 1ê°œì˜ ìŠ¤íŠ¸ë§ì´ë‚˜ ë¦¬ìŠ¤íŠ¸ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ”ë°, ì´ëŠ” ...\"],[\"ëª¨ë¸ ìì²´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” [Pytorch `nn.Module`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fnn.html#torch.nn.Module)ë‚˜ [T...\"],[\"1. ë” ì ì€ ê³„ì‚° ë¹„ìš©, ë” ì ì€ íƒ„ì†Œ ë°œìêµ­:\\n    - ì—°êµ¬ìë“¤ì€ ëª¨ë¸ì„ ê³„ì† ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ëŠ” ëŒ€ì‹  í•™ìŠµëœ ëª¨ë¸ì„ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n    - ì‹¤ë¬´ìë“¤ì€ í•™ìŠµì— í•„ìš”í•œ ì‹œ...\"],[\"1. í•„ìš”í•œ ëŒ€ë¡œ ëª¨ë¸ì´ë‚˜ ì˜ˆì‹œë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ì„¸ìš”:\\n    - ìš°ë¦¬ëŠ” ì €ìê°€ ê³µê°œí•œ ê²°ê³¼ë¥¼ ì¬í˜„í•˜ê¸° ìœ„í•´ ê° ëª¨ë¸ êµ¬ì¡°ì˜ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\\n    - ëª¨ë¸ ë‚´ë¶€ êµ¬ì¡°ëŠ” ê°€ëŠ¥í•œ ...\"],[\"## ì™œ transformersë¥¼ ì‚¬ìš©í•˜ì§€ ë§ì•„ì•¼ í• ê¹Œìš”?\\n\\n- ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì‹ ê²½ë§ ë¸”ë¡ì„ ë§Œë“¤ê¸° ìœ„í•œ ëª¨ë“ˆì´ ì•„ë‹™ë‹ˆë‹¤. ì—°êµ¬ìë“¤ì´ ì—¬ëŸ¬ íŒŒì¼ì„ ì‚´í´ë³´ì§€ ì•Šê³  ë°”ë¡œ ê° ëª¨ë¸ì„ ...\"],[\"## ì„¤ì¹˜\\n\\n### pipë¡œ ì„¤ì¹˜í•˜ê¸°\\n\\nì´ ì €ì¥ì†ŒëŠ” Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, TensorFlow 2.6+ì—ì„œ í…ŒìŠ¤íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\n...\"],[\"ì´ë“¤ ì¤‘ ì ì–´ë„ í•˜ë‚˜ê°€ ì„¤ì¹˜ë˜ì—ˆë‹¤ë©´, ğŸ¤— TransformersëŠ” ë‹¤ìŒê³¼ ê°™ì´ pipì„ ì´ìš©í•´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\\n\\n```bash\\npip install transformers...\"],[\"```\\n\\nì˜ˆì‹œë“¤ì„ ì²´í—˜í•´ë³´ê³  ì‹¶ê±°ë‚˜, ìµœìµœìµœì²¨ë‹¨ ì½”ë“œë¥¼ ì›í•˜ê±°ë‚˜, ìƒˆë¡œìš´ ë²„ì „ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦´ ìˆ˜ ì—†ë‹¤ë©´ [ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì†ŒìŠ¤ì—ì„œ ë°”ë¡œ ì„¤ì¹˜](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\nFlax, PyTorch, TensorFlow ì„¤ì¹˜ í˜ì´ì§€ì—ì„œ ì´ë“¤ì„ condaë¡œ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.\\n\\n## ëª¨ë¸ êµ¬ì¡°\\n\\n**ğŸ¤— Transformersê°€ ì œê³µí•˜ëŠ”...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BigBird-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbig_bird)** (from Google R...\"],[\"1. **[BlenderbotSmall](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot-small)** (from ...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[CamemBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcamembert)** (Inria\\u002fFacebook\\u002fSor...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (OpenAI ì—ì„œ) Alec Radford, Jon...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (MetaAI ì—ì„œ ì œê³µ)ì€ Ba...\"],[\"1. **[ConvNeXT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnext)** (Facebook AI ì—ì„œ) Zhua...\"],[\"1. **[CTRL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fctrl)** (Salesforce ì—ì„œ) Nitish Shiris...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (Microsoft ì—ì„œ) Pe...\"],[\"1. **[DePlot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeplot)** (Google AI ì—ì„œ ì œê³µ)ì€ Fangyu...\"],[\"1. **[DiNAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinat)** (SHI Labs ì—ì„œ) Ali Hassani a...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (HuggingFace ì—ì„œ) ...\"],[\"1. **[DPR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdpr)** (Facebook ì—ì„œ) Vladimir Karpukhi...\"],[\"1. **[ELECTRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002felectra)** (Google Research\\u002fStanfo...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (Baidu ì—ì„œ ì œê³µ)ì€ Xuan Ouya...\"],[\"1. **[Falcon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffalcon)** (from Technology Innovati...\"],[\"1. **[FLAN-UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-ul2)** (from Google AI) rele...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (from Google Research) releas...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT NeoX Japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox_japanese)** (fr...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (BigCode ì—ì„œ ì œê³µ)ì€...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[I-BERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fibert)** (Berkeley ì—ì„œ) Sehoon Kim, ...\"],[\"1. **[InstructBLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finstructblip)** (Salesforce ì—...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (Microsoft Resear...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (Meta AI ì—ì„œ) Ben Graham, Al...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (UNC Chapel Hill ì—ì„œ) Hao ...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (Meta and UIUC ì—ì„œ...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (Facebook ì—ì„œ ì œê³µ)ì€ Xuezhe Ma, ...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (Facebook ì—ì„œ ì œê³µ)ì€ Vineel Pratap...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (Google Inc. ì—...\"],[\"1. **[MPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmpt)** (MosaiML ì—ì„œ ì œê³µ)ì€ the MosaicML N...\"],[\"1. **[MVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmvp)** (RUC AI Box ì—ì„œ) Tianyi Tang, Ju...\"],[\"1. **[NLLB-MOE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnllb-moe)** (Meta ì—ì„œ ì œê³µ)ì€ the NLL...\"],[\"1. **[OpenLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopen-llama)** (from [s-JoL](http...\"],[\"1. **[PatchTSMixer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtsmixer)** ( IBM Researc...\"],[\"1. **[Perceiver IO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fperceiver)** (Deepmind ì—ì„œ) An...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (UCLA NLP ì—ì„œ) Wasi Uddin ...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (Nanjing University, The Univer...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (Google Research ì—ì„œ) ...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (Facebook ì—ì„œ) Yinhan Li...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (Bo Peng ì—ì„œ ì œê³µ)ì€ Bo Peng.ì˜ [t...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (Meta AI ì—ì„œ ì œê³µ)ì€ A...\"],[\"1. **[SpeechT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeecht5)** (Microsoft Research ì—...\"],[\"1. **[SqueezeBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsqueezebert)** (Berkeley ì—ì„œ) F...\"],[\"1. **[Swin2SR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin2sr)** (University of WÃ¼rzburg...\"],[\"1. **[Table Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftable-transformer)** (Mi...\"],[\"1. **[Trajectory Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrajectory_transfor...\"],[\"1. **[TVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftvp)** (Intel ì—ì„œ) Yimeng Zhang, Xin Ch...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (Microsoft R...\"],[\"1. **[VideoMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvideomae)** (Multimedia Computing...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (Google AI...\"],[\"1. **[VitDet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitdet)** (Meta AI ì—ì„œ ì œê³µ)ì€ Yanghao ...\"],[\"1. **[VITS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvits)** (Kakao Enterprise ì—ì„œ ì œê³µ)ì€ Jae...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (Faceb...\"],[\"1. **[X-MOD](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxmod)** (Meta AI ì—ì„œ ì œê³µ)ì€ Jonas Pfeif...\"],[\"1. **[XLM-ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-prophetnet)** (Microsof...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (Meta AI ì—ì„œ) Davis Liang, H...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (Facebook A...\"],[\"1. ìƒˆë¡œìš´ ëª¨ë¸ì„ ì˜¬ë¦¬ê³  ì‹¶ë‚˜ìš”? ìš°ë¦¬ê°€ **ìƒì„¸í•œ ê°€ì´ë“œì™€ í…œí”Œë¦¿** ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì˜¬ë¦¬ë„ë¡ ë„ì™€ë“œë¦´ê²Œìš”. ê°€ì´ë“œì™€ í…œí”Œë¦¿ì€ ì´ ì €ì¥ì†Œì˜ [`templates`](.\\u002fte...\"],[\"ê° ëª¨ë¸ì´ Flax, PyTorch, TensorFlowìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ ë˜ëŠ” ğŸ¤— Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì§€ì›í•˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´, [ì´ í‘œ](https...\"],[\"| ì„¹ì…˜ | ì„¤ëª… |\\n|-|-|\\n| [ë„íë¨¼íŠ¸](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002f) | ì „ì²´ API ë„íë¨¼íŠ¸ì™€ íŠœí† ë¦¬ì–¼ |\\n| [ê³¼ì œ ìš”ì•½](htt...\"],[\"| [ë§ˆì´ê·¸ë ˆì´ì…˜](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmigration) | `pytorch-transformers`ë‚˜ `pytorch-pr...\"],[\"## ì¸ìš©\\n\\nğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¸ìš©í•˜ê³  ì‹¶ë‹¤ë©´, ì´ [ë…¼ë¬¸](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)ì„...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### IntelÂ® oneCCL Bindings for PyTorch installation\\n\\nWheel files are available for the following Pyt...\"],[\"```\\npip install oneccl_bind_pt=={pytorch_version} -f https:\\u002f\\u002fdeveloper.intel.com\\u002fipex-whl-stable-cpu...\"],[\"```\\n\\n#### IntelÂ® Extension for PyTorch installation\\n\\nIntel Extension for PyTorch (IPEX) provides per...\"],[\"```\\nThe following command enables training with a total of four processes on two Xeons (node0 and no...\"],[\"```\\n\\n## Usage with Kubernetes\\n\\nThe same distributed training job from the previous section can be de...\"],[\"```\\nThe image needs to be built and copied to the cluster's nodes or pushed to a container registry ...\"],[\"The snippet below is an example of a yaml file for a PyTorchJob with 4 workers running the\\n[question...\"],[\"- name: TRANSFORMERS_CACHE\\n                value: \\\"\\u002ftmp\\u002fpvc-mount\\u002ftransformers_cache\\\"\\n              ...\"],[\"```\\nTo run this example, update the yaml based on your training script and the nodes in your cluster...\"],[\"```\\n\\nThe logs for worker can be viewed using `kubectl logs -n kubeflow \\u003cpod name\\u003e`. Add `-f` to stre...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Model description\\n\\nMatCha is a model that is trained using `Pix2Struct` architecture. You can fin...\"],[\"You can use these models as follows (example on a ChatQA dataset):\\n\\n```python\\nfrom transformers impo...\"],[\"```\\n\\n## Fine-tuning\\n\\nTo fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https:\\u002f\\u002fgit...\"],[\"!--Copyright 2022 The HuggingFace Team and The OpenBMB Team. All rights reserved.\\n\\nLicensed under th...\"],[\"[[autodoc]] CpmAntTokenizer\\n    - all\\n\\n## CpmAntModel\\n\\n[[autodoc]] CpmAntModel\\n    - all\\n    \\n## Cpm...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recent breakthroughs in natural language process...\"],[\"processor = AutoImageProcessor.from_pretrained('facebook\\u002fdinov2-base')\\nmodel = AutoModel.from_pretra...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model is exceptionally versatile and can be used for a wide range of image and multimodal tasks...\"],[\"```\\n\\n\\u003cTip\\u003e\\nTo run the following examples with a non-quantized version of the model checkpoint you wi...\"],[\"```\\n\\nNow that you have the model loaded in one of the suggested ways, let's move on to exploring tas...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gen...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is a good idea to include the `bad_words_ids` in the call to `generate` to avoid erro...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gen...\"],[\"```\\n\\n## Few-shot prompting\\n\\nWhile IDEFICS demonstrates great zero-shot results, your task may requir...\"],[\"Photo by [Juan Mayobre](https:\\u002f\\u002funsplash.com\\u002f@jmayobres).\\n  \\n```py\\n\\u003e\\u003e\\u003e prompt = [\\\"User:\\\",\\n...       ...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gen...\"],[\"```\\n\\nNotice that just from a single example (i.e., 1-shot) the model has learned how to perform the ...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gen...\"],[\"```\\n\\n## Image classification\\n\\nIDEFICS is capable of classifying images into different categories wit...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gene...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\nLooks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story a...\"],[\"## Running inference in batch mode\\n\\nAll of the earlier sections illustrated IDEFICS for a single exa...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gen...\"],[\"```\\n\\n## IDEFICS instruct for conversational use\\n\\nFor conversational use cases, you can find fine-tun...\"],[\"...         \\\"\\\\nAssistant:\\\",\\n...     ],\\n... ]\\n\\n\\u003e\\u003e\\u003e # --batched mode\\n\\u003e\\u003e\\u003e inputs = processor(prompts, a...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003eState-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\u003c\\u002fp\\u003e\\n\\u003c\\u002fh...\"],[\"In Natural Language Processing:\\n- [Masked word completion with BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-bas...\"],[\"- [Natural Language Inference with RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+w...\"],[\"- [Question answering with...\"],[\"- [Translation with T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin...\"],[\"In Computer Vision:\\n- [Image classification with ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16...\"],[\"In Multimodal tasks:\\n- [Table Question Answering with TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002ftapas-bas...\"],[\"## Quick tour\\n\\nTo immediately use a model on a given input (text, image, audio, ...), we provide the...\"],[\"```\\n\\nThe second line of code downloads and caches the pretrained model used by the pipeline, while t...\"],[\"# Allocate a pipeline for object detection\\n\\u003e\\u003e\\u003e object_detector = pipeline('object-detection')\\n\\u003e\\u003e\\u003e ob...\"],[\"```\\n\\nHere, we get a list of objects detected in the image, with a box surrounding the object and a c...\"],[\"```\\n\\nThe tokenizer is responsible for all the preprocessing the pretrained model expects and can be ...\"],[\"## Why shouldn't I use transformers?\\n\\n- This library is not a modular toolbox of building blocks for...\"],[\"When one of those backends has been installed, ğŸ¤— Transformers can be installed using pip as follows:...\"],[\"```\\n\\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait fo...\"],[\"```\\n\\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with co...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BEiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbeit)** (from Microsoft) released wit...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (from OpenAI) released with t...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (from OpenAI) released w...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (from OpenAI) released with ...\"],[\"1. **[Jukebox](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fjukebox)** (from OpenAI) released ...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (from Microsoft R...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (from South China University ...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (from Microsoft Research & ...\"],[\"1. **[M-CTC-T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmctct)** (from Facebook) released ...\"],[\"1. **[MarkupLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarkuplm)** (from Microsoft Resea...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (from NVIDI...\"],[\"1. **[Mixtral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmixtral)** (from Mistral AI) by Th...\"],[\"1. **[MobileBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilebert)** (from CMU\\u002fGoogle ...\"],[\"1. **[MobileViTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevitv2)** (from Apple) re...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[NLLB-MOE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnllb-moe)** (from Meta) released ...\"],[\"1. **[OPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmaster\\u002fmodel_doc\\u002fopt)** (from Meta AI) released ...\"],[\"1. **[PatchTST](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtst)** (from IBM) released w...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpersimmon)** (from ADEPT) releas...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (from Google) rel...\"],[\"1. **[ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fprophetnet)** (from Microsoft R...\"],[\"1. **[REALM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frealm.html)** (from Google Research)...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (from Bo Peng), released on [...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (from Meta AI) rel...\"],[\"1. **[SpeechToTextTransformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text)** ...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (from MBZUAI) r...\"],[\"1. **[SwitchTransformers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswitch_transformers)** ...\"],[\"1. **[TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapas)** (from Google AI) released w...\"],[\"1. **[Transformer-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftransfo-xl)** (from Google\\u002f...\"],[\"1. **[UnivNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funivnet)** (from Kakao Corporation...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (from NAVER AI Lab\\u002fKakao Ente...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (from Google AI) ...\"],[\"1. **[ViTMSN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_msn)** (from Meta AI) released ...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (from Microsoft Research) ...\"],[\"1. **[XLM-ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-prophetnet)** (from Mic...\"],[\"1. **[XLNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlnet)** (from Google\\u002fCMU) released ...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (from the University of Wisco...\"],[\"To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated to...\"],[\"## Citation\\n\\nWe now have a [paper](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f) you can cit...\"],[\"CodeParrot ğŸ¦œ\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flvwerra\\u002frepo-images\\u002fra...\"],[\"```\\n\\nAdditionally, sure you have git-lfs installed. You can find instructions for how to install it ...\"],[\"- exact deduplication using each file's hash after having removed whistespaces.\\n- near deduplication...\"],[\"```\\nDuring preprocessing the dataset is downloaded and stored locally as well as caches of the compu...\"],[\"```\\nThis will initialize a new model with the architecture and configuration of `gpt2-large` and use...\"],[\"```\\n\\nRecall that you can see the full set of possible options with descriptions (for all scripts) by...\"],[\"```\\nIn addition we evaluate the model on OpenAI's _HumanEval_ benchmark. You can run the evaluation ...\"],[\"```\\n\\nThe results as well as reference values are shown in the following table:\\n\\n| Model | pass@1 | p...\"],[\"## Training with Megatron\\n[Megatron](https:\\u002f\\u002fgithub.com\\u002fNVIDIA\\u002fMegatron-LM) is a framework developed...\"],[\"```\\n\\nYou also need to add the vocabulary file and merges table of the tokenizer that you trained on ...\"],[\"```\\nThis outputs two files `codeparrot_content_document.idx` and `codeparrot_content_document.bin` w...\"],[\"### Training\\nYou can configure the model architecture and training parameters as shown below, or put...\"],[\"--pipeline-model-parallel-size 1 \\\\\\n        $GPT_ARGS \\\\\\n        --vocab-file $VOCAB_FILE \\\\\\n        --...\"],[\"```\\nThe training takes almost 12 hours in this setting.\\n\\n### Convert model to `transformers`\\nAfter t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Check more detailed information for [Auto Mixed Precision](https:\\u002f\\u002fintel.github.io\\u002fintel-extension-f...\"],[\"```\\npip install intel_extension_for_pytorch==\\u003cversion_name\\u003e -f https:\\u002f\\u002fdeveloper.intel.com\\u002fipex-whl-...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nTo print summary statistics for the GPU utilization and the training run with the [`Trainer`] w...\"],[\"```\\n\\nWe see that the kernels alone take up 1.3GB of GPU memory. Now let's see how much space the mod...\"],[\"```\\n\\n```bash\\nTue Jan 11 08:58:05 2022\\n+-------------------------------------------------------------...\"],[\"+-----------------------------------------------------------------------------+\\n| Processes:        ...\"],[\"```\\n\\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of...\"],[\"```\\n\\nWe see that already a relatively small batch size almost fills up our GPU's entire memory. Howe...\"],[\"Let's look at the details.\\n\\n**Model Weights:**\\n\\n- 4 bytes * number of parameters for fp32 training\\n-...\"],[\"As you can see, there are potentially a few places where we could save GPU memory or speed up operat...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"[GPT Neo](..\\u002fmodel_doc\\u002fgpt_neo), [GPT NeoX](..\\u002fmodel_doc\\u002fgpt_neox), [GPT-J](..\\u002fmodel_doc\\u002fgptj), [I-B...\"],[\"[OpenAI GPT](..\\u002fmodel_doc\\u002fopenai-gpt), [OPT](..\\u002fmodel_doc\\u002fopt), [Perceiver](..\\u002fmodel_doc\\u002fperceiver),...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThere are two fields in this dataset:\\n\\n- `text`: the movie review text.\\n- `label`: a value that...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Evaluate\\n\\nIncluding a metric during training is often helpful for ...\"],[\"```\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with the [`Traine...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n[`Trainer`] applies dynamic padding by default when you pass `tokenizer` to it. In this ...\"],[\"```\\n\\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr...\"],[\"```\\n\\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nPass your inputs to the model and return the `logits`:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import TFAu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n *The design choices in the Transformer attention mec...\"],[\"## MegaModel\\n\\n[[autodoc]] MegaModel\\n    - forward\\n\\n## MegaForCausalLM\\n\\n[[autodoc]] MegaForCausalLM\\n ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequ...\"],[\"The Authors' code can be found [here](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fProphetNet).\\n\\n## Resources\\n\\n- [Ca...\"],[\"!--Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache L...\"],[\"The abstract from the paper is the following:\\n\\n*Grouping and recognition are important components of...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThis will create a `imagenette2` dir with two subdirectories `train` and `val` each with multip...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] PreTrainedModel\\n    - push_to_hub\\n    - all\\n\\n\\u003ca id='from_pretrained-torch-dtype'\\u003e\\u003c\\u002fa\\u003e\\n\\n#...\"],[\"```\\n\\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM ...\"],[\"```\\n\\nYou can inspect how the model was split across devices by looking at its `hf_device_map` attrib...\"],[\"```\\n\\nYou can also write your own device map following the same format (a dictionary layer name to de...\"],[\"```\\n\\nDue to Pytorch design, this functionality is only available for floating dtypes.\\n\\n\\n## ModuleUti...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The \\\"Roaring 20s\\\" of visual recognition began with t...\"],[\"\\u003csmall\\u003e ConvNeXT architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2201.03545\\\"\\u003eoriginal pa...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## ConvNextModel\\n\\n[[autodoc]] ConvNextModel\\n    - forward\\n\\n## ConvNextForIm...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"In this example we will use the vision model from [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=clip)\\n...\"],[\"```\\nhuggingface-cli repo create clip-roberta-base\\n```\\nNext we clone the model repository to add the ...\"],[\"```\\n\\nIf the checkpoints are in PyTorch then one could pass `text_from_pt=True` and `vision_from_pt=T...\"],[\"```\\n\\n### Prepare dataset files and split the dataset.\\n\\n```python\\nimport json\\nimport collections\\n\\nima...\"],[\"```\\n\\n\\u003e Note: The data loading and processing part of this script can still be improved for maximum p...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fvilt_archite...\"],[\"[[autodoc]] ViltFeatureExtractor\\n    - __call__\\n\\n## ViltImageProcessor\\n\\n[[autodoc]] ViltImageProcess...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Zero-shot image classification pipeline\\n\\nThe simplest way to try out inference with a model ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import r...\"],[\"```\\n\\nPass the inputs through the model, and post-process the results:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e w...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recently-proposed Perceiver model obtains good r...\"],[\"Internally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch_size, num...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fperceiver_ar...\"],[\"[[autodoc]] models.perceiver.modeling_perceiver.PerceiverDecoderOutput\\n\\n[[autodoc]] models.perceiver...\"],[\"## PerceiverOpticalFlowDecoder\\n\\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverOpticalFlow...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Development of the model was led by [Shinya Otani](https:\\u002f\\u002fgithub.com\\u002fSO0529), [Takayoshi Makabe](ht...\"],[\"```\\n\\n## Resources\\n\\n- [Causal language modeling task guide](..\\u002ftasks\\u002flanguage_modeling)\\n\\n## GPTNeoXJa...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"All community leaders are obligated to respect the privacy and security of the\\nreporter of any incid...\"],[\"Community Impact Guidelines were inspired by\\n[Mozilla's code of conduct enforcement ladder][Mozilla ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## With Trainer\\n\\nHere is an example on a summarization task:\\n```bash\\npython examples\\u002fpytorch\\u002fsummari...\"],[\"```\\n\\nOnly T5 models `t5-small`, `t5-base`, `t5-large`, `t5-3b` and `t5-11b` must use an additional a...\"],[\"```\\n\\nThe task of summarization supports custom CSV and JSONLINES formats.\\n\\n#### Custom CSV Files\\n\\nIf...\"],[\"```\\n\\nand you wanted to select only `text` and `summary`, then you'd pass these additional arguments:...\"],[\"```\\n\\n## With Accelerate\\n\\nBased on the script [`run_summarization_no_trainer.py`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"```\\n\\nThis command is the same and will work for:\\n\\n- a CPU-only setup\\n- a setup with one GPU\\n- a dist...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Many real-world applications require the prediction ...\"],[\"## InformerConfig\\n\\n[[autodoc]] InformerConfig\\n\\n## InformerModel\\n\\n[[autodoc]] InformerModel\\n    - for...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nHaving downloaded COCO dataset manually you should be able to load with the `ydshieh\\u002fcoc_datase...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The methods and tools covered in this guide can be classified based on the effect they have on the t...\"],[\"If these methods do not result in sufficient gains, you can explore the following options: \\n* [Look ...\"],[\"For parameters that are small, consider also [Dimension Quantization Effects](https:\\u002f\\u002fdocs.nvidia.co...\"],[\"```\\n\\nIn the above example, your effective batch size becomes 4. \\n\\nAlternatively, use ğŸ¤— Accelerate to...\"],[\"To enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArgu...\"],[\"```\\n\\nAlternatively, use ğŸ¤— Accelerate - find the ğŸ¤— Accelerate example [further in this guide](#using-...\"],[\"```\\n\\nIf you prefer to use ğŸ¤— Accelerate, find the ğŸ¤— Accelerate example [further in this guide](#using...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\ntf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an inter...\"],[\"Let's take a closer look at two alternatives to AdamW optimizer:\\n1. `adafactor` which is available i...\"],[\"```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis...\"],[\"```\\n\\nHowever, we can also use a third-party implementation of the 8-bit optimizer for demonstration ...\"],[\"optimizer_kwargs = {\\n    \\\"betas\\\": (training_args.adam_beta1, training_args.adam_beta2),\\n    \\\"eps\\\": t...\"],[\"```\\n\\nFinally, pass the custom optimizer as an argument to the `Trainer`:\\n\\n```py\\ntrainer = Trainer(mo...\"],[\"```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis...\"],[\"DeepSpeed is an open-source deep learning optimization library that is integrated with ğŸ¤— Transformer...\"],[\"```\\n\\n`torch.compile` uses Python's frame evaluation API to automatically create a graph from existin...\"],[\"**Training & inference backends**:\\n* `dynamo.optimize(\\\"inductor\\\")` - Uses TorchInductor backend with...\"],[\"**Inference-only backend**s:\\n* `dynamo.optimize(\\\"ofi\\\")` -  Uses Torchscript optimize_for_inference. ...\"],[\"```\\n\\nThe full example training loop with ğŸ¤— Accelerate is only a handful of lines of code long:\\n\\n```p...\"],[\"```\\n\\nFirst we wrap the dataset in a [`DataLoader`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fdata.html#torch.u...\"],[\"At times, additional efforts may be required to pre-build some components. For instance, if you're u...\"],[\"(source: [GLAM](https:\\u002f\\u002fai.googleblog.com\\u002f2021\\u002f12\\u002fmore-efficient-in-context-learning-with.html))\\n\\nYo...\"],[\"And for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing Mixture-of-Experts Infere...\"],[\"```\\n\\nOnce converted, train the model as usual.\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe PyTorch-native `scaled_dot...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Motivation\\nWithout processing, english-\\u003e romanian mbart-large-en-ro gets BLEU score 26.8 on the W...\"],[\"```\\n\\n(2) define a function for post processing.\\n It removes diacritics and does other things I don't...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"* \\\"I would like to use a BertModel within a RL-Agent for a customer support service. How can I use a...\"],[\"```\\n    \\\"huggingface\\\" \\\"transformers\\\" your query\\n    ```\\n\\n    The first two quoted words tell Google ...\"],[\"```\\n\\n    And now we can use it to do the searching on your favorite search engine:\\n\\n    1. first for...\"],[\"```\\n\\n   then you'd search for `\\\"ValueError\\\" \\\"cannot be found\\\"`\\n\\n   As you search you will notice tha...\"],[\"```\\n   ```\\n   git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\n   cd transformers\\n   pip instal...\"],[\"```\\n\\n   which would result in the following entry, which can be opened if desired, but otherwise tak...\"],[\"8. Before reporting an issue, first, always try to update your environment to the latest official ve...\"],[\"If you see a certain developer doing multiple and\\u002for recent commits into a specific area of the proj...\"],[\"To get the link to the specific comment do not copy the url from the location bar of your browser, b...\"],[\"```\\n    \\u003e How big is your gpu cluster?\\n\\n    Our cluster is made of 256 gpus.\\n    ```\\n\\n    If you are...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*With the success of language pretraining, it is high...\"],[\"## Usage tips\\n\\n- Since Funnel Transformer uses pooling, the sequence length of the hidden states cha...\"],[\"## FunnelConfig\\n\\n[[autodoc]] FunnelConfig\\n\\n## FunnelTokenizer\\n\\n[[autodoc]] FunnelTokenizer\\n    - bui...\"],[\"[[autodoc]] TFFunnelForSequenceClassification\\n    - call\\n\\n## TFFunnelForMultipleChoice\\n\\n[[autodoc]] ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nor for an editable install:\\n\\n```bash\\npip install -e .[quality]\\n```\\n\\n\\n## Tests\\n\\nAll the jobs tha...\"],[\"```\\n\\nJust in case anything slipped through the cracks, the full test suite is also run daily.\\n\\n## Do...\"],[\"```\\n\\nThis checks that:\\n\\n- All objects added to the init are documented (performed by `utils\\u002fcheck_re...\"],[\"```\\n\\nAdditional checks concern PRs that add new models, mainly that:\\n\\n- All models added are in an A...\"],[\"```\\n\\nNote that instead of applying this to a whole class, you can apply it to the relevant methods t...\"],[\"```\\n\\nNote that there shouldn't be any spaces around the arrow (unless that space is part of the patt...\"],[\"```\\n\\nIn this case, the code is copied from `BertForSequenceClassification` by replacing:\\n- `Bert` by...\"],[\"Movement Pruning: Adaptive Sparsity by Fine-Tuning\\n\\nAuthor: @VictorSanh\\n\\n*Magnitude pruning is a wid...\"],[\"| Fine-pruning+Distillation\\u003cbr\\u003e(Teacher=BERT-base fine-tuned) | BERT base\\u003cbr\\u003efine-tuned | Remaining\\u003c...\"],[\"For more information, we invite you to check out [our paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2005.07683).\\nYou ...\"],[\"While movement pruning does not directly optimize for memory footprint (but rather the number of non...\"],[\"## How to fine-prune?\\n\\n### Setup\\n\\nThe code relies on the ğŸ¤— Transformers library. In addition to the ...\"],[\"```bash\\nSERIALIZATION_DIR=\\u003cOUTPUT_DIR\\u003e\\nSQUAD_DATA=\\u003cSQUAD_DATA\\u003e\\n\\npython examples\\u002fmovement-pruning\\u002fmas...\"],[\"```\\n\\n### Fine-pruning with other methods\\n\\nWe can also explore other fine-pruning methods by changing...\"],[\"```\\n\\nL0 regularization\\n```bash\\npython examples\\u002fmovement-pruning\\u002fmasked_run_squad.py \\\\\\n    --output_d...\"],[\"```\\n\\n### After fine-pruning\\n\\n**Counting parameters**\\n\\nRegularization based pruning methods (soft mov...\"],[\"```\\n@article{sanh2020movement,\\n    title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},\\n    a...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n    \\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembed\\u002fH39Z_72...\"],[\"### Encoder[[cv-encoder]]\\n\\nThe [Vision Transformer (ViT)](model_doc\\u002fvit) opened the door to computer...\"],[\"### Decoder[[cv-decoder]]\\n\\nDecoder-only vision models are rare because most vision models rely on an...\"],[\"### Encoder[[nlp-encoder]]\\n\\n[BERT](model_doc\\u002fbert) is an encoder-only Transformer that randomly mask...\"],[\"### Decoder[[nlp-decoder]]\\n\\n[GPT-2](model_doc\\u002fgpt2) is a decoder-only Transformer that predicts the ...\"],[\"### Encoder-decoder[[nlp-encoder-decoder]]\\n\\n[BART](model_doc\\u002fbart) keeps the original Transformer ar...\"],[\"### Encoder[[audio-encoder]]\\n\\n[Wav2Vec2](model_doc\\u002fwav2vec2) uses a Transformer encoder to learn spe...\"],[\"## Multimodal\\n\\n\\u003ciframe style=\\\"border: 1px solid rgba(0, 0, 0, 0.1);\\\" width=\\\"1000\\\" height=\\\"450\\\" src=\\\"...\"],[\"[CLIP](model_doc\\u002fclip) takes a different approach and makes a pair prediction of (`image`, `text`) ....\"],[\"## Reinforcement learning\\n\\n\\u003ciframe style=\\\"border: 1px solid rgba(0, 0, 0, 0.1);\\\" width=\\\"1000\\\" height...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"```\\n\\nIf you want to try out the brand new features, you might be interested in installing the librar...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou can load a PEFT adapter with either an `AutoModelFor` class or the base model class ...\"],[\"```\\n\\n## Add a new adapter\\n\\nYou can use [`~peft.PeftModel.add_adapter`] to add a new adapter to a mod...\"],[\"```\\n\\n## Enable and disable adapters\\n\\nOnce you've added an adapter to a model, you can enable or disa...\"],[\"```\\n\\n2. Add adapter to the model.\\n\\n```py\\nmodel.add_adapter(peft_config)\\n```\\n\\n3. Now you can pass the...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [jegormeister](https:\\u002f\\u002fhuggingface.co\\u002fjegormeister). The original code...\"],[\"!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apac...\"],[\"These ready-to-use checkpoints can be downloaded and used via the HuggingFace Hub:\\n\\n```python\\n\\u003e\\u003e\\u003e fr...\"],[\"```\\n\\nRaw weights for `Mistral-7B-v0.1` and `Mistral-7B-Instruct-v0.1` can be downloaded from:\\n\\n| Mod...\"],[\"```\\n\\n## Combining Mistral and Flash Attention 2\\n\\nFirst, make sure to install the latest version of F...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is a expected speedup diagram that compares pure inference time be...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Previous approaches preprocessed the audio to extract useful features from it. It is now more common...\"],[\"```\\n\\n### Automatic speech recognition\\n\\nAutomatic speech recognition (ASR) transcribes speech into te...\"],[\"```\\n\\n## Computer vision\\n\\nOne of the first and earliest successful computer vision tasks was recogniz...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e classifier = pipeline(task=\\\"image-classification\\\")\\n...\"],[\"```\\n\\n### Object detection\\n\\nUnlike image classification, object detection identifies multiple objects...\"],[\"```\\n\\n### Image segmentation\\n\\nImage segmentation is a pixel-level task that assigns every pixel in an...\"],[\"```\\n\\n### Depth estimation\\n\\nDepth estimation predicts the distance of each pixel in an image from the...\"],[\"```\\n\\n## Natural language processing\\n\\nNLP tasks are among the most common types of tasks because text...\"],[\"```\\n\\n### Token classification\\n\\nIn any NLP task, text is preprocessed by separating the sequence of t...\"],[\"\\u003e\\u003e\\u003e classifier = pipeline(task=\\\"ner\\\")\\n\\u003e\\u003e\\u003e preds = classifier(\\\"Hugging Face is a French company based...\"],[\"```\\n\\n### Question answering\\n\\nQuestion answering is another token-level task that returns an answer t...\"],[\"```\\n\\n### Summarization\\n\\nSummarization creates a shorter version of a text from a longer one while tr...\"],[\"```\\n\\n### Translation\\n\\nTranslation converts a sequence of text in one language to another. It is impo...\"],[\"```\\n\\n* masked: the model's objective is to predict a masked token in a sequence with full access to ...\"],[\"```\\n\\n## Multimodal\\n\\nMultimodal tasks require a model to process multiple data modalities (text, imag...\"],[\"```\\n\\nHopefully, this page has given you some more background information about all the types of task...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work in language modeling demonstrates that t...\"],[\"## Usage tips\\n\\nWe have provided pretrained [GPT2-345M](https:\\u002f\\u002fngc.nvidia.com\\u002fcatalog\\u002fmodels\\u002fnvidia:...\"],[\"```\\n\\nOnce you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a ...\"],[\"Summarization (Seq2Seq model) training examples\\n\\nThe following example showcases how to finetune a s...\"],[\"```\\n\\nThis should finish in 37min, with validation loss and ROUGE2 score of 1.7785 and 17.01 respecti...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Usage example\\n\\nThe `generate()` method can be used to generate text using GPTSAN-Japanese model....\"],[\"```\\n\\n## GPTSAN Features\\n\\nGPTSAN has some unique features. It has a model structure of Prefix-LM. It ...\"],[\"\\u003e\\u003e\\u003e x_token = tokenizer(\\\"\\\", prefix_text=\\\"ï½±ï½²ï½³ï½´\\\")\\ninput_ids:      | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG |\\ntoken_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multimodal pre-training with text, layout, and image...\"],[\"## Usage: MarkupLMProcessor\\n\\nThe easiest way to prepare data for the model is to use [`MarkupLMProce...\"],[\"```\\n\\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import MarkupLMProcessor\\n\\n\\u003e\\u003e\\u003e processor = MarkupLMProcessor.from_pre...\"],[\"```\\n\\n**Use case 2: web page classification (training, inference) + token classification (inference),...\"],[\"```\\n\\n**Use case 3: token classification (training), parse_html=False**\\n\\nFor token classification tas...\"],[\"```\\n\\n**Use case 4: web page question answering (inference), parse_html=True**\\n\\nFor question answerin...\"],[\"```\\n\\n**Use case 5: web page question answering (inference), parse_html=False**\\n\\nFor question answeri...\"],[\"```\\n\\n## Resources\\n\\n- [Demo notebooks](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransformers-Tutorials\\u002ftree\\u002fmast...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- The implementation is the same as [Roberta](roberta) except instead of using _Add a...\"],[\"[[autodoc]] RobertaPreLayerNormForQuestionAnswering\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFRobertaPreLayerN...\"],[\"[[autodoc]] FlaxRobertaPreLayerNormForSequenceClassification\\n    - __call__\\n\\n## FlaxRobertaPreLayerN...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Summary\\nIn Phi-1 and Phi-1.5 papers, the authors showed how important the quality of the data is...\"],[\"The abstract from the Phi-1.5 paper is the following:\\n\\n*We continue the investigation into the power...\"],[\"### Example :\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import PhiForCausalLM, AutoTokenizer\\n\\n\\u003e\\u003e\\u003e # define th...\"],[\"```\\n\\n\\n## Combining Phi and Flash Attention 2\\n\\nFirst, make sure to install the latest version of Flas...\"],[\"```\\n\\n### Expected speedups\\nBelow is an expected speedup diagram that compares pure inference time be...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n{\\\"sentence1\\\": \\\"COVID-19 vaccine updates: How is the rollout proceeding?\\\", \\\"label\\\": \\\"news\\\"}\\n{\\\"sen...\"],[\"```\\npython run_text_classification.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--train_file tr...\"],[\"```\\n\\n## run_glue.py\\n\\nThis script handles training on the GLUE dataset for various text classificatio...\"],[\"```\\npython run_glue.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--task_name mnli \\\\\\n--do_train ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*State-of-the-art computer vision systems are trained...\"],[\"To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-ove...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"**Image retrieval**\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1bLVwVKpAndpEDHqjzxVPr_9...\"],[\"## CLIPTextConfig\\n\\n[[autodoc]] CLIPTextConfig\\n\\n## CLIPVisionConfig\\n\\n[[autodoc]] CLIPVisionConfig\\n\\n##...\"],[\"## FlaxCLIPTextModelWithProjection\\n\\n[[autodoc]] FlaxCLIPTextModelWithProjection\\n    - __call__\\n\\n## F...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For something slightly more challenging, you can also take a look at the [Good Second Issue](https:\\u002f...\"],[\"```\\n\\nYou can also run the same command from the root of the repository:\\n\\n```bash\\npython src\\u002ftransfor...\"],[\"```\\n\\n### Do you want a new feature?\\n\\nIf there is a new feature you'd like to see in ğŸ¤— Transformers, ...\"],[\"For more details about how to generate, build, and write the documentation, take a look at the docum...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   git checkout -b a-descr...\"],[\"```\\n\\n   Finally, we have a lot of scripts to make sure we don't forget to update\\n   some files when ...\"],[\"```\\n\\n   If you've already opened a pull request, you'll need to force push with the `--force` flag. ...\"],[\"### Pull request checklist\\n\\nâ˜ The pull request title should summarize your contribution.\\u003cbr\\u003e\\nâ˜ If yo...\"],[\"â˜ All public methods must have informative docstrings (see\\n[`modeling_bert.py`](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"```\\n\\nSimilarly, for the `examples` directory, specify a *path to a subfolder or test file* to run th...\"],[\"```\\n\\nLike the slow tests, there are other environment variables available which not enabled by defau...\"],[\"```\\n\\nOne way to run the `make` command on Windows is with MSYS2:\\n\\n1. [Download MSYS2](https:\\u002f\\u002fwww.ms...\"],[\"!--Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace I...\"],[\"This paper has been accepted to the [AAAI'23](https:\\u002f\\u002faaai.org\\u002fConferences\\u002fAAAI-23\\u002f) conference. \\n\\nT...\"],[\"\\u003csmall\\u003e BridgeTower architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2206.08657\\\"\\u003eoriginal...\"],[\"\\u003e\\u003e\\u003e processor = BridgeTowerProcessor.from_pretrained(\\\"BridgeTower\\u002fbridgetower-large-itm-mlm-itc\\\")\\n\\u003e\\u003e...\"],[\"```\\n\\nThe following example shows how to run image-text retrieval using [`BridgeTowerProcessor`] and ...\"],[\"```\\n\\nThe following example shows how to run masked language modeling using [`BridgeTowerProcessor`] ...\"],[\"```\\n\\nTips:\\n\\n- This implementation of BridgeTower uses [`RobertaTokenizer`] to generate text embeddin...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 NVIDIA Corporation. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"```\\n\\n## Quantization Aware Training (QAT)\\n\\nCalibrate the pretrained model and finetune with quantiza...\"],[\"```\\ntrtexec --onnx=model.onnx --explicitBatch --workspace=16384 --int8 --shapes=input_ids:64x128,att...\"],[\"```\\n\\n## Post Training Quantization (PTQ)\\n\\n### PTQ by calibrating and evaluating the finetuned FP32 m...\"],[\"```\\n\\n### Quantization options\\n\\nSome useful options to support different implementations and optimiza...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Contrastive learning has shown remarkable success in...\"],[\"## ClapTextModel\\n\\n[[autodoc]] ClapTextModel\\n    - forward\\n\\n## ClapTextModelWithProjection\\n\\n[[autodoc...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### XLM with language embeddings\\n\\nThe following XLM models use language embeddings to specify the la...\"],[\"```\\n\\nThe `lang2id` attribute of the tokenizer displays this model's languages and their ids:\\n\\n```py\\n...\"],[\"```\\n\\nThe [run_generation.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002f...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\n\\n\\u003e\\u003e\\u003e en_text = \\\"D...\"],[\"```\\n\\nTokenize the text:\\n\\n```py\\n\\u003e\\u003e\\u003e encoded_zh = tokenizer(chinese_text, return_tensors=\\\"pt\\\")\\n```\\n\\nM2...\"],[\"```\\n\\n## MBart\\n\\nThe following MBart models can be used for multilingual translation:\\n\\n- `facebook\\u002fmba...\"],[\"```\\n\\nTokenize the text:\\n\\n```py\\n\\u003e\\u003e\\u003e encoded_en = tokenizer(en_text, return_tensors=\\\"pt\\\")\\n```\\n\\nMBart f...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\n## Generate text\\n\\nA language model trained for [causal language modeling](tasks\\u002flanguage_model...\"],[\"Properly setting up the token selection step and the stopping condition is essential to make your mo...\"],[\"```\\n\\nYou'll notice two flags in the `from_pretrained` call:\\n\\n - `device_map` ensures the model is mo...\"],[\"```\\n\\nFinally, you don't need to do it one sequence at a time! You can batch your inputs, which will ...\"],[\"```\\n\\n### Generated output is too short\\u002flong\\n\\nIf not specified in the [`~generation.GenerationConfig`...\"],[\"```\\n\\n### Incorrect generation mode\\n\\nBy default, and unless specified in the [`~generation.Generation...\"],[\"```\\n\\n### Wrong padding side\\n\\nLLMs are [decoder-only](https:\\u002f\\u002fhuggingface.co\\u002flearn\\u002fnlp-course\\u002fchapter...\"],[\"```\\n\\n### Wrong prompt\\n\\nSome models and tasks expect a certain input prompt format to work properly. ...\"],[\"\\u003e\\u003e\\u003e set_seed(0)\\n\\u003e\\u003e\\u003e messages = [\\n...     {\\n...         \\\"role\\\": \\\"system\\\",\\n...         \\\"content\\\": \\\"You...\"],[\"```\\n\\n## Further resources\\n\\nWhile the autoregressive generation process is relatively straightforward...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## DataCollatorForSeq2Seq\\n\\n[[autodoc]] data.data_collator.DataCollatorForSeq2Seq\\n\\n## DataCollatorFor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We release Code Llama, a family of large language mo...\"],[\"The `Llama2` family models, on which Code Llama is based, were trained using `bfloat16`, but the ori...\"],[\"Here is a sample usage:\\n\\n```bash\\npython src\\u002ftransformers\\u002fmodels\\u002fllama\\u002fconvert_llama_weights_to_hf.py...\"],[\"```\\n\\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci...\"],[\"```\\n\\nIf you only want the infilled part:\\n```python\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\u003e\\u003e\\u003e import ...\"],[\"```\\n\\nUnder the hood, the tokenizer [automatically splits by `\\u003cFILL_ME\\u003e`](https:\\u002f\\u002fhuggingface.co\\u002fdocs...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nIf you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix`...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Illustrating feature maps of the first stage looks like below.\\n\\u003cdiv style=\\\"text-align: center\\\"\\u003e\\n\\u003cimg...\"],[\"```\\n`feature_maps` object now has three feature maps, each can be accessed like below. Say we would ...\"],[\"```\\n\\n`timm` models are also supported in transformers through `TimmBackbone` and `TimmBackboneConfig...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Arthur Zucker](https:\\u002f\\u002fhuggingface.co\\u002fArthurZ), [Younes Belkada](http...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\" \\u002f\\u003e\\n\\n- A notebook on [fine-tuning OPT with PEFT, bitsandbytes...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\" \\u002f\\u003e\\n\\n- [Text classification task guide](sequence_classifi...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is an expected speedup diagram that compares pure inference time b...\"],[\"## FlaxOPTForCausalLM\\n\\n[[autodoc]] FlaxOPTForCausalLM\\n    - __call__\\n\\n\\u003c\\u002fjax\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Existing work in translation demonstrated the potent...\"],[\"**Supervised Training**\\n\\n```python\\nfrom transformers import M2M100Config, M2M100ForConditionalGenera...\"],[\"```\\n\\n**Generation**\\n\\nM2M100 uses the `eos_token_id` as the `decoder_start_token_id` for generation w...\"],[\"```\\n\\n## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nFLAN-T5 includes the same improvements as T5 version 1.1 (see [here](https:\\u002f\\u002fhuggingface.co\\u002fdoc...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe RoBERTa model was proposed in [RoBERTa: A Robustly Optimized BERT Pretraining Appro...\"],[\"## Usage tips\\n\\n- This implementation is the same as [`BertModel`] with a tiny embeddings tweak as we...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog on [Getting Started with Sentiment Analysis ...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`RobertaForTokenClassification`] is supported by ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- A blog on [How to train a new language model from scratch usi...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- A blog on [Accelerated Inference with Optimum and Tr...\"],[\"**Multiple choice**\\n- [`RobertaForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002fgit...\"],[\"## RobertaForQuestionAnswering\\n\\n[[autodoc]] RobertaForQuestionAnswering\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n#...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"Please discuss on the [forum](https:\\u002f\\u002fdiscuss.huggingface.co\\u002f) or in an [issue](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt...\"],[\"```\\n\\nTo browse the examples corresponding to released versions of ğŸ¤— Transformers, click on the line ...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eExamples for older versions of ğŸ¤— Transformers\\u003c\\u002fsummary\\u003e\\n\\t\\u003cul\\u003e\\n\\t    \\u003cli\\u003e\\u003ca href=...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv4.13.0\\u002fexamples\\\"\\u003ev4.13.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003c...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv4.4.2\\u002fexamples\\\"\\u003ev4.4.2\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv3.1.0\\u002fexamples\\\"\\u003ev3.1.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv2.4.0\\u002fexamples\\\"\\u003ev2.4.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"Alternatively, you can switch your cloned ğŸ¤— Transformers to a specific version (for instance with v3...\"],[\"```\\nand run the example command as usual afterward.\\n\\n## Running the Examples on Remote Hardware with...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [HuggingFaceM4](https:\\u002f\\u002fhuggingface.co\\u002fHuggingFaceM4). The original co...\"],[\"!---\\nCopyright 2022 The Microsoft Inc. and The HuggingFace Inc. Team. All rights reserved.\\n\\nLicensed...\"],[\"### What Questions Can be Answered\\n\\nBenefiting from the powerfulness of generative models, TAPEX can...\"],[\"```bash\\nexport EXP_NAME=wikisql_tapex_base\\n\\npython run_wikisql_with_tapex.py \\\\\\n  --do_train \\\\\\n  --do...\"],[\"```\\n\\n#### TAPEX-Large on WikiSQL\\n\\nHere is how to run the script on the WikiSQL with `tapex-large`:\\n\\u003e...\"],[\"```\\n\\n#### TAPEX-Base on WikiTableQuestions\\n\\nHere is how to run the script on the WikiTableQuestions ...\"],[\"```\\n\\n#### TAPEX-Large on WikiTableQuestions\\n\\nHere is how to run the script on the WikiTableQuestions...\"],[\"```\\n\\n### How to Evaluate TAPEX Fine-tuned Models on TableQA\\n\\nWe provide fine-tuned model weights to ...\"],[\"```\\n\\n## Table Fact Verification Tasks\\n\\n### What is Table Fact Verification\\n\\n![Example](https:\\u002f\\u002ftable...\"],[\"```\\n\\n#### TAPEX-Large on TabFact\\n\\nHere is how to run the script on the TabFact:\\n\\u003e The default hyper-...\"],[\"```\\n\\n### How to Evaluate TAPEX Fine-tuned Models on TableFV\\n\\nWe provide fine-tuned model weights to ...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ğŸ¤— Transformers aporta miles de modelos preentrenados Para realizar tareas en diferentes modalidades ...\"],[\"## Demostraciones en lÃ­nea\\n\\nPuedes probar la mayorÃ­a de nuestros modelos directamente en sus pÃ¡ginas...\"],[\"En procesamiento del lenguaje natural:\\n- [TerminaciÃ³n de palabras enmascaradas con BERT](https:\\u002f\\u002fhug...\"],[\"- [Inferencia del lenguaje natural con RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+d...\"],[\"- [Responder a preguntas con...\"],[\"- [TraducciÃ³n con T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"En visiÃ³n de ordenador:\\n- [ClasificaciÃ³n de imÃ¡genes con ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base...\"],[\"## Si estÃ¡ buscando soporte personalizado del equipo de Hugging Face\\n\\n\\u003ca target=\\\"_blank\\\" href=\\\"https...\"],[\"```\\n\\nLa segunda lÃ­nea de cÃ³digo descarga y almacena en cachÃ© el modelo previamente entrenado que usa...\"],[\"# Allocate a pipeline for object detection\\n\\u003e\\u003e\\u003e object_detector = pipeline('object_detection')\\n\\u003e\\u003e\\u003e ob...\"],[\"```\\n\\nAquÃ­ obtenemos una lista de objetos detectados en la imagen, con un cuadro que rodea el objeto ...\"],[\"```\\n\\nY aquÃ­ estÃ¡ el cÃ³digo equivalente para TensorFlow:\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoT...\"],[\"```\\n\\nEl tokenizador es responsable de todo el preprocesamiento que espera el modelo preentrenado y s...\"],[\"1. Menores costes de cÃ³mputo, menor huella de carbono:\\n    - Los investigadores pueden compartir mod...\"],[\"## Â¿Por quÃ© no deberÃ­a usar transformers?\\n\\n- Esta biblioteca no es una caja de herramientas modular ...\"],[\"Primero, crea un entorno virtual con la versiÃ³n de Python que vas a usar y actÃ­valo.\\n\\nLuego, deberÃ¡s...\"],[\"```\\n\\nSi deseas jugar con los ejemplos o necesitas la Ãºltima versiÃ³n del cÃ³digo y no puedes esperar a...\"],[\"```\\n\\nSigue las pÃ¡ginas de instalaciÃ³n de Flax, PyTorch o TensorFlow para ver cÃ³mo instalarlos con co...\"],[\"1. **[BlenderbotSmall](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot-small)** (from ...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[CANINE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcanine)** (from Google Research) re...\"],[\"1. **[CLIPSeg](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclipseg)** (from University of GÃ¶t...\"],[\"1. **[Conditional DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconditional_detr)** (from...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[Data2Vec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdata2vec)** (from Facebook) relea...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (from S...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (from NVIDI...\"],[\"1. **[mLUKE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmluke)** (from Studio Ousia) release...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (from Google I...\"],[\"1. **[MRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmra)** (from the University of Wiscons...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (from Huawei Noahâ€™s Ark Lab...\"],[\"1. **[NystrÃ¶mformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (from the U...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (from UCLA NLP) released ...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SEW-D](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew_d)** (from ASAPP) released with ...\"],[\"1. **[Splinter](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsplinter)** (from Tel Aviv Univer...\"],[\"1. **[Swin Transformer V2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswinv2)** (from Micros...\"],[\"1. **[T5v1.1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5v1.1)** (from Google AI) released...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[TVLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftvlt)** (from UNC Chapel Hill) releas...\"],[\"1. **[UniSpeech](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech)** (from Microsoft Res...\"],[\"1. **[VAN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvan)** (from Tsinghua University and N...\"],[\"1. **[ViTMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_mae)** (from Meta AI) released ...\"],[\"1. Â¿Quieres aportar un nuevo modelo? Hemos agregado una **guÃ­a detallada y plantillas** para guiarte...\"],[\"Para comprobar si cada modelo tiene una implementaciÃ³n en Flax, PyTorch o TensorFlow, o tiene un tok...\"],[\"## Aprender mÃ¡s\\n\\n| SecciÃ³n | DescripciÃ³n |\\n|-|-|\\n| [DocumentaciÃ³n](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftrans...\"],[\"## CitaciÃ³n\\n\\nAhora nosotros tenemos un [papel](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"GPT-NeoX-20B was trained with fp16, thus it is recommended to initialize the model as follows:\\n\\n```p...\"],[\"```\\n\\nGPT-NeoX-20B also has a different tokenizer from the one used in GPT-J-6B and GPT-Neo. The new ...\"],[\"```\\n\\n## Using Flash Attention 2\\n\\nFlash Attention 2 is an faster, optimized version of the model.\\n\\n##...\"],[\"```\\n\\n\\n### Expected speedups\\n\\nBelow is an expected speedup diagram that compares pure inference time ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nBefore diving deeper into the specifics of each technique, let's go over the rough decision ...\"],[\"**Parallelization strategy for a multi-Node \\u002f multi-GPU setup**\\n\\n* When you have fast inter-node con...\"],[\"[DP](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fmaster\\u002fgenerated\\u002ftorch.nn.DataParallel.html):\\n\\nFor each batch:\\n   1. G...\"],[\"Let's illustrate the differences between DP and DDP with an experiment. We'll benchmark the differen...\"],[\"```\\nrm -r \\u002ftmp\\u002ftest-clm; CUDA_VISIBLE_DEVICES=0,1 \\\\\\npython examples\\u002fpytorch\\u002flanguage-modeling\\u002frun_cl...\"],[\"```\\n\\n**DDP w\\u002fo NVlink**\\n\\n```\\nrm -r \\u002ftmp\\u002ftest-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \\\\\\ntorc...\"],[\"```\\n\\nHere are the same benchmarking results gathered in a table for convenience:\\n\\n| Type   | NVlink ...\"],[\"```\\n\\nIf we have 3 GPUs, ZeRO-DP splits the model onto 3 GPUs like so:\\n\\n```\\nGPU0:\\nLa | Lb | Lc\\n---|--...\"],[\"```\\n\\nThe inputs are passed without modifications as if they would be processed by the original model...\"],[\"Implementations:\\n\\n- [DeepSpeed](https:\\u002f\\u002fwww.deepspeed.ai\\u002ftutorials\\u002fzero\\u002f) ZeRO-DP stages 1+2+3\\n- [`A...\"],[\"```\\n===================  ===================\\n|  0 | 1 | 2 | 3  |  |  4 | 5 | 6 | 7  |\\n==============...\"],[\"```\\n\\nIn this example, when data moves from layer 0 to 3, it's no different from regular forward pass...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"Note that this is the same concept as gradient accumulation steps. PyTorch uses `chunks`, while Deep...\"],[\"Pipeline API solutions have been implemented in:\\n- PyTorch\\n- DeepSpeed\\n- Megatron-LM\\n\\nThese come wit...\"],[\"More recent solutions include:\\n- Varuna\\n- Sagemaker\\n\\nWe have not experimented with Varuna and SageMa...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"Alternative names:\\n- DeepSpeed calls it [tensor slicing](https:\\u002f\\u002fwww.deepspeed.ai\\u002ftraining\\u002f#model-pa...\"],[\"Here it's important to see how DP rank 0 doesn't see GPU2 and DP rank 1 doesn't see GPU3. To DP ther...\"],[\"Since each dimension requires at least 2 GPUs, here you'd need at least 8 GPUs.\\n\\nImplementations:\\n- ...\"],[\"ZeRO stage 3 is not a good choice either for the same reason - more inter-node communications requir...\"],[\"Examples:\\n* Sample\\n\\nLet's take 10 batches of sequence length 512. If we parallelize them by sample d...\"],[\"ğŸ¤— Transformers status: Transformers models are FX-trace-able via [transformers.utils.fx](https:\\u002f\\u002fgit...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Accelerate\\\"\\u003e\\n\\nUse `--num_processes` to select how many GPUs to use.\\n\\n...\"],[\"```\\n\\nYou can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Memory usage and data loading\\n\\nOne thing to note is that all data is loaded into memory in this ...\"],[\"```\\npython run_qa.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--output_dir output \\\\\\n--dataset_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this paper, we design and train a Generative Imag...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage examples and tips\\n\\nThe model can be used in combination with the [`EncoderDecoderModel`] to...\"],[\"```\\n\\nPretrained [`EncoderDecoderModel`] are also directly available in the model hub, e.g.:\\n\\n```pyth...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained Language Models (PLMs) have proven to be...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recently-developed DETR approach applies the tra...\"],[\"## Resources\\n\\n- [Object detection task guide](..\\u002ftasks\\u002fobject_detection)\\n\\n## ConditionalDetrConfig\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Convolutional Neural Networks (ConvNets) are commonl...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nWe now have a tokenizer trained on the files we defined. We can either continue using it in tha...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n**Note:** This script only works with models that have a fast tokenizer (backed by the [ğŸ¤— Token...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npython run_audio_classification.py \\\\\\n    --model_name_or_path facebook\\u002fwav2vec2-base \\\\\\n    -...\"],[\"```\\n\\nOn a single V100 GPU (16GB), this script should run in ~14 minutes and yield accuracy of **98.2...\"],[\"```\\n\\nOn 4 V100 GPUs (16GB), this script should run in ~1 hour and yield accuracy of **79.45%**.\\n\\nğŸ‘€ S...\"],[\"```\\n\\n### Examples\\n\\nThe following table shows a couple of demonstration fine-tuning runs.\\nIt has been...\"],[\"| Dataset | Pretrained Model | # transformer layers | Accuracy on eval | GPU setup | Training time |...\"],[\"| Common Language | [facebook\\u002fwav2vec2-base](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fwav2vec2-base) | 12 | 0...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*Recent advancements in automatic speech translation have dramatically expanded language coverage, i...\"],[\"from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly availabl...\"],[\"## Usage\\n\\nIn the following example, we'll load an Arabic audio sample and an English text sample and...\"],[\"```\\n\\nYou can seamlessly use this model on text or on audio, to generated either translated text or t...\"],[\"```\\n\\nWith basically the same code, I've translated English text and Arabic speech to Russian speech ...\"],[\"```\\n\\nFeel free to try out [`SeamlessM4Tv2ForSpeechToText`] and [`SeamlessM4Tv2ForTextToSpeech`] as w...\"],[\"### Difference with SeamlessM4T-v1\\n\\nThe architecture of this new version differs from the first in a...\"],[\"## SeamlessM4Tv2Model\\n\\n[[autodoc]] SeamlessM4Tv2Model\\n    - generate\\n\\n\\n## SeamlessM4Tv2ForTextToSpee...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten).\\n\\n## Usage...\"],[\"\\u003cPipelineTag pipeline=\\\"automatic-speech-recognition\\\"\\u002f\\u003e\\n\\n- A blog post on [boosting Wav2Vec2 with n-g...\"],[\"## Wav2Vec2Config\\n\\n[[autodoc]] Wav2Vec2Config\\n\\n## Wav2Vec2CTCTokenizer\\n\\n[[autodoc]] Wav2Vec2CTCToken...\"],[\"\\u003e\\u003e\\u003e # import model, feature extractor, tokenizer\\n\\u003e\\u003e\\u003e model = AutoModelForCTC.from_pretrained(\\\"patric...\"],[\"...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text\\n...     batch[\\\"trans...\"],[\"```\\n\\n## Wav2Vec2 specific outputs\\n\\n[[autodoc]] models.wav2vec2_with_lm.processing_wav2vec2_with_lm.W...\"],[\"[[autodoc]] TFWav2Vec2ForSequenceClassification\\n    - call\\n\\n## TFWav2Vec2ForCTC\\n\\n[[autodoc]] TFWav2V...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This is also equivalent to the exponentiation of the cross-entropy between the data and model predic...\"],[\"This is quick to compute since the perplexity of each segment can be computed in one forward pass, b...\"],[\"```\\n\\nWe'll load in the WikiText-2 dataset and evaluate the perplexity using a few different sliding-...\"],[\"```\\n\\nWith ğŸ¤— Transformers, we can simply pass the `input_ids` as the `labels` to our model, and the a...\"],[\"```\\n\\nRunning this with the stride length equal to the max input length is equivalent to the suboptim...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"| Task | Example model | Example dataset | ğŸ¤— Datasets | Colab\\n|---|---|---|:---:|:---:|\\n| [**`causal...\"],[\"## Intro: JAX and Flax\\n\\n[JAX](https:\\u002f\\u002fgithub.com\\u002fgoogle\\u002fjax) is a numerical computation library that...\"],[\"Each example README contains more details on the specific model and training\\nprocedure.\\n\\n\\n## Running...\"],[\"To specify a given repository name, use the `--hub_model_id` argument. You will need to specify the ...\"],[\"# ğŸ”¥ Model cards now live inside each huggingface.co model repo ğŸ”¥\\n\\n\\nFor consistency, ease of use and ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Dictionary\\n\\nHugging Face: à¤—à¤²à¥‡ à¤²à¤—à¤¾à¤“ à¤šà¥‡à¤¹à¤°à¤¾\\ntoken: à¤¶à¤¬à¥à¤¦ (à¤”à¤° à¤®à¥‚à¤² à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¥€ à¤•à¥‹ à¤•à¥‹à¤·à¥à¤ à¤• à¤®à¥‡à¤‚ à¤šà¤¿à¤¹à¥à¤¨à¤¿à¤¤ à¤•à¤°à¥‡à¤‚ï¼‰\\nto...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ğŸ¤— Transformers 100 à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤ªà¤¾à¤  à¤µà¤°à¥à¤—à¥€à¤•à¤°à¤£, à¤¸à¥‚à¤šà¤¨à¤¾ à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤·à¤£, à¤ªà¥à¤°à¤¶à¥à¤¨ à¤‰à¤¤à¥à¤¤à¤°, à¤¸à¤¾à¤°à¤¾à¤‚à¤¶à¥€à¤•à¤°à¤£, à¤…à¤¨à¥à¤µà¤¾...\"],[\"ğŸ¤— Transformers à¤¤à¥€à¤¨ à¤¸à¤¬à¤¸à¥‡ à¤²à¥‹à¤•à¤ªà¥à¤°à¤¿à¤¯ à¤—à¤¹à¤¨ à¤¶à¤¿à¤•à¥à¤·à¤£ à¤ªà¥à¤¸à¥à¤¤à¤•à¤¾à¤²à¤¯à¥‹à¤‚ à¤•à¤¾ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆï¼š [Jax](https:\\u002f\\u002fjax.readthe...\"],[\"à¤¯à¤¹à¤¾à¤ à¤•à¥à¤› à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤¹à¥ˆà¤‚ï¼š\\n- [à¤¶à¤¬à¥à¤¦ à¤•à¥‹ à¤­à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤®à¤¾à¤¸à¥à¤• à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ BERT à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚](https:\\u002f\\u002fhuggingfac...\"],[\"- [à¤¬à¤¾à¤°à¥à¤Ÿ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¤¾à¤  à¤¸à¤¾à¤°à¤¾à¤‚à¤¶](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+met...\"],[\"- [à¤¡à¤¿à¤¸à¥à¤Ÿà¤¿à¤²à¤¬à¤°à¥à¤Ÿ à¤•à¥‡ à¤¸à¤¾à¤¥...\"],[\"à¤ªà¥à¤°à¤¶à¥à¤¨à¥‹à¤¤à¥à¤¤à¤°](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+...\"],[\"orest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+sq...\"],[\"ounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departme...\"],[\"- [à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤²à¤¿à¤ T5 à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+li...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**ï¼Œà¤¹à¤—à¤¿à¤‚à¤— à¤«à¥‡à¤¸ à¤Ÿà¥€à¤® à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤¬à¤¨à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾, à¤¯à¤¹ ...\"],[\"```\\n\\nà¤•à¥‹à¤¡ à¤•à¥€ à¤¦à¥‚à¤¸à¤°à¥€ à¤ªà¤‚à¤•à¥à¤¤à¤¿ à¤ªà¤¾à¤‡à¤ªà¤²à¤¾à¤‡à¤¨ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤ à¤—à¤ à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤¡à¤¾à¤‰à¤¨à¤²à¥‹à¤¡ à¤”à¤° à¤•à¥ˆà¤¶ à¤•à¤°à¤¤à¥€ à¤¹...\"],[\"```\\n\\nà¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤¨à¥‡ à¤•à¥‡ à¤…à¤²à¤¾à¤µà¤¾, à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤² à¤¸à¤‚à¤—à¤¤ à¤†à¤¤à¥à¤®à¤µà¤¿à¤¶à¥à¤µà¤¾à¤¸ à¤¸à¥à¤•à¥‹à¤° à¤­à¥€ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¹à¤¾à¤‚ à¤‰à¤¤à¥à¤¤à¤° à¤Ÿà¥‹à¤•à¤¨à¤¯à¥...\"],[\"```\\n\\nà¤Ÿà¥‹à¤•à¤¨à¤¨à¤¾à¤‡à¤œà¤¼à¤° à¤¸à¤­à¥€ à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤²à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥à¤°à¥€à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸à¤¿à¤‚à¤— à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤”à¤° à¤‡à¤¸à¥‡ à¤¸à¥€à¤§à¥‡ à¤à¤• à¤¸à¥à¤Ÿà¥...\"],[\"## à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¤¾à¤°à¥à¤®à¤° à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥à¤¯à¥‹à¤‚ à¤•à¤°à¥‡à¤‚?\\n\\n1. à¤‰à¤ªà¤¯à¥‹à¤— à¤®à¥‡à¤‚ à¤†à¤¸à¤¾à¤¨à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤¨à¥à¤¨à¤¤ à¤®à¥‰à¤¡à¤²:\\n    - à¤à¤¨à¤à¤²à¤¯à¥‚ à¤”à¤° à¤à¤¨à¤à¤²à¤œà¥€ à¤ª...\"],[\"1. à¤†à¤¸à¤¾à¤¨à¥€ à¤¸à¥‡ à¤…à¤¨à¤¨à¥à¤¯ à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤…à¤¨à¥à¤•à¥‚à¤²à¤¿à¤¤ à¤•à¤°à¥‡à¤‚ à¤”à¤° à¤…à¤ªà¤¨à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚:\\n    - à¤¹à¤® à¤®à¥‚à¤²...\"],[\"## à¤®à¥à¤à¥‡ à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¥‰à¤°à¥à¤®à¤° à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¬ à¤¨à¤¹à¥€à¤‚ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤?\\n\\n- à¤¯à¤¹ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤®à¥‰à¤¡à¥à¤¯à¥‚à¤²à¤° à¤¨à¥à¤¯à¥‚à¤°à¤² à¤¨à¥‡à¤Ÿà¤µà¤°à¥à¤• à¤Ÿà¥‚à¤²à¤¬à¥‰à¤•à¥à¤¸ à¤¨...\"],[\"à¤†à¤ª [à¤µà¤°à¥à¤šà¥à¤…à¤² à¤à¤¨à¤µà¤¾à¤¯à¤°à¤¨à¤®à¥‡à¤‚à¤Ÿ](https:\\u002f\\u002fdocs.python.org\\u002f3\\u002flibrary\\u002fvenv.html) à¤®à¥‡à¤‚ ğŸ¤— à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¥‰à¤°à¥à¤®à¤° à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤° ...\"],[\"```\\n\\nà¤¯à¤¦à¤¿ à¤†à¤ª à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥‡ à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤•à¥‹ à¤†à¤œà¤¼à¤®à¤¾à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤¯à¤¾ à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤• à¤°à¤¿à¤²à¥€à¤œà¤¼ à¤¸à¥‡ à¤ªà¤¹à¤²à¥‡ à¤¨à¤µà¥€à¤¨à¤¤à¤® à¤‡à¤¨-à¤¡à¥‡à¤µà¤²à¤ªà¤®à¥‡à¤‚à¤Ÿ à¤•à¥‹à¤¡ ...\"],[\"```\\n\\nà¤•à¥‹à¤‚à¤¡à¤¾ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ Flax, PyTorch, à¤¯à¤¾ TensorFlow à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¤¿à¤¸à¥€ à¤à¤• à¤•à¥‹ à¤¸à¥à¤¥à¤¾à¤ªà¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶à¥‹...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (Google Research and the ...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (VinAI Research à¤¸à¥‡) à¤¸à¤¾à¤¥...\"],[\"1. **[BERT For Sequence Generation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbert-generati...\"],[\"1. **[BigBird-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbig_bird)** (à¤—à¥‚à¤—à¤² à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (Salesforce à¤¸à¥‡) Junnan Li...\"],[\"1. **[BROS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbros)** (NAVER CLOVA à¤¸à¥‡) Teakgyu Hong...\"],[\"1. **[CANINE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcanine)** (Google à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (OpenAI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [à¤²à¤°...\"],[\"1. **[CodeGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcodegen)** (à¤¸à¥‡à¤²à¥à¤¸à¤«à¥‹à¤°à¥à¤¸ à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚...\"],[\"1. **[Conditional DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconditional_detr)** (à¤®à¤¾à¤‡à¤•...\"],[\"1. **[ConvNeXTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnextv2)** (from Facebook AI...\"],[\"1. **[CTRL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fctrl)** (à¤¸à¥‡à¤²à¥à¤¸à¤«à¥‹à¤°à¥à¤¸ à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° ...\"],[\"1. **[DeBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta)** (Microsoft à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ ...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (à¤¸à¥‡à¤‚à¤¸à¤Ÿà¤¾...\"],[\"1. **[DETA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeta)** (from The University of Texas...\"],[\"1. **[DiNAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinat)** (from SHI Labs) released wi...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (à¤¹à¤—à¤¿à¤‚à¤—à¤«à¥‡à¤¸ à¤¸à¥‡), à¤¸à¤¾...\"],[\"1. **[Donut](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdonut)** (NAVER à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•à¤¾à¤—à¤œ [OC...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[EncoderDecoder](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fencoder-decoder)** (Google ...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (à¤®à¥‡à¤Ÿà¤¾ AI à¤¸à¥‡) à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¥‰à¤°à¥à¤®à¤° à¤ªà¥à¤°à¥‹à¤Ÿ...\"],[\"à¤®à¥‰à¤¡à¤² à¤µà¤¿à¤•à¤¾à¤¸ à¤•à¥‡ à¤ªà¥ˆà¤®à¤¾à¤¨à¥‡ à¤ªà¤° à¤ªà¥à¤°à¥‹à¤Ÿà¥€à¤¨ à¤…à¤¨à¥à¤•à¥à¤°à¤® à¤¸à¤Ÿà¥€à¤• à¤¸à¤‚à¤°à¤šà¤¨à¤¾ à¤­à¤µà¤¿à¤·à¥à¤¯à¤µà¤¾à¤£à¥€ à¤•à¥‹ à¤¸à¤•à¥à¤·à¤® à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚](https:\\u002f\\u002fdoi.org\\u002f10...\"],[\"1. **[FLAN-UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-ul2)** (from Google AI) rele...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (FLAVA: A à¤«à¤¾à¤‰à¤‚à¤¡à¥‡à¤¶à¤¨à¤² à¤²à¥ˆà¤‚à¤—à¥à¤µà¥‡...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (à¤¸à¥€à¤à¤®à¤¯à¥‚\\u002fà¤—à¥‚à¤—à¤² ...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (KAIST à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [à¤µà¤°à¥...\"],[\"1. **[GPT NeoX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox)** (EleutherAI à¤¸à¥‡) à¤ªà¥‡à¤ªà¤° ...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (EleutherAI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ª...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (BigCode à¤¸à¥‡) Lou...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[Hubert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fhubert)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° ...\"],[\"1. **[ImageGPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fimagegpt)** (from OpenAI) release...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤¸à¥‰à¤«à¥à¤Ÿ à¤°à¤¿à¤¸...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (à¤®à¥‡à¤Ÿà¤¾ AI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° ...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (UNC à¤šà¥ˆà¤ªà¤² à¤¹à¤¿à¤² à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[Mask2Former](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmask2former)** (FAIR and UIUC ...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤¨...\"],[\"1. **[Megatron-BERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron-bert)** (NVIDIA à¤¸à¥‡)...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (Facebook à¤¸à¥‡) Vineel Pratap, An...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (from Google I...\"],[\"1. **[MPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmpt)** (MosaiML à¤¸à¥‡) the MosaicML NLP T...\"],[\"1. **[MVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmvp)** (from RUC AI Box) released with...\"],[\"1. **[NLLB-MOE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnllb-moe)** (Meta à¤¸à¥‡) the NLLB te...\"],[\"1. **[OneFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002foneformer)** (SHI Labs à¤¸à¥‡) à¤ªà¥‡à¤ªà¤° ...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (Google AI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•...\"],[\"1. **[PatchTST](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtst)** (IBM à¤¸à¥‡) Yuqi Nie, Na...\"],[\"1. **[Perceiver IO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fperceiver)** (à¤¦à¥€à¤ªà¤®à¤¾à¤‡à¤‚à¤¡ à¤¸à¥‡) à¤¸à¤¾...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (Google à¤¸à¥‡) Kento...\"],[\"1. **[Pop2Piano](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpop2piano)** released with the p...\"],[\"1. **[QDQBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fqdqbert)** (NVIDIA à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡), à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•à¤¾...\"],[\"1. **[RoFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froformer)** (à¤à¥à¤ˆà¤ˆ à¤Ÿà¥‡à¤•à¥à¤¨à¥‹à¤²à¥‰à¤œà¥€ à¤¸à¥‡),...\"],[\"1. **[SegFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsegformer)** (from NVIDIA) relea...\"],[\"1. **[SEW-D](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew_d)** (ASAPP à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤­à¤¾...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (MBZUAI à¤¸à¥‡) Abd...\"],[\"1. **[Swin2SR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin2sr)** (from University of WÃ¼r...\"],[\"1. **[T5v1.1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5v1.1)** (Google AI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ª...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤¸à¥‰à¤«à¥à¤Ÿ à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸à¥‡) à¤¸à¤¾...\"],[\"1. **[Transformer-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftransfo-xl)** (Google\\u002fCMU à¤•...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤¸à¥‰à¤«à¥...\"],[\"1. **[VAN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvan)** (à¤¸à¤¿à¤‚à¤˜à¥à¤† à¤¯à¥‚à¤¨à¤¿à¤µà¤°à¥à¤¸à¤¿à¤Ÿà¥€ à¤”à¤° à¤¨à¤¨à¤•à¤¾à¤ˆ à¤¯à¥‚...\"],[\"1. **[VipLlava](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvipllava)** (University of Wiscon...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (from Google AI) ...\"],[\"1. **[ViTMatte](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitmatte)** (HUST-VL à¤¸à¥‡) Jingfeng...\"],[\"1. **[ViViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvivit)** (from Google Research) rele...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (Faceb...\"],[\"1. **[Whisper](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwhisper)** (OpenAI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•à¤¾à¤—...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤à¤†à¤ˆ à¤¸à¥‡)...\"],[\"1. **[XLNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlnet)** (Google\\u002fCMU à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤à¤†à¤ˆ...\"],[\"1. à¤à¤• à¤¨à¤ à¤®à¥‰à¤¡à¤² à¤®à¥‡à¤‚ à¤¯à¥‹à¤—à¤¦à¤¾à¤¨ à¤¦à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚? à¤¨à¤ à¤®à¥‰à¤¡à¤² à¤œà¥‹à¤¡à¤¼à¤¨à¥‡ à¤®à¥‡à¤‚ à¤†à¤ªà¤•à¤¾ à¤®à¤¾à¤°à¥à¤—à¤¦à¤°à¥à¤¶à¤¨ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¤®à¤¾à¤°à¥‡ à¤ªà¤¾à¤¸ à¤à¤•...\"],[\"à¤¯à¤¹ à¤œà¤¾à¤‚à¤šà¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿ à¤•à¥à¤¯à¤¾ à¤•à¤¿à¤¸à¥€ à¤®à¥‰à¤¡à¤² à¤®à¥‡à¤‚ à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤¹à¥€ Flax, PyTorch à¤¯à¤¾ TensorFlow à¤•à¤¾ à¤•à¤¾à¤°à¥à¤¯à¤¾à¤¨à¥à¤µà¤¯à¤¨ à¤¹à¥ˆ, à¤¯à¤¾ ...\"],[\"## à¤…à¤§à¤¿à¤• à¤¸à¤®à¤à¥‡à¤‚\\n\\n|à¤…à¤§à¥à¤¯à¤¾à¤¯ | à¤µà¤¿à¤µà¤°à¤£ |\\n|-|-|\\n| [à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¤¼à¥€à¤•à¤°à¤£](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002f) | à¤ªà¥‚à¤°...\"],[\"## à¤‰à¤¦à¥à¤§à¤°à¤£\\n\\nà¤¹à¤®à¤¨à¥‡ à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤• à¤¤à¥Œà¤° à¤ªà¤° à¤‡à¤¸ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤•à¤¾ [à¤ªà¥‡à¤ªà¤°](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-d...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Language models have become a key step to achieve st...\"],[\"## FlaubertConfig\\n\\n[[autodoc]] FlaubertConfig\\n\\n## FlaubertTokenizer\\n\\n[[autodoc]] FlaubertTokenizer\\n\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"ğŸ¤— Transformers integrates [DeepSpeed](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fDeepSpeed) via 2 options:\\n\\n1. Int...\"],[\"```\\n\\nor via `transformers`' `extras`:\\n\\n```bash\\npip install transformers[deepspeed]\\n```\\n\\nor find more...\"],[\"```\\n\\nSo if you get `8, 6`, then use `TORCH_CUDA_ARCH_LIST=\\\"8.6\\\"`. If you have multiple different car...\"],[\"```\\n\\nthen you know that this card's arch is `8.6`.\\n\\nYou can also leave `TORCH_CUDA_ARCH_LIST` out co...\"],[\"```\\n\\nAs you can see the arguments aren't the same, but for most needs either of them works. The\\nfull...\"],[\"```\\n\\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed_config ...\"],[\"```\\n\\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use jus...\"],[\"```\\n\\nwhich enables optimizer offload and some other important features. You may experiment with the ...\"],[\"```\\n\\n  In this example, we tell DeepSpeed to use GPU 1 (second gpu).\\n\\n\\n\\n\\u003ca id='deepspeed-multi-node'...\"],[\"```\\nhostname1 slots=8\\nhostname2 slots=8\\n```\\nand then you can launch it as:\\n\\n```bash\\ndeepspeed --num_...\"],[\"```\\n\\nUnlike the `torch.distributed.run` launcher, `deepspeed` will automatically launch this command...\"],[\"```\\n\\nAll is left is to schedule it to run:\\n```bash\\nsbatch launch.slurm\\n```\\n\\n`srun` will take care of...\"],[\"```\\n\\nNote: `...` stands for the normal arguments that you'd pass to the functions.\\n\\nIf you want to u...\"],[\"\\\"zero_optimization\\\": {\\n        \\\"stage\\\": 3,\\n        \\\"offload_optimizer\\\": {\\n            \\\"device\\\": \\\"cpu...\"],[\"```\\n\\nIf the training script is in a normal file and not in the notebook cells, you can launch `deeps...\"],[\"```\\n\\nSome more examples are to be found in the [main repo](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fDeepSpeed) a...\"],[\"```\\n\\nWhen you execute the program, DeepSpeed will log the configuration it received from the [`Train...\"],[\"```\\n\\n\\u003ca id='deepspeed-config-shared'\\u003e\\u003c\\u002fa\\u003e\\n\\n### Shared Configuration\\n\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThis sec...\"],[\"\\u003ca id='deepspeed-zero'\\u003e\\u003c\\u002fa\\u003e\\n\\n### ZeRO\\n\\n[Zero Redundancy Optimizer (ZeRO)](https:\\u002f\\u002fwww.deepspeed.ai\\u002ft...\"],[\"```\\n\\n**Performance tuning:**\\n\\n- enabling `offload_optimizer` should reduce GPU RAM usage (it require...\"],[\"```\\n\\nThis is a stage 2 optimization for CPU offloading that parallelizes gradient copying to CPU mem...\"],[\"```\\n\\nIf you are getting OOMs, because your model or activations don't fit into the GPU memory and yo...\"],[\"The following configuration values depend on the model's hidden size:\\n\\n- `reduce_bucket_size`: `hidd...\"],[\"#### ZeRO-0 Config\\n\\nNote that we're listing Stage 0 and 1 last since they are rarely used.\\n\\nStage 0 ...\"],[\"```\\n\\nThis will essentially disable ZeRO without you needing to change anything else.\\n\\n\\n#### ZeRO-1 C...\"],[\"```\\n\\n\\n\\n\\u003ca id='deepspeed-nvme'\\u003e\\u003c\\u002fa\\u003e\\n\\n### NVMe Support\\n\\nZeRO-Infinity allows for training incredibly l...\"],[\"```\\n\\nYou can choose to offload both optimizer states and params to NVMe, or just one of them or none...\"],[\"The performance will likely improve significantly with just `offload_params` turned off, even if you...\"],[\"```\\n\\nHere is a full ZeRO-2 all-enabled manually set configuration file. It is here mainly for you to...\"],[\"```\\n\\n\\u003ca id='deepspeed-zero3-example'\\u003e\\u003c\\u002fa\\u003e\\n\\n#### ZeRO-3 Example\\n\\nHere is a full ZeRO-3 auto-configura...\"],[\"\\\"gradient_accumulation_steps\\\": \\\"auto\\\",\\n    \\\"gradient_clipping\\\": \\\"auto\\\",\\n    \\\"steps_per_print\\\": 2000,...\"],[\"```\\n\\nHere is a full ZeRO-3 all-enabled manually set configuration file. It is here mainly for you to...\"],[\"```\\n\\n#### How to Choose Which ZeRO Stage and Offloads To Use For Best Performance\\n\\nSo now you know t...\"],[\"8. Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher GPUs and fp16 on old...\"],[\"These notes were written primarily for the training mode, but they should mostly apply for inference...\"],[\"Therefore you have two ways to take advantage of this very beneficial feature:\\n\\n1. If you want to us...\"],[\"If you don't configure the `optimizer` entry in the configuration file, the [`Trainer`] will\\nautomat...\"],[\"```\\n\\nNote that the command line arguments will set the values in the configuration file. This is so ...\"],[\"```\\nto the top level configuration.\\n\\n\\n\\n\\u003ca id='deepspeed-scheduler'\\u003e\\u003c\\u002fa\\u003e\\n\\n#### Scheduler\\n\\nDeepSpeed s...\"],[\"```\\n\\nSince *\\\"auto\\\"* is used the [`Trainer`] arguments will set the correct values in the configurati...\"],[\"```\\n\\nand `total_num_steps`, `warmup_max_lr`, `warmup_num_steps` and `total_num_steps` will be set at...\"],[\"```\\n\\nIf you're using the Ampere-architecture based GPU, pytorch version 1.7 and higher will automati...\"],[\"```\\n\\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSp...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nAs of `deepspeed==0.6.0` the bf16 support is new and experimental.\\n\\nIf you use [gradient...\"],[\"```\\nThe valid values as of this writing are \\\"fp16\\\", \\\"bfp16\\\", \\\"fp32\\\".\\n\\nnote: stage zero 3 had a bug w...\"],[\"```\\n\\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSp...\"],[\"```\\n\\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSp...\"],[\"```\\n\\n**FP32 Weights:**\\n\\nWhile the fp16 weights are fine for resuming training, if you finished finet...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNote, that once `load_state_dict_from_zero_checkpoint` was run, the `model` will no long...\"],[\"```\\n\\n**Offline FP32 Weights Recovery:**\\n\\nDeepSpeed creates a special conversion script `zero_to_fp32...\"],[\"```\\n\\nIn this example there is just one DeepSpeed checkpoint sub-folder *global_step1*. Therefore to ...\"],[\"```\\n\\nAs you can see this gives you a randomly initialized model.\\n\\nIf you want to use a pretrained mo...\"],[\"```\\n\\nIf you're using the official example scripts and your command line arguments include `--deepspe...\"],[\"```\\n\\nstress on `tensor([1.])`, or if you get an error where it says the parameter is of size `1`, in...\"],[\"```\\n\\nSince for inference there is no need for additional large memory used by the optimizer states a...\"],[\"Let's estimate how much memory is needed to finetune \\\"bigscience\\u002fT0_3B\\\" on a single GPU:\\n\\n```bash\\n$ ...\"],[\"```\\n\\nSo you can fit it on a single 80GB GPU and no CPU offload, or a tiny 8GB GPU but then need ~60G...\"],[\"If you have enough GPU memory make sure to disable the CPU\\u002fNVMe offload as it'll make everything fas...\"],[\"```\\n\\nSo here you'd want 2x 32GB GPUs or higher without offloading to CPU.\\n\\nFor full information plea...\"],[\"```\\n\\n4. If possible include a link to a Google Colab notebook that we can reproduce the problem with...\"],[\"### Troubleshooting\\n\\n#### the `deepspeed` process gets killed at startup without a traceback\\n\\nIf the...\"],[\"```\\n\\nand you see in your log that Deepspeed reports `OVERFLOW!` as follows:\\n\\n```\\n0%|                ...\"],[\"```\\n\\nthat means that the Deepspeed loss scaler can't figure out a scaling co-efficient that overcome...\"],[\"For example for a pretrained model:\\n\\n```python\\nfrom transformers.integrations import HfDeepSpeedConf...\"],[\"```\\n\\nor for non-pretrained model:\\n\\n```python\\nfrom transformers.integrations import HfDeepSpeedConfig...\"],[\"```\\n\\nPlease note that if you're not using the [`Trainer`] integration, you're completely on your own...\"],[\"```python\\n#!\\u002fusr\\u002fbin\\u002fenv python\\n\\n# This script demonstrates how to use Deepspeed ZeRO in an inferenc...\"],[\"os.environ[\\\"TOKENIZERS_PARALLELISM\\\"] = \\\"false\\\"  # To avoid warnings about parallelism in tokenizers\\n...\"],[\"# keeping the same format as json for consistency, except it uses lower case for true\\u002ffalse\\n# fmt: o...\"],[\"# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at onc...\"],[\"```\\n\\nLet's save it as `t0.py` and run it:\\n```\\n$ deepspeed --num_gpus 2 t0.py\\nrank0:\\n   in=Is this re...\"],[\"```\\nRUN_SLOW=1 pytest tests\\u002fdeepspeed\\n```\\n\\n\\n\\n\\n## Main DeepSpeed Resources\\n\\n- [Project's github](http...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n### Model checkpoints\\n\\n|     Model Name      | Language |           Description           |\\n|:-...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Compression plays an important role on the efficient...\"],[\"## Resources\\n\\nDemo notebooks for Swin2SR can be found [here](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransform...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\npython run_mlm.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--output_dir output \\\\\\n--dataset...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Natural Language Processing (NLP) has recently achie...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## TFMobileBertForPreTraining\\n\\n[[autodoc]] TFMobileBertForPreTraining\\n    - call\\n\\n## TFMobileBertFor...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recent \\\"Text-to-Text Transfer Transformer\\\" (T5) ...\"],[\"## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002ftasks...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have demonstrated the efficiency of g...\"],[\"* Causal language modeling (CLM) which is the traditional autoregressive training (so this model cou...\"],[\"[[autodoc]] XLMForSequenceClassification\\n    - forward\\n\\n## XLMForMultipleChoice\\n\\n[[autodoc]] XLMForM...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\nNew behaviour\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import NllbTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = NllbTokeniz...\"],[\"```\\n\\nFor more details, feel free to check the linked [PR](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformer...\"],[\"The abstract of the paper is the following:\\n\\n*Driven by the goal of eradicating language barriers on...\"],[\"Note that we're using the BCP-47 code for French `fra_Latn`. See [here](https:\\u002f\\u002fgithub.com\\u002ffacebookr...\"],[\"```\\n\\n### Generating from any other language than English\\n\\nEnglish (`eng_Latn`) is set as the default...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [zphang](https:\\u002f\\u002fhuggingface.co\\u002fzphang) with contributions from [Black...\"],[\"```\\n\\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fgith...\"],[\"ğŸš€ Deploy\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002flxe\\u002fsimple-llama-finetuner\\u002fblob\\u002fmas...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper shows that masked autoencoders (MAE) are ...\"],[\"## Usage tips\\n\\n- MAE (masked auto encoding) is a method for self-supervised pre-training of Vision T...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Also introduced in this paper was ESMFold. It uses an ESM-2 stem with a head that can predict folded...\"],[\"The abstract from\\n\\\"Language models of protein sequences at the scale of evolution enable accurate st...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We design a family of image classification architect...\"],[\"- Compared to ViT, LeViT models use an additional distillation head to effectively learn from a teac...\"],[\"Techniques like data augmentation, optimization, and regularization were used in order to simulate t...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer with...\"],[\"- `static_categorical_features`: categorical features which are static over time (i.e., have the sam...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Note on custom data\\n\\nIn case you'd like to use the script with custom data, there are 2 things re...\"],[\"# step 2: create DatasetDict\\ndataset = DatasetDict({\\n    \\\"train\\\": train_dataset,\\n    \\\"validation\\\": v...\"],[\"```\\n\\nAn example of such a dataset can be seen at [nielsr\\u002fade20k-demo](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nYou can easily upload this by clicking on \\\"Add file\\\" in the \\\"Files and versions\\\" tab of your re...\"],[\"```\\n\\nThe resulting model can be seen here: https:\\u002f\\u002fhuggingface.co\\u002fnielsr\\u002fsegformer-finetuned-sidewal...\"],[\"```\\n\\nand reply to the questions asked regarding the environment on which you'd like to train. Then\\n\\n...\"],[\"```\\n\\nand perform inference as follows:\\n\\n```python\\nfrom PIL import Image\\nimport requests\\nimport torch...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you run into any issues running this model, please reinstall the last version that supported...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have a potential of learning longer-ter...\"],[\"## Usage tips\\n\\n- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done ...\"],[\"[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput\\n\\n[[autodoc]]...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] pytorch_utils.prune_linear_layer\\n\\n## TensorFlow custom layers\\n\\n[[autodoc]] modeling_tf_u...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For any feature you'd like to implement in an example script, please discuss it on the [forum](https...\"],[\"```\\n\\nFor older versions of the example scripts, click on the toggle below:...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eExamples for older versions of ğŸ¤— Transformers\\u003c\\u002fsummary\\u003e\\n\\t\\u003cul\\u003e\\n\\t\\t\\u003cli\\u003e\\u003ca href=\\\"ht...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv3.3.1\\u002fexamples\\\"\\u003ev3.3.1\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv2.6.0\\u002fexamples\\\"\\u003ev2.6.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"Then switch your current clone of ğŸ¤— Transformers to a specific version, like v3.5.1 for example:\\n\\n``...\"],[\"```\\n\\nAfter you've setup the correct library version, navigate to the example folder of your choice a...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nThe example script downloads and preprocesses a dataset from the ğŸ¤— [Datasets](https:\\u002f...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Distributed training and mixed precision\\n\\nThe [Trainer](https:\\u002f\\u002fhu...\"],[\"```\\n\\nTensorFlow scripts utilize a [`MirroredStrategy`](https:\\u002f\\u002fwww.tensorflow.org\\u002fguide\\u002fdistributed_...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nTensor Processing Units (TPUs) are specifically designed to accelerate performance. T...\"],[\"```\\n\\nTest your setup to make sure it is configured correctly:\\n\\n```bash\\naccelerate test\\n```\\n\\nNow you ...\"],[\"```\\n\\n## Test a script\\n\\nIt is often a good idea to run your script on a smaller number of dataset exa...\"],[\"```\\n\\n## Resume training from checkpoint\\n\\nAnother helpful option to enable is resuming training from ...\"],[\"```\\n\\nThen add the `push_to_hub` argument to the script. This argument will create a repository with ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import DistilBertConfig\\n\\n\\u003e\\u003e\\u003e config = DistilBertConfig()\\n\\u003e\\u003e\\u003e print(confi...\"],[\"```\\n\\n[`DistilBertConfig`] displays all the default attributes used to build a base [`DistilBertModel...\"],[\"```\\n\\nTo reuse the configuration file, load it with [`~PretrainedConfig.from_pretrained`]:\\n\\n```py\\n\\u003e\\u003e\\u003e...\"],[\"```\\n\\nThis creates a model with random values instead of pretrained weights. You won't be able to use...\"],[\"```\\n\\nWhen you load pretrained weights, the default model configuration is automatically loaded if th...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nFor example, [`TFDistilBertForSequenceClassification`] is a base DistilBERT model wit...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Tokenizer\\n\\nThe last base class you need before using a model for t...\"],[\"```\\n\\nCreate a fast tokenizer with the [`DistilBertTokenizerFast`] class:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformer...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf you aren't looking for any customization, just use the `from_pretrained` method to lo...\"],[\"```\\n\\n## Feature Extractor\\n\\nA feature extractor processes audio inputs. It inherits from the base [`~...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf you aren't looking for any customization, just use the `from_pretrained` method to lo...\"],[\"```\\n\\nCombine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from transform...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Implementation Notes\\n\\n- FSMT uses source and target vocabulary pairs that aren't combined into on...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the...\"],[\"New in v2:\\n\\n- **Vocabulary** In v2 the tokenizer is changed to use a new vocabulary of size 128K bui...\"],[\"## DebertaV2TokenizerFast\\n\\n[[autodoc]] DebertaV2TokenizerFast\\n    - build_inputs_with_special_tokens...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr). The original code can be foun...\"],[\"```\\n\\nNote that LayoutXLM has its own tokenizer, based on\\n[`LayoutXLMTokenizer`]\\u002f[`LayoutXLMTokenizer...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present in this paper a new architecture, named C...\"],[\"## Usage tips\\n\\n- CvT models are regular Vision Transformers, but trained with convolutions. They out...\"],[\"## CvtConfig\\n\\n[[autodoc]] CvtConfig\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## CvtModel\\n\\n[[autodoc]] CvtModel\\n    ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\nthis should make a directory called `cnn_dm\\u002f` with 6 files.\\n\\n#### WMT16 English-Romanian Transla...\"],[\"```\\nThe `.source` files are the input, the `.target` files are the desired output.\\n\\n### Potential is...\"],[\"Summarization Tips:\\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB...\"],[\"### Fine-tuning using Seq2SeqTrainer\\nTo use `Seq2SeqTrainer` for fine-tuning you should use the `fin...\"],[\"```\\n\\nFor multi-gpu training use `torch.distributed.launch`, e.g. with 2 gpus:\\n```bash\\ntorchrun --npr...\"],[\"```\\n\\n## Evaluation Commands\\n\\nTo create summaries for each article in dataset, we use `run_eval.py`, ...\"],[\"```\\n\\n### Multi-GPU Evaluation\\nhere is a command to run xsum evaluation on 8 GPUS. It is more than li...\"],[\"```\\n\\n   `--info` is an additional argument available for the same purpose of tracking the conditions...\"],[\"```\\n    --search \\\"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\\\"\\n   ```\\n   whi...\"],[\"```\\n\\nIf you pass `--info \\\"some experiment-specific info\\\"` it will get printed before the results tab...\"],[\"```\\nsplits `wmt_en_ro\\u002ftrain` into 11,197 uneven lengthed batches and can finish 1 epoch in 8 minutes...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Driven by the goal of eradicating language barriers on...\"],[\"## Implementation differences with SwitchTransformers\\n\\nThe biggest difference is the way the tokens ...\"],[\"\\u003e\\u003e\\u003e translated_tokens = model.generate(\\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to...\"],[\"```\\n\\n### Generating from any other language than English\\n\\nEnglish (`eng_Latn`) is set as the default...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by repl...\"],[\"## FNetConfig\\n\\n[[autodoc]] FNetConfig\\n\\n## FNetTokenizer\\n\\n[[autodoc]] FNetTokenizer\\n    - build_input...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We tackle the task of conditional music generation. ...\"],[\"```\\n\\n## Generation\\n\\nMusicGen is compatible with two generation modes: greedy and sampling. In practi...\"],[\"```\\n\\nOr save them as a `.wav` file using a third-party library, e.g. `scipy`:\\n\\n```python\\n\\u003e\\u003e\\u003e import ...\"],[\"```\\npip install --upgrade pip\\npip install datasets[audio]\\n```\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers impor...\"],[\"```\\n\\nFor batched audio-prompted generation, the generated `audio_values` can be post-processed to re...\"],[\"```\\n\\n### Generation Configuration\\n\\nThe default parameters that control the generation process, such ...\"],[\"```\\n\\nNote that any arguments passed to the generate method will **supersede** those in the generatio...\"],[\"```\\n\\nSince the text encoder and audio encoder\\u002fdecoder models are frozen during training, the MusicGe...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] generation.GenerationMixin\\n\\t- generate\\n\\t- compute_transition_scores\\n\\t- greedy_search\\n\\t- ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that ca...\"],[\"```\\n\\nThen tokenize your input as shown below:\\n\\n```py\\n\\u003e\\u003e\\u003e sequence = \\\"In a hole in the ground there l...\"],[\"```\\n\\n## AutoProcessor\\n\\nMultimodal tasks require a processor that combines two types of preprocessing...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nFor PyTorch models, the `from_pretrained()` method uses `torch.load()` wh...\"],[\"```\\n\\nEasily reuse the same checkpoint to load an architecture for a different task:\\n\\n```py\\n\\u003e\\u003e\\u003e from ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"[NystrÃ¶mformer](..\\u002fmodel_doc\\u002fnystromformer), [Perceiver](..\\u002fmodel_doc\\u002fperceiver), [QDQBert](..\\u002fmodel...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to log in to your Hugging Face account so you can upload and share your model ...\"],[\"```\\n\\nThen take a look at an example:\\n\\n```py\\n\\u003e\\u003e\\u003e eli5[\\\"train\\\"][0]\\n{'answers': {'a_id': ['c3d1aib', 'c...\"],[\"```\\n\\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool ...\"],[\"```\\n\\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. Thi...\"],[\"```py\\n\\u003e\\u003e\\u003e eli5 = eli5.flatten()\\n\\u003e\\u003e\\u003e eli5[\\\"train\\\"][0]\\n{'answers.a_id': ['c3d1aib', 'c3d4lya'],\\n 'answ...\"],[\"```\\n\\nEach subfield is now a separate column as indicated by the `answers` prefix, and the `text` fie...\"],[\"```\\n\\nThis dataset contains the token sequences, but some of these are longer than the maximum input ...\"],[\"```\\n\\nNow create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient t...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nThen share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so every...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nPass your inputs to the model and return the `logits` of the masked token:\\n\\n```py\\n\\u003e\\u003e\\u003e from tran...\"],[\"```\\n\\nPass your inputs to the model and return the `logits` of the masked token:\\n\\n```py\\n\\u003e\\u003e\\u003e from tran...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Existing pre-trained models are generally geared tow...\"],[\"## Usage tips\\n\\n- UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions as ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- [self-hosted (push)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002f.github\\u002fworkflows\\u002fself-...\"],[\"```\\n\\n   The results can be observed [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002factions).\\n\\n\\n\\n...\"],[\"```\\n\\nwill run all the tests inside that class.\\n\\nAs mentioned earlier you can see what tests are cont...\"],[\"```\\n\\n\\n### Run documentation tests \\n\\nIn order to test whether the documentation examples are correct,...\"],[\"```\\n\\nJust run the following line to automatically test every docstring example in the desired file: ...\"],[\"```\\n\\nor `pytest.ini`\\u002f``tox.ini`` files:\\n\\n```ini\\n[pytest]\\nlooponfailroots = transformers tests\\n```\\n\\nT...\"],[\"```\\n\\n### Clearing state\\n\\nCI builds and when isolation is important (against speed), cache should be ...\"],[\"```\\n\\nImportant: the presence of `pytest-random-order` will automatically randomize tests, no configu...\"],[\"```\\n\\nTo disable the shuffling for all tests:\\n\\n```bash\\npytest --random-order-bucket=none\\n```\\n\\nBy defa...\"],[\"```\\n\\n```bash\\npytest --instafail\\n```\\n\\n### To GPU or not to GPU\\n\\nOn a GPU-enabled setup, to test in CP...\"],[\"```\\n\\nThis is handy when you want to run different tasks on different GPUs.\\n\\nSome tests must be run o...\"],[\"```\\n\\nThese decorators can be stacked. For example, if a test is slow and requires at least one GPU u...\"],[\"```\\nAlternative backends may also require the replacement of device-specific functions. For example ...\"],[\"```\\nThis format also allows for specification of any additional imports required. To use this file t...\"],[\"```\\n\\nTo send test results to JUnit format output:\\n\\n```bash\\npy.test tests --junitxml=result.xml\\n```\\n\\n...\"],[\"```\\n\\nNow, by default this test will be run 3 times, each time with the last 3 arguments of `test_flo...\"],[\"```\\n\\nThe module [parameterized](https:\\u002f\\u002fpypi.org\\u002fproject\\u002fparameterized\\u002f) which is already in the dev...\"],[\"```\\n\\nas in the previous example.\\n\\n\\n\\n### Files and directories\\n\\nIn tests often we need to know where ...\"],[\"```\\n\\nIf you don't need to manipulate paths via `pathlib` or you just need a path as a string, you ca...\"],[\"```\\n\\n`tmp_dir` will contain the path to the created temporary dir. It will be automatically removed ...\"],[\"```\\n\\n### Skipping tests\\n\\nThis is useful when a bug is found and a new test is written, yet the bug i...\"],[\"```\\n\\nor the `xfail` way:\\n\\n```python\\ndef test_feature_x():\\n    pytest.xfail(\\\"expected to fail until b...\"],[\"```\\n\\nOnce a test is marked as `@slow`, to run such tests set `RUN_SLOW=1` env var, e.g.:\\n\\n```bash\\nRU...\"],[\"```\\n\\nAs explained at the beginning of this document, slow tests get to run on a scheduled basis, rat...\"],[\"```\\n\\nHere is a an example of a [script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fscript...\"],[\"```\\n\\nAnd, of course, most of the time, `stderr` will come as a part of an exception, so try\\u002fexcept h...\"],[\"```\\n\\nHere is a full test example:\\n\\n```python\\nfrom transformers.testing_utils import CaptureStdout\\n\\nm...\"],[\"```\\n\\nAt times an external program needs to be called, which requires setting `PYTHONPATH` in `os.env...\"],[\"```\\n\\n## Working with github actions workflows\\n\\nTo trigger a self-push workflow CI job, you must:\\n\\n1....\"],[\"Some CIs, like TravisCI support ignore-step-failure and will report the overall job as successful, b...\"],[\"```\\n\\nFor simple commands you could also do:\\n\\n```bash\\ncmd_that_may_fail || true\\n```\\n\\nOf course, once ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce dense vision transformers, an architect...\"],[\"```python\\nfrom transformers import Dinov2Config, DPTConfig, DPTForDepthEstimation\\n\\n# initialize with...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"Awesome projects built with Transformers\\n\\nThis page lists awesome projects built on top of Transform...\"],[\"FLAIR is a powerful PyTorch NLP framework, convering several important tasks: NER, sentiment-analysi...\"],[\"Keywords: Dialogue, Chatbots, VQA, Datasets, Agents\\n\\n## [sentence-transformers](https:\\u002f\\u002fgithub.com\\u002fU...\"],[\"Keywords: NLP, Chinese, Research, Industry\\n\\n## [stanza](https:\\u002f\\u002fgithub.com\\u002fstanfordnlp\\u002fstanza)\\n\\nThe ...\"],[\"Keywords: Adapters, LoRA, Parameter-efficient fine-tuning, Hub\\n\\n## [NeMo](https:\\u002f\\u002fgithub.com\\u002fNVIDIA\\u002f...\"],[\"Keywords: Healthcare imaging, Training, Evaluation\\n\\n## [simpletransformers](https:\\u002f\\u002fgithub.com\\u002fThili...\"],[\"Haystack is an open source NLP framework to interact with your data using Transformer models and LLM...\"],[\"Keywords: Visualization, Transformers\\n\\n## [mesh-transformer-jax](https:\\u002f\\u002fgithub.com\\u002fkingoflolz\\u002fmesh-...\"],[\"Keywords: Data augmentation, Synthetic data generation, Audio, NLP\\n\\n## [dream-textures](https:\\u002f\\u002fgith...\"],[\"Stable-Dreamfusion is a pytorch implementation of the text-to-3D model Dreamfusion, powered by the S...\"],[\"Keywords: Training, LLM, Megatron, DeepSpeed\\n\\n## [muzic](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fmuzic)\\n\\nMuzic ...\"],[\"## [open_clip](https:\\u002f\\u002fgithub.com\\u002fmlfoundations\\u002fopen_clip)\\n\\nOpenCLIP is an open source implementatio...\"],[\"[TextAttack](https:\\u002f\\u002fgithub.com\\u002fQData\\u002fTextAttack) ğŸ™ is a Python framework for adversarial attacks, d...\"],[\"Keywords: Model inspection, Model interpretation, Black-box, White-box\\n\\n## [tortoise-tts](https:\\u002f\\u002fgi...\"],[\"Keywords: Adversarial, Outlier, Drift detection\\n\\n## [FARM](https:\\u002f\\u002fgithub.com\\u002fdeepset-ai\\u002fFARM)\\n\\n[FAR...\"],[\"Keywords: Model explainability\\n\\n## [s3prl](https:\\u002f\\u002fgithub.com\\u002fs3prl\\u002fs3prl)\\n\\n[s3prl](https:\\u002f\\u002fgithub.c...\"],[\"Keywords: Stable Diffusion, CLI, Python API\\n\\n## [sparseml](https:\\u002f\\u002fgithub.com\\u002fneuralmagic\\u002fsparseml)\\n...\"],[\"Keywords: Rust, BERT, Inference\\n\\n## [EasyNLP](https:\\u002f\\u002fgithub.com\\u002falibaba\\u002fEasyNLP)\\n\\n[EasyNLP](https:\\u002f...\"],[\"Keywords: Semi-structured documents, Unstructured documents, LLM, Document Question Answering\\n\\n## [C...\"],[\"Keywords: Model deployment, CLoud, Mobile, Edge\\n\\n## [underthesea](https:\\u002f\\u002fgithub.com\\u002fundertheseanlp\\u002f...\"],[\"Keywords: Model interpretation, Visualization\\n\\n## [mlrun](https:\\u002f\\u002fgithub.com\\u002fmlrun\\u002fmlrun)\\n\\nMLRun is ...\"],[\"Keywords: Large models, Training, Fine-tuning, Deployment, Multi-modal\\n\\n## [pyserini](https:\\u002f\\u002fgithub...\"],[\"Keywords: Data-Centric AI, Data Quality, Noisy Labels, Outlier Detection, Active Learning  \\n\\n## [Ben...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003eAprendizado de mÃ¡quina de Ãºltima geraÃ§Ã£o para JAX, PyTorch e TensorFlow\\u003c\\u002f...\"],[\"A biblioteca ğŸ¤— Transformers oferece APIs para baixar e usar rapidamente esses modelos prÃ©-treinados ...\"],[\"- [Completar palavra mascarada com BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+...\"],[\"- [InferÃªncia de Linguagem Natural com RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+d...\"],[\"- [Resposta a perguntas com...\"],[\"- [TraduÃ§Ã£o com T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"Em VisÃ£o Computacional:\\n- [ClassificaÃ§Ã£o de Imagens com ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-...\"],[\"Em Tarefas Multimodais:\\n- [Respostas de Perguntas em Tabelas com TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fgoogl...\"],[\"## Se vocÃª estÃ¡ procurando suporte personalizado da equipe Hugging Face\\n\\n\\u003ca target=\\\"_blank\\\" href=\\\"ht...\"],[\"```\\n\\nA segunda linha de cÃ³digo baixa e armazena em cache o modelo prÃ©-treinado usado pelo pipeline, ...\"],[\"```\\n\\n\\nAqui obtemos uma lista de objetos detectados na imagem, com uma caixa envolvendo o objeto e um...\"],[\"```\\n\\nE aqui estÃ¡ o cÃ³digo equivalente para TensorFlow:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoT...\"],[\"```\\n\\nO tokenizador Ã© responsÃ¡vel por todo o prÃ©-processamento que o modelo prÃ©-treinado espera, e po...\"],[\"1. Menores custos de computaÃ§Ã£o, menor pegada de carbono:\\n    - Pesquisadores podem compartilhar mod...\"],[\"## Por que nÃ£o devo usar transformers?\\n\\n- Esta biblioteca nÃ£o Ã© uma caixa de ferramentas modular par...\"],[\"Primeiro, crie um ambiente virtual com a versÃ£o do Python que vocÃª vai usar e ative-o.\\n\\nEm seguida, ...\"],[\"```\\nSe vocÃª deseja experimentar com os exemplos ou precisa da versÃ£o mais recente do cÃ³digo e nÃ£o po...\"],[\"```\\n\\nSiga as pÃ¡ginas de instalaÃ§Ã£o do Flax, PyTorch ou TensorFlow para ver como instalÃ¡-los com cond...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (from CMU\\u002fGoo...\"],[\"1. **[GPT Neo](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neo)** (from EleutherAI) relea...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (from EleutherAI) released i...\"],[\"1. **[MGP-STR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmgp-str)** (from Alibaba Research)...\"],[\"1. **[OPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmaster\\u002fmodel_doc\\u002fopt)** (from Meta AI) released ...\"],[\"1. **[Perceiver IO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fperceiver)** (from Deepmind) ...\"],[\"1. **[ViTMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_mae)** (from Meta AI) released ...\"],[\"1. **[XLS-R](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxls_r)** (from Facebook AI) released...\"],[\"1. Quer contribuir com um novo modelo? Adicionamos um **guia detalhado e modelos de exemplo** para o...\"],[\"## Saiba mais\\n\\n| SeÃ§Ã£o | DescriÃ§Ã£o |\\n|-|-|\\n| [DocumentaÃ§Ã£o](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers...\"],[\"## CitaÃ§Ã£o\\n\\nAgora temos um [artigo](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f) que vocÃª p...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fi.ibb.co\\u002fW6PQMdC\\u002fScreenshot-2022-09-13-at-9-08-40-AM.png\\\" alt=\\\"drawing\\\" width=\\\"600...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Although convolutional neural networks (CNNs) have a...\"],[\"- PVTv1 on ImageNet-1K\\n\\n| **Model variant**  |**Size** |**Acc@1**|**Params (M)**|\\n|-----------------...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--This tip is automatically generated by `make fix-copies`, do not fill manually!--\\u003e...\"],[\"[BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [Bert Generation](..\\u002fmodel_doc\\u002fbert-generation...\"],[\"[GPT-J](..\\u002fmodel_doc\\u002fgptj), [LLaMA](..\\u002fmodel_doc\\u002fllama), [Marian](..\\u002fmodel_doc\\u002fmarian), [mBART](..\\u002fm...\"],[\"[Speech2Text2](..\\u002fmodel_doc\\u002fspeech_to_text_2), [Transformer-XL](..\\u002fmodel_doc\\u002ftransfo-xl), [TrOCR](.....\"],[\"```\\n\\nWe encourage you to log in to your Hugging Face account so you can upload and share your model ...\"],[\"```\\n\\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool ...\"],[\"```\\n\\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. Thi...\"],[\"```\\n\\nThis dataset contains the token sequences, but some of these are longer than the maximum input ...\"],[\"```\\n\\nNow create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient t...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nThen share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so every...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nUse the [`~transformers.generation_utils.GenerationMixin.generate`] method to generate text.\\nFo...\"],[\"```\\n\\nUse the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the s...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Semi-supervised learning through pseudo-labeling has...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Let's start by creating a model repository to save the trained model and logs.\\nHere we call the mode...\"],[\"```\\nhuggingface-cli repo create english-roberta-base-dummy\\n```\\n\\nNext we clone the model repository t...\"],[\"```\\n\\n### Train model\\n\\nNext we can run the example script to pretrain the model.\\nCompared to the defa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are widely used in natural ...\"],[\"## Usage tips\\n\\n- The YOSO attention algorithm is implemented through custom CUDA kernels, functions ...\"],[\"## YosoForMultipleChoice\\n\\n[[autodoc]] YosoForMultipleChoice\\n    - forward\\n\\n## YosoForTokenClassifica...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n1. CLVP is an integral part of the Tortoise TTS model.\\n2. CLVP can be used to compare...\"],[\"\\u003e\\u003e\\u003e # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using ...\"],[\"```\\n\\n\\n## ClvpConfig\\n\\n[[autodoc]] ClvpConfig\\n    - from_sub_model_configs\\n\\n## ClvpEncoderConfig\\n\\n[[au...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The tremendous success of CLIP (Radford et al., 2021...\"],[\"\\u003e\\u003e\\u003e url = \\\"https:\\u002f\\u002fclip-cn-beijing.oss-cn-beijing.aliyuncs.com\\u002fpokemon.jpeg\\\"\\n\\u003e\\u003e\\u003e image = Image.open(...\"],[\"```\\n\\nCurrently, following scales of pretrained Chinese-CLIP models are available on ğŸ¤— Hub:\\n\\n- [OFA-S...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pretrained multilingual large language models have t...\"],[\"## Usage tips \\n\\n- UMT5 was only pre-trained on [mC4](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmc4) excluding ...\"],[\"```\\n\\n\\u003cTip\\u003e \\n\\nRefer to [T5's documentation page](t5) for more tips, code examples and notebooks.\\n\\u003c\\u002fTi...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"ProphetNet is an encoder-decoder model and can predict n-future tokens for \\\"ngram\\\" language modeling...\"],[\"[[autodoc]] ProphetNetTokenizer\\n\\n## ProphetNet specific outputs\\n\\n[[autodoc]] models.prophetnet.model...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [sshleifer](https:\\u002f\\u002fhuggingface.co\\u002fsshleifer). The Authors' code can b...\"],[\"## Implementation Notes\\n\\n- All models are transformer encoder-decoders with 16 layers in each compon...\"],[\"... model_name = \\\"google\\u002fpegasus-xsum\\\"\\n... device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n....\"],[\"```\\n\\n## Resources\\n\\n- [Script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fresearc...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [anton-l](https:\\u002f\\u002fhuggingface.co\\u002fanton-l).\\n\\n## Usage tips\\n\\n- SEW-D is ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the general idea of self-supervised learning i...\"],[\"## Usage tips\\n\\n- Data2VecAudio, Data2VecText, and Data2VecVision have all been trained using the sam...\"],[\"**Data2VecAudio documentation resources**\\n- [Audio classification task guide](..\\u002ftasks\\u002faudio_classif...\"],[\"## Data2VecTextForSequenceClassification\\n\\n[[autodoc]] Data2VecTextForSequenceClassification\\n    - fo...\"],[\"Token classification\\n\\n## PyTorch version, no Trainer\\n\\nFine-tuning (m)LUKE for token classification t...\"],[\"```\\n\\nand reply to the questions asked. Then\\n\\n```bash\\naccelerate test\\n```\\n\\nthat will check everything...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Building open-domain chatbots is a challenging area fo...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## BlenderbotSmallModel\\n\\n[[autodoc]] BlenderbotSmallModel\\n    - forward\\n\\n##...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--This tip is automatically generated by `make fix-copies`, do not fill manually!--\\u003e\\n\\n[BART](..\\u002fmo...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThe preprocessing function you want to create needs to:\\n\\n1. Prefix the input with a prompt so T...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import DataCollatorForSeq2Seq\\n\\n\\u003e\\u003e\\u003e data_collator = DataC...\"],[\"```\\n\\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule....\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`Seq2SeqTr...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nUse the [`~transformers.generation_utils.GenerationMixin.generate`] method to create the transl...\"],[\"```\\n\\nDecode the generated token ids back into text:\\n\\n```py\\n\\u003e\\u003e\\u003e tokenizer.decode(outputs[0], skip_spe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n| **Input**                                                                                    ...\"],[\"```\\n| **Input**                                                                                     ...\"],[\"```\\n| **Input**                                                                                     ...\"],[\"```\\n\\nTo use BigCode or OpenAssistant, start by logging in to have access to the Inference API:\\n\\n```p...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransfo...\"],[\"```\\n\\nHere, the model could interpret in two ways:\\n- Have the `text-to-image` generate a capybara swi...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransfo...\"],[\"This is using brand-new tools and not pipelines, because the agent writes better code with very atom...\"],[\"### A curated set of tools\\n\\nWe identify a set of tools that can empower such agents. Here is an upda...\"],[\"```\\n\\n### Custom tools\\n\\nWhile we identify a curated set of tools, we strongly believe that the main v...\"],[\"```\\n\\nthat you can then modify and execute yourself....\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [aps](https:\\u002f\\u002fhuggingface.co\\u002faps). The original code can be found [her...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale language models show promising text gene...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Causal languag...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\\n\\n- GEGLU ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe encoded versions have different lengths:\\n\\n```python\\n\\u003e\\u003e\\u003e len(encoded_sequence_a), len(encode...\"],[\"```\\n\\n### autoencoding models\\n\\nSee [encoder models](#encoder-models) and [masked language modeling](#...\"],[\"### convolution\\n\\nA type of layer in a neural network where the input matrix is multiplied element-wi...\"],[\"\\u003cYoutube id=\\\"d_ixlCubqQw\\\"\\u002f\\u003e\\n\\n### deep learning (DL)\\n\\nMachine learning algorithms which uses neural n...\"],[\"For models employing the function [`apply_chunking_to_forward`], the `chunk_size` defines the number...\"],[\"### input IDs\\n\\nThe input ids are often the only required parameters to be passed to the model as inp...\"],[\"```\\n\\nThe tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocab...\"],[\"```\\n\\nbecause this is the way a [`BertModel`] is going to expect its inputs.\\n\\n## L\\n\\n### labels\\n\\nThe l...\"],[\"- For sequence classification models, ([`BertForSequenceClassification`]), the model expects a tenso...\"],[\"- For automatic speech recognition models, ([`Wav2Vec2ForCTC`]), the model expects a tensor of dimen...\"],[\"Each model's labels may be different, so be sure to always check the documentation of each model for...\"],[\"### pixel values\\n\\nA tensor of the numerical representations of an image that is passed to a model. T...\"],[\"Speech and vision models have their own pretraining objectives. For example, Wav2Vec2 is a speech mo...\"],[\"An example of a semi-supervised learning approach is \\\"self-training\\\", in which a model is trained on...\"],[\"### token\\n\\nA part of a sentence, usually a word, but can also be a subword (non-common words are oft...\"],[\"```\\n\\nWe can use our tokenizer to automatically generate such a sentence by passing the two sequences...\"],[\"```\\n\\nThe first sequence, the \\\"context\\\" used for the question, has all its tokens represented by a `0...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Try out the inference widget here: https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224\\n\\nContent:\\n- [P...\"],[\"```\\n\\nğŸ‘€ See the results here: [nateraw\\u002fvit-base-beans](https:\\u002f\\u002fhuggingface.co\\u002fnateraw\\u002fvit-base-beans)...\"],[\"```\\n\\nInternally, the script will use the [`ImageFolder`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.0.0...\"],[\"# example 4: providing several splits\\ndataset = load_dataset(\\\"imagefolder\\\", data_files={\\\"train\\\": [\\\"p...\"],[\"```\\n\\n`ImageFolder` will create a `label` column, and the label name is based on the directory name.\\n...\"],[\"```\\n\\n## PyTorch version, no Trainer\\n\\nBased on the script [`run_image_classification_no_trainer.py`](...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nHere's a code snippet you can use to listen to the resulting audio in a notebook: \\n\\n```python\\n\\u003e...\"],[\"```\\n\\nor alternatively for AMD GPUs:\\n\\n```bash\\n!rocm-smi\\n```\\n\\n\\u003c\\u002fTip\\u003e\\n\\nWe encourage you to log in to yo...\"],[\"```\\n\\n### Text cleanup for SpeechT5 tokenization \\n\\nStart by cleaning up the text data. You'll need th...\"],[\"```\\n\\nThe dataset examples contain `raw_text` and `normalized_text` features. When deciding which fea...\"],[\"```\\n\\nNow you have two sets of characters: one with the vocabulary from the dataset and one with the ...\"],[\"```\\n\\nNow that you have dealt with special characters in the text, it's time to shift focus to the au...\"],[\"```\\n\\nLet's check how many speakers remain: \\n\\n```py\\n\\u003e\\u003e\\u003e len(set(dataset[\\\"speaker_id\\\"]))\\n42\\n```\\n\\nLet's...\"],[\"```\\n\\nYou are left with just under 10,000 examples from approximately 40 unique speakers, which shoul...\"],[\"\\u003e\\u003e\\u003e def create_speaker_embedding(waveform):\\n...     with torch.no_grad():\\n...         speaker_embedd...\"],[\"```\\n\\nIt's important to note that the `speechbrain\\u002fspkrec-xvect-voxceleb` model was trained on Englis...\"],[\"```\\n\\nSpeaker embeddings should be a 512-element vector:\\n\\n```py\\n\\u003e\\u003e\\u003e processed_example[\\\"speaker_embedd...\"],[\"```\\n\\nNext, create a basic train\\u002ftest split: \\n\\n```py\\n\\u003e\\u003e\\u003e dataset = dataset.train_test_split(test_size...\"],[\"```\\n\\n### Data collator\\n\\nIn order to combine multiple examples into a batch, you need to define a cus...\"],[\"...         # not used during fine-tuning\\n...         del batch[\\\"decoder_attention_mask\\\"]\\n\\n...      ...\"],[\"```\\n\\nIn SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other word...\"],[\"```\\n\\nThe `use_cache=True` option is incompatible with gradient checkpointing. Disable it for trainin...\"],[\"```\\n\\nAnd with that, you're ready to start training! Training will take several hours. Depending on y...\"],[\"```\\n\\nNow you can pass the text and speaker embeddings to the pipeline, and it will take care of the ...\"],[\"```\\n\\nCreate a spectrogram with your model: \\n\\n```py\\n\\u003e\\u003e\\u003e spectrogram = model.generate_speech(inputs[\\\"i...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised learning (SSL) is a long-standing go...\"],[\"## UniSpeechSatConfig\\n\\n[[autodoc]] UniSpeechSatConfig\\n\\n## UniSpeechSat specific outputs\\n\\n[[autodoc]]...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It achieves state-of-the-art on both SQA and WTQ, while having comparable performance to SOTA on Wik...\"],[\"In addition, the authors have further pre-trained TAPAS to recognize **table entailment**, by creati...\"],[\"- TAPAS is a model that uses relative position embeddings by default (restarting the position embedd...\"],[\"- TAPAS is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It ...\"],[\"## Usage: fine-tuning\\n\\nHere we explain how you can fine-tune [`TapasForQuestionAnswering`] on your o...\"],[\"To summarize:\\n\\n| **Task**                            | **Example dataset** | **Description**        ...\"],[\"\\u003e\\u003e\\u003e # or, the base sized model with WikiSQL configuration\\n\\u003e\\u003e\\u003e config = TapasConfig(\\\"google-base-fine...\"],[\"```\\n\\nOf course, you don't necessarily have to follow one of these three ways in which TAPAS was fine...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nInitializing a model with a pre-trained base and randomly initialized classification ...\"],[\"```\\n\\nOf course, you don't necessarily have to follow one of these three ways in which TAPAS was fine...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nWhat you can also do is start from an already fine-tuned checkpoint. ...\"],[\"- `id`: optional, id of the table-question pair, for bookkeeping purposes.\\n- `annotator`: optional, ...\"],[\"**STEP 3: Convert your data into tensors using TapasTokenizer**\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\nThird, give...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasTokenizer\\n\\u003e\\u003e\\u003e import pandas as pd\\n\\n\\u003e\\u003e\\u003e model_name = \\\"google\\u002f...\"],[\"```\\n\\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.a...\"],[\"...     def __len__(self):\\n...         return len(self.data)\\n\\n\\n\\u003e\\u003e\\u003e data = pd.read_csv(tsv_path, sep=...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nThird, given that you've prepared your data in this TSV\\u002fCSV format (and corresponding...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasTokenizer\\n\\u003e\\u003e\\u003e import pandas as pd\\n\\n\\u003e\\u003e\\u003e model_name = \\\"google\\u002f...\"],[\"```\\n\\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.a...\"],[\"\\u003e\\u003e\\u003e class TableDataset:\\n...     def __init__(self, data, tokenizer):\\n...         self.data = data\\n.....\"],[\"...     def __len__(self):\\n...         return len(self.data)\\n\\n\\n\\u003e\\u003e\\u003e data = pd.read_csv(tsv_path, sep=...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nNote that here, we encode each table-question pair independently. Thi...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasConfig, TapasForQuestionAnswering, AdamW\\n\\n\\u003e\\u003e\\u003e # this is the ...\"],[\"...         # zero the parameter gradients\\n...         optimizer.zero_grad()\\n\\n...         # forward ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nYou can then fine-tune [`TFTapasForQuestionAnswering`] as follows (shown here for the...\"],[\"...         # forward + backward + optimize\\n...         with tf.GradientTape() as tape:\\n...         ...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Usage: inference\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\nHere we explain how you ...\"],[\"\\u003e\\u003e\\u003e data = {\\\"Actors\\\": [\\\"Brad Pitt\\\", \\\"Leonardo Di Caprio\\\", \\\"George Clooney\\\"], \\\"Number of movies\\\": [\\\"8...\"],[\"\\u003e\\u003e\\u003e display(table)\\n\\u003e\\u003e\\u003e print(\\\"\\\")\\n\\u003e\\u003e\\u003e for query, answer, predicted_agg in zip(queries, answers, aggre...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nHere we explain how you can use [`TFTapasForQuestionAnswering`] for inference (i.e. m...\"],[\"\\u003e\\u003e\\u003e data = {\\\"Actors\\\": [\\\"Brad Pitt\\\", \\\"Leonardo Di Caprio\\\", \\\"George Clooney\\\"], \\\"Number of movies\\\": [\\\"8...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nIn case of a conversational set-up, then each table-question pair mus...\"],[\"## TFTapasModel\\n[[autodoc]] TFTapasModel\\n    - call\\n    \\n## TFTapasForMaskedLM\\n[[autodoc]] TFTapasFo...\"],[\"## Translating the Transformers documentation into your language\\n\\nAs part of our mission to democrat...\"],[\"```\\n\\nHere, `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https:...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmodel_doc\\u002fbig_bird), [CamemBE...\"],[\"[RoBERTa](..\\u002fmodel_doc\\u002froberta), [RoBERTa-PreLayerNorm](..\\u002fmodel_doc\\u002froberta-prelayernorm), [RoCBert...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThe preprocessing function you want to create needs to:\\n\\n1. Make four copies of the `sent1` fie...\"],[\"```\\n\\nğŸ¤— Transformers doesn't have a data collator for multiple choice, so you'll need to adapt the [`...\"],[\"...         batch = self.tokenizer.pad(\\n...             flattened_features,\\n...             padding=...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e from dataclasses import dataclass\\n\\u003e\\u003e\\u003e from transformers.tokenization_utils_...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Evaluate\\n\\nIncluding a metric during training is often helpful for ...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr...\"],[\"```\\n\\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nPass your inputs and labels to the model and return the `logits`:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Fine-tuning BERT on SQuAD1.0\\n\\nThe [`run_qa.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob...\"],[\"```\\n\\nTraining with the previously defined hyper-parameters yields the following results:\\n\\n```bash\\nf1...\"],[\"```\\n\\n### Fine-tuning T5 on SQuAD2.0\\n\\nThe [`run_seq2seq_qa.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransf...\"],[\"```\\n\\n## Accelerate-based scripts\\n\\nBased on the scripts `run_qa_no_trainer.py` and `run_qa_beam_searc...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [weiweishi](https:\\u002f\\u002fhuggingface.co\\u002fweiweishi).\\n\\n## Resources\\n\\n- [Text ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transfer of pre-trained representations improves sam...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Some preselected input tokens are also given global attention: for those few tokens, the attention m...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Types of Segmentation\\n\\nSemantic segmentation assigns a label or class to every single pixel ...\"],[\"```\\n\\nThe segmentation pipeline output includes a mask for every predicted class. \\n```bash\\n[{'score':...\"],[\"```\\n\\nTaking a look at the mask for the car class, we can see every car is classified with the same m...\"],[\"```\\nChecking out one of the car masks below.\\n\\n```python\\nresults[2][\\\"mask\\\"]\\n```\\n\\u003cdiv class=\\\"flex just...\"],[\"```\\nAs you can see below, we have more classes. We will later illustrate to see that every pixel is ...\"],[\"```\\n\\nLet's have a side by side comparison for all types of segmentation.\\n\\n\\u003cdiv class=\\\"flex justify-c...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\n### Load SceneParse150 dataset\\n\\nStart by loading a smaller...\"],[\"```\\n\\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_...\"],[\"```\\n\\n- `image`: a PIL image of the scene.\\n- `annotation`: a PIL image of the segmentation map, which...\"],[\"```\\n\\n#### Custom dataset\\n\\nYou could also create and use your own dataset if you prefer to train with...\"],[\"# step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal\\u002fnotebook...\"],[\"```\\n\\n2. an id2label dictionary mapping the class integers to their class names\\n\\n     ```py\\n     impo...\"],[\"```\\n\\nNow create two preprocessing functions to prepare the images and annotations for the model. The...\"],[\"```\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\nIt is common to apply some data augmentation...\"],[\"```\\n\\nNext, create two preprocessing functions to prepare batches of images and annotations for the m...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n### Evaluate\\n\\nIncluding a metric during training is often helpful for...\"],[\"```\\n\\nThen create a function to [`~evaluate.EvaluationModule.compute`] the metrics. Your predictions ...\"],[\"```\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\n\\n```py\\n\\u003e\\u003e\\u003e def compute_metrics(eval_pred):\\n...\"],[\"```\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nYour `compute_metrics` function is ready to go now, and you'll retur...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nThen, load SegFormer with [`TFAutoModelForSemanticSegmentation`] along with the label mappings,...\"],[\"```\\n\\nTo compute the accuracy from the predictions and push your model to the ğŸ¤— Hub, use [Keras callb...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nPass your input to the model and return the `logits`:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import TFAut...\"],[\"```\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nTo visualize the results, load the [dataset color palette](https:\\u002f\\u002fg...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Scene text recognition (STR) has been an active rese...\"],[\"\\u003csmall\\u003e MGP-STR architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2209.03592\\\"\\u003eoriginal pap...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\n\\u003e\\u003e\\u003e import request...\"],[\"```\\n\\n## MgpstrConfig\\n\\n[[autodoc]] MgpstrConfig\\n\\n## MgpstrTokenizer\\n\\n[[autodoc]] MgpstrTokenizer\\n    ...\"],[\"Zero-shot classifier distillation\\n\\nAuthor: @joeddav \\n\\nThis script provides a way to improve the spee...\"],[\"```\\n\\n`\\u003cunlabeled_data.txt\\u003e` should be a text file with a single unlabeled example per line. `\\u003cclass_...\"],[\"Any of the arguments in the ğŸ¤— Trainer's\\n[`TrainingArguments`](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fma...\"],[\"\\u003e\\u003e\\u003e zero_shot_classifier = pipeline(\\\"zero-shot-classification\\\", model=\\\"roberta-large-mnli\\\")\\n\\u003e\\u003e\\u003e zero...\"],[\"```\\n\\nUnfortunately, inference is slow since each of our 4 class names must be fed through the large ...\"],[\"```\\n\\nand even used trivially with a `TextClassificationPipeline`:\\n\\n```python\\n\\u003e\\u003e\\u003e distilled_classifie...\"],[\"```\\n\\n```python\\n%%time\\nfor _ in range(1000):\\n    distilled_classifier([sequence] * 16)\\n# runs in 10.3...\"],[\"!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apac...\"],[\"This model was contributed by [Younes Belkada](https:\\u002f\\u002fhuggingface.co\\u002fybelkada) and [Arthur Zucker](...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\u003e\\u003e\\u003e device = \\\"cuda\\\" # the...\"],[\"```\\n\\nTo use the raw checkpoints with HuggingFace you can use the `convert_mixtral_weights_to_hf.py` ...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is a expected speedup diagram that compares pure inference time be...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Structure of the prompt\\n\\nLet's take a closer look at how the prompt is structured to understand ...\"],[\"```\\n\\nTask: \\\"Identify the oldest person in the `document` and create an image showcasing the result a...\"],[\"```\\n\\nWe can see that the tool name is short and precise. The description includes two parts, the fir...\"],[\"```\\n\\n````\\n\\nThe pattern the model is prompted to repeat has three parts: The task statement, the agen...\"],[\"```\\n\\nHuman: I tried this code, it worked but didn't give me a good result. The question is in French...\"],[\"```\\nwhich the agent completes. Contrary to the `run` command, the `chat` command then appends the co...\"],[\"```\\n\\n```text\\n'This is a tool that creates an image according to a prompt, which is a text descriptio...\"],[\"```\\nreturns\\n```text\\n==Explanation from the agent== \\nI will use the following tools `image_generator`...\"],[\"```\\n\\n```text\\n==Explanation from the agent==\\nI will use the following tool: `image_generator` to gene...\"],[\"```\\ntemplate = \\\"\\\"\\\" [...] \\\"\\\"\\\"\\n\\nagent = HfAgent(url_endpoint=your_endpoint, chat_prompt_template=templ...\"],[\"```\\n\\nUpon adding custom tools to an agent, the tools' descriptions and names are automatically\\ninclu...\"],[\"```\\n\\nThe set of curated tools already has an `image_transformer` tool which is hereby replaced with ...\"],[\"```\\n\\n```text\\n==Explanation from the agent==\\nI will use the following tool: `image_transformer` to tr...\"],[\"```\\n\\nFor the task `text-classification`, this returns `'facebook\\u002fbart-large-mnli'`, for `translation...\"],[\"```\\n\\nWe now have our tool handy. Save it in a file and import it from your main script. Let's name t...\"],[\"```\\n\\nand generates the following audio.\\n\\n| **Audio**                                                ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nBeware when replacing tools with others! This will also adjust the agent's prompt. This ...\"],[\"```\\n\\nThe model adequately leverages the tool:\\n```text\\n==Explanation from the agent==\\nI will use the ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nHereby, _inference_ is defined by a single forward pass, and _training_ is defined by a singl...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments\\n...\"],[\"```\\n\\nAn instantiated benchmark object can then simply be run by calling `benchmark.run()`.\\n\\n```py\\n\\u003e\\u003e...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```bash\\npython examples\\u002ftensorflow\\u002fbenchmarking\\u002frun_benchmark_tf.py --help...\"],[\"```\\n\\nAn instantiated benchmark object can then simply be run by calling `benchmark.run()`.\\n\\n```py\\n\\u003e\\u003e...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nBy default, the _time_ and the _required memory_ for _inference_ are ...\"],[\"\\u003e\\u003e\\u003e benchmark = PyTorchBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\\n\\u003e\\u003e\\u003e benc...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments,...\"],[\"\\u003e\\u003e\\u003e benchmark = TensorFlowBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\\n\\u003e\\u003e\\u003e b...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nAgain, _inference time_ and _required memory_ for _inference_ are mea...\"],[\"With the new _benchmark_ tools, it is easier than ever to share your benchmark results with the comm...\"],[\"!--Copyright 2023 IBM and HuggingFace Inc. team. All rights reserved.\\n\\nLicensed under the Apache Lic...\"],[\"The abstract from the paper is the following:\\n\\n*TSMixer is a lightweight neural architecture exclusi...\"],[\"## Sample usage \\n```python\\n\\nfrom transformers import PatchTSMixerConfig, PatchTSMixerForPrediction\\nf...\"],[\"```\\n\\n## Usage tips\\n\\nThe model can also be used for time series classification and time series regres...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*GPT-2 is a large transformer-based language model wi...\"],[\"## Usage tips\\n\\n- GPT-2 is a model with absolute position embeddings so it's usually advised to pad t...\"],[\"- [`GPT2LMHeadModel`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgithub....\"],[\"## GPT2Config\\n\\n[[autodoc]] GPT2Config\\n\\n## GPT2Tokenizer\\n\\n[[autodoc]] GPT2Tokenizer\\n    - save_vocabu...\"],[\"## TFGPT2Tokenizer\\n\\n[[autodoc]] TFGPT2Tokenizer\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003cjax\\u003e\\n\\n## FlaxGPT2Model\\n\\n[[autodoc]] FlaxGPT2...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nActivate the virtual environment. On Linux and MacOs:\\n\\n```bash\\nsource .env\\u002fbin\\u002factivate\\n```\\nAct...\"],[\"```\\n\\n## Install from source\\n\\nInstall ğŸ¤— Transformers from source with the following command:\\n\\n```bash...\"],[\"```\\n\\nYour Python environment will find the `main` version of ğŸ¤— Transformers on the next run.\\n\\n## Ins...\"],[\"```\\n\\nThis script should run without hanging or waiting to timeout because it won't attempt to downlo...\"],[\"```\\n\\n    3. Now when you're offline, reload your files with [`PreTrainedModel.from_pretrained`] from...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e input_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\n\\n\\u003e\\u003e\\u003e gen_tokens = model.generate(\\n....\"],[\"```\\n\\n## Combining GPT-Neo and Flash Attention 2\\n\\nFirst, make sure to install the latest version of F...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is an expected speedup diagram that compares pure inference time b...\"],[\"## FlaxGPTNeoForCausalLM\\n\\n[[autodoc]] FlaxGPTNeoForCausalLM\\n    - __call__\\n\\n\\u003c\\u002fjax\\u003e\\n\\u003c\\u002fframeworkconten...\"],[\"DeeBERT: Early Exiting for *BERT\\n\\nThis is the code base for the paper [DeeBERT: Dynamic Early Exitin...\"],[\"```\\n@inproceedings{xin-etal-2020-deebert,\\n    title = \\\"{D}ee{BERT}: Dynamic Early Exiting for Accele...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nYou'll use ğŸ¤— Datasets to load a dataset from the Hugging Face Hub, ğŸ¤— Transformers to train your...\"],[\"```\\n\\nYou'll see that this dataset already comes with a training set containing 1000 images and a tes...\"],[\"```\\n\\nThe examples in the dataset have the following fields:\\n- `image_id`: the example image id\\n- `im...\"],[\"\\u003e\\u003e\\u003e for i in range(len(annotations[\\\"id\\\"])):\\n...     box = annotations[\\\"bbox\\\"][i]\\n...     class_idx =...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fi.imgur.com\\u002fTdaqPJO.png\\\" alt=\\\"CPPE-5 Im...\"],[\"```\\n\\n## Preprocess the data\\n\\nTo finetune a model, you must preprocess the data you plan to use to ma...\"],[\"```\\n\\nBefore passing the images to the `image_processor`, apply two preprocessing transformations to ...\"],[\"```\\n\\nThe `image_processor` expects the annotations to be in the following format: `{'image_id': int,...\"],[\"```\\n\\nNow you can combine the image and annotation transformations to use on a batch of examples:\\n\\n``...\"],[\"```\\n\\nApply this preprocessing function to the entire dataset using ğŸ¤— Datasets [`~datasets.Dataset.wi...\"],[\"[[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\\n          [ 1.3081,  1.3081,  1.3081,...\"],[\"[[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\\n          [ 1.4200,  1.4200,  1.4200,...\"],[\"```\\n\\nYou have successfully augmented the individual images and prepared their annotations. However, ...\"],[\"```\\n\\n## Training the DETR model\\nYou have done most of the heavy lifting in the previous sections, so...\"],[\"```\\n\\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, then configu...\"],[\"```\\n\\n## Evaluate\\nObject detection models are commonly evaluated with a set of \\u003ca href=\\\"https:\\u002f\\u002fcocod...\"],[\"...     return annotations\\n\\n\\n\\u003e\\u003e\\u003e # Save images and annotations into the files torchvision.datasets.C...\"],[\"...     with open(path_anno, \\\"w\\\") as file:\\n...         json.dump(output_json, file, ensure_ascii=Fal...\"],[\"```\\n\\nNext, prepare an instance of a `CocoDetection` class that can be used with `cocoevaluator`.\\n\\n``...\"],[\"```\\n\\nFinally, load the metrics and run the evaluation.\\n\\n```py\\n\\u003e\\u003e\\u003e import evaluate\\n\\u003e\\u003e\\u003e from tqdm impo...\"],[\"...         module.add(prediction=results, reference=labels)\\n...         del batch\\n\\n\\u003e\\u003e\\u003e results = mo...\"],[\"```\\nThese results can be further improved by adjusting the hyperparameters in [`~transformers.Traini...\"],[\"```\\n\\nYou can also manually replicate the results of the pipeline if you'd like:\\n\\n```py\\n\\u003e\\u003e\\u003e image_pro...\"],[\"```\\n\\nLet's plot the result:\\n```py\\n\\u003e\\u003e\\u003e draw = ImageDraw.Draw(image)\\n\\n\\u003e\\u003e\\u003e for score, label, box in zip...\"],[\"Fine-Tuning week of XLSR-Wav2Vec2 on 60 languages ğŸŒ\\n\\nWelcome to the fine-tuning week! The goal of th...\"],[\"**Please keep in mind:**\\nThe spirit of the fine-tuning week is to provide state-of-the-art speech re...\"],[\"## Table of Contents\\n\\n- [Organization of the fine tuning week](#organization-of-the-fine-tuning-week...\"],[\"## Organization of the fine tuning week\\n\\nThe week officially starts on 22.03.2021 and ends on 29.03....\"],[\"Two possible setups can be used to fine-tune Wav2Vec2. The easiest setup is to simply use [google co...\"],[\"**2.**: Next, head over to the official [Fine-Tune XLSR-Wav2Vec2 with ğŸ¤— Transformes](https:\\u002f\\u002fcolab.r...\"],[\"**5.**: It is time to start running the google colab! Make sure that you have selected \\\"GPU\\\" as your...\"],[\"When running the google colab make sure that you uncomment the cell corresponding to mounting your g...\"],[\"```\\n\\nand the line:\\n\\n```python\\n  output_dir=\\\"\\u002fcontent\\u002fgdrive\\u002fMyDrive\\u002fwav2vec2-large-xlsr-turkish-demo...\"],[\"```\\n\\nfurther below (which should already be uncommented).\\n\\nHaving finished the training you should f...\"],[\"1. To begin with, we should clone transformers localy and install all the required packages.\\n\\nFirst,...\"],[\"```\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n```\\n\\nSecond, head over to the `examp...\"],[\"```\\n$ pip install -r requirements.txt\\n```\\n\\n\\t**Note**: Installing the latest version of `torchaudio` ...\"],[\"```\\n\\n\\t**To lanuch fine-tuninig on multiple GPUs:**\\n\\t\\n\\t```bash\\n\\tpython -m torch.distributed.launch \\\\\\n...\"],[\"```\\n\\n\\tThe above command will launch the training on 4 GPUs. Use the `--nproc_per_node` option to spe...\"],[\"```\\n\\nThen and add the following files that fully define a XLSR-Wav2Vec2 checkpoint into the reposito...\"],[\"```\\n\\nThe next **very important** step is to create the model card. For people to use your fine-tuned...\"],[\"\\u003c======================Copy **raw** version from here=========================\\n---\\nlanguage: {lang_i...\"],[\"metrics:\\n       - name: Test WER\\n         type: wer\\n         value: {wer_result_on_test} #TODO (IMPO...\"],[\"# Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, *e.g.* French\\n\\nFin...\"],[\"resampler = torchaudio.transforms.Resample(48_000, 16_000)\\n\\n# Preprocessing the datasets.\\n# We need ...\"],[\"```\\n\\n\\n## Evaluation\\n\\nThe model can be evaluated as follows on the {language} test data of Common Voi...\"],[\"# Preprocessing the datasets.\\n# We need to read the aduio files as arrays\\ndef speech_file_to_array_f...\"],[\"```\\n\\n**Test Result**: XX.XX %  # TODO: write output of print here. IMPORTANT: Please remember to als...\"],[\"Second, the rules regarding the preprocessing are not that as straight-forward. It is allowed (and r...\"],[\"### How to effectively preprocess the data\\n\\n\\n### How to do efficiently load datasets with limited ra...\"],[\"- How was XLSR-Wav2Vec2 pretrained? -\\u003e Feature vectors were masked and had to be predicted by the mo...\"],[\"## FAQ\\n\\n- Can a participant fine-tune models for more than one language? \\nYes! A participant can fin...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Read more about it [in the release blogpost](https:\\u002f\\u002fwww.mosaicml.com\\u002fblog\\u002fmpt-7b)\\n\\n## Usage tips\\n\\n-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Open-vocabulary object detection has benefited great...\"],[\"## Usage example\\n\\nOWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditione...\"],[\"\\u003e\\u003e\\u003e url = \\\"http:\\u002f\\u002fimages.cocodataset.org\\u002fval2017\\u002f000000039769.jpg\\\"\\n\\u003e\\u003e\\u003e image = Image.open(requests.g...\"],[\"```\\n\\n## Resources\\n\\n- A demo notebook on using OWLv2 for zero- and one-shot (image-guided) object det...\"],[\"Flax\\u002fJAX community week ğŸ¤—\\n\\nWelcome to the Flax\\u002fJAX community week! The goal of this week is to make ...\"],[\"Don't forget to sign up [here](https:\\u002f\\u002fforms.gle\\u002ftVGPhjKXyEsSgUcs8)! \\n\\n## Table of Contents\\n\\n- [Orga...\"],[\"## Organization\\n\\nParticipants can propose ideas for an interesting NLP and\\u002for CV project. Teams of 3...\"],[\"## Important dates\\n\\n- **23.06.** Official announcement of the community week. Make sure to sign-up i...\"],[\"For issues with Flax\\u002fJAX, Transformers, Datasets or for questions that are specific to your project ...\"],[\"## Projects\\n\\nDuring the first week after the community week announcement, **23.06. - 30.06.**, teams...\"],[\"### How to form a team around a project\\n\\nYou can check out all existing projects ideas on the forum ...\"],[\"Once created, the team can start refining their project:\\n\\n- What is the goal of the project? *E.g.*,...\"],[\"## Tips on how to organize the project\\n\\nThis section gives you some tips on how to most efficiently ...\"],[\"To give an example, a well-defined project would be the following:\\n\\n- task: summarization\\n- model: [...\"],[\"It is recommended that the motivated and experienced team members take the lead in dividing the work...\"],[\"### Other tips\\n\\nHere is a collection of some more tips:\\n\\n- We strongly recommend to work as publicly...\"],[\"```\\n- Ask for help. If you are stuck, use the public Slack channel or the [forum](https:\\u002f\\u002fdiscuss.hu...\"],[\"```\\n\\nYou can activate your venv by running\\n\\n```bash\\nsource ~\\u002f\\u003cyour-venv-name\\u003e\\u002fbin\\u002factivate\\n```\\n\\nWe s...\"],[\"```\\n\\n4. Set up a flax environment by running the following command in a virtual environment:\\n\\n   ```...\"],[\"```\\n$ cd ~\\u002f\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets.git\\n$ cd datasets\\n$ pip install -e \\\"...\"],[\"```\\nlibtpu.so already in used by another process. Not attempting to load libtpu.so in this process.\\n...\"],[\"```\\n\\nand then:\\n\\n```\\n$ pip install \\\"jax[tpu]\\u003e=0.2.16\\\" -f https:\\u002f\\u002fstorage.googleapis.com\\u002fjax-releases\\u002f...\"],[\"```\\nJax should have been installed correctly nevertheless.\\n\\nTo verify that JAX was correctly install...\"],[\"```\\n$ cd ~\\u002f\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets.git\\n$ cd datasets\\n$ pip install -e \\\"...\"],[\"```\\n\\n## Quickstart flax and jax\\n\\n[JAX](https:\\u002f\\u002fjax.readthedocs.io\\u002fen\\u002flatest\\u002findex.html) is Autograd ...\"],[\"- [BART](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fsrc\\u002ftransformers\\u002fmodels\\u002fbart\\u002fmodeling...\"],[\"- [ViT](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fsrc\\u002ftransformers\\u002fmodels\\u002fvit\\u002fmodeling_f...\"],[\"You can find all available training scripts for JAX\\u002fFlax under the \\nofficial [flax example folder](h...\"],[\"### **Flax design philosophy in Transformers**\\n\\nThis section will explain how Flax models are implem...\"],[\"```\\n\\nInstantiating an object `model_pytorch` of the class `ModelPyTorch` would actually allocate mem...\"],[\"```\\n\\nIn a more abstract way, this can be represented as passing the word embeddings to the model fun...\"],[\"```\\n\\nAt first glance the linear layer class `flax.linen.Dense` looks very similar to PyTorch's `torc...\"],[\"```\\n\\nVisually, the forward pass would now be represented as passing all tensors required for the com...\"],[\"The `FlaxPreTrainedModel` is an abstract class that holds a Flax module, handles weights initializat...\"],[\"class MLPModule(nn.Module):\\n   config: MLPConfig\\n   dtype: jnp.dtype = jnp.float32\\n\\n   def setup(sel...\"],[\"```\\n\\nNow let's define the `FlaxPreTrainedModel` model class.\\n\\n```python\\nfrom transformers.modeling_f...\"],[\"```\\n\\nNow the `FlaxMLPModel` will have a similar interface as PyTorch or Tensorflow models and allows...\"],[\"Another significant difference between Flax and PyTorch models is that, we can pass the `labels` dir...\"],[\"model = FlaxRobertaModel.from_pretrained(\\\"julien-c\\u002fdummy-unknown\\\")\\n\\n@jax.jit\\ndef run_model(input_ids...\"],[\"```\\n\\nWe use `jax.jit` to compile the function to get maximum performance. Note that in the above exa...\"],[\"```\\n\\nAs explained above we don't compute the loss inside the model, but rather in the task-specific ...\"],[\"```\\n\\nFinally, let's run our training loop.\\n\\n```python\\n# train loop\\nfor i in range(10):\\n   params, op...\"],[\"```\\n\\nNote that, as JAX is backed by the [XLA](https:\\u002f\\u002fwww.tensorflow.org\\u002fxla) compiler any JAX\\u002fFlax ...\"],[\"Speaker        | Topic                           | Time                  |  Video |\\n|-------------|-...\"],[\"### Thursday, July 1st\\n- [Watch the talks on YouTube](https:\\u002f\\u002fwww.youtube.com\\u002fwatch?v=__eG63ZP_5g)\\n-...\"],[\"Speaker        | Topic                           | Time                  | Video |\\n|-------------|--...\"],[\"| Rohan Anil, Google Brain | Scalable Second Order Optimization for Deep Learning      | 7.00pm-7.30...\"],[\"### Friday, July 2nd\\n- [Watch the talks on YouTube](https:\\u002f\\u002fwww.youtube.com\\u002fwatch?v=ZCMOPkcTu3s)\\n- [...\"],[\"Speaker        | Topic                           | Time                  |  Video |\\n|-------------|-...\"],[\"| Siddhartha Kamalakara, Joanna Yoo & JoÃ£o G M AraÃºjo, Cohere | Training large scale language models...\"],[\"### Talks & Speakers\\n\\n#### Skye Wanderman-Milne, JAX developer, Google Brain\\n- Talk: Intro to JAX on...\"],[\"#### Pablo Castro, Staff Research Software Developer; Google Research, Brain Team\\n- Talk: Using Jax ...\"],[\"#### Sabrina J. Mielke, PhD student at The Johns Hopkins University & Part-time research intern at H...\"],[\"#### Mostafa Dehghani, Research Scientist, Google Brain\\n- Talk: Long Range Arena: Benchmarking Effic...\"],[\"#### Rohan Anil, Senior Staff Software Engineer, Google Research, Brain Team\\n- Talk: Scalable Second...\"],[\"#### Ben Wang, Independent AI Researcher, EleutherAI\\n- Talk: Multihost Training in Mesh Transformer ...\"],[\"#### Siddhartha Kamalakara, Joanna Yoo, JoÃ£o G M AraÃºjo, MLE at Cohere\\n- Talk: Training large scale ...\"],[\"Now let's explain in more detail how a project can be created on the hub. Having an officially defin...\"],[\"Great, now we have a project directory with integrated git version control and a public model page, ...\"],[\"```\\n\\nNext we can clone the repo:\\n\\n```bash\\n$ git clone https:\\u002f\\u002fhuggingface.co\\u002fflax-community\\u002froberta-...\"],[\"```\\n\\nCool! The file is now displayed on the model page under the [files tab](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"```\\n\\nThis creates and saves our tokenizer directly in the cloned repository.\\nFinally, we can start t...\"],[\"```\\n\\nSince the dataset is tiny this command should actually run in less than 5 minutes. Note that we...\"],[\"bytes_output = serialization.to_bytes(params)\\n\\nrepo = Repository(\\\"flax-model\\\", clone_from=\\\"flax-comm...\"],[\"```\\n\\n**Note**: Make sure to have `huggingface_hub \\u003e= 0.0.13` to make this command work.\\n\\nFor more in...\"],[\"```\\n\\n3. Let's also make sure the correct project is set in case your email is used for multiple gclo...\"],[\"```\\n\\t\\nThis should ssh you into the TPU VM!\\nNow you can follow the steps of the section [How to insta...\"],[\"**NLP**\\n* **Conversational:** To have the best conversations!. [Example](https:\\u002f\\u002fhuggingface.co\\u002fmicr...\"],[\"**Speech**\\n* **Audio to Audio:** For tasks such as audio source separation or speech enhancement. \\n*...\"],[\"```\\npip install huggingface_hub\\n```\\n\\nHere is an example downloading (and caching!) a specific file d...\"],[\"```\\n\\n\\nWe'll provide more examples on Streamlit demos next week. Stay tuned!\\n\\n### Using a Gradio demo...\"],[\"### Process\\n\\n* **July 17, 12h00 CEST**: TPU VM access closes.\\n* **July 19, 12h00 CEST**: Project com...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The cost of vision-and-language pre-training has bec...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"Text Summarization with Pretrained Encoders\\n\\nThis folder contains part of the code necessary to repr...\"],[\"```\\n\\nAnd move all the stories to the same folder. We will refer as `$DATA_PATH` the path to where yo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents a new vision Transformer, called...\"],[\"\\u003csmall\\u003e Swin Transformer architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.03334\\\"\\u003eori...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The Intel Team Authors and HuggingFace Inc. team. All rights reserved.\\n\\nLicensed u...\"],[\"The abstract from the paper is the following:\\n\\n*In this paper, we study the problem of temporal vide...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"def pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\\n    '''\\n    ...\"],[\"def decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\\n    '''\\n    Decod...\"],[\"decoder_kwargs = dict(\\n    container=av.open(file, metadata_errors=\\\"ignore\\\"),\\n    sampling_rate=1,\\n ...\"],[\"```\\n\\nTips:\\n\\n- This implementation of TVP uses [`BertTokenizer`] to generate text embeddings and Resn...\"],[\"# Adversarial evaluation of model performances\\n\\nHere is an example on evaluating a model using adver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Running on low resource devices\\n\\nThe model is pretty heavy (~40GB in half precision) so if you ju...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nRefer to [T5's documentation page](t5) for API reference, tips, code examples and notebo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"FlashAttention-2 is currently supported for the following architectures:\\n* [Bark](https:\\u002f\\u002fhuggingfac...\"],[\"* [OPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopt#transformers.OPTModel)\\n* [Phi](https:...\"],[\"You can request to add FlashAttention-2 support for another model by opening a GitHub Issue or Pull ...\"],[\"```\\n\\nWe strongly suggest referring to the detailed [installation instructions](https:\\u002f\\u002fgithub.com\\u002fDa...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFlashAttention-2 can only be used when the model's dtype is `fp16` or `bf16`. Make sure ...\"],[\"```\\n\\n### Expected speedups\\n\\nYou can benefit from considerable speedups for inference, especially for...\"],[\"For sequences with padding tokens (generating with padding tokens), you need to unpad\\u002fpad the input ...\"],[\"For now, Transformers supports SDPA inference and training for the following architectures:\\n* [Bart]...\"],[\"input_text = \\\"Hello my dog is cute and\\\"\\ninputs = tokenizer(input_text, return_tensors=\\\"pt\\\").to(\\\"cuda...\"],[\"```\\n\\nIf you see a bug with the traceback below, try using the nightly version of PyTorch which may h...\"],[\"```\\n\\n## BetterTransformer\\n\\n\\u003cTip warning={true}\\u003e\\n\\nSome BetterTransformer features are being upstreame...\"],[\"```\\n\\nYou can return the original Transformers model with the [`~PreTrainedModel.reverse_bettertransf...\"],[\"```\\n\\n### 8-bit\\n\\n\\u003cTip\\u003e\\n\\nIf you're curious and interested in learning more about the concepts underlyi...\"],[\"```\\n\\nTo load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFeel free to try running a 11 billion parameter [T5 model](https:\\u002f\\u002fcolab.research.google...\"],[\"ORT is supported by ğŸ¤— Optimum which can be used in ğŸ¤— Transformers. You'll need to use an [`~optimum....\"],[\"```\\n\\nNow you're free to use the model for inference:\\n\\n```py\\nfrom optimum.pipelines import pipeline\\nf...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cYoutube id=\\\"nhJxYji1aho\\\"\\u002f\\u003e\\n\\nA simple way of tokenizing this text is to split it by spaces, which wo...\"],[\"```\\n[\\\"Don't\\\", \\\"you\\\", \\\"love\\\", \\\"ğŸ¤—\\\", \\\"Transformers?\\\", \\\"We\\\", \\\"sure\\\", \\\"do.\\\"]\\n```\\n\\nThis is a sensible firs...\"],[\"```\\n\\nAs can be seen space and punctuation tokenization, as well as rule-based tokenization, is used ...\"],[\"## Subword tokenization\\n\\n\\u003cYoutube id=\\\"zHvTiHr506c\\\"\\u002f\\u003e\\n\\nSubword tokenization algorithms rely on the pr...\"],[\"```\\n\\nBecause we are considering the uncased model, the sentence was lowercased first. We can see tha...\"],[\"```\\n\\nWe'll get back to the meaning of those `\\\"â–\\\"` when we look at [SentencePiece](#sentencepiece). A...\"],[\"```\\n(\\\"hug\\\", 10), (\\\"pug\\\", 5), (\\\"pun\\\", 12), (\\\"bun\\\", 4), (\\\"hugs\\\", 5)\\n```\\n\\nConsequently, the base vocabu...\"],[\"```\\n\\nBPE then identifies the next most common symbol pair. It's `\\\"u\\\"` followed by `\\\"n\\\"`, which occur...\"],[\"```\\n\\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rule...\"],[\"\\u003ca id='wordpiece'\\u003e\\u003c\\u002fa\\u003e\\n\\n### WordPiece\\n\\nWordPiece is the subword tokenization algorithm used for [BER...\"],[\"At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) ov...\"],[\"```\\n[\\\"b\\\", \\\"g\\\", \\\"h\\\", \\\"n\\\", \\\"p\\\", \\\"s\\\", \\\"u\\\", \\\"ug\\\", \\\"un\\\", \\\"hug\\\"],...\"],[\"```\\n\\n`\\\"hugs\\\"` could be tokenized both as `[\\\"hug\\\", \\\"s\\\"]`, `[\\\"h\\\", \\\"ug\\\", \\\"s\\\"]` or `[\\\"h\\\", \\\"u\\\", \\\"g\\\", \\\"s\\\"]...\"],[\"The [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained language models like BERT and its varian...\"],[\"[[autodoc]] ConvBertTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"After such a [`VisionEncoderDecoderModel`] has been trained\\u002ffine-tuned, it can be saved\\u002floaded just ...\"],[\"```\\n\\n## Initialising `VisionEncoderDecoderModel` from a pretrained encoder and a pretrained decoder....\"],[\"```\\n\\n## Loading an existing `VisionEncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load f...\"],[\"```\\n\\n## Loading a PyTorch checkpoint into `TFVisionEncoderDecoderModel`.\\n\\n[`TFVisionEncoderDecoderMo...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"```\\n\\nThis model was contributed by [nielsr](https:\\u002f\\u002fgithub.com\\u002fnielsrogge). This model's TensorFlow ...\"],[\"Distil*\\n\\nAuthor: @VictorSanh\\n\\nThis folder contains the original code used to train Distil* as well a...\"],[\"**September 19, 2019 - Update:** We fixed bugs in the code and released an updated version of the we...\"],[\"We have applied the same method to other Transformer architectures and released the weights:\\n- GPT2:...\"],[\"| Model                     | Macro-score                    | CoLA | MNLI | MRPC | QNLI | QQP  | RT...\"],[\"| DistilRoBERTa\\u003csup\\u003e1\\u003c\\u002fsup\\u003e |  **79.0**\\u002f**82.3**\\u003csup\\u003e2\\u003c\\u002fsup\\u003e | 59.3 | 84.0 | 86.6 | 90.8 | 89.4 | 67...\"],[\"\\u003csup\\u003e1\\u003c\\u002fsup\\u003e We did not use the MNLI checkpoint for fine-tuning but directly perform transfer learni...\"],[\"- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to p...\"],[\"- `distilgpt2`: DistilGPT2 English language model pretrained with the supervision of `gpt2` (the sma...\"],[\"Using DistilBERT is very similar to using BERT. DistilBERT share the same tokenizer as BERT's `bert-...\"],[\"```\\n\\nSimilarly, using the other Distil* models simply consists in calling the base classes with a di...\"],[\"```\\n\\n### B. Training\\n\\nTraining with distillation is really simple once you have pre-processed the da...\"],[\"```\\n\\nBy default, this will launch a training on a single GPU (even if more are available on the clus...\"],[\"```\\n\\n**Tips:** Starting distilled training with good initialization of the model weights is crucial ...\"],[\"How to propose a Flax\\u002fJAX + Transformers project \\n\\nGreat that you've opened this document! \\nWhile we...\"],[\"Second, make sure that your project idea doesn't already exist by checking [existing projects](https...\"],[\"1. *A clear description of the project*\\n2. *In which language should the project be conducted?* Engl...\"],[\"4. *What data should be used?* It is important to state at least what kind of data you would like to...\"],[\"Feel free to copy-paste the following format for your project proposal and fill out the respective s...\"],[\"```\\n# \\u003cFILL ME: Name of project\\u003e\\n\\n\\u003cFILL ME: A clear description of the project\\u003e\\n\\n## 2. Language\\n\\nThe...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- We have released a series of models [here](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=mvp...\"],[\"```\\n\\nFor data-to-text generation, it is an example to use MVP and multi-task pre-trained variants.\\n`...\"],[\"```\\n\\nFor lightweight tuning, *i.e.*, fixing the model and only tuning prompts, you can load MVP with...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Question ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been o...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Question answe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nTake a look at the example again:\\n\\n```py\\n\\u003e\\u003e\\u003e minds[\\\"train\\\"][0]\\n{'audio': {'array': array([-0.00...\"],[\"```\\n\\nThe MInDS-14 dataset has a sampling rate of 8000kHz (you can find this information in its [data...\"],[\"```\\n\\nAs you can see in the `transcription` above, the text contains a mix of upper and lowercase cha...\"],[\"```\\n\\nğŸ¤— Transformers doesn't have a data collator for ASR, so you'll need to adapt the [`DataCollator...\"],[\"```\\n\\nNow instantiate your `DataCollatorForCTCWithPadding`:\\n\\n```py\\n\\u003e\\u003e\\u003e data_collator = DataCollatorCT...\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThe transcription is decent, but it could be better! Try finetuning your model on more e...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\nRoFormer is a BERT-like autoencoding model with rotary position embeddings. Rotary pos...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFRoFormerModel\\n\\n[[autodoc]] TFRoFormerModel\\n    - call\\n\\n## TFRoFormerForMaskedLM\\n\\n[[...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fpytorch\\u002ffairseq\\u002ftree\\u002fmaster\\u002ffairseq\\u002fmodels\\u002f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Program synthesis strives to generate a computer pro...\"],[\"## Checkpoint Naming\\n\\n* CodeGen model [checkpoints](https:\\u002f\\u002fhuggingface.co\\u002fmodels?other=codegen) are...\"],[\"```\\n\\n## Resources\\n\\n- [Causal language modeling task guide](..\\u002ftasks\\u002flanguage_modeling)\\n\\n## CodeGenCo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nAre you unsure whether the model you wish to use already has a corresponding TensorFlow archi...\"],[\"### 1.-3. Prepare your model contribution\\n\\n**1. Select the model you wish to convert**\\n\\nLet's start ...\"],[\"```\\n\\n3. Set up a development environment, for instance by running the following command:\\n\\n```bash\\npy...\"],[\"```\\n\\n8. Once you are satisfied, go to the webpage of your fork on GitHub. Click on â€œPull requestâ€. M...\"],[\"### 4. Model implementation\\n\\nNow it's time to finally start coding. Our suggested starting point is ...\"],[\"Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow...\"],[\"Double-check the documentation!\\n- PyTorch's `nn.Parameter` variables typically need to be initialize...\"],[\"- Keras models need to be built in order to load pretrained weights. For that reason, `TFBrandNewBer...\"],[\"In addition to the model file itself, you will also need to add the pointers to the model classes an...\"],[\"### 5. Add model tests\\n\\nHurray, you've implemented a TensorFlow model! Now it's time to add tests to...\"],[\"```\\n\\nThe most likely outcome is that you'll see a bunch of errors. Don't worry, this is expected! De...\"],[\"```\\n\\nand we will merge your PR! Congratulations on the milestone ğŸ‰\\n\\n**7. (Optional) Build demos and ...\"],[\"First of all, let's talk about why understanding these mismatches matters. Many community members wi...\"],[\"Image Captioning (vision-encoder-text-decoder model) training example\\n\\nThe following example showcas...\"],[\"```\\n\\n### Create a model from a vision encoder model and a text decoder model\\nNext, we create a [Flax...\"],[\"```\\n\\n### Train the model\\nFinally, we can run the example script to train the model:\\n\\n```bash\\npython3...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003csmall\\u003e SimMIM framework. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2111.09886\\\"\\u003eoriginal paper\\u003c\\u002f...\"],[\"```\\n\\nHere, we train for 100 epochs with a learning rate of 2e-5. Note that the SimMIM authors used a...\"],[\"```\\n\\nNext, we can run the script by providing the path to this custom configuration (replace `path_t...\"],[\"```\\n\\n## MAE\\n\\nThe `run_mae.py` script can be used to pre-train a Vision Transformer as a masked autoe...\"],[\"One can use the following command to pre-train a `ViTMAEForPreTraining` model from scratch on the [c...\"],[\"```\\n\\nHere we set:\\n- `mask_ratio` to 0.75 (to mask 75% of the patches for each image)\\n- `norm_pix_los...\"],[\"```\\n\\nNote that you can put images in dummy subfolders, whose names will be ignored by default (as la...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [sijunhe](https:\\u002f\\u002fhuggingface.co\\u002fsijunhe). The original code can be fo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In deep learning, models typically reuse the same pa...\"],[\"## SwitchTransformersConfig\\n\\n[[autodoc]] SwitchTransformersConfig\\n\\n## SwitchTransformersTop1Router\\n\\n...\"],[\"Simple VQGAN CLIP\\n\\nAuthor: @ErwannMillon \\n\\nThis is a very simple VQGAN-CLIP implementation that was ...\"],[\"```\\nfrom VQGAN_CLIP import VQGAN_CLIP\\nvqgan_clip = VQGAN_CLIP()\\nvqgan_clip.generate(\\\"a picture of a ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [stancld](https:\\u002f\\u002fhuggingface.co\\u002fstancld).\\nThe original code can be fo...\"],[\"## Usage tips\\n\\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`...\"],[\"```python\\n\\u003e\\u003e\\u003e import evaluate\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e from transformers import Aut...\"],[\"```\\n\\n\\n## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](.....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [novice03](https:\\u002f\\u002fhuggingface.co\\u002fnovice03).\\nThe original code can be ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note tha...\"],[\"```\\n\\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two f...\"],[\"```\\n\\nThe main advantage of doing this for big models is that during step 2 of the workflow shown abo...\"],[\"```\\n\\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrain...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- The checkpoints are named **mobilenet\\\\_v1\\\\_*depth*\\\\_*size***, for example **mobilen...\"],[\"- The original TensorFlow checkpoints include quantized models. We do not support these models as th...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"[[autodoc]] BloomForCausalLM\\n    - forward\\n\\n## BloomForSequenceClassification\\n\\n[[autodoc]] BloomForS...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Language model pre-training has been shown to captur...\"],[\"## RealmEmbedder\\n\\n[[autodoc]] RealmEmbedder\\n    - forward\\n\\n## RealmScorer\\n\\n[[autodoc]] RealmScorer\\n ...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003eJAX, PyTorch à°®à°°à°¿à°¯à± TensorFlow à°•à±‹à°¸à°‚ à°…à°¤à±à°¯à°¾à°§à±à°¨à°¿à°• à°¯à°‚à°¤à±à°° à°…à°­à±à°¯à°¾à°¸à°‚\\u003c\\u002fp\\u003e\\n\\u003c\\u002fh3\\u003e\\n\\n\\u003ch...\"],[\"à°¸à°¹à°œ à°­à°¾à°·à°¾ à°ªà±à°°à°¾à°¸à±†à°¸à°¿à°‚à°—à±â€Œà°²à±‹:\\n- [BERT à°¤à±‹ à°®à°¾à°¸à±à°•à±â€Œà°¡à± à°µà°°à±à°¡à± à°•à°‚à°ªà±à°²à±€à°·à°¨à±](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-unca...\"],[\"- [RoBERTa à°¤à±‹ à°¸à°¹à°œ à°­à°¾à°·à°¾ à°…à°¨à±à°®à°¿à°¤à°¿](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+Lost.+Nob...\"],[\"- [DistilBERT à°¤à±‹ à°ªà±à°°à°¶à±à°¨...\"],[\"à°¸à°®à°¾à°§à°¾à°¨à°‚](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used...\"],[\"st+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+squar...\"],[\"ts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments...\"],[\"- [T5 à°¤à±‹ à°…à°¨à±à°µà°¾à°¦à°‚](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"à°•à°‚à°ªà±à°¯à±‚à°Ÿà°°à± à°¦à±ƒà°·à±à°Ÿà°¿à°²à±‹:\\n- [VIT à°¤à±‹ à°šà°¿à°¤à±à°° à°µà°°à±à°—à±€à°•à°°à°£](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224)\\n- ...\"],[\"à°®à°²à±à°Ÿà±€à°®à±‹à°¡à°²à± à°Ÿà°¾à°¸à±à°•à±â€Œà°²à°²à±‹:\\n- [TAPAS à°¤à±‹ à°Ÿà±‡à°¬à±à°²à± à°ªà±à°°à°¶à±à°¨ à°¸à°®à°¾à°§à°¾à°¨à°¾à°²à±](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002ftapas-base...\"],[\"```\\n\\nà°°à±†à°‚à°¡à°µ à°²à±ˆà°¨à± à°•à±‹à°¡à± à°¡à±Œà°¨à±â€Œà°²à±‹à°¡à± à°®à°°à°¿à°¯à± à°ªà±ˆà°ªà±â€Œà°²à±ˆà°¨à± à°‰à°ªà°¯à±‹à°—à°¿à°‚à°šà±‡ à°ªà±à°°à±€à°Ÿà±à°°à±ˆà°¨à±à°¡à± à°®à±‹à°¡à°²à±â€Œà°¨à± à°•à°¾à°·à± à°šà±‡à°¸à±à°¤à±à°‚à°¦à°¿, à°®à±‚à°¡à°µà°¦...\"],[\"```\\n\\nà°‡à°•à±à°•à°¡ à°®à°¨à°‚ à°†à°¬à±à°œà±†à°•à±à°Ÿà± à°šà±à°Ÿà±à°Ÿà±‚ à°‰à°¨à±à°¨ à°¬à°¾à°•à±à°¸à± à°®à°°à°¿à°¯à± à°•à°¾à°¨à±à°«à°¿à°¡à±†à°¨à±à°¸à± à°¸à±à°•à±‹à°°à±â€Œà°¤à±‹ à°šà°¿à°¤à±à°°à°‚à°²à±‹ à°—à±à°°à±à°¤à°¿à°‚à°šà°¬à°¡à°¿à°¨ à°µà°¸à±à°¤à±...\"],[\"```\\n\\nà°ªà±à°°à°¿à°Ÿà±à°°à±ˆà°¨à±à°¡à± à°®à±‹à°¡à°²à± à°†à°¶à°¿à°‚à°šà±‡ à°…à°¨à±à°¨à°¿ à°ªà±à°°à±€à°ªà±à°°à°¾à°¸à±†à°¸à°¿à°‚à°—à±â€Œà°²à°•à± à°Ÿà±‹à°•à±†à°¨à±ˆà°œà°°à± à°¬à°¾à°§à±à°¯à°¤ à°µà°¹à°¿à°¸à±à°¤à±à°‚à°¦à°¿ à°®à°°à°¿à°¯à± à°¨à±‡à°°à±à°—à°¾ à°’à°•...\"],[\"4. à°®à±€ à°…à°µà°¸à°°à°¾à°²à°•à± à°…à°¨à±à°—à±à°£à°‚à°—à°¾ à°®à±‹à°¡à°²à± à°²à±‡à°¦à°¾ à°‰à°¦à°¾à°¹à°°à°£à°¨à± à°¸à±à°²à°­à°‚à°—à°¾ à°…à°¨à±à°•à±‚à°²à±€à°•à°°à°¿à°‚à°šà°‚à°¡à°¿:\\n    - à°ªà±à°°à°¤à°¿ à°†à°°à±à°•à°¿à°Ÿà±†à°•à±à°šà°°à± à°¦à°¾à°¨à°¿ ...\"],[\"```\\n\\nà°®à±€à°°à± à°‰à°¦à°¾à°¹à°°à°£à°²à°¤à±‹ à°ªà±à°²à±‡ à°šà±‡à°¯à°¾à°²à°¨à±à°•à±à°‚à°Ÿà±‡ à°²à±‡à°¦à°¾ à°•à±‹à°¡à± à°¯à±Šà°•à±à°• à°¬à±à°²à±€à°¡à°¿à°‚à°—à± à°à°¡à±à°œà± à°…à°µà°¸à°°à°‚ à°®à°°à°¿à°¯à± à°•à±Šà°¤à±à°¤ à°µà°¿à°¡à±à°¦à°² à°•à±‹à°¸à°‚ ...\"],[\"```\\n\\nFlax, PyTorch à°²à±‡à°¦à°¾ TensorFlow à°¯à±Šà°•à±à°• à°‡à°¨à±â€Œà°¸à±à°Ÿà°¾à°²à±‡à°·à°¨à± à°ªà±‡à°œà±€à°²à°¨à± à°•à±Šà°‚à°¡à°¾à°¤à±‹ à°à°²à°¾ à°‡à°¨à±â€Œà°¸à±à°Ÿà°¾à°²à± à°šà±‡à°¯à°¾à°²à±‹ à°šà±‚à°¡à°Ÿà°¾à°¨à°¿...\"],[\"1. **[OPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmaster\\u002fmodel_doc\\u002fopt)** (from Meta AI) released ...\"],[\"1. **[PEGASUS-X](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus_x)** (from Google) relea...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (from Bo Peng), released on [...\"],[\"1. **[SEW](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew)** (from ASAPP) released with the ...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin)** (from Microsoft) ...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (from Microsoft Research) r...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (from Microsoft), released ...\"],[\"1. **[UniSpeech](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech)** (from Microsoft Res...\"],[\"1. **[VideoMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvideomae)** (from Multimedia Comp...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (from the University of Wisco...\"],[\"à°ªà±à°°à°¤à°¿ à°®à±‹à°¡à°²à± à°«à±à°²à°¾à°•à±à°¸à±, à°ªà±ˆà°Ÿà°¾à°°à±à°šà± à°²à±‡à°¦à°¾ à°Ÿà±†à°¨à±à°¸à°°à±â€Œà°«à±à°²à±‹à°²à±‹ à°…à°®à°²à± à°šà±‡à°¯à°¬à°¡à°¿à°‚à°¦à°¾ à°²à±‡à°¦à°¾ ğŸ¤— Tokenizers à°²à±ˆà°¬à±à°°à°°à±€ à°¦à±à°µà°¾à°°à°¾ à°…...\"],[\"## à°…à°¨à±à°²à±‡à°–à°¨à°‚\\n\\nğŸ¤— à°Ÿà±à°°à°¾à°¨à±à°¸à±â€Œà°«à°¾à°°à±à°®à°°à±à°¸à± à°²à±ˆà°¬à±à°°à°°à±€ à°•à±‹à°¸à°‚ à°®à±€à°°à± à°‰à°¦à°¹à°°à°¿à°‚à°šà°—à°² [à°ªà±‡à°ªà°°à±](https:\\u002f\\u002fwww.aclweb.org\\u002fantholo...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Most widely-used pre-trained language models operate...\"],[\"## Usage example\\n\\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\\n\\n```python\\n\\u003e...\"],[\"```\\n\\nFor batched inference and training it is however recommended to make use of the tokenizer:\\n\\n```...\"],[\"```\\n\\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \\nsince the mode...\"],[\"\\u003e\\u003e\\u003e # ByT5 produces only one char at a time so we need to produce many more output characters here -...\"],[\"\\u003e\\u003e\\u003e output_ids_list.append(output_ids[start_token:])\\n\\u003e\\u003e\\u003e output_string = tokenizer.batch_decode(outp...\"],[\"```\\n\\n\\n## ByT5Tokenizer\\n\\n[[autodoc]] ByT5Tokenizer\\n\\nSee [`ByT5Tokenizer`] for all details....\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoModel, AutoTokenizer\\n\\n\\u003e\\u003e\\u003e bartpho = Auto...\"],[\"```\\n\\n## Usage tips\\n\\n- Following mBART, BARTpho uses the \\\"large\\\" architecture of BART with an additio...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ğŸ¤— Transformers æä¾›äº†æ•¸ä»¥åƒè¨ˆçš„é è¨“ç·´æ¨¡å‹ï¼Œæ”¯æ´ 100 å¤šç¨®èªè¨€çš„æ–‡æœ¬åˆ†é¡ã€è³‡è¨Šæ“·å–ã€å•ç­”ã€æ‘˜è¦ã€ç¿»è­¯ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®“æœ€å…ˆé€²çš„ NLP æŠ€è¡“äººäººæ˜“ç”¨ã€‚\\n\\nğŸ¤— Transform...\"],[\"é€™è£¡æ˜¯ä¸€äº›ç¯„ä¾‹ï¼š\\n- [ç”¨ BERT åšé®è“‹å¡«è©](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F...\"],[\"- [ç”¨ RoBERTa åšè‡ªç„¶èªè¨€æ¨è«–](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+a...\"],[\"åšå•ç­”](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+...\"],[\"- [ç”¨ T5 åšç¿»è­¯](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**ï¼Œç”± Hugging Face åœ˜éšŠæ‰€æ‰“é€ ï¼Œæ˜¯ä¸€å€‹æ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ dem...\"],[\"```\\n\\nç¬¬äºŒè¡Œç¨‹å¼ç¢¼ä¸‹è¼‰ä¸¦å¿«å– pipeline ä½¿ç”¨çš„é è¨“ç·´æ¨¡å‹ï¼Œè€Œç¬¬ä¸‰è¡Œç¨‹å¼ç¢¼å‰‡åœ¨çµ¦å®šçš„æ–‡æœ¬ä¸Šé€²è¡Œäº†è©•ä¼°ã€‚é€™è£¡çš„ç­”æ¡ˆâ€œæ­£é¢â€ (positive) å…·æœ‰ 99.97% çš„ä¿¡è³´åº¦ã€‚\\n\\nè¨±å¤šçš„ NL...\"],[\"```\\né€™è£¡æ˜¯å°æ‡‰çš„ TensorFlow ç¨‹å¼ç¢¼ï¼š\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, TFAutoModel\\n\\n\\u003e\\u003e\\u003e to...\"],[\"```\\n\\nTokenizer ç‚ºæ‰€æœ‰çš„é è¨“ç·´æ¨¡å‹æä¾›äº†é è™•ç†ï¼Œä¸¦å¯ä»¥ç›´æ¥è½‰æ›å–®ä¸€å­—ä¸²ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–ä¸²åˆ— (list)ã€‚å®ƒæœƒè¼¸å‡ºä¸€å€‹çš„å­—å…¸ (dict) è®“ä½ å¯ä»¥åœ¨ä¸‹æ¸¸ç¨‹å¼ç¢¼è£¡ä½¿ç”¨æˆ–ç›´æ¥è—‰ç”± `*...\"],[\"1. å°æ–¼æ¨¡å‹ç”Ÿå‘½é€±æœŸçš„æ¯ä¸€å€‹éƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š\\n    - è¨“ç·´å…ˆé€²çš„æ¨¡å‹ï¼Œåªéœ€ 3 è¡Œç¨‹å¼ç¢¼\\n    - æ¨¡å‹å¯ä»¥åœ¨ä¸åŒæ·±åº¦å­¸ç¿’æ¡†æ¶ä¹‹é–“ä»»æ„è½‰æ›\\n    - ç‚ºè¨“ç·´ã€è©•ä¼°å’Œç”Ÿç”¢é¸æ“‡æœ€é©åˆçš„æ¡†æ¶ï¼Œä¸¦å®Œ...\"],[\"## å®‰è£\\n\\n### ä½¿ç”¨ pip\\n\\né€™å€‹ Repository å·²åœ¨ Python 3.8+ã€Flax 0.4.1+ã€PyTorch 1.10+ å’Œ TensorFlow 2.6+ ä¸‹ç¶“éæ¸¬è©¦ã€‚\\n\\n...\"],[\"```\\n\\nå¦‚æœä½ æƒ³è¦è©¦è©¦ç¯„ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼ç™¼å¸ƒå‰ä½¿ç”¨æœ€æ–°é–‹ç™¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œä½ å¿…é ˆ[å¾åŸå§‹ç¢¼å®‰è£](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002finstallation...\"],[\"```\\n\\nè¦è—‰ç”± conda å®‰è£ Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè«‹åƒé–±å®ƒå€‘å„è‡ªå®‰è£é é¢çš„èªªæ˜ã€‚\\n\\n## æ¨¡å‹æ¶æ§‹\\n\\n**ğŸ¤— Transformers æ”¯æ´çš„[æ‰€æœ‰çš„æ¨¡...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (from OpenAI) released with ...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (from BigCode) r...\"],[\"1. **[MarkupLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarkuplm)** (from Microsoft Resea...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (from Google I...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[SeamlessM4T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t)** (from Meta AI)...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (from Microsoft) released w...\"],[\"1. **[UMT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fumt5)** (from Google Research) releas...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[VipLlava](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvipllava)** (from University of W...\"],[\"1. **[XLM-ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-prophetnet)** (from Mic...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (from the University of Wisco...\"],[\"è¦æª¢æŸ¥æŸå€‹æ¨¡å‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å¯¦ä½œï¼Œæˆ–å…¶æ˜¯å¦åœ¨ğŸ¤— Tokenizers å‡½å¼åº«ä¸­æœ‰å°æ‡‰çš„ tokenizerï¼Œæ•¬è«‹åƒé–±[æ­¤è¡¨](https:\\u002f\\u002fhugg...\"],[\"## å¼•ç”¨\\n\\næˆ‘å€‘å·²å°‡æ­¤å‡½å¼åº«çš„[è«–æ–‡](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)æ­£å¼ç™¼è¡¨ã€‚å¦‚æœä½ ä½¿ç”¨äº† ğŸ¤— Transformers...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training techniques have been verified successfu...\"],[\"```python\\ndef normalize_bbox(bbox, width, height):\\n    return [\\n        int(1000 * (bbox[0] \\u002f width)...\"],[\"```\\n\\nHere, `width` and `height` correspond to the width and height of the original document in which...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"**Other resources**\\n- [Masked language modeling task guide](..\\u002ftasks\\u002fmasked_language_modeling)\\n\\nğŸš€ De...\"],[\"Wav2Vec2 Contrastive Loss PreTraining examples\\n\\nThe following example showcases how to pretrain a wa...\"],[\"```\\ncd wav2vec2-base-robust\\ngit lfs track \\\"*tfevents*\\\"\\n```\\n\\nGreat, we have set up our model reposito...\"],[\"```\\n\\n### Create a feature extractor configuration\\n\\nBefore we can start the training, we need to defi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training video transformers on extra large-scale...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fDialoGPT).\\n\\n## Usage tips\\n\\n- Dial...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Humans read and write hundreds of billions of messag...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## OpenLlamaConfig\\n\\n[[autodoc]] OpenLlamaConfig\\n\\n## OpenLlamaModel\\n\\n[[autodoc]] OpenLlamaModel\\n    -...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### RemoteTool\\n\\n[[autodoc]] RemoteTool\\n\\n### launch_gradio_demo\\n\\n[[autodoc]] launch_gradio_demo\\n\\n## A...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Let's install the libraries needed for distillation and evaluating the process. \\n\\n```bash\\npip instal...\"],[\"```\\n\\nIn this example, we are using the `merve\\u002fbeans-vit-224` model as teacher model. It's an image c...\"],[\"```\\n\\nEssentially, we want the student model (a randomly initialized MobileNet) to mimic the teacher ...\"],[\"with torch.no_grad():\\n          teacher_output = self.teacher(**inputs)\\n\\n        # Compute soft targ...\"],[\"```\\n\\nWe will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through ...\"],[\"```\\n\\nWe can use `compute_metrics` function to evaluate our model on the test set. This function will...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Connectionist Temporal Classification\\n\\nThe script [`run_speech_recognition_ctc.py`](https:\\u002f\\u002fgithu...\"],[\"```\\n\\nIf the environment variable is not set, the training script might freeze, *i.e.* see: https:\\u002f\\u002fg...\"],[\"```\\n\\nOn a single V100 GPU, this script should run in *ca.* 1 hour 20 minutes and yield a CTC loss of...\"],[\"```\\n\\nOn 8 V100 GPUs, this script should run in *ca.* 18 minutes and yield a CTC loss of **0.39** and...\"],[\"```bash\\n**torchrun \\\\\\n\\t--nproc_per_node 4 run_speech_recognition_ctc_streaming.py \\\\\\n\\t--dataset_name=\\\"...\"],[\"```\\n\\nOn 4 V100 GPUs, this script should run in *ca.* 3h 31min and yield a CTC loss of **0.35** and w...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [TIMIT](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftimit_asr)| -  | [unispeech-large-1500h-cv](https:\\u002f\\u002fhuggin...\"],[\"#### Librispeech CTC\\n\\n- [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)| `\\\"clean\\\"` - `\\\"train.100\\\"` |  [micr...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)| `\\\"clean\\\"` - `\\\"train.100\\\"` |  [face...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)| `\\\"clean\\\"` - `\\\"train.100\\\"` |  [asap...\"],[\"#### Common Voice CTC\\n\\n- [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmozilla-foundation\\u002fcommon_voice_3_0)| `\\\"it\\\"`  | [fa...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"`  | [facebook\\u002fwav2vec2-large-x...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"`  | [facebook\\u002fwav2vec2-large-x...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"`  | [facebook\\u002fwav2vec2-xls-r-1...\"],[\"#### Multilingual Librispeech CTC\\n\\n- [Multilingual Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmult...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Multilingual Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmultilingual_librispeech)| `\\\"german\\\"`  ...\"],[\"## Connectionist Temporal Classification With Adapters\\n\\nThe script [`run_speech_recognition_ctc_adap...\"],[\"#### Common Voice CTC Adapter\\n\\nAs in the examples [above](#examples-ctc), we fine-tune on Common Voi...\"],[\"```\\nhuggingface-cli login\\n```\\n\\nNow, let's run an example and upload it to the Hub under `wav2vec2-co...\"],[\"```\\n\\nThis should take less than 10 minutes on most GPUs and you should very quickly get word error r...\"],[\"```sh\\npython run_speech_recognition_ctc.py \\\\\\n\\t--dataset_name=\\\"common_voice\\\" \\\\\\n\\t--model_name_or_path=...\"],[\"```\\n\\nNow you should have both `adapter.tur.safetensors` and `adapter.swe.safetensors` in the model r...\"],[\"```\\nrespectively.\\n\\n## Sequence to Sequence\\n\\nThe script [`run_speech_recognition_seq2seq.py`](https:\\u002f...\"],[\"#### Single GPU Whisper Training\\nThe following example shows how to fine-tune the [Whisper small](ht...\"],[\"```\\nOn a single V100, training should take approximately 8 hours, with a final cross-entropy loss of...\"],[\"#### Multi GPU Whisper Training\\nThe following example shows how to fine-tune the [Whisper small](htt...\"],[\"```\\nOn two V100s, training should take approximately 4 hours, with a final cross-entropy loss of **1...\"],[\"```bash\\nhuggingface-cli repo create wav2vec2-2-bart-base\\ngit clone https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-user...\"],[\"```\\n\\nNext, run the following script **inside** the just cloned repo:\\n\\n```python\\nfrom transformers im...\"],[\"```\\n\\nNote that we have added a randomly initialized _adapter layer_ to `wav2vec2-base` with the argu...\"],[\"Having warm-started the speech-encoder-decoder model under `\\u003cyour-user-name\\u003e\\u002fwav2vec2-2-bart`, we ca...\"],[\"```\\n\\nIf the environment variable is not set, the training script might freeze, *i.e.* see: https:\\u002f\\u002fg...\"],[\"```bash\\npython run_speech_recognition_seq2seq.py \\\\\\n\\t--dataset_name=\\\"librispeech_asr\\\" \\\\\\n\\t--model_name...\"],[\"```\\n\\nOn a single V100 GPU, this script should run in *ca.* 5 hours and yield a \\ncross-entropy loss o...\"],[\"```\\n\\nOn 8 V100 GPUs, this script should run in *ca.* 45 minutes and yield a cross-entropy loss of **...\"],[\"| Dataset                                                        | Dataset Config            | Pretr...\"],[\"|----------------------------------------------------------------|---------------------------|------...\"],[\"----------------------------------------------------------------------------------------------------...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr) | `\\\"clean\\\"` - `\\\"train.100\\\"` | [face...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr) | `\\\"clean\\\"` - `\\\"train.100\\\"` | [face...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-atten...\"],[\"## Usage tips\\n\\n- MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequen...\"],[\"LXMERT DEMO\\n\\n1. make a virtualenv: ``virtualenv venv`` and activate ``source venv\\u002fbin\\u002factivate``\\n2. ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n### Train tokenizer\\n\\nIn the first step, we train a tokenizer to efficiently process the text in...\"],[\"```\\n\\nGreat, we have set up our model repository. During training, we will automatically\\npush the tra...\"],[\"```\\n\\nTraining should converge at a loss and accuracy \\nof 1.78 and 0.64 respectively after 18 epochs ...\"],[\"```\\n\\n### Train tokenizer\\n\\nIn the first step, we train a tokenizer to efficiently process the text in...\"],[\"```\\n\\n### Create configuration\\n\\nNext, we create the model's configuration file. This is as simple \\nas...\"],[\"```\\n\\nTraining should converge at a loss and perplexity \\nof 3.24 and 25.72 respectively after 20 epoc...\"],[\"```\\n\\n### Train tokenizer\\n\\nIn the first step, we train a tokenizer to efficiently process the text in...\"],[\"# Train tokenizer\\ntokenizer.train_from_iterator(\\n    iterator=batch_iterator(input_sentence_size=inp...\"],[\"```\\n\\n### Create configuration\\n\\nNext, we create the model's configuration file. This is as simple \\nas...\"],[\"```\\n\\nTraining should converge at a loss and accuracy \\nof 2.36 and 57.0 respectively after 3 epochs o...\"],[\"```\\n\\n### Train tokenizer\\nIn the first step, we train a tokenizer to efficiently process the text inp...\"],[\"```\\n\\nGreat, we have set up our model repository. During training, we will automatically\\npush the tra...\"],[\"```\\n\\nTraining should converge at a loss and accuracy \\nof 1.36 and 0.77 respectively after 3 epochs o...\"],[\"### Script to run MLM with PyTorch\\u002fXLA on TPUv3-8\\n\\nFor comparison one can run the same pre-training ...\"],[\"```\\n\\n, set the following environment variables:\\n\\n```bash\\nexport XRT_TPU_CONFIG=\\\"localservice;0;local...\"],[\"```\\n\\n### Script to compare pre-training with PyTorch on 8 GPU V100's\\n\\nFor comparison you can run the...\"],[\"```\\n\\n, and can start training as follows:\\n\\n```bash\\npython3 -m torch.distributed.launch --nproc_per_n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large pre-trained language models have been shown to...\"],[\"## RagConfig\\n\\n[[autodoc]] RagConfig\\n\\n## RagTokenizer\\n\\n[[autodoc]] RagTokenizer\\n\\n## Rag specific outp...\"],[\"Robust Speech Challenge ğŸ¤—\\n\\nWelcome to the robust speech recognition challenge ğŸ™ï¸ !\\n\\nThe goal of this...\"],[\"Speech recognition systems should be trained using **PyTorch**, **ğŸ¤— Transformers**, and, **ğŸ¤— Dataset...\"],[\"During the event, the speech recognition system will be evaluated on both the Common Voice `\\\"test\\\"` ...\"],[\"![timeline](https:\\u002f\\u002fgithub.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fraw\\u002fmaster\\u002fRobush%20Speech%20Chall...\"],[\"```\\n\\nincludes more or less the same data as\\n\\n```python\\nload_dataset(\\\"mozilla-foundation\\u002fcommon_voice...\"],[\"```\\n\\nHowever, we strongly encourage participants to make use of Common Voice's other splits, *e.g.* ...\"],[\"Next, let's talk about preprocessing. Audio data and transcriptions have to be brought into the corr...\"],[\"It is allowed (and recommended) to normalize the data to only have lower-case characters. It is also...\"],[\"Since those choices are not always obvious when in doubt feel free to ask on Discord or even better ...\"],[\"```\\n\\nYou can activate your venv by running\\n\\n```bash\\nsource ~\\u002f\\u003cyour-venv-name\\u003e\\u002fbin\\u002factivate\\n```\\n\\nTo b...\"],[\"```\\n\\n4. Set up a PyTorch environment by running the following command your virtual environment:\\n\\n   ...\"],[\"```\\n$ cd ~\\u002f\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets.git\\n$ cd datasets\\n$ pip install -e \\\"...\"],[\"```\\n\\n## How to finetune an acoustic model\\n\\nIn this section, we show you how to fine-tune a pre-train...\"],[\"The blog can also be opened and directly fine-tuned in a google colab notebook.\\nIn this section, we ...\"],[\"```\\n\\nto login. It is recommended to login with your access token that can be found under your huggin...\"],[\"```\\n\\n3. **Add your training script and `run`-command to the repository**\\n\\nWe encourage participants ...\"],[\"```\\n\\nAlright, finally we can define the training script. We'll simply use some \\ndummy hyper-paramete...\"],[\"```\\n\\n4. **Start training**\\n\\nNow all that is left to do is to start training the model by executing t...\"],[\"```\\n\\n, clone it locally (assuming the `\\u003cusername\\u003e` is `hf-test`)\\n\\n```bash\\ngit clone hf-test\\u002fxls-r-30...\"],[\"```\\n\\n, and, define the following hyperparameters for training\\n\\n```bash\\necho '''python run_speech_rec...\"],[\"```\\n\\nThe training takes *ca.* 7 hours and yields a reasonable test word \\nerror rate of 27% as can be...\"],[\"### Setting up an AI notebook\\n1. Go to the `Public Cloud` page and select `Project Management` -\\u003e `U...\"],[\"For more quick tutorials about OVHcloud AI products, check out the showcase https:\\u002f\\u002fvimeo.com\\u002fshowca...\"],[\"## Evaluation\\n\\nFinally, we have arrived at the most fun part of the challenge - sitting back and\\nwat...\"],[\"```\\n\\nNext, we should adapt `eval.py` so that it fits our evaluation data. Here it is \\nimportant to k...\"],[\"- 1. The following input arguments should not be changed and keep their original functionality\\u002fmeani...\"],[\"- a. Somehow giving the model access to the target transcriptions to improve performance. The model ...\"],[\"Uff, that was a lot of text describing how to make sure your `eval.py` script \\nis in the correct for...\"],[\"```\\n\\nTo log each of the model's predictions with the target transcriptions, you can just \\nadd the `-...\"],[\"```\\n- \\\"sv\\\"\\n- \\\"robust-speech-event\\\"\\n```\\n\\nunder `tags:` as done [here](https:\\u002f\\u002fhuggingface.co\\u002fhf-test\\u002f...\"],[\"```\\n\\nThe dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \\nat the end of the robus...\"],[\"The following table summarizes what platform to use for which problem.\\n\\n- Problem\\u002fquestion\\u002fbug with ...\"],[\"## Talks\\n\\nWe are very excited to be hosting 2 days of talks from Kensho-Technologies, Mozilla's Comm...\"],[\"### Friday, January 21th\\n\\n Speaker        | Topic                           | Time                  ...\"],[\"#### Raymond Grossman, Jeremy Lopez, Machine Learning Engineer, Kensho Technologies\\n- Talk: PyCTCDec...\"],[\"#### Changhan Wang, Main author of XLS-R and Research Engineer, Meta AI Research\\n- Talk: XLS-R: Larg...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [AI Sweden](https:\\u002f\\u002fhuggingface.co\\u002fAI-Sweden).\\n\\n## Usage example\\n\\n```p...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"Testing mixed int8 quantization\\n\\n![HFxbitsandbytes.png](https:\\u002f\\u002fcdn-uploads.huggingface.co\\u002fproductio...\"],[\"```\\nFor the latest pytorch instructions please see [this](https:\\u002f\\u002fpytorch.org\\u002fget-started\\u002flocally\\u002f)\\n...\"],[\"```\\nls -l $CONDA_PREFIX\\u002flib\\u002flibcudart.so\\n```\\nor \\n```\\nls -l $LD_LIBRARY_PATH\\n```\\nCheck if `libcudart....\"],[\"```\\n\\nOn each path (`$path`) separated by `:`.\\nIf not, simply run\\n```bash\\nls -l $LD_LIBRARY_PATH\\u002flibc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Driven by improved architectures and better represen...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pipelined NLP systems have largely been superseded b...\"],[\"## Usage tips\\n\\n- CANINE uses no less than 3 Transformer encoders internally: 2 \\\"shallow\\\" encoders (w...\"],[\"\\u003e\\u003e\\u003e outputs = model(input_ids)  # forward pass\\n\\u003e\\u003e\\u003e pooled_output = outputs.pooler_output\\n\\u003e\\u003e\\u003e sequenc...\"],[\"```\\n\\nFor batched inference and training, it is however recommended to make use of the tokenizer (to ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"Plug and Play Language Models: a Simple Approach to Controlled Text Generation\\n\\nAuthors: [Sumanth Da...\"],[\"```\\n\\n### Tuning hyperparameters for bag-of-words control\\n\\n1. Increase `--stepsize` to intensify topi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### GPT-2\\u002fGPT and causal language modeling\\n\\nThe following example fine-tunes GPT-2 on WikiText-2. We...\"],[\"```\\n\\nThis takes about half an hour to train on a single K80 GPU and about one minute for the evaluat...\"],[\"```\\n\\n### RoBERTa\\u002fBERT\\u002fDistilBERT and masked language modeling\\n\\nThe following example fine-tunes RoBE...\"],[\"```\\n\\nIf your dataset is organized with one sample per line, you can use the `--line_by_line` flag (o...\"],[\"```\\n\\n**Note:** On TPU, you should use the flag `--pad_to_max_length` in conjunction with the `--line...\"],[\"```\\n\\nIf your dataset is organized with one sample per line, you can use the `--line_by_line` flag (o...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Reinforcement learning (RL) is typically concerned w...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview transformers docs\\u002fsource\\u002fen\\u002f\\n```\\n\\nThe docs will be vi...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"### Adding a new model\\n\\nWhen adding a new model:\\n\\n- Create a file `xxx.md` or under `.\\u002fsource\\u002fmodel_...\"],[\"```\\n## XXXConfig\\n\\n[[autodoc]] XXXConfig\\n```\\n\\nThis will include every public method of the configurat...\"],[\"```\\n## XXXTokenizer\\n\\n[[autodoc]] XXXTokenizer\\n    - all\\n    - __call__\\n```\\n\\n### Writing source docum...\"],[\"```\\n\\nIf the description is too long to fit in one line, another indentation is necessary before writ...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\nWe follow the [doctest](https:\\u002f\\u002fdocs.pyth...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"```\\n    Example:\\n\\n    ```python\\n    \\u003e\\u003e\\u003e from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\n ...\"],[\"```\\n```\\n\\nThe docstring should give a minimal, clear example of how the respective model \\nis to be us...\"],[\"```\\n\\n### Writing doctests\\n\\nHere are a few tips to help you debug the doctests and make them pass:\\n\\n-...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] data.processors.utils.DataProcessor\\n\\n[[autodoc]] data.processors.utils.InputExample\\n\\n[[a...\"],[\"[[autodoc]] data.processors.glue.glue_convert_examples_to_features\\n\\n\\n## XNLI\\n\\n[The Cross-Lingual NLI...\"],[\"This library hosts a processor for each of the two versions:\\n\\n### Processors\\n\\nThose processors are:\\n...\"],[\"```\\n\\nUsing *tensorflow_datasets* is as easy as using a data file:\\n\\n```python\\n# tensorflow_datasets o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"`BrosForTokenClassification` has a simple linear layer on top of BrosModel. It predicts the label of...\"],[\"BROS achieves comparable or better result on Key Information Extraction (KIE) benchmarks such as FUN...\"],[\"```python\\ndef expand_and_normalize_bbox(bboxes, doc_width, doc_height):\\n    # here, bboxes are numpy...\"],[\"```\\n\\n- [`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassi...\"],[\"```\\n\\n## Resources\\n\\n- Demo scripts can be found [here](https:\\u002f\\u002fgithub.com\\u002fclovaai\\u002fbros).\\n\\n## BrosConf...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [zphang](\\u003chttps:\\u002f\\u002fhuggingface.co\\u002fzphang). The original code can be fou...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"inputs = tokenizer(\\\"This is an example.\\\", return_tensors=\\\"pt\\\")\\n# Feed everything to the model\\noutput...\"],[\"```\\n\\nIf you want to make sure the model stops generating when `'\\\\n\\\\n'` is detected, we recommend usi...\"],[\"```\\n\\n## RwkvConfig\\n\\n[[autodoc]] RwkvConfig\\n\\n## RwkvModel\\n\\n[[autodoc]] RwkvModel\\n    - forward\\n\\n## Rw...\"],[\"In comparison, the RWKV attention is given by\\n\\n$$O_{i} = \\\\sigma(R_{i}) \\\\frac{\\\\sum_{j=1}^{i} e^{W_{i-...\"],[\"and\\n\\n$$D_{i} = e^{u + K_{i}} + \\\\hat{D}_{i} \\\\hbox{  where  } \\\\hat{D}_{i} = e^{K_{i-1}} + e^{w + K_{i-...\"],[\"defined by the following recurrent formulas:\\n\\n$$\\\\tilde{N}_{0} = 0 \\\\hbox{  and  } \\\\tilde{N}_{j+1} = e...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Universal Image Segmentation is not a new concept. P...\"],[\"## Usage tips\\n\\n-  OneFormer requires two inputs during inference: *image* and *task token*. \\n- Durin...\"],[\"- Demo notebooks regarding inference + fine-tuning on custom data can be found [here](https:\\u002f\\u002fgithub...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n***** Eval results *****\\neval_acc = 0.8338998300509847\\neval_loss = 0.44457291918821606\\n```\\n\\n## W...\"],[\"```\\n\\nand reply to the questions asked. Then\\n\\n```bash\\naccelerate test\\n```\\n\\nthat will check everything...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"...\"],[\"```\\n\\nNotice how the entire chat is condensed into a single string. If we use `tokenize=True`, which ...\"],[\"```\\n\\nNote that this time, the tokenizer has added the control tokens [INST] and [\\u002fINST] to indicate ...\"],[\"```\\n\\nNow that our input is formatted correctly for Zephyr, we can use the model to generate a respon...\"],[\"```\\n\\n```text\\nConversation id: 76d886a0-74bd-454e-9804-0467041a63dc\\nsystem: You are a friendly chatbo...\"],[\"```\\n\\nAnd here's what it looks like **with** a generation prompt:\\n\\n```python\\ntokenizer.apply_chat_tem...\"],[\"```\\n\\nNote that this time, we've added the tokens that indicate the start of a bot response. This ens...\"],[\"```\\nAnd we get:\\n```text\\n\\u003c|user|\\u003e\\nWhich is bigger, the moon or the sun?\\u003c\\u002fs\\u003e\\n\\u003c|assistant|\\u003e\\nThe sun.\\u003c\\u002fs...\"],[\"```\\n\\nIf you've never seen one of these before, this is a [Jinja template](https:\\u002f\\u002fjinja.palletsproje...\"],[\"```\\n{% for message in messages %}\\n    {% if message['role'] == 'user' %}\\n        {{ bos_token + '[IN...\"],[\"```\\n\\nNow, simply set the `tokenizer.chat_template` attribute. Next time you use [`~PreTrainedTokeniz...\"],[\"```\\n\\nThe method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called...\"],[\"If you're training a model from scratch, or fine-tuning a base language model for chat, on the other...\"],[\"```\\n{% for message in messages %}\\n    {{'\\u003c|im_start|\\u003e' + message['role'] + '\\\\n' + message['content']...\"],[\"```\\n\\nThe \\\"user\\\", \\\"system\\\" and \\\"assistant\\\" roles are the standard for chat, and we recommend using th...\"],[\"```\\n{% for message in messages %}\\n{{ message['content'] }}\\n{% endfor %}\\n```\\n\\nNote that whatever's in...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡ã®ã‚ˆã†ãªå ´åˆã«é©ç”¨ã§ãã¾ã™:\\n\\n* ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡ã€æƒ…å ±æŠ½å‡ºã€è³ªå•å¿œç­”ã€è¦ç´„ã€ç¿»è¨³ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãªã©ã®ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«ã€100ä»¥ä¸Šã®è¨€èªã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚\\n* ğŸ–¼ï¸ ç”»...\"],[\"ğŸ¤—Transformersã¯[Jax](https:\\u002f\\u002fjax.readthedocs.io\\u002fen\\u002flatest\\u002f)ã€[PyTorch](https:\\u002f\\u002fpytorch.org\\u002f)ã€[TensorFl...\"],[\"è‡ªç„¶è¨€èªå‡¦ç†ã«ã¦:\\n- [BERTã«ã‚ˆã‚‹ãƒã‚¹ã‚¯ãƒ‰ãƒ¯ãƒ¼ãƒ‰è£œå®Œ](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D...\"],[\"- [RoBERTaã«ã‚ˆã‚‹è‡ªç„¶è¨€èªæ¨è«–](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+an...\"],[\"-...\"],[\"[DistilBERTã«ã‚ˆã‚‹è³ªå•å¿œç­”](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+i...\"],[\"roadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C00...\"],[\"h+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+o...\"],[\"- [T5ã«ã‚ˆã‚‹ç¿»è¨³](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã«ã¦:\\n- [ViTã«ã‚ˆã‚‹ç”»åƒåˆ†é¡](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224)\\n- [DETRã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡º](htt...\"],[\"## Hugging Faceãƒãƒ¼ãƒ ã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ»ã‚µãƒãƒ¼ãƒˆã‚’ã”å¸Œæœ›ã®å ´åˆ\\n\\n\\u003ca target=\\\"_blank\\\" href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fsupport\\\"\\u003e\\n   ...\"],[\"```\\n\\n2è¡Œç›®ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€pipelineã§ä½¿ç”¨ã•ã‚Œã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€3è¡Œç›®ã§ã¯ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ãã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€ç­”ãˆã¯99.97%ã®ä¿¡...\"],[\"```\\n\\nã“ã“ã§ã¯ã€ç”»åƒã‹ã‚‰æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆãŒå¾—ã‚‰ã‚Œã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å›²ã‚€ãƒœãƒƒã‚¯ã‚¹ã¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚å·¦å´ãŒå…ƒç”»åƒã€å³å´ãŒäºˆæ¸¬çµæœã‚’è¡¨ç¤ºã—ãŸã‚‚ã®ã§ã™:\\n\\n\\u003ch3 align=\\\"c...\"],[\"```\\n\\nãã—ã¦ã“ã¡ã‚‰ã¯TensorFlowã¨åŒç­‰ã®ã‚³ãƒ¼ãƒ‰ã¨ãªã‚Šã¾ã™:\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, TFAutoMode...\"],[\"```\\n\\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ã™ã¹ã¦ã®å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€å˜ä¸€ã®æ–‡å­—åˆ— (ä¸Šè¨˜ã®ä¾‹ã®ã‚ˆã†ã«) ã¾ãŸã¯ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ç›´æ¥å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ä¸‹æµã®ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã§ãã‚‹è¾æ›¸ã‚’å‡ºåŠ›ã—ã¾...\"],[\"## ãªãœtransformersã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\\n\\n1. ä½¿ã„ã‚„ã™ã„æœ€æ–°ãƒ¢ãƒ‡ãƒ«:\\n    - è‡ªç„¶è¨€èªç†è§£ãƒ»ç”Ÿæˆã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®å„ã‚¿ã‚¹ã‚¯ã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—...\"],[\"1. ãƒ¢ãƒ‡ãƒ«ã‚„ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ã¦ç°¡å˜ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½:\\n    - åŸè‘—è€…ãŒç™ºè¡¨ã—ãŸçµæœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä¾‹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\\n    - ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã¯å¯èƒ½ãªé™ã‚Šä¸€è²«ã—ã¦å…¬...\"],[\"## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\\n\\n### pipã«ã¦\\n\\nã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, TensorFlow 2.6+ ã§ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\\n...\"],[\"```\\n\\nã‚‚ã—ã‚µãƒ³ãƒ—ãƒ«ã‚’è©¦ã—ãŸã„ã€ã¾ãŸã¯ã‚³ãƒ¼ãƒ‰ã®æœ€å…ˆç«¯ãŒå¿…è¦ã§ã€æ–°ã—ã„ãƒªãƒªãƒ¼ã‚¹ã‚’å¾…ã¦ãªã„å ´åˆã¯ã€[ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftran...\"],[\"```\\n\\nFlaxã€PyTorchã€TensorFlowã‚’condaã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹æ–¹æ³•ã¯ã€ãã‚Œãã‚Œã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«å¾“ã£ã¦ãã ã•ã„ã€‚\\n\\n\\u003e **_æ³¨æ„:_**  Windowsã§ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (Google Research and the ...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (VinAI Research ã‹ã‚‰) Ngu...\"],[\"1. **[BigBird-Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbigbird_pegasus)** (Google...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (Google AI ã‹ã‚‰) Alexander Kolesn...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (Salesforce ã‹ã‚‰) Junnan Li...\"],[\"1. **[BROS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbros)** (NAVER CLOVA ã‹ã‚‰) Teakgyu Hong...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (OFA-Sys ã‹ã‚‰) ...\"],[\"1. **[CLVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclvp)** released with the paper [Bett...\"],[\"1. **[Conditional DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconditional_detr)** (Micr...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (Tsinghua University ã‹ã‚‰) Zhengy...\"],[\"1. **[Data2Vec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdata2vec)** (Facebook ã‹ã‚‰) Alexei ...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (SenseT...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (Facebook ã‹ã‚‰) Nicolas Carion,...\"],[\"1. **[DINOv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinov2)** (Meta AI ã‹ã‚‰) Maxime Oquab...\"],[\"1. **[DiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdit)** (Microsoft Research ã‹ã‚‰) Junlong...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (Snap R...\"],[\"1. **[EncoderDecoder](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fencoder-decoder)** (Google ...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (Meta AI ã‹ã‚‰) ã¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³è¨€èªãƒ¢...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (Google AI ã‹ã‚‰) Hyung Wo...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (CNRS ã‹ã‚‰) Hang Le, Lo...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (CMU\\u002fGoogle B...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (OpenAI ã‹ã‚‰) Alec Radford...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (OpenAI ã‹ã‚‰) Alec Radford*, J...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (BigCode ã‹ã‚‰) Lou...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (Microsoft ã‹ã‚‰) Ch...\"],[\"1. **[I-BERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fibert)** (Berkeley ã‹ã‚‰) Sehoon Kim, ...\"],[\"1. **[InstructBLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finstructblip)** (Salesforce ã‹...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (Microsoft Resear...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (Meta AI ã‹ã‚‰) Ben Graham, Al...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (UNC Chapel Hill ã‹ã‚‰) Hao ...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (Meta and UIUC ã‹ã‚‰...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (Facebook ã‹ã‚‰) Xuezhe Ma, Chun...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (Facebook ã‹ã‚‰) Vineel Pratap, An...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (Google Inc. ã‹...\"],[\"1. **[MRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmra)** (the University of Wisconsin - ...\"],[\"1. **[NAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnat)** (SHI Labs ã‹ã‚‰) Ali Hassani, Stev...\"],[\"1. **[NystrÃ¶mformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (the Univer...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (Google AI ã‹ã‚‰) Matthias ...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (Google ã‹ã‚‰) Jingqing Zh...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (UCLA NLP ã‹ã‚‰) Wasi Uddin ...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (Nanjing University, The Univer...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (Google Research ã‹ã‚‰) ...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (Facebook ã‹ã‚‰), Yinhan L...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (Bo Peng ã‹ã‚‰) Bo Peng. ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸ...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (Meta AI ã‹ã‚‰) Alexa...\"],[\"1. **[SpeechToTextTransformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text)** ...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (MBZUAI ã‹ã‚‰) Abd...\"],[\"1. **[SwitchTransformers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswitch_transformers)** ...\"],[\"1. **[TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapas)** (Google AI ã‹ã‚‰) Jonathan Her...\"],[\"1. **[Transformer-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftransfo-xl)** (Google\\u002fCMU ã‹...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (Google Research ã‹ã‚‰) Yi Tay, Mo...\"],[\"1. **[UnivNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funivnet)** (from Kakao Corporation...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (NAVER AI Lab\\u002fKakao Enterpris...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (Google AI ã‹ã‚‰) Al...\"],[\"1. **[ViTMSN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_msn)** (Meta AI ã‹ã‚‰) Mahmoud Ass...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[Whisper](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwhisper)** (OpenAI ã‹ã‚‰) Alec Radfor...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) Xi Victori...\"],[\"1. **[XLM-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta)** (Facebook AI ã‹ã‚‰...\"],[\"1. **[XLNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlnet)** (Google\\u002fCMU ã‹ã‚‰) Zhilin Yang...\"],[\"1. **[YOLOS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyolos)** (Huazhong University of Sci...\"],[\"å„ãƒ¢ãƒ‡ãƒ«ãŒFlaxã€PyTorchã€TensorFlowã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‹ã€ğŸ¤—Tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«æ”¯ãˆã‚‰ã‚ŒãŸé–¢é€£ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’æŒã£ã¦ã„ã‚‹ã‹ã¯ã€[ã“ã®è¡¨](https:\\u002f\\u002fhuggingfa...\"],[\"## ã•ã‚‰ã«è©³ã—ã\\n\\n| ã‚»ã‚¯ã‚·ãƒ§ãƒ³ | æ¦‚è¦ |\\n|-|-|\\n| [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002f) | å®Œå…¨ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ...\"],[\"## å¼•ç”¨\\n\\nğŸ¤— ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å¼•ç”¨ã§ãã‚‹[è«–æ–‡](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)ãŒå‡ºæ¥ã¾ã—ãŸ:\\n```bi...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It should be noted that each of the first three modules can support conditional speaker embeddings t...\"],[\"```\\n\\n#### Using CPU offload\\n\\nAs mentioned above, Bark is made up of 4 sub-models, which are called u...\"],[\"```\\n\\n\\n##### Usage\\n\\nTo load a model using Flash Attention 2, we can pass the `attn_implementation=\\\"fl...\"],[\"```\\n\\n##### Performance comparison\\n\\n\\nThe following diagram shows the latency for the native attention...\"],[\"```\\n\\nFind out more on inference optimization techniques [here](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransform...\"],[\"```\\n\\nBark can generate highly realistic, **multilingual** speech as well as other audio - including ...\"],[\"```\\n\\n## BarkConfig\\n\\n[[autodoc]] BarkConfig\\n    - all\\n\\n## BarkProcessor\\n\\n[[autodoc]] BarkProcessor\\n  ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"MPNet adopts a novel pre-training method, named masked and permuted language modeling, to inherit th...\"],[\"## MPNetConfig\\n\\n[[autodoc]] MPNetConfig\\n\\n## MPNetTokenizer\\n\\n[[autodoc]] MPNetTokenizer\\n    - build_i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"BLIP is a model that is able to perform various multi-modal tasks including:\\n- Visual Question Answe...\"],[\"## BlipConfig\\n\\n[[autodoc]] BlipConfig\\n    - from_text_vision_configs\\n\\n## BlipTextConfig\\n\\n[[autodoc]]...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have shown that multilingual pretrain...\"],[\"```\\n\\nNote that mLUKE has its own tokenizer, [`MLukeTokenizer`]. You can initialize it as follows:\\n\\n`...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [sshleifer](https:\\u002f\\u002fhuggingface.co\\u002fsshleifer). The authors' code can b...\"],[\"```\\n\\n## Implementation Notes\\n\\n- Blenderbot uses a standard [seq2seq model transformer](https:\\u002f\\u002farxiv...\"],[\"## TFBlenderbotForConditionalGeneration\\n\\n[[autodoc]] TFBlenderbotForConditionalGeneration\\n    - call...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"A Hugging Face team member will be available to help you along the way so you'll never be alone. ğŸ¤— â¤...\"],[\"With this in mind, let's go a bit deeper into the general library design.\\n\\n### Overview of models\\n\\nT...\"],[\"```python\\nmodel = BrandNewBertModel.from_pretrained(\\\"brandy\\u002fbrand_new_bert\\\")\\nmodel.config  # model h...\"],[\"```\\n\\nSimilar to the model, the configuration inherits basic serialization and deserialization functi...\"],[\"### Overview of tokenizers\\n\\nNot quite ready yet :-( This section will be added soon!\\n\\n## Step-by-ste...\"],[\"In the following, we try to give you a general recipe that we found most useful when porting a model...\"],[\"-  What type of model is *brand_new_bert*? BERT-like encoder-only model? GPT2-like decoder-only mode...\"],[\"```\\n\\n3. Set up a development environment, for instance by running the following command:\\n\\n```bash\\npy...\"],[\"```\\n\\nNow you have set up a development environment to port *brand_new_bert* to ğŸ¤— Transformers.\\n\\n### ...\"],[\"It is very important that before you start the porting process, you can **efficiently** debug code i...\"],[\"```python\\nmodel = BrandNewBertModel.load_pretrained_checkpoint(\\\"\\u002fpath\\u002fto\\u002fcheckpoint\\u002f\\\")\\ninput_ids = [...\"],[\"```\\n\\nNext, regarding the debugging strategy, there are generally a few from which to choose from:\\n\\n-...\"],[\"No matter which strategy you choose, the recommended procedure is often the same that you should sta...\"],[\"```\\n[[\\n [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\\n [-0.4417, -0.5920,  0.3450,  ...\"],[\"```\\n\\nWe expect that every model added to ğŸ¤— Transformers passes a couple of integration tests, meanin...\"],[\"- Find the best way of debugging intermediate results. Is the original repository written in PyTorch...\"],[\"original code so that you can directly input the ids instead of an input string.\\n- Make sure that th...\"],[\"The following section gives you more specific details\\u002ftips on how you can do this for *brand_new_ber...\"],[\"```\\n\\nIn the special case that you are adding a model whose architecture exactly matches the model ar...\"],[\"```\\n\\n4. Push the changes to your account using:\\n\\n```bash\\ngit push -u origin a-descriptive-name-for-m...\"],[\"```\\n\\nIn general, all questions you might have regarding the model or your implementation should be a...\"],[\"**Note** that at this point, you don't have to be very sure that your code is fully correct or clean...\"],[\"```\\n\\nThe above command will create a model according to the default parameters as defined in `BrandN...\"],[\"```\\n\\nYou can have some more custom schemes if you need a special initialization for some modules. Fo...\"],[\"```\\n\\nThe `_is_hf_initialized` flag is internally used to make sure we only initialize a submodule on...\"],[\"```python\\nfrom torch import nn\\n\\n\\nclass SimpleModel(nn.Module):\\n    def __init__(self):\\n        super...\"],[\"```\\n\\nNow we can create an instance of this model definition which will fill all weights: `dense`, `i...\"],[\"```\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0.207...\"],[\"0.0333, -0.0536],\\n        [-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,\\n ...\"],[\"```\\n\\nIn the conversion script, you should fill those randomly initialized weights with the exact wei...\"],[\"```\\n\\nIf either the shape or the name doesn't match, you probably assigned the wrong checkpoint weigh...\"],[\"```\\n\\n**7. Implement the forward pass**\\n\\nHaving managed to correctly load the pretrained weights into...\"],[\"```\\n\\nIt is very likely that the ğŸ¤— Transformers implementation and the original model implementation ...\"],[\"The best way to fix the problem is usually to look at the forward pass of the original implementatio...\"],[\"```\\n\\nHaving fixed all common tests, it is now crucial to ensure that all the nice work you have done...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIn case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n\\n\\u003c\\u002f...\"],[\"```\\n\\nYou might have to take a deeper look again into the original repository to find the correct tok...\"],[\"```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer test file should also ...\"],[\"Next, make sure that the docstring added to `src\\u002ftransformers\\u002fmodels\\u002fbrand_new_bert\\u002fmodeling_brand_n...\"],[\"```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality\\n```\\n\\nThere ar...\"],[\"```\\n\\nIt is worth spending some time to create fitting model cards for each checkpoint. The model car...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Self-training\\n\\nThis is an implementation of the self-training algorithm (without task augmentation) ...\"],[\"```\\nThis will install PyTorch as a backend.\\n\\n## Self-training\\n### Running self-training with a base ...\"],[\"```python\\nimport os\\nfrom selftraining import selftrain\\n\\ndata_dir = '\\u002fpath\\u002fto\\u002fyour\\u002fdata\\u002fdir'\\nparamete...\"],[\"```\\n\\n**Note**: We checkpoint periodically during self-training. In case of preemptions, just re-run ...\"],[\"```\\n\\n2. Run your script with the following command:\\n\\n```sh\\npython -m torch.distributed.launch --nnod...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n2. Pass your input to the [`pipeline`]. In the case of speech recognition, this is an audio inp...\"],[\"```\\n\\nNow this result looks more accurate! For a deep-dive comparison on Wav2Vec2 vs Whisper, refer t...\"],[\"```\\n\\nLet's check out 3 important ones:\\n\\n### Device\\n\\nIf you use `device=n`, the pipeline automaticall...\"],[\"```\\n\\nThis runs the pipeline on the 4 provided audio files, but it will pass them in batches of 2\\nto ...\"],[\"```\\n\\nAs you can see, the model inferred the text and also outputted **when** the various sentences w...\"],[\"```\\n\\nThe iterator `data()` yields each result, and the pipeline automatically\\nrecognizes the input i...\"],[\"```\\n\\n\\n## Using pipelines for a webserver\\n\\n\\u003cTip\\u003e\\nCreating an inference engine is a complex topic whic...\"],[\"```\\n\\n## Text pipeline\\n\\nUsing a [`pipeline`] for NLP tasks is practically identical.\\n\\n```py\\n\\u003e\\u003e\\u003e from ...\"],[\"```\\n\\n## Multimodal pipeline\\n\\nThe [`pipeline`] supports more than one modality. For example, a visual...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n## Using `pipeline` on large models with ğŸ¤— `accelerate`:\\n\\nYou can easily run `pipeline`...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"From the abstract of the XLM-V paper:\\n\\n*Large multilingual language models typically rely on a singl...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Combining simple architectures with large-scale pre-...\"],[\"## Usage tips\\n\\nOWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](c...\"],[\"\\u003e\\u003e\\u003e url = \\\"http:\\u002f\\u002fimages.cocodataset.org\\u002fval2017\\u002f000000039769.jpg\\\"\\n\\u003e\\u003e\\u003e image = Image.open(requests.g...\"],[\"```\\n\\n## Resources\\n\\nA demo notebook on using OWL-ViT for zero- and one-shot (image-guided) object det...\"],[\"Security Policy\\n\\n## Reporting a Vulnerability\\n\\nğŸ¤— We have our bug bounty program set up with HackerOn...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*The MobileNetV2 architecture is based on an inverted residual structure where the input and output ...\"],[\"- One can use [`MobileNetV2ImageProcessor`] to prepare images for the model.\\n\\n- The available image ...\"],[\"- The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract\\n\\n*Code summarization and generation empower conversion between programming...\"],[\"In cases where the language code is needed, the regular [`~PLBartTokenizer.__call__`] will encode so...\"],[\"```\\n\\n### Generation\\n\\n  While generating the target text set the `decoder_start_token_id` to the targ...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Causal la...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a self-supervised vision representation...\"],[\"- BEiT models are regular Vision Transformers, but pre-trained in a self-supervised way rather than ...\"],[\"images and 1,000 classes).\\n- BEiT uses relative position embeddings, inspired by the T5 model. Durin...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## BeitModel\\n\\n[[autodoc]] BeitModel\\n    - forward\\n\\n## BeitForMaskedImageMod...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This version of the model is for tasks where the state is a vector.\\n\\nThis model was contributed by [...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents XLS-R, a large-scale model for c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\\n\\u003e\\u003e\\u003e line = \\\"TÃ´i lÃ  sinh_viÃªn trÆ°á»ng Ä‘áº¡i_há»c CÃ´ng_ng...\"],[\"```\\n\\n\\u003cTip\\u003e \\n\\nPhoBERT implementation is the same as BERT, except for tokenization. Refer to [EART doc...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftrocr_archit...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\"\\u002f\\u003e\\n\\n- [Casual language modeling](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002f...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\u003e\\u003e\\u003e generated_ids = model.gene...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=trocr) to look for TrOCR checkpoints.\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n`compile()`Â comes with multiple modes for compiling, which essentially differ in compilation ti...\"],[\"```\\n\\n#### Object Detection with DETR\\n\\n```python \\nfrom transformers import AutoImageProcessor, AutoMo...\"],[\"```\\n\\nBelow you can find the list of the models we benchmarked.\\n\\n**Image Classification** \\n- [google\\u002f...\"],[\"Below you can find visualization of inference durations with and without `torch.compile()`Â and perce...\"],[\"Below you can find inference durations in milliseconds for each model with and without `compile()`. ...\"],[\"### A100 (batch size: 4)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecomp...\"],[\"### A100 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"### V100 (batch size: 1)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecomp...\"],[\"### V100 (batch size: 4)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecomp...\"],[\"### V100 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"### T4 (batch size: 1)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompil...\"],[\"### T4 (batch size: 4)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompil...\"],[\"### T4 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompi...\"],[\"### A100\\n\\n| **Task\\u002fModel** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0 -\\u003cbr\\u003e compile...\"],[\"###Â V100\\n\\n| **Task\\u002fModel** | **Batch Size** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"### T4\\n\\n| **Task\\u002fModel** | **Batch Size** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Get started by making sure you have PyTorch installed. MPS acceleration is supported on macOS 12.3+....\"],[\"```\\n\\n[`TrainingArguments`] uses the `mps` device by default if it's available which means you don't ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003csmall\\u003e Nougat high-level overview. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2308.13418\\\"\\u003eorigin...\"],[\"\\u003e\\u003e\\u003e from transformers import NougatProcessor, VisionEncoderDecoderModel\\n\\u003e\\u003e\\u003e from datasets import loa...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=nougat) to look for Nougat checkpoints...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised approaches for speech representation...\"],[\"## HubertModel\\n\\n[[autodoc]] HubertModel\\n    - forward\\n\\n## HubertForCTC\\n\\n[[autodoc]] HubertForCTC\\n   ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to capture global context,\\n...\"],[\"\\u003csmall\\u003e Neighborhood Attention with different dilation values.\\nTaken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[`SeamlessM4TModel`] can perform all the above tasks, but each task also has its own dedicated sub-m...\"],[\"## Usage\\n\\nFirst, load the processor and a checkpoint of the model:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers ...\"],[\"```\\n\\nYou can seamlessly use this model on text or on audio, to generated either translated text or t...\"],[\"```\\n\\nWith basically the same code, I've translated English text and Arabic speech to Russian speech ...\"],[\"```\\n\\nFeel free to try out [`SeamlessM4TForSpeechToText`] and [`SeamlessM4TForTextToSpeech`] as well....\"],[\"This model was contributed by [ylacombe](https:\\u002f\\u002fhuggingface.co\\u002fylacombe). The original code can be ...\"],[\"## SeamlessM4TTextToUnitModel\\n\\n[[autodoc]] SeamlessM4TTextToUnitModel\\n\\n## SeamlessM4TTextToUnitForCo...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"3. Check the [Migration](migration) guide if you use an older version of ğŸ¤— Transformers since some i...\"],[\"```\\nValueError: Connection error, and we cannot find the requested files in the cached path.\\nPlease ...\"],[\"```\\n\\nHere are some potential solutions you can try to lessen memory use:\\n\\n- Reduce the [`per_device_...\"],[\"```\\n\\n- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTra...\"],[\"```\\n\\n## Incorrect output when padding tokens aren't masked\\n\\nIn some cases, the output `hidden_state`...\"],[\"```\\n\\nMost of the time, you should provide an `attention_mask` to your model to ignore the padding to...\"],[\"```\\n\\nğŸ¤— Transformers doesn't automatically create an `attention_mask` to mask a padding token if it i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n\\u003e Visually-situated language is ubiquitous -- sources...\"],[\"If you want to use the model to perform conditional text captioning, make sure to use the processor ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"## The Big Table of Tasks\\n\\nHere is the list of all our examples:\\n\\n| Task | Example datasets |\\n|---|-...\"],[\"Patience-based Early Exit\\n\\nPatience-based Early Exit (PABEE) is a plug-and-play inference method for...\"],[\"```\\n\\n## Inference\\n\\nYou can inference with different patience settings by:\\n```bash\\nexport GLUE_DIR=\\u002fp...\"],[\"```\\nwhere `patience` can be a list of patience settings, separated by a comma. It will help determin...\"],[\"| Model         | \\\\#Param | Speed\\\\-up | MNLI  | SST\\\\-2 | STS\\\\-B |\\n|---------------|---------|-------...\"],[\"Long Form Question Answering\\n\\nAuthor: @yjernite\\n\\nThis folder contains the code for the Long Form Que...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, neural networks purely based on attention ...\"],[\"- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teache...\"],[\"augmentation, optimization, and regularization were used in order to simulate training on a much lar...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"[[autodoc]] TFDeiTModel\\n    - call\\n\\n## TFDeiTForMaskedImageModeling\\n\\n[[autodoc]] TFDeiTForMaskedImag...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Light-weight convolutional neural networks (CNNs) ar...\"],[\"## Usage tips\\n\\n- MobileViT is more like a CNN than a Transformer model. It does not work on sequence...\"],[\"model_ckpt = \\\"apple\\u002fmobilevit-xx-small\\\"\\nmodel = TFMobileViTForImageClassification.from_pretrained(mo...\"],[\"```\\n\\n  The resulting model will be just **about an MB** making it a good fit for mobile applications...\"],[\"[[autodoc]] TFMobileViTModel\\n    - call\\n\\n## TFMobileViTForImageClassification\\n\\n[[autodoc]] TFMobileV...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"[LayoutLMv3](..\\u002fmodel_doc\\u002flayoutlmv3), [LED](..\\u002fmodel_doc\\u002fled), [LiLT](..\\u002fmodel_doc\\u002flilt), [Longform...\"],[\"[SqueezeBERT](..\\u002fmodel_doc\\u002fsqueezebert), [T5](..\\u002fmodel_doc\\u002ft5), [UMT5](..\\u002fmodel_doc\\u002fumt5), [XLM](..\\u002f...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThere are several important fields here:\\n\\n- `answers`: the starting location of the answer toke...\"],[\"```\\n\\nThere are a few preprocessing steps particular to question answering tasks you should be aware ...\"],[\"...         # Find the start and end of the context\\n...         idx = 0\\n...         while sequence_i...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets [`~datasets.Dataset...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use ...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nTokenize the text and return TensorFlow tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here are the different models open sourced in the MMS project. The models and code are originally re...\"],[\"```py\\nfrom transformers import Wav2Vec2ForCTC, AutoProcessor\\n\\nmodel_id = \\\"facebook\\u002fmms-1b-all\\\"\\ntarge...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou can safely ignore a warning such as:\\n\\n```text\\nSome weights of Wav2Vec2ForCTC were no...\"],[\"```\\n\\n#### Inference\\n\\nNext, let's look at how we can run MMS in inference and change adapter layers a...\"],[\"```\\n\\nNow we process the audio data, pass the processed audio data to the model and transcribe the mo...\"],[\"```\\n\\nIn the same way the language can be switched out for all other supported languages. Please have...\"],[\"```\\n\\nThe resulting waveform can be saved as a `.wav` file:\\n\\n```python\\nimport scipy\\n\\nscipy.io.wavfile...\"],[\"```\\n\\n**Tips:**\\n\\n* The MMS-TTS checkpoints are trained on lower-cased, un-punctuated text. By default...\"],[\"```\\n\\n### Language Identification (LID)\\n\\nDifferent LID models are available based on the number of la...\"],[\"```\\n\\nNext, we load the model and processor\\n\\n```py\\nfrom transformers import Wav2Vec2ForSequenceClassi...\"],[\"```\\n\\nTo see all the supported languages of a checkpoint, you can print out the language ids as follo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## BetterTransformer\\n\\nBetterTransformer accelerates inference with its fastpath (native PyTorch spec...\"],[\"```\\n\\n## TorchScript\\n\\nTorchScript is an intermediate PyTorch model representation that can be run in ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nFor PyTorch \\u003e= 1.14.0, JIT-mode could benefit any model for prediction an...\"],[\"```\\n\\n## ğŸ¤— Optimum\\n\\n\\u003cTip\\u003e\\n\\nLearn more details about using ORT with ğŸ¤— Optimum in the [Optimum Inferenc...\"],[\"!--Copyright 2023 The HuggingFace and Baidu Team. All rights reserved.\\n\\nLicensed under the Apache Li...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have demonstrated that pre-trained cr...\"],[\"## ErnieMConfig\\n\\n[[autodoc]] ErnieMConfig\\n\\n\\n## ErnieMTokenizer\\n\\n[[autodoc]] ErnieMTokenizer\\n    - bu...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Contrastive language-image pretraining has shown gre...\"],[\"\\u003csmall\\u003e X-CLIP architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.02816\\\"\\u003eoriginal pape...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nHaving downloaded COCO dataset manually you should be able to load with the `ydshieh\\u002fcoc_datase...\"],[\"```\\n\\nThis loads both the text and vision encoders using pre-trained weights, the projection layers a...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n\\n### Upload converted models\\nSince version v3.5.0, the model sharing workflow is switched to gi...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] thus implement the main\\nmethods for using al...\"],[\"## PreTrainedTokenizerFast\\n\\nThe [`PreTrainedTokenizerFast`] depend on the [tokenizers](https:\\u002f\\u002fhuggi...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"async def server_loop(q):\\n    pipe = pipeline(model=\\\"bert-base-uncased\\\")\\n    while True:\\n        (st...\"],[\"```\\n\\nNow you can start it with:\\n```bash\\nuvicorn server:app\\n```\\n\\nAnd you can query it:\\n```bash\\ncurl -...\"],[\"```\\n\\nAgain, the proposed code is optimized for readability, not for being the best code.\\nFirst of al...\"],[\"This would be important if the inference of single items were long (\\u003e 1s) because \\nin this case, it ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Inference\\n\\nSpeech2Text is a speech model that accepts a float tensor of log-mel filter-bank featu...\"],[\"\\u003e\\u003e\\u003e ds = load_dataset(\\\"hf-internal-testing\\u002flibrispeech_asr_demo\\\", \\\"clean\\\", split=\\\"validation\\\")\\n\\n\\u003e\\u003e\\u003e ...\"],[\"```\\n\\n- Multilingual speech translation\\n\\n  For multilingual speech translation models, `eos_token_id`...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=speech_to_text) to look for Speech2Tex...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While originally designed for natural language proce...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, significant progress has been made applyin...\"],[\"## Resources\\n\\n\\u003cPipelineTag pipeline=\\\"object-detection\\\"\\u002f\\u003e\\n\\n- A demo notebook for the Table Transforme...\"],[\"# Sequence to Sequence Training and Evaluation\\n\\nThis directory contains examples for finetuning and ...\"],[\"```\\nthis should make a directory called `wmt_en_ro\\u002f` with 6 files.\\n\\n#### WMT English-German\\n\\n```bash...\"],[\"```\\nThe `.source` files are the input, the `.target` files are the desired output.\\n\\n### Potential is...\"],[\"Summarization Tips:\\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB...\"],[\"```\\n\\n### Finetuning Training Params\\n\\nTo override the pretrained model's training params, you can pas...\"],[\"```\\nThis should take \\u003c 6h\\u002fepoch on a 16GB v100 and achieve test BLEU above 26\\nTo get results in line...\"],[\"```\\n### Finetuning Outputs\\nAs you train, `output_dir` will be filled with files, that look kind of l...\"],[\"```\\n\\n### Converting pytorch-lightning checkpoints\\npytorch lightning ``-do_predict`` often fails, aft...\"],[\"```\\nuses 12,723 batches of length 48 and takes slightly more time 9.5 minutes.\\n\\nThe feature is still...\"],[\"![DBART](https:\\u002f\\u002fhuggingface.co\\u002ffront\\u002fthumbnails\\u002fdistilbart_large.png)\\n\\n+ For the CNN\\u002fDailyMail data...\"],[\"### Evaluation\\n\\nuse [run_distributed_eval](.\\u002frun_distributed_eval.py), with the following convenient...\"],[\"```\\nOn a 1 GPU system, here are four commands (that assume `xsum`, `cnn_dm` are downloaded, cmd-F fo...\"],[\"```\\n\\n### Distillation\\n+ For all of the following commands, you can get roughly equivalent result and...\"],[\"```\\nor for `pegasus-xsum`\\n```bash\\npython make_student.py google\\u002fpegasus-xsum --save_path dpx_xsum_16...\"],[\"```\\n\\n+ Note: The command that produced `sshleifer\\u002fdistilbart-cnn-12-6` is at [train_distilbart_cnn.s...\"],[\"```\\n\\u003c!--- runtime: 6H on NVIDIA RTX 24GB GPU --\\u003e\\n+ Tip: You can get the same simple distillation log...\"],[\"```\\n\\n\\n\\nTo combine datasets, as in Section 6.2, try something like:\\n```bash\\ncurl -S https:\\u002f\\u002fcdn-datas...\"],[\"```\\n\\n+ Expected ROUGE-2 between 21.3 and 21.6, run time ~13H.\\n+ direct KD + Pegasus is VERY slow and...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\\\"auto\\\"` wh...\"],[\"```\\n\\nFor the chat model:\\n```bash\\nwget https:\\u002f\\u002faxtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-o...\"],[\"```\\n\\nThis model was contributed by [Molbap](https:\\u002f\\u002fhuggingface.co\\u002fMolbap).\\nThe original code can be...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cbr\\u003e\\n\\nFeel free to check out the [API reference](.\\u002fmain_classes\\u002ftrainer) for these other [`Trainer`]...\"],[\"```\\n\\nThis guide provides an overview of the [`Trainer`] class.\\n\\n## Basic usage\\n\\n[`Trainer`] includes...\"],[\"```\\n\\nPass `training_args` to the [`Trainer`] along with a model, dataset, something to preprocess th...\"],[\"```\\n\\nYou can save your checkpoints (the optimizer state is not saved by default) to the Hub by setti...\"],[\"* [`~Trainer.get_train_dataloader`] creates a training DataLoader\\n* [`~Trainer.get_eval_dataloader`]...\"],[\"```\\n\\n### Callbacks\\n\\nAnother option for customizing the [`Trainer`] is to use [callbacks](callbacks)....\"],[\"```\\n\\n## Logging\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [logging](.\\u002fmain_classes\\u002flogging) API reference for more infor...\"],[\"log_level = training_args.get_process_log_level()\\nlogger.setLevel(log_level)\\ndatasets.utils.logging....\"],[\"```\\n\\nUse different combinations of `log_level` and `log_level_replica` to configure what gets logged...\"],[\"```\\n\\nNEFTune is disabled after training to restore the original embedding layer to avoid any unexpec...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"FSDP\\\"\\u003e\\n\\n```yml\\ncompute_environment: LOCAL_MACHINE\\ndistributed_type: F...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"DeepSpeed with Accelerate plugin\\\"\\u003e\\n\\n```yml\\ncompute_environment: LOCAL...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nThe [`accelerate_launch`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002fpack...\"],[\"```\\n\\nYou could also specify the parameters from the `config_file.yaml` file directly in the command ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"---\\n**NOTE 1**\\n\\nWav2Vec2's pre-training is known to be quite unstable.\\nIt is advised to do a couple ...\"],[\"The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available \\nper device, ...\"],[\"```\\n\\nThe results of this run can be seen [here](https:\\u002f\\u002fwandb.ai\\u002fpatrickvonplaten\\u002fwav2vec2-pretraine...\"],[\"```bash\\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\\\\n\\t--dataset_name=librispeech_asr \\\\...\"],[\"```\\n\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. \\nIn case you have more than 8...\"],[\"```bash\\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\\ \\n\\t--dataset_name=librispeech_asr ...\"],[\"```\\n\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. \\nIn case you have more than 8...\"],[\"p align=\\\"center\\\"\\u003e \\u003cimg src=\\\"http:\\u002f\\u002fsayef.tech:8082\\u002fuploads\\u002fFSNER-LOGO-2.png\\\" alt=\\\"FSNER LOGO\\\"\\u003e \\u003c\\u002fp\\u003e\\n...\"],[\"or\\n\\n2. Install from source: `python setup.py install` and import the model as shown in the code exam...\"],[\"device = 'cpu'\\n\\nW_query = tokenizer.tokenize(query).to(device)\\nW_supports = tokenizer.tokenize(suppo...\"],[\"!--âš ï¸ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar t...\"],[\"| Notebook     |      Description      |      Author      |      |\\n|:----------|:-------------|:----...\"],[\"| [Train T5 on TPU](https:\\u002f\\u002fgithub.com\\u002fpatil-suraj\\u002fexploring-T5\\u002fblob\\u002fmaster\\u002fT5_on_TPU.ipynb)  | How ...\"],[\"| [Fine-tune DialoGPT on New Datasets and Languages](https:\\u002f\\u002fgithub.com\\u002fncoop57\\u002fi-am-a-nerd\\u002fblob\\u002fmas...\"],[\"| [Fine-tune BART for Summarization](https:\\u002f\\u002fgithub.com\\u002fohmeow\\u002fohmeow_website\\u002fblob\\u002fmaster\\u002fposts\\u002f2021...\"],[\"| [Optimize ğŸ¤— Hugging Face models with Weights & Biases](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fwa...\"],[\"| [Fine-tune Longformer for QA](https:\\u002f\\u002fgithub.com\\u002fpatil-suraj\\u002fNotebooks\\u002fblob\\u002fmaster\\u002flongformer_qa_t...\"],[\"| [Fine-tune T5 for Sentiment Span Extraction](https:\\u002f\\u002fgithub.com\\u002fenzoampil\\u002ft5-intro\\u002fblob\\u002fmaster\\u002ft5_...\"],[\"|[Fine-tune BERT for Multi-label Classification](https:\\u002f\\u002fgithub.com\\u002fabhimishra91\\u002ftransformers-tutori...\"],[\"|[Speed up Fine-Tuning in Transformers with Dynamic Padding \\u002f Bucketing](https:\\u002f\\u002fgithub.com\\u002fELS-RD\\u002ft...\"],[\"|[Expand and Fine Tune Sci-BERT](https:\\u002f\\u002fgithub.com\\u002flordtt13\\u002fword-embeddings\\u002fblob\\u002fmaster\\u002fCOVID-19%20...\"],[\"|[Fine-tune Electra and interpret with Integrated Gradients](https:\\u002f\\u002fgithub.com\\u002felsanns\\u002fxai-nlp-note...\"],[\"|[Fine-tune a DistilBERT Model for Multi Label Classification task](https:\\u002f\\u002fgithub.com\\u002fDhavalTaunk08...\"],[\"|[Fine-tune Roberta for sentiment analysis](https:\\u002f\\u002fgithub.com\\u002fDhavalTaunk08\\u002fNLP_scripts\\u002fblob\\u002fmaster...\"],[\"|[Leverage BERT for Encoder-Decoder Summarization on CNN\\u002fDailymail](https:\\u002f\\u002fgithub.com\\u002fpatrickvonpla...\"],[\"|[Fine-tune TAPAS on Sequential Question Answering (SQA)](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransformers...\"],[\"|[Fine-tuning mBART for translation](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fvasudevgupta7\\u002fhuggingf...\"],[\"|[Fine-Tune DistilGPT2 and Generate Text](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002ftripathiaakash\\u002fDi...\"],[\"|[Evaluate LED on Arxiv](https:\\u002f\\u002fgithub.com\\u002fpatrickvonplaten\\u002fnotebooks\\u002fblob\\u002fmaster\\u002fLED_on_Arxiv.ipyn...\"],[\"|[Wav2Vec2 CTC decoding with GPT2 adjustment](https:\\u002f\\u002fgithub.com\\u002fvoidful\\u002fhuggingface_notebook\\u002fblob\\u002fm...\"],[\"|[Evaluate Big Bird on Trivia QA](https:\\u002f\\u002fgithub.com\\u002fpatrickvonplaten\\u002fnotebooks\\u002fblob\\u002fmaster\\u002fEvaluati...\"],[\"| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https:\\u002f\\u002fgithub.com\\u002fNielsRog...\"],[\"| [Fine-tune the Vision Transformer on CIFAR-10 using the ğŸ¤— Trainer](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fT...\"],[\"| [Evaluate LUKE on TACRED, a relation extraction dataset](https:\\u002f\\u002fgithub.com\\u002fstudio-ousia\\u002fluke\\u002fblob...\"],[\"| [Evaluate BigBird-Pegasus on PubMed dataset](https:\\u002f\\u002fgithub.com\\u002fvasudevgupta7\\u002fbigbird\\u002fblob\\u002fmain\\u002fno...\"],[\"| [Detect objects in an image with DETR](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransformers-Tutorials\\u002fblob\\u002fm...\"],[\"| [Finetune T5 for Named Entity Recognition](https:\\u002f\\u002fgithub.com\\u002fToluClassics\\u002fNotebooks\\u002fblob\\u002fmain\\u002fT5_...\"],[\"\\u003c!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ve...\"],[\"Join the growing community on the [Hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels), [forum](https:\\u002f\\u002fdiscuss.hugg...\"],[\"\\u003c!--This table is updated automatically from the auto modules with _make fix-copies_. Do not update ...\"],[\"|                                  Model                                   | PyTorch support | Tenso...\"],[\"|               [BigBird-Pegasus](model_doc\\u002fbigbird_pegasus)               |       âœ…        |       ...\"],[\"|                      [ConvBERT](model_doc\\u002fconvbert)                      |       âœ…        |       ...\"],[\"|                        [DINOv2](model_doc\\u002fdinov2)                        |       âœ…        |       ...\"],[\"|                  [Funnel Transformer](model_doc\\u002ffunnel)                  |       âœ…        |       ...\"],[\"|                       [Jukebox](model_doc\\u002fjukebox)                       |       âœ…        |       ...\"],[\"|                   [Mask2Former](model_doc\\u002fmask2former)                   |       âœ…        |       ...\"],[\"|                           [MT5](model_doc\\u002fmt5)                           |       âœ…        |       ...\"],[\"|                     [Persimmon](model_doc\\u002fpersimmon)                     |       âœ…        |       ...\"],[\"|                          [RWKV](model_doc\\u002frwkv)                          |       âœ…        |       ...\"],[\"|                         [TAPAS](model_doc\\u002ftapas)                         |       âœ…        |       ...\"],[\"|                   [VisualBERT](model_doc\\u002fvisual_bert)                    |       âœ…        |       ...\"],[\"|                [XLM-RoBERTa-XL](model_doc\\u002fxlm-roberta-xl)                |       âœ…        |       ...\"],[\"\\u003c!-- End table--\\u003e...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Once exported to ONNX format, a model can be:\\n- optimized for inference via techniques such as [grap...\"],[\"```\\n\\nTo check out all available arguments, refer to the [ğŸ¤— Optimum docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs...\"],[\"```\\n\\nThe example above illustrates exporting a checkpoint from ğŸ¤— Hub. When exporting a local model, ...\"],[\"```\\n\\nThe process is identical for TensorFlow checkpoints on the Hub. For instance, here's how you wo...\"],[\"```\\n\\n### Exporting a model for an unsupported architecture\\n\\nIf you wish to contribute by adding supp...\"],[\"```\\n\\nThe required output names (like `[\\\"last_hidden_state\\\"]`) can be obtained by taking a look at th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"To explain how tasks are solved, we'll walk through what goes on inside the model to output useful p...\"],[\"This model has four main components:\\n\\n1. A *feature encoder* takes the raw audio waveform, normalize...\"],[\"Ready to try your hand at audio classification? Check out our complete [audio classification guide](...\"],[\"### Image classification\\n\\nViT and ConvNeXT can both be used for image classification; the main diffe...\"],[\"4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer percep...\"],[\"You can feed this output to another convolutional layer, and with each successive layer, the network...\"],[\"### Object detection\\n\\n[DETR](model_doc\\u002fdetr), *DEtection TRansformer*, is an end-to-end object detec...\"],[\"3. DETR uses a *bipartite matching loss* during training to compare a fixed number of predictions wi...\"],[\"There are three main components to Mask2Former:\\n\\n1. A [Swin](model_doc\\u002fswin) backbone accepts an ima...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"### Text classification\\n\\n[BERT](model_doc\\u002fbert) is an encoder-only model and is the first model to e...\"],[\"Ready to try your hand at text classification? Check out our complete [text classification guide](ta...\"],[\"1. GPT-2 uses [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) to tokenize words ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"BART adapts to translation by adding a separate randomly initialized encoder to map a source languag...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Soonhwan-Kwon](https:\\u002f\\u002fgithub.com\\u002fSoonhwan-Kwon) and [stefan-it](http...\"],[\"## XLMRobertaXLForTokenClassification\\n\\n[[autodoc]] XLMRobertaXLForTokenClassification\\n    - forward\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo export a model's checkpoint from the ğŸ¤— Hub, for example, `bert-base-uncased`, run the follow...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present a new method that views object detection ...\"],[\"## How DETR works\\n\\nHere's a TLDR explaining how [`~transformers.DetrForObjectDetection`] works:\\n\\nFir...\"],[\"Next, this is sent through the encoder, outputting `encoder_hidden_states` of the same shape (you ca...\"],[\"DETR can be naturally extended to perform panoptic segmentation (which unifies semantic segmentation...\"],[\"- DETR uses so-called **object queries** to detect objects in an image. The number of queries determ...\"],[\"- [`~transformers.DetrForObjectDetection`] and [`~transformers.DetrForSegmentation`] can be initiali...\"],[\"There are three ways to instantiate a DETR model (depending on what you prefer):\\n\\nOption 1: Instanti...\"],[\"```\\n\\nOption 2: Instantiate DETR with randomly initialized weights for Transformer, but pre-trained w...\"],[\"```\\n\\nAs a summary, consider the following table:...\"],[\"| Task | Object detection | Instance segmentation | Panoptic segmentation |\\n|------|----------------...\"],[\"| **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformer...\"],[\"In short, one should prepare the data either in COCO detection or COCO panoptic format, then use\\n[`~...\"],[\"## DetrFeatureExtractor\\n\\n[[autodoc]] DetrFeatureExtractor\\n    - __call__\\n    - post_process_object_d...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In our example, we will take a couple of arguments of the ResNet class that we might want to tweak. ...\"],[\"```\\n\\nThe three important things to remember when writing you own configuration are the following:\\n- ...\"],[\"```\\n\\nYou can also use any other method of the [`PretrainedConfig`] class, like [`~PretrainedConfig.p...\"],[\"```\\n\\nFor the model that will classify images, we just change the forward method:\\n\\n```py\\nimport torch...\"],[\"```\\n\\nAgain, you can use any of the methods of [`PreTrainedModel`], like [`~PreTrainedModel.save_pret...\"],[\"```\\n\\nNote that the first argument used when registering your custom config to [`AutoConfig`] needs t...\"],[\"```\\n\\nThen you have to tell the library you want to copy the code files of those objects when using t...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\nNext, let's create the config and models as we did before:\\n\\n```py\\nresnet50d_config = Re...\"],[\"```\\n\\nYou can then push to your own namespace (or an organization you are a member of) like this:\\n\\n``...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Open-domain question answering relies on efficient p...\"],[\"[[autodoc]] DPRReaderTokenizer\\n\\n## DPRReaderTokenizerFast\\n\\n[[autodoc]] DPRReaderTokenizerFast\\n\\n## DP...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThen take a look at an example:...\"],[\"```py\\n\\u003e\\u003e\\u003e billsum[\\\"train\\\"][0]\\n{'summary': 'Existing law authorizes state agencies to enter into cont...\"],[\"'text': 'The people of the State of California do enact as follows:\\\\n\\\\n\\\\nSECTION 1.\\\\nSection 10295.3...\"],[\"to enter into a specific contract with the state agency.\\\\n(2) The contract is necessary to respond t...\"],[\"contained in Article 9 (commencing with Section 10420) to a contract subject to this chapter shall n...\"],[\"'title': 'An act to add Section 10295.35 to the Public Contract Code, relating to public contracts.'...\"],[\"```\\n\\nThere are two fields that you'll want to use:\\n\\n- `text`: the text of the bill which'll be the i...\"],[\"```\\n\\nNow create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynami...\"],[\"```\\n\\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule....\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`Seq2SeqTr...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nYou can also manually replicate the results of the `pipeline` if you'd like:\\n\\n\\n\\u003cframeworkconten...\"],[\"```\\n\\nUse the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the s...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"* [Methods and tools for efficient training on a single GPU](perf_train_gpu_one): start here to lear...\"],[\"When making contributions that A is better than B, please try to include a reproducible benchmark an...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work in language modeling demonstrates that t...\"],[\"## Usage tips\\n\\nWe have provided pretrained [BERT-345M](https:\\u002f\\u002fngc.nvidia.com\\u002fcatalog\\u002fmodels\\u002fnvidia:...\"],[\"```\\n\\nBERT-345M-cased:\\n\\n```bash\\nwget --content-disposition https:\\u002f\\u002fapi.ngc.nvidia.com\\u002fv2\\u002fmodels\\u002fnvidi...\"],[\"```\\n\\n```bash\\npython3 $PATH_TO_TRANSFORMERS\\u002fmodels\\u002fmegatron_bert\\u002fconvert_megatron_bert_checkpoint.py ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## With Trainer\\n\\nHere is an example of a translation fine-tuning with a MarianMT model:\\n\\n```bash\\npyt...\"],[\"```\\n\\nMBart and some T5 models require special handling.\\n\\nT5 models `t5-small`, `t5-base`, `t5-large`...\"],[\"```\\n\\nIf you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix`...\"],[\"```\\n\\nAnd here is how you would use the translation finetuning on your own files, after adjusting the...\"],[\"```\\nHere the languages are Romanian (`ro`) and English (`en`).\\n\\nIf you want to use a pre-processed d...\"],[\"```\\n\\nthen\\n\\n```bash\\npython run_translation_no_trainer.py \\\\\\n    --model_name_or_path Helsinki-NLP\\u002fopus...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Try out the inference widget here: https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224\\n\\n## TensorFlow...\"],[\"```\\n\\nğŸ‘€ See the results here: [amyeroberts\\u002fvit-base-beans](https:\\u002f\\u002fhuggingface.co\\u002famyeroberts\\u002fvit-bas...\"],[\"```\\n\\nInternally, the script will use the [`ImageFolder`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.0.0...\"],[\"```\\n\\n`ImageFolder` will create a `label` column, and the label name is based on the directory name.\\n...\"],[\"ere is how to convert a GPT2 model generated outside of `transformers`\\n\\n* [Megatron-LM](https:\\u002f\\u002fgith...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract, MBART is a sequence-to-sequence denoising auto-encoder pretrained on larg...\"],[\"\\u003e\\u003e\\u003e inputs = tokenizer(example_english_phrase, text_target=expected_translation_romanian, return_ten...\"],[\"```\\n\\n- Generation\\n\\n  While generating the target text set the `decoder_start_token_id` to the target...\"],[\"```\\n\\n## Overview of MBart-50\\n\\nMBart-50 was introduced in the [Multilingual Translation with Extensib...\"],[\"src_text = \\\" UN Chief Says There Is No Military Solution in Syria\\\"\\ntgt_text = \\\"Åeful ONU declarÄƒ cÄƒ ...\"],[\"```\\n\\n- Generation\\n\\n  To generate using the mBART-50 multilingual translation models, `eos_token_id` ...\"],[\"# translate Arabic to English\\ntokenizer.src_lang = \\\"ar_AR\\\"\\nencoded_ar = tokenizer(article_ar, return...\"],[\"```\\n\\n## Documentation resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification...\"],[\"## FlaxMBartForConditionalGeneration\\n\\n[[autodoc]] FlaxMBartForConditionalGeneration\\n    - __call__\\n ...\"],[\"# MM-IMDb\\n\\nBased on the script [`run_mmimdb.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fma...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"XTREME-S covers speech recognition with Fleurs, Multilingual LibriSpeech (MLS) and VoxPopuli, speech...\"],[\"## Fine-tuning for the XTREME-S tasks\\n\\nBased on the [`run_xtreme_s.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingfa...\"],[\"```\\n\\nwhere `TASK_NAME` can be one of: `mls, voxpopuli, covost2, fleurs-asr, fleurs-lang_id, minds14`...\"],[\"### Speech Recognition with MLS\\n\\nThe following command shows how to fine-tune the [XLS-R](https:\\u002f\\u002fhu...\"],[\"```\\n\\nOn 8 V100 GPUs, this script should run in ~19 hours and yield a cross-entropy loss of **0.6215*...\"],[\"```bash\\npython -m torch.distributed.launch \\\\\\n    --nproc_per_node=2 \\\\\\n    run_xtreme_s.py \\\\\\n    --ta...\"],[\"```\\n\\nOn 2 A100 GPUs, this script should run in ~5 hours and yield a cross-entropy loss of **0.4119**...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e ## Input Japanese Text\\n\\u003e\\u003e\\u003e line = \\\"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚\\\"\\n\\n\\u003e\\u003e\\u003e inputs = tokenizer(line, return_tensors=\\\"pt\\\")\\n...\"],[\"```\\n\\nExample of using a model with Character tokenization:\\n\\n```python\\n\\u003e\\u003e\\u003e bertjapanese = AutoModel.f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003csmall\\u003e LiLT architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2202.13669\\\"\\u003eoriginal paper\\u003c...\"],[\"```\\nfrom transformers import LiltModel\\n\\nmodel = LiltModel.from_pretrained(\\\"path_to_your_files\\\")\\nmode...\"],[\"```\\n\\n- When preparing data for the model, make sure to use the token vocabulary that corresponds to ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Begin by loading the [Yelp Reviews](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyelp_review_full) dataset:\\n\\n```p...\"],[\"```\\n\\nAs you now know, you need a tokenizer to process the text and include a padding and truncation ...\"],[\"```\\n\\n\\u003ca id='trainer'\\u003e\\u003c\\u002fa\\u003e\\n\\n## Train\\n\\nAt this point, you should follow the section corresponding to t...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou will see a warning about some of the pretrained weights not being used and some weig...\"],[\"```\\n\\nCall [`~evaluate.compute`] on `metric` to calculate the accuracy of your predictions. Before pa...\"],[\"```\\n\\nThen fine-tune your model by calling [`~transformers.Trainer.train`]:\\n\\n```py\\n\\u003e\\u003e\\u003e trainer.train(...\"],[\"```\\n\\nFinally, load, [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_apis\\u002f#compile-method), an...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou don't have to pass a loss argument to your models when you `compile()` them! Hugging...\"],[\"```\\n\\nRemember that Hugging Face datasets are stored on disk by default, so this will not inflate you...\"],[\"```\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003ca id='pytorch_native'\\u003e\\u003c\\u002fa\\u003e\\n\\n## Train in native PyTorch\\n\\n\\u003cframeworkc...\"],[\"```\\n\\n### DataLoader\\n\\nCreate a `DataLoader` for your training and test datasets so you can iterate ov...\"],[\"```\\n\\nLastly, specify `device` to use a GPU if you have access to one. Otherwise, training on a CPU m...\"],[\"```\\n\\n### Evaluate\\n\\nJust like how you added an evaluation function to [`Trainer`], you need to do the...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract,\\n\\n- Bart uses a standard seq2seq\\u002fmachine translation architecture with a b...\"],[\"## Implementation Notes\\n\\n- Bart doesn't use `token_type_ids` for sequence classification. Use [`Bart...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"- A blog post on [Distributed Training: Train BART\\u002fT5 for Summarization using ğŸ¤— Transformers and Ama...\"],[\"- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`BartForConditionalGeneration`] is supported by this [exampl...\"],[\"\\u003cPipelineTag pipeline=\\\"translation\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune mBART using Seq2SeqTrainer f...\"],[\"## BartModel\\n\\n[[autodoc]] BartModel\\n    - forward\\n\\n## BartForConditionalGeneration\\n\\n[[autodoc]] Bart...\"],[\"Intro\\n\\nAuthors: @patrickvonplaten and @lhoestq\\n\\nAimed at tackling the knowledge-intensive NLP tasks ...\"],[\"```\\nWe publish two `base` models which can serve as a starting point for finetuning on downstream ta...\"],[\"```\\nYou will then be able to pass `path\\u002fto\\u002fcheckpoint` as `model_name_or_path` to the `finetune_rag....\"],[\"```\\n\\nUsing Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load th...\"],[\"```\\nDoes He Love You\\tDoes He Love You\\tRed Sandy Spika dress of Reba McEntire\\tGreatest Hits Volume Tw...\"],[\"```\\n   ```bash\\n   # EXPLANATION\\n    python examples\\u002fresearch_projects\\u002frag\\u002feval_rag.py \\\\\\n        --mo...\"],[\"```\\n- `ans` - where a single line contains a single expected answer, e.g.:\\n```\\nXiu Li Dai\\n```\\n\\nPredi...\"],[\"```\\n\\nThe created outputs in `path\\u002fto\\u002fmy_knowledge_dataset` can then be used to finetune RAG as follo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Tips:\\n\\n- At the moment, only the backbone is available.\\n\\n## VitDetConfig\\n\\n[[autodoc]] VitDetConfig\\n\\n...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nDeepSpeed compiles CUDA C++ code and it can be a potential source of errors when building PyTor...\"],[\"```\\n\\n`PATH` lists the locations of the executables and `LD_LIBRARY_PATH` lists where to look for sha...\"],[\"```\\n\\n## Multi-GPU Network Issues Debug\\n\\nWhen training or inferencing with `DistributedDataParallel` ...\"],[\"```\\n\\nThis will dump a lot of NCCL-related debug information, which you can then search online if you...\"],[\"```\\nDetected inf\\u002fnan during batch_number=0\\nLast 21 forward frames:\\nabs min  abs max  metadata\\n      ...\"],[\"1.79e-06 4.65e+00 input[0]\\n3.18e-04 6.27e+04 output\\n                  encoder.block.2.layer.1.dropou...\"],[\"```\\n\\nThe example output has been trimmed in the middle for brevity.\\n\\nThe second column shows the val...\"],[\"```\\nDetected inf\\u002fnan during batch_number=0\\nLast 21 forward frames:\\nabs min  abs max  metadata\\n[...]\\n...\"],[\"```\\n\\nThe last frame reports for `Dropout.forward` function with the first entry for the only input a...\"],[\"def forward(self, hidden_states):\\n        hidden_gelu = self.gelu_act(self.wi_0(hidden_states))\\n    ...\"],[\"```\\n\\nNow it's easy to see the `dropout` call, and all the previous calls as well.\\n\\nSince the detecti...\"],[\"```\\n\\nYou can see that we added 2 of these and now we track if `inf` or `nan` for `forwarded_states` ...\"],[\"```\\n                  *** Starting batch number=1 ***\\nabs min  abs max  metadata\\n                  s...\"],[\"# Information Gain Filtration(IGF)\\n\\nAuthors @Tuko @mraunak\\n\\nThis folder contains the code how to imp...\"],[\"![IGF performance](result_igf.png)\\n\\nFigure 1: Comparing IGF to Standard Fine-tuning:\\nIGF with consta...\"],[\"```python\\npython run_clm_igf.py\\\\\\n--model_name_or_path \\\"gpt2\\\" \\\\\\n--data_file=\\\"data\\u002ftokenized_stories_t...\"],[\"```\\n\\n## Citation\\n\\nIf you find the resource useful, please cite the following paper\\n\\n```\\n@inproceedin...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified...\"],[\"## Usage tips\\n\\n- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised ...\"],[\"- [t5-11b](https:\\u002f\\u002fhuggingface.co\\u002ft5-11b).\\n\\nBased on the original T5 model, Google has released some...\"],[\"## Training\\n\\nT5 is an encoder-decoder model and converts all NLP problems into a text-to-text format...\"],[\"\\u003e\\u003e\\u003e tokenizer = T5Tokenizer.from_pretrained(\\\"t5-small\\\")\\n\\u003e\\u003e\\u003e model = T5ForConditionalGeneration.from_...\"],[\"```\\n\\nIf you're interested in pre-training T5 on a new corpus, check out the [run_t5_mlm_flax.py](htt...\"],[\"```\\n\\nAs you can see, only 2 inputs are required for the model in order to compute a loss: `input_ids...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e...\"],[\"\\u003e\\u003e\\u003e # replace padding token id's of the labels by -100 so it's ignored by the loss\\n\\u003e\\u003e\\u003e labels[labels...\"],[\"```\\n\\nAdditional training tips:\\n\\n- T5 models need a slightly higher learning rate than the default on...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\n\\u003e\\u003e\\u003e tokenizer = T5To...\"],[\"```\\n\\nNote that T5 uses the `pad_token_id` as the `decoder_start_token_id`, so when doing generation ...\"],[\"```\\n\\nBecause T5 has been trained with the span-mask denoising objective,\\nit can be used to predict t...\"],[\"```\\n\\n## Performance\\n\\nIf you'd like a faster training and inference performance, install [NVIDIA APEX...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\"\\u002f\\u003e\\n\\n- A notebook for [Finetuning CodeT5 for generating docstr...\"],[\"- A notebook to [Finetune T5-base-dutch to perform Dutch abstractive summarization on a TPU](https:\\u002f...\"],[\"- [Summarization](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fchapter7\\u002f5?fw=pt#summarization) chapter of the ğŸ¤— Hug...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`FlaxT5ForConditionalGeneration`] is supported by this [exam...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune T5 for question answe...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFT5Model\\n\\n[[autodoc]] TFT5Model\\n    - call\\n\\n## TFT5ForConditionalGeneration\\n\\n[[autod...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"The following example shows how to fine-tune the [Whisper small](https:\\u002f\\u002fhuggingface.co\\u002fopenai\\u002fwhisp...\"],[\"```\\n\\nOn a TPU v4-8, training should take approximately 25 minutes, with a final cross-entropy loss o...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"tokenizer = GPT2Tokenizer.from_pretrained(\\\"gpt2\\\")\\nmodel = GPT2LMHeadModel.from_pretrained(\\\"gpt2\\\")\\n\\ni...\"],[\"```\\n\\nThe `generation_output` object is a [`~generation.GreedySearchDecoderOnlyOutput`], as we can\\nse...\"],[\"```\\n\\nwill return the tuple `(generation_output.sequences, generation_output.scores)` for instance.\\n\\n...\"],[\"[[autodoc]] generation.TFContrastiveSearchDecoderOnlyOutput\\n\\n### FLAX\\n\\n[[autodoc]] generation.FlaxSa...\"],[\"[[autodoc]] MinLengthLogitsProcessor\\n    - __call__\\n\\n[[autodoc]] MinNewTokensLengthLogitsProcessor\\n ...\"],[\"[[autodoc]] TFMinLengthLogitsProcessor\\n    - __call__\\n\\n[[autodoc]] TFNoBadWordsLogitsProcessor\\n    -...\"],[\"[[autodoc]] FlaxTopKLogitsWarper\\n    - __call__\\n\\n[[autodoc]] FlaxTopPLogitsWarper\\n    - __call__\\n\\n[[...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*With the capability of modeling bidirectional contex...\"],[\"## Usage tips\\n\\n- The specific attention pattern can be controlled at training and test time using th...\"],[\"## XLNetTokenizerFast\\n\\n[[autodoc]] XLNetTokenizerFast\\n\\n## XLNet specific outputs\\n\\n[[autodoc]] models...\"],[\"[[autodoc]] XLNetForSequenceClassification\\n    - forward\\n\\n## XLNetForMultipleChoice\\n\\n[[autodoc]] XLN...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nLayoutLMv2 solves the document question-answering task by a...\"],[\"```\\n\\n```bash\\npip install 'git+https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002fdetectron2.git'\\npip install torchv...\"],[\"```\\n\\nAs you can see, the dataset is split into train and test sets already. Take a look at a random ...\"],[\"```\\n\\nNote that the LayoutLMv2 checkpoint that we use in this guide has been trained with `max_positi...\"],[\"```\\n\\nFinally, the data exploration won't be complete if we don't peek at an image example.\\n\\n```py\\n\\u003e\\u003e...\"],[\"```\\n\\nTo apply this preprocessing to the entire dataset in a fast way, use [`~datasets.Dataset.map`]....\"],[\"```\\n\\nOn top of the preprocessing mentioned above, we also need to add the labels for the model. For ...\"],[\"```\\n\\nTo illustrate how this function finds the position of the answer, let's use it on an example:...\"],[\"```py\\n\\u003e\\u003e\\u003e example = dataset_with_ocr[\\\"train\\\"][1]\\n\\u003e\\u003e\\u003e words = [word.lower() for word in example[\\\"word...\"],[\"Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'develo...\"],[\"'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'gro...\"],[\"'*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'u...\"],[\"Answer:  T.F. Riehl\\nstart_index 17\\nend_index 18...\"],[\"```\\n\\nOnce examples are encoded, however, they will look like this:\\n\\n```py\\n\\u003e\\u003e\\u003e encoding = tokenizer(e...\"],[\"```\\n\\nWe'll need to find the position of the answer in the encoded input.\\n* `token_type_ids` tells us...\"],[\"...         if match:\\n...             # if match is found, use `token_type_ids` to find where words ...\"],[\"...         else:\\n...             start_positions.append(cls_index)\\n...             end_positions.ap...\"],[\"```\\n\\nNow that we have this preprocessing function, we can encode the entire dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e enco...\"],[\"```\\n\\n## Evaluation\\n\\nEvaluation for document question answering requires a significant amount of post...\"],[\"```\\n\\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, and configur...\"],[\"```\\n\\nTo add the final model to ğŸ¤— Hub, create a model card and call `push_to_hub`:\\n\\n```py\\n\\u003e\\u003e\\u003e trainer...\"],[\"```\\n\\nYou can also manually replicate the results of the pipeline if you'd like:\\n1. Take an image and...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We extract an optimal subset of architectural parame...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- Splitting the embedding matrix into two smaller matrices.\\n- Using repeating layers split among gro...\"],[\"## Usage tips\\n\\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad ...\"],[\"- [`TFAlbertForSequenceClassification`] is supported by this [example script](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`AlbertForMaskedLM`] is supported by this [example script](h...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`AlbertForQuestionAnswering`] is supported by this ...\"],[\"**Multiple choice**\\n\\n- [`AlbertForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002fgit...\"],[\"## AlbertForMultipleChoice\\n\\n[[autodoc]] AlbertForMultipleChoice\\n\\n## AlbertForTokenClassification\\n\\n[[...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmodel_doc\\u002fbig_bird), [BioGpt]...\"],[\"[LayoutLM](..\\u002fmodel_doc\\u002flayoutlm), [LayoutLMv2](..\\u002fmodel_doc\\u002flayoutlmv2), [LayoutLMv3](..\\u002fmodel_doc\\u002f...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThe letter that prefixes each `ner_tag` indicates the token position of the entity:\\n\\n- `B-` ind...\"],[\"```\\n\\nHowever, this adds some special tokens `[CLS]` and `[SEP]` and the subword tokenization creates...\"],[\"...     labels = []\\n...     for i, label in enumerate(examples[f\\\"ner_tags\\\"]):\\n...         word_ids =...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets [`~datasets.Dataset...\"],[\"```\\n\\nGet the NER labels first, and then create a function that passes your true predictions and true...\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your...\"],[\"```\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with the [`Traine...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr...\"],[\"```\\n\\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nPass your inputs to the model and return the `logits`:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import Auto...\"],[\"```\\n\\nGet the class with the highest probability, and use the model's `id2label` mapping to convert i...\"],[\"## Saved Pseudo-Labels\\nThese are the generations of various large models on various large **training...\"],[\"### Available Pseudo-labels\\n| Dataset | Model                       | Link                          ...\"],[\"| CNN\\u002fDM  | `sshleifer\\u002fpegasus-cnn-ft-v2` | [download](https:\\u002f\\u002fcdn-datasets.huggingface.co\\u002fpseudo\\u002fcn...\"],[\"(EN_RO = WMT 2016 English-Romanian).\\n\\nExample Download Command:\\n```bash\\ncurl -S https:\\u002f\\u002fcdn-datasets...\"],[\"```\\n### Generating New Pseudolabels\\nHere is the command I used to generate the pseudolabels in the s...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"If you would like to list benchmark results on your favorite models of the [model hub](https:\\u002f\\u002fhuggi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## How to enable Hyperparameter search in example\\n\\nDefine the hyperparameter search space, diff...\"],[\"```\\n\\nOptuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search` and de...\"],[\"```\\n\\nFor wandb, see wandb [object_parameter](https:\\u002f\\u002fdocs.wandb.ai\\u002fguides\\u002fsweeps\\u002fconfiguration), it'...\"],[\"```\\n\\nCreate a [`Trainer`] with your `model_init` function, training arguments, training and test dat...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image Transformer has recently achieved significant ...\"],[\"```\\n\\nThis will load the model pre-trained on masked image modeling. Note that this won't include the...\"],[\"```\\n\\nThis particular checkpoint was fine-tuned on [RVL-CDIP](https:\\u002f\\u002fwww.cs.cmu.edu\\u002f~aharley\\u002frvl-cdi...\"],[\"urrently the following model proposals are available:\\n\\n- \\u003cs\\u003e[BigBird (Google)](.\\u002fADD_BIG_BIRD.md)\\u003c\\u002fs...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present SegFormer, a simple, efficient yet powerf...\"],[\"- SegFormer consists of a hierarchical Transformer encoder, and a lightweight all-MLP decoder head.\\n...\"],[\"to try out a SegFormer model on custom images.\\n- SegFormer works on any input size, as it pads the i...\"],[\"| **Model variant** | **Depths**    | **Hidden sizes**    | **Decoder hidden size** | **Params (M)**...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"## SegformerConfig\\n\\n[[autodoc]] SegformerConfig\\n\\n## SegformerFeatureExtractor\\n\\n[[autodoc]] Segformer...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Motivated by the success of T5 (Text-To-Text Transfe...\"],[\"## SpeechT5FeatureExtractor\\n\\n[[autodoc]] SpeechT5FeatureExtractor\\n    - __call__\\n\\n## SpeechT5Process...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow, let's load an image.\\n\\n```python\\nfrom PIL import Image\\nimport requests\\n\\nurl = \\\"https:\\u002f\\u002fhugg...\"],[\"```\\n\\nWe can now infer the image by passing pixel values to the model.\\n\\n```python\\nimport torch\\n\\nwith ...\"],[\"```\\n\\nWe need to squeeze the output and get rid of axis 0, clip the values, then convert it to be num...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Entity representations are useful in natural languag...\"],[\"- *Inputting [MASK] entities to compute entity representations*: The [MASK] entity is used to mask e...\"],[\"Usage example:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import LukeTokenizer, LukeModel, LukeForEntityPairCl...\"],[\"\\u003e\\u003e\\u003e model = LukeForEntityPairClassification.from_pretrained(\\\"studio-ousia\\u002fluke-large-finetuned-tacre...\"],[\"```\\n\\n## Resources\\n\\n- [A demo notebook on how to fine-tune [`LukeForEntityPairClassification`] for re...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[BEiT](..\\u002fmodel_doc\\u002fbeit), [BiT](..\\u002fmodel_doc\\u002fbit), [ConvNeXT](..\\u002fmodel_doc\\u002fconvnext), [ConvNeXTV2](...\"],[\"Transformer](..\\u002fmodel_doc\\u002fswin), [Swin Transformer V2](..\\u002fmodel_doc\\u002fswinv2), [VAN](..\\u002fmodel_doc\\u002fvan)...\"],[\"```\\n\\nWe encourage you to log in to your Hugging Face account to upload and share your model with the...\"],[\"```\\n\\nNow you can convert the label id to a label name:\\n\\n```py\\n\\u003e\\u003e\\u003e id2label[str(79)]\\n'prime_rib'\\n```\\n...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets [`~datasets.Dataset...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\n\\nTo avoid overfitting and to make the model ...\"],[\"```\\n\\nNext, create functions to apply appropriate transformations to a batch of images, instead of on...\"],[\"```\\n\\nAs a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike o...\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you set up you...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"\\u003e\\u003e\\u003e trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     data_collator=data_c...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nThen, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\\n\\n```py...\"],[\"```\\n\\nTo compute the accuracy from the predictions and push your model to the ğŸ¤— Hub, use [Keras callb...\"],[\"```\\n\\nFinally, you are ready to train your model! Call `fit()` with your training and validation data...\"],[\"```\\n\\nCongratulations! You have fine-tuned your model and shared it on the ğŸ¤— Hub. You can now use it ...\"],[\"```\\n\\nYou can also manually replicate the results of the `pipeline` if you'd like:\\n\\n\\u003cframeworkcontent...\"],[\"```\\n\\nGet the predicted label with the highest probability, and use the model's `id2label` mapping to...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- The model usually performs well without requiring any finetuning.\\n- The architectur...\"],[\"```\\nThe script will automatically determine all necessary parameters from the OpenAI checkpoint. A `...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"```\\n\\n## WhisperConfig\\n\\n[[autodoc]] WhisperConfig\\n\\n## WhisperTokenizer\\n\\n[[autodoc]] WhisperTokenizer\\n...\"],[\"## FlaxWhisperForAudioClassification\\n\\n[[autodoc]] FlaxWhisperForAudioClassification\\n    - __call__\\n\\n...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import GPTJForCausalLM\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e device = \\\"cuda\\\"\\n\\u003e\\u003e\\u003e mod...\"],[\"```\\n\\n- The model should fit on 16GB GPU for inference. For training\\u002ffine-tuning it would take much m...\"],[\"\\u003e\\u003e\\u003e prompt = (\\n...     \\\"In a shocking finding, scientists discovered a herd of unicorns living in a ...\"],[\"```\\n\\n...or in float16 precision:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import GPTJForCausalLM, AutoTokeni...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"- Description of [GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fEleutherAI\\u002fgpt-j-6B).\\n- A blog on how to [Deploy GPT...\"],[\"- [`GPTJForCausalLM`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgithub....\"],[\"**Documentation resources**\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe `outputs` object is a [`~modeling_outputs.SequenceClassifierOutput`], as we can see in the\\n...\"],[\"```\\n\\nwill return the tuple `(outputs.loss, outputs.logits)` for instance.\\n\\nWhen considering our `out...\"],[\"## NextSentencePredictorOutput\\n\\n[[autodoc]] modeling_outputs.NextSentencePredictorOutput\\n\\n## Sequenc...\"],[\"[[autodoc]] modeling_outputs.Seq2SeqTSPredictionOutput\\n\\n## SampleTSPredictionOutput\\n\\n[[autodoc]] mod...\"],[\"## TFNextSentencePredictorOutput\\n\\n[[autodoc]] modeling_tf_outputs.TFNextSentencePredictorOutput\\n\\n## ...\"],[\"## FlaxCausalLMOutputWithCrossAttentions\\n\\n[[autodoc]] modeling_flax_outputs.FlaxCausalLMOutputWithCr...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog post on how to [Accelerate Large Model Train...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\" \\u002f\\u003e\\n\\n- [`DebertaForTokenClassification`] is supported by...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`DebertaForMaskedLM`] is supported by this [example script](...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`DebertaForQuestionAnswering`] is supported by this...\"],[\"## DebertaForSequenceClassification\\n\\n[[autodoc]] DebertaForSequenceClassification\\n    - forward\\n\\n## ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Matthijs](https:\\u002f\\u002fhuggingface.co\\u002fMatthijs), [Patrick Von Platen](http...\"],[\"```\\n\\n## EncodecConfig\\n\\n[[autodoc]] EncodecConfig\\n\\n## EncodecFeatureExtractor\\n\\n[[autodoc]] EncodecFea...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"emb = jnp.zeros((50264, model.config.hidden_size))\\n# update the first 50257 weights using pre-traine...\"],[\"```\\n\\n\\n### Train Model\\n\\n```bash\\npython run_clm_mp.py \\\\\\n    --model_name_or_path gpt-neo-1.3B  \\\\\\n    -...\"],[\"VisualBERT Demo\\n\\nThis demo shows usage of VisualBERT VQA model and is adapted from LXMERT demo prese...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n1. Most of the checkpoints provided work with the [`VisualBertForPreTraining`] config...\"],[\"- [VisualBERT VQA demo notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002frese...\"],[\"```\\n\\n## VisualBertConfig\\n\\n[[autodoc]] VisualBertConfig\\n\\n## VisualBertModel\\n\\n[[autodoc]] VisualBertMo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have emerged as a powerful tool for a b...\"],[\"[[autodoc]] NystromformerForMaskedLM\\n    - forward\\n\\n## NystromformerForSequenceClassification\\n\\n[[aut...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce the Segment Anything (SA) project: a ne...\"],[\"Below is an example on how to run mask generation given an image and a 2D point:\\n\\n```python\\nimport t...\"],[\"```\\n\\nResources:\\n\\n- [Demo notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexamples\\u002fsegme...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003csmall\\u003e UPerNet framework. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1807.10221\\\"\\u003eoriginal paper\\u003c...\"],[\"```\\n\\nTo use another vision backbone, like [ConvNeXt](convnext), simply instantiate the model with th...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have shown great potential in computer ...\"],[\"This model was contributed by [heytanay](https:\\u002f\\u002fhuggingface.co\\u002fheytanay). The original code can be ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```\\n\\nor directly on the hub under *Training metrics*.\\n\\nsample Metrics - [tfhub.dev](https:\\u002f\\u002ftensorbo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your...\"],[\"### Wrapping policy\\n\\nFSDP is applied by wrapping each layer in the network. The wrapping is usually ...\"],[\"```\\n\\nHowever, when training ends, you want to save the full state dict because sharded state dict is...\"],[\"```\\n\\nThe [`xla_fsdp_settings`](https:\\u002f\\u002fgithub.com\\u002fpytorch\\u002fxla\\u002fblob\\u002f2e6e183e0724818f137c8135b34ef273d...\"],[\"```\\n\\n```bash\\naccelerate launch --fsdp=\\\"full shard\\\" --fsdp_config=\\\"path\\u002fto\\u002ffsdp_config\\u002f my-trainer-sc...\"],[\"Testing new Hugging Face Deep Learning Container.\\n\\nThis document explains the testing strategy for r...\"],[\"```\\nThese tests take around 10-15 minutes to finish. Preferably make a screenshot of the successfull...\"],[\"repository_info:\\n  training_repository: &TRAINING_REPOSITORY\\n    image_type: &TRAINING_IMAGE_TYPE tr...\"],[\"```\\n2. In the PR comment describe what test, we ran and with which package versions. Here you can co...\"],[\"```\\nThese tests take around 10-15 minutes to finish. Preferably make a screenshot of the successfull...\"],[\"repository_info:\\n  training_repository: &TRAINING_REPOSITORY\\n    image_type: &TRAINING_IMAGE_TYPE tr...\"],[\"```\\n2. In the PR comment describe what test we ran and with which framework versions. Here you can c...\"],[\"| ID                                  | Description                                                 ...\"],[\"| tensorflow-transfromers-test-2-smd  | test bert finetuning using BERT from transformer lib+ TF SM ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fpytorch\\u002ffairseq\\u002ftree\\u002fmaster\\u002ffairseq\\u002fmodels\\u002f...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nYou can convert custom code checkpoints to full Transformers checkpoints using the `convert_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Depth estimation from a single image is an important...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained representations are becoming crucial for...\"],[\"[`AlignProcessor`] wraps [`EfficientNetImageProcessor`] and [`BertTokenizer`] into a single instance...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The library was designed with two strong goals in mind:\\n\\n1. Be as easy and fast to use as possible:\\n...\"],[\"2. Provide state-of-the-art models with performances as close as possible to the original models:\\n\\n ...\"],[\"- Easily switch between PyTorch, TensorFlow 2.0 and Flax, allowing training with one framework and i...\"],[\"All these classes can be instantiated from pretrained instances, saved locally, and shared on the Hu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Natural Language Processing\\n\\n\\u003cYoutube id=\\\"Yffk5aydLzg\\\"\\u002f\\u003e\\n\\nThe main tool for preprocessing te...\"],[\"```\\n\\nThe tokenizer returns a dictionary with three important items:\\n\\n* [input_ids](glossary#input-id...\"],[\"```\\n\\nAs you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separa...\"],[\"```\\n\\n### Pad\\n\\nSentences aren't always the same length which can be an issue because tensors, the mod...\"],[\"```\\n\\nThe first and third sentences are now padded with `0`'s because they are shorter.\\n\\n### Truncati...\"],[\"Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by th...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [Padding and truncation](.\\u002fpad_truncation) concept guide to learn more dif...\"],[\"Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:\\n\\n\\u003cframeworkco...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e batch_sentences = [\\n...     \\\"But what about second breakfast?\\\",\\n...     \\\"Do...\"],[\"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Audio\\n\\nFor audio tasks, you'll need a [feature extractor](main_cla...\"],[\"```\\n\\nThis returns three items:\\n\\n* `array` is the speech signal loaded - and potentially resampled - ...\"],[\"```\\n\\nNext, load a feature extractor to normalize and pad the input. When padding textual data, a `0`...\"],[\"```\\n\\nCreate a function to preprocess the dataset so the audio samples are the same lengths. Specify ...\"],[\"```\\n\\n## Computer vision\\n\\nFor computer vision tasks, you'll need an [image processor](main_classes\\u002fim...\"],[\"```\\n\\nNext, take a look at the image with ğŸ¤— Datasets [`Image`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fp...\"],[\"```\\n\\nFirst, let's add some image augmentation. You can use any library you prefer, but in this tutor...\"],[\"```\\n\\n2. The model accepts [`pixel_values`](model_doc\\u002fvisionencoderdecoder#transformers.VisionEncoder...\"],[\"```\\n\\nHere is what the image looks like after the transforms are applied. The image has been randomly...\"],[\"```\\n\\n## Multimodal\\n\\nFor tasks involving multimodal inputs, you'll need a [processor](main_classes\\u002fpr...\"],[\"```\\n\\nNow take a look at the `audio` and `text` columns:\\n\\n```py\\n\\u003e\\u003e\\u003e lj_speech[0][\\\"audio\\\"]\\n{'array': a...\"],[\"```\\n\\n1. Create a function to process the audio data contained in `array` to `input_values`, and toke...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nPrompt engineering is only a part of the LLM output optimization process. Another essential c...\"],[\"\\u003e\\u003e\\u003e generator = pipeline('text-generation', model = 'gpt2')\\n\\u003e\\u003e\\u003e prompt = \\\"Hello, I'm a language mode...\"],[\"```\\n\\nTo run inference with an encoder-decoder, use the `text2text-generation` pipeline:\\n\\n```python\\n\\u003e...\"],[\"```\\n\\nNext, let's load the model with the appropriate pipeline (`\\\"text-generation\\\"`): \\n\\n```python\\n\\u003e\\u003e\\u003e...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNote that Falcon models were trained using the `bfloat16` datatype, so we recommend you ...\"],[\"```\\n\\nAs a result, the output contains a classification label from the list we have provided in the i...\"],[\"```\\n\\nAs you can see, the model correctly identified two named entities from the given text.\\n\\n#### Tr...\"],[\"```\\n\\nHere we've added a `do_sample=True` and `top_k=10` to allow the model to be a bit more flexible...\"],[\"```\\n\\n#### Question answering\\n\\nFor question answering task we can structure the prompt into the follo...\"],[\"```\\n\\n#### Reasoning\\n\\nReasoning is one of the most difficult tasks for LLMs, and achieving good resul...\"],[\"```\\n\\nThis is a wrong answer, it should be 12. In this case, this can be due to the prompt being too ...\"],[\"In few-shot prompting, we provide examples in the prompt giving the model more context to improve th...\"],[\"```\\n\\nIn the above code snippet we used a single example to demonstrate the desired output to the mod...\"],[\"```\\n\\n## Prompting vs fine-tuning\\n\\nYou can achieve great results by optimizing your prompts, however,...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Several TensorFlow methods in ğŸ¤— Transformers have been rewritten to be XLA-compatible, including tex...\"],[\"```\\n\\nThe above model accepts inputs having a dimension of `(10, )`. We can use the model for running...\"],[\"```\\n\\nAnd then you can run the following code:\\n\\n```py\\nimport tensorflow as tf\\nfrom transformers impor...\"],[\"```\\n\\nAs you can notice, enabling XLA on `generate()` is just a single line of code. The rest of the ...\"],[\"# Here, we call the tokenizer with padding options.\\ntokenized_input = tokenizer(input_string, pad_to...\"],[\"```\\n\\nThis way, you can ensure that the inputs to `xla_generate()` will always receive inputs with th...\"],[\"```\\nThe first call to `xla_generate()` is time-consuming because of tracing, but the successive call...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"| Task | Example datasets | Trainer support | ğŸ¤— Accelerate | ğŸ¤— Datasets | Colab\\n|---|---|:---:|:---:...\"],[\"| [**`summarization`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002fsumma...\"],[\"| [**`token-classification`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorc...\"],[\"| [**`multi-lingual speech-recognition`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexa...\"],[\"| [**`image-classification`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorc...\"],[\"## Running quick tests\\n\\nMost examples are equipped with a mechanism to truncate the number of datase...\"],[\"```\\nexamples\\u002fpytorch\\u002ftoken-classification\\u002frun_ner.py \\\\\\n--max_train_samples 50 \\\\\\n--max_eval_samples 5...\"],[\"```\\n\\n## Resuming training\\n\\nYou can resume training from a previous checkpoint like this:\\n\\n1. Pass `-...\"],[\"A few notes on this integration:\\n\\n- you will need to be logged in to the Hugging Face website locall...\"],[\"```\\n\\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) o...\"],[\"```\\n\\nIf you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you c...\"],[\"```\\n\\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) o...\"],[\"```\\n\\nthat will check everything is ready for training. Finally, you can launch training with\\n\\n```bas...\"],[\"```\\n\\nIf you are in Jupyter or Colab, you should login with:\\n\\n```python\\nimport wandb\\nwandb.login()\\n``...\"],[\"```\\n\\nNext, in your model training script, import `NeptuneCallback`:\\n\\n```python\\nfrom transformers.int...\"],[\"```\\n\\nNow, when you start the training with `trainer.train()`, your metadata will be logged in Neptun...\"],[\"```\\n\\n\\nTo enable logging to ClearML, include `\\\"clearml\\\"` in the `report_to` of your `TrainingArgument...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up t...\"],[\"```\\nnvidia-smi topo -m\\n```\\n\\nand it will tell you how the GPUs are inter-connected. On a machine with...\"],[\"```\\n\\nSo the first report `NV2` tells us the GPUs are interconnected with 2 NVLinks, and the second r...\"],[\"Let's compare the execution of a gpt2 language model training over a small sample of wikitext.\\n\\nThe ...\"],[\"```\\n\\nHardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (`NV2` in `nvidia-smi topo -m`)\\nSoftwa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Modern approaches typically formulate semantic segme...\"],[\"## Usage tips\\n\\n-  MaskFormer's Transformer decoder is identical to the decoder of [DETR](detr). Duri...\"],[\"## MaskFormer specific outputs\\n\\n[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerModelOut...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the Transformer architecture has become the de...\"],[\"## ViTHybridImageProcessor\\n\\n[[autodoc]] ViTHybridImageProcessor\\n    - preprocess\\n\\n## ViTHybridModel\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised pre-training techniques have achieve...\"],[\"## Usage tips\\n\\n- In terms of data processing, LayoutLMv3 is identical to its predecessor [LayoutLMv2...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- [`LayoutLMv2ForSequenceClassification`] is supporte...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`LayoutLMv3ForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`LayoutLMv2ForQuestionAnswering`] is supported by t...\"],[\"## TFLayoutLMv3Model\\n\\n[[autodoc]] TFLayoutLMv3Model\\n    - call\\n\\n## TFLayoutLMv3ForSequenceClassifica...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale autoregressive language models such as G...\"],[\"## Resources\\n\\n- [Causal language modeling task guide](..\\u002ftasks\\u002flanguage_modeling)\\n\\n## XGLMConfig\\n\\n[[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This is a machine translation model that supports many low-resource languages, and that is competiti...\"],[\"```\\n\\nGoogle has released the following variants:\\n\\n- [google\\u002fmadlad400-3b-mt](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"def preprocess(self, inputs, maybe_arg=2):\\n        model_input = Tensor(inputs[\\\"input_ids\\\"])\\n       ...\"],[\"```\\n\\nThe structure of this breakdown is to support relatively seamless support for CPU\\u002fGPU, while su...\"],[\"\\u003e\\u003e\\u003e pipe(\\\"This is a test\\\", top_k=2)\\n[{\\\"label\\\": \\\"1-star\\\", \\\"score\\\": 0.8}, {\\\"label\\\": \\\"2-star\\\", \\\"score\\\":...\"],[\"```\\n\\nIn order to achieve that, we'll update our `postprocess` method with a default parameter to `5`...\"],[\"```\\n\\nYou can specify a default model if you want, in which case it should come with a specific revis...\"],[\"```\\n\\n## Share your pipeline on the Hub\\n\\nTo share your custom pipeline on the Hub, you just have to s...\"],[\"```\\n\\nThe implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If w...\"],[\"```\\n\\n## Add the pipeline to ğŸ¤— Transformers\\n\\nIf you want to contribute your pipeline to ğŸ¤— Transformer...\"],[\"Training a masked language model end-to-end from scratch on TPUs\\n\\nIn this example, we're going to de...\"],[\"## Setting up a TPU-VM\\n\\nSince this example focuses on using TPUs, the first step is to set up access...\"],[\"```\\n\\nThe script will automatically load the `train` split of the WikiText dataset and train a [Unigr...\"],[\"```\\n\\n**Notes**:\\n\\n* While running the above script, you need to specify the `split` accordingly. The ...\"],[\"```\\n\\nIf you had specified a `hub_model_id` while launching training, then your model will be pushed ...\"],[\"```\\n\\nYou can also try out inference using the [Inference Widget](https:\\u002f\\u002fhuggingface.co\\u002ftf-tpu\\u002frober...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n```bash\\nDatasetDict({\\n    train: Dataset({\\n        features: ['image', 'text'],\\n        num_rows...\"],[\"```\\n    \\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface...\"],[\"```\\n\\n## Evaluate\\n\\nImage captioning models are typically evaluated with the [Rouge Score](https:\\u002f\\u002fhug...\"],[\"```\\n\\nThen pass them along with the datasets and the model to ğŸ¤— Trainer. \\n\\n```python\\ntrainer = Traine...\"],[\"!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved.\\n\\nLicensed under the MIT L...\"],[\"This model was contributed by [clefourrier](https:\\u002f\\u002fhuggingface.co\\u002fclefourrier). The original code c...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Can Transformer perform 2D object- and region-level ...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Masked language modeling (MLM) pretraining methods s...\"],[\"## Usage tips\\n\\n- ELECTRA is the pretraining approach, therefore there is nearly no changes done to t...\"],[\"## ElectraConfig\\n\\n[[autodoc]] ElectraConfig\\n\\n## ElectraTokenizer\\n\\n[[autodoc]] ElectraTokenizer\\n\\n## E...\"],[\"## TFElectraForMultipleChoice\\n\\n[[autodoc]] TFElectraForMultipleChoice\\n    - call\\n\\n## TFElectraForTok...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present a conceptually simple and e...\"],[\"The [`AltCLIPProcessor`] wraps a [`CLIPImageProcessor`] and a [`XLMRobertaTokenizer`] into a single ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThis model is based on `CLIPModel`, use it like you would use the original [CLIP](clip)....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Tips:\\n\\n- The architecture is similar than llava architecture except that the multi-modal projector t...\"],[\"```\\n\\nFor multiple turns conversation:\\n\\n```bash\\nA chat between a curious human and an artificial inte...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nOnce the installation is done, you can use the CLI command `add-new-model` to generate your mod...\"],[\"```\\n--\\u003e\\n\\nOnce the command has finished, you should have a total of 7 new files spread across the rep...\"],[\"```\\n\\nFeel free to modify each file to mimic the behavior of your model. \\n\\nâš  You should be careful ab...\"],[\"```\\n\\nThis will start a small questionnaire you have to fill.\\n\\n```\\nWhat identifier would you like to ...\"],[\"```\\nWill your new model use the same processing class as Xxx (XxxTokenizer\\u002fXxxFeatureExtractor\\u002fXxxIm...\"],[\"```\\n\\nâš  You should be careful about the classes preceded by the following line:ï¸ \\n\\n```python\\n# Copied...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] get_cosine_with_hard_restarts_schedule_with_warmup\\n\\n\\u003cimg alt=\\\"\\\" src=\\\"https:\\u002f\\u002fhuggingface...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce Kosmos-2, a Multimodal Large Language M...\"],[\"## Example\\n\\n```python\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import requests\\n\\u003e\\u003e\\u003e from transformers import Aut...\"],[\"\\u003e\\u003e\\u003e caption, entities = processor.post_process_generation(generated_text)\\n\\u003e\\u003e\\u003e caption\\n'An image of a...\"],[\"```\\n\\nThis model was contributed by [Yih-Dar SHIEH](https:\\u002f\\u002fhuggingface.co\\u002fydshieh). The original cod...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- information extraction from scanned documents: the [FUNSD](https:\\u002f\\u002fguillaumejaume.github.io\\u002fFUNSD\\u002f...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training of text and layout has proved effective...\"],[\"```\\npython -m pip install 'git+https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002fdetectron2.git'\\npython -m pip ins...\"],[\"```\\n(If you are developing for LayoutLMv2, note that passing the doctests also requires the installa...\"],[\"- The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates visual embed...\"],[\"of the input text tokens. This is identical to [`LayoutLMModel`]. These can be obtained using an\\n  e...\"],[\"```\\n\\nHere, `width` and `height` correspond to the width and height of the original document in which...\"],[\"```\\n\\nHowever, this model includes a brand new [`~transformers.LayoutLMv2Processor`] which can be use...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune LayoutLMv2 for text-...\"],[\"## Usage: LayoutLMv2Processor\\n\\nThe easiest way to prepare data for the model is to use [`LayoutLMv2P...\"],[\"```\\n\\nIn short, one can provide a document image (and possibly additional data) to [`LayoutLMv2Proces...\"],[\"```python\\nfrom transformers import LayoutLMv2Processor\\nfrom PIL import Image\\n\\nprocessor = LayoutLMv2...\"],[\"```\\n\\n**Use case 2: document image classification (training, inference) + token classification (infer...\"],[\"```\\n\\n**Use case 3: token classification (training), apply_ocr=False**\\n\\nFor token classification task...\"],[\"```\\n\\n**Use case 4: visual question answering (inference), apply_ocr=True**\\n\\nFor visual question answ...\"],[\"```\\n\\n**Use case 5: visual question answering (inference), apply_ocr=False**\\n\\nFor visual question ans...\"],[\"```\\n\\n## LayoutLMv2Config\\n\\n[[autodoc]] LayoutLMv2Config\\n\\n## LayoutLMv2FeatureExtractor\\n\\n[[autodoc]] L...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n**Note:** This script only works with models that have a fast tokenizer (backed by the ğŸ¤— Tokeni...\"],[\"```\\n\\nthen\\n\\n```bash\\nexport TASK_NAME=ner\\n\\npython run_ner_no_trainer.py \\\\\\n  --model_name_or_path bert-...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper:\\n\\n\\n*Inductive transfer learning, enabled by self-supervised learning, have...\"],[\"## BarthezTokenizer\\n\\n[[autodoc]] BarthezTokenizer\\n\\n## BarthezTokenizerFast\\n\\n[[autodoc]] BarthezToken...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In recent years, a series of Transformer-based model...\"],[\"\\u003e\\u003e\\u003e # HerBERT can also be loaded using AutoTokenizer and AutoModel:\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transf...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nHerbert implementation is the same as `BERT` except for the tokenization method. Refer t...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [anton-l](https:\\u002f\\u002fhuggingface.co\\u002fanton-l).\\n\\n## Usage tips\\n\\n- SEW is a ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- accessing all the hidden-states of BERT\\u002fGPT\\u002fGPT-2,\\n- accessing all the attention weights for each ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are unable to process long ...\"],[\"## Longformer Self Attention\\n\\nLongformer self attention employs self attention on both a \\\"local\\\" con...\"],[\"For more information, please refer to the official [paper](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2004.05150.pdf).\\n\\n\\n...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput\\n\\n[[aut...\"],[\"## TFLongformerForMultipleChoice\\n\\n[[autodoc]] TFLongformerForMultipleChoice\\n    - call\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002ffram...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```\\n\\nor directly on the hub under *Training metrics*.\\n\\nTraining with the previously defined hyper-pa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolv...\"],[\"[[autodoc]] TvltProcessor\\n    - __call__\\n\\n## TvltImageProcessor\\n\\n[[autodoc]] TvltImageProcessor\\n    ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: ...\"],[\"## Usage tips\\n\\n- DistilBERT doesn't have `token_type_ids`, you don't need to indicate which token be...\"],[\"- A blog post on [Getting Started with Sentiment Analysis using Python](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002f...\"],[\"- [`TFDistilBertForSequenceClassification`] is supported by this [example script](https:\\u002f\\u002fgithub.com...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`DistilBertForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`DistilBertForMaskedLM`] is supported by this [example scrip...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`DistilBertForQuestionAnswering`] is supported by t...\"],[\"**Multiple choice**\\n- [`DistilBertForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002f...\"],[\"ğŸš€ Deploy\\n\\n- A blog post on how to [deploy DistilBERT on Google Cloud](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fho...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n\\n## DistilBertConfig\\n\\n[[autodoc]] DistilBertConfig\\n\\n## DistilBertTokenizer\\n\\n[[autodoc]] DistilB...\"],[\"## FlaxDistilBertForMaskedLM\\n\\n[[autodoc]] FlaxDistilBertForMaskedLM\\n    - __call__\\n\\n## FlaxDistilBer...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, e...\"],[\"At the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parame...\"],[\"To give some examples of how much VRAM it roughly takes to load a model in bfloat16:\\n\\n-   **GPT3** r...\"],[\"Naive pipeline parallelism is supported out of the box. For this, simply load the model with `device...\"],[\"```\\n```python\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretr...\"],[\"```\\n\\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be q...\"],[\"$$ Y = X * \\\\text{dequantize}(W) $$\\n\\nfor every matrix multiplication. Dequantization and re-quantizat...\"],[\"```\\n\\nOverall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32...\"],[\"## 2. Flash Attention\\n\\nToday's top-performing LLMs share more or less the same fundamental architect...\"],[\"with \\\\\\\\( s^a_{ij} \\\\\\\\) and \\\\\\\\( s^b_{ij} \\\\\\\\) being some softmax normalization statistics that need to ...\"],[\"```\\nFor demonstration purposes, we duplicate the system prompt by ten so that the input length is lo...\"],[\"```\\n\\n**Output**:\\n```bash\\n37.668193340301514\\n```\\n\\nAs we can see the peak GPU memory requirement is no...\"],[\"```\\n\\nFor more information on how to use Flash Attention, please have a look at [this doc page](https...\"],[\"```\\n\\n**Output**:\\n```\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 20\\nshape of inp...\"],[\"```\\n\\nAs one can see, when using the key-value cache the text input tokens are *not* increased in len...\"],[\"```\\n\\nIn this chat, the LLM runs auto-regressive decoding twice:\\n  1. The first time, the key-value c...\"],[\"```python\\n# Generation as usual\\nprompt = system_prompt + \\\"Question: Please write a function in Pytho...\"],[\"```\\n\\n**Output**:\\n```\\n is a modified version of the function that returns Mega bytes instead.\\n\\ndef by...\"],[\"```\\n\\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires ...\"],[\"The important part to understand here is that reducing the number of key-value attention heads to 1 ...\"],[\"GQA was only recently proposed which is why there is less adoption at the time of writing this noteb...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- [`LEDForConditionalGeneration`] is an extension of\\n  [`BartForConditionalGeneration...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten).\\n\\n## Resou...\"],[\"[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput\\n\\n[[autodoc]] models.led.modeling_tf_l...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper states the following:\\n\\n*Visual language such as charts and plots is ubiqui...\"],[\"- `google\\u002fdeplot`: DePlot fine-tuned on ChartQA dataset \\n\\n\\n```python\\nfrom transformers import AutoPr...\"],[\"```\\n\\n## Fine-tuning\\n\\nTo fine-tune DePlot, refer to the pix2struct [fine-tuning notebook](https:\\u002f\\u002fgit...\"],[\"*NOTE**: This example is outdated and is not longer actively maintained. Please \\nfollow the new inst...\"],[\"```bash\\n#!\\u002fusr\\u002fbin\\u002fenv bash\\npython run_asr.py \\\\\\n--output_dir=\\\".\\u002fwav2vec2-base-timit-asr\\\" \\\\\\n--num_tra...\"],[\"```\\n\\nThe resulting model and inference examples can be found [here](https:\\u002f\\u002fhuggingface.co\\u002felgeish\\u002fw...\"],[\"```\\n\\nThe instance above is used as follows:\\n* creates a tokenizer with `do_lower_case=True` (ignores...\"],[\"```bash\\n#!\\u002fusr\\u002fbin\\u002fenv bash\\npython run_asr.py \\\\\\n--output_dir=\\\".\\u002fwav2vec2-large-xlsr-53-arabic-speech...\"],[\"```\\n\\nFirst, let's understand how this dataset represents Arabic text; it uses a format called\\n[Buckw...\"],[\"```\\n\\nThe instance above is used as follows:\\n* creates a tokenizer with Buckwalter vocabulary and `wo...\"],[\"```\\nPYTHONPATH=..\\u002f..\\u002f..\\u002fsrc deepspeed --num_gpus 2 \\\\\\nrun_asr.py \\\\\\n--output_dir=output_dir --num_trai...\"],[\"```\\n    \\\"zero_optimization\\\": {\\n        ...\\n        \\\"find_unused_parameters\\\": true,\\n        ...\\n    }...\"],[\"```\\nPYTHONPATH=..\\u002f..\\u002f..\\u002fsrc deepspeed --num_gpus 4 run_pretrain.py \\\\\\n--output_dir=\\\".\\u002fwav2vec2-base-l...\"],[\"```\\n\\n\\n### Forced Alignment\\n\\nCharacter level forced alignment for audio and text pairs with wav2vec2 ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multilingual pre-trained models are known to suffer ...\"],[\"```\\n\\n2. By explicitly passing the index of the language adapter for each sample:\\n\\n```python\\nimport t...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\nAll the methods of this logging module are documented below, the main ones are\\n[`logging.get_v...\"],[\"See reference of the `captureWarnings` method below.\\n\\n[[autodoc]] logging.captureWarnings\\n\\n## Base s...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- [`DefaultFlowCallback`] which handles the default behavior for logging, saving and evaluation.\\n- [...\"],[\"If a package is installed but you don't wish to use the accompanying integration, you can change `Tr...\"],[\"def on_train_begin(self, args, state, control, **kwargs):\\n        print(\\\"Starting training\\\")\\n\\n\\ntrain...\"],[\"```\\n\\nAnother way to register a callback is to call `trainer.add_callback()` as follows:\\n\\n```python\\nt...\"],[\"ow to add BigBird to ğŸ¤— Transformers?\\n=====================================\\n\\nMentor: [Patrick](https:...\"],[\"-   Composition is generally favored over abstraction\\n-   Duplicating code is not always bad if it s...\"],[\"10. [ ] Run end-to-end integration tests\\n\\n11. [ ] Finished docs\\n\\n12. [ ] Uploaded model weights to t...\"],[\"After you feel like you have gotten a good overview of the architecture\\nof the model, you might want...\"],[\"#### Make sure you've understood the fundamental aspects of BigBird\\n\\nAlright, now you should be read...\"],[\"```\\n\\n3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    `...\"],[\"```\\n\\nNow you have set up a development environment to port *BigBird*\\nto ğŸ¤— Transformers.\\n\\n### Run a p...\"],[\"It is very important that before you start the porting process, that you\\ncan **efficiently** debug c...\"],[\"```python\\nfrom bigbird.core import modeling\\nmodel = modeling.BertModel(bert_config)\\nfrom bigbird.cor...\"],[\"No matter which strategy you choose, the recommended procedure is often\\nthe same in that you should ...\"],[\"```\\n\\nWe expect that every model added to ğŸ¤— Transformers passes a couple of\\nintegration tests, meanin...\"],[\"#### (Important) More details on how to create a debugging environment for BigBird \\n\\n- BigBird has m...\"],[\"Otherwise, let's start generating a new model with the amazing\\nCookiecutter!\\n\\n**Use the Cookiecutter...\"],[\"```\\n    git checkout -b add_big_bird\\n```\\n\\n2.  Commit the automatically generated code:\\n\\n```\\n    git ...\"],[\"```\\n\\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\\n    Click on \\\"Pull reques...\"],[\"Now you can finally start coding :). The generated code in\\n`src\\u002ftransformers\\u002fmodels\\u002fbig_bird\\u002fmodelin...\"],[\"```\\n\\nThe above command will create a model according to the default\\nparameters as defined in `BigBir...\"],[\"You can copy paste the conversion function into `modeling_big_bird.py` and then adapt it \\nto your ne...\"],[\"```\\n\\nIf either the shape or the name doesn't match, you probably assigned\\nthe wrong checkpoint weigh...\"],[\"```\\n\\n**7. Implement the forward pass**\\n\\nHaving managed to correctly load the pretrained weights into...\"],[\"```\\n\\nIt is very likely that the ğŸ¤— Transformers implementation and the\\noriginal model implementation ...\"],[\"The best way to fix the problem is usually to look at the forward pass\\nof the original implementatio...\"],[\"```\\n\\nHaving fixed all common tests, it is now crucial to ensure that all the\\nnice work you have done...\"],[\"```\\n\\n**Note**: In case you are using Windows, you should replace `RUN_SLOW=1` with\\n`SET RUN_SLOW=1`\\n...\"],[\"```\\n\\nTo ensure that the tokenizer works correctly, it is recommended to first\\ncreate a script in the...\"],[\"```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer\\ntest file should also ...\"],[\"Next, make sure that the docstring added to\\n`src\\u002ftransformers\\u002fmodels\\u002fbig_bird\\u002fmodeling_big_bird.py` ...\"],[\"```\\n\\nThere are a couple of other very strict design tests in ğŸ¤— Transformers\\nthat might still be fail...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The BigCode project is an open-scientific collaborat...\"],[\"The model is an optimized [GPT2 model](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2) with...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is a expected speedup diagram that compares pure inference time be...\"],[\"Examples\\nIn this folder we showcase some examples to use code models for downstream tasks.\\n\\n## Compl...\"],[\"```\\n\\n## Code generation: text to python\\nIn this task we want to train a model to generate code from ...\"],[\"```\\n\\n## Code explanation: python to text\\nIn this task we want to train a model to explain python cod...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [yuvalkirstain](https:\\u002f\\u002fhuggingface.co\\u002fyuvalkirstain) and [oriram](htt...\"],[\"## SplinterTokenizerFast\\n\\n[[autodoc]] SplinterTokenizerFast\\n\\n## SplinterModel\\n\\n[[autodoc]] SplinterM...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They ...\"],[\"## Usage tips\\n\\n- This model only supports inference. This is for a few reasons, mostly because it re...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self atten...\"],[\"## Usage tips\\n\\n- One can use the [`AutoImageProcessor`] API to prepare images for the model.\\n- NAT c...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"```\\n\\nFor multiple turns conversation:\\n\\n```bash\\n\\\"USER: \\u003cimage\\u003e\\\\n\\u003cprompt1\\u003eASSISTANT: \\u003canswer1\\u003eUSER: \\u003cp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*General-purpose language models that can solve vario...\"],[\"## Usage tips\\n\\nInstructBLIP uses the same architecture as [BLIP-2](blip2) with a tiny but important ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou will then be able to use the auto classes like you would usually do!\\n\\n\\u003cTip warning={true}\\u003e\\n...\"],[\"[[autodoc]] AutoModelForCausalLM\\n\\n### TFAutoModelForCausalLM\\n\\n[[autodoc]] TFAutoModelForCausalLM\\n\\n##...\"],[\"### FlaxAutoModelForMultipleChoice\\n\\n[[autodoc]] FlaxAutoModelForMultipleChoice\\n\\n### AutoModelForNext...\"],[\"### TFAutoModelForImageClassification\\n\\n[[autodoc]] TFAutoModelForImageClassification\\n\\n### FlaxAutoMo...\"],[\"## Audio\\n\\nThe following auto classes are available for the following audio tasks.\\n\\n### AutoModelForA...\"],[\"### AutoModelForVisualQuestionAnswering\\n\\n[[autodoc]] AutoModelForVisualQuestionAnswering\\n\\n### AutoMo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [camembert](https:\\u002f\\u002fhuggingface.co\\u002fcamembert). The original code can b...\"],[\"[[autodoc]] CamembertForQuestionAnswering\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFCamembertModel\\n\\n[[autodoc]] TFCamembertM...\"],[\"# Token classification\\n\\nBased on the scripts [`run_ner.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransform...\"],[\"```\\n\\nThe GermEval 2014 dataset contains some strange \\\"control character\\\" tokens like `'\\\\x96', '\\\\u200...\"],[\"```\\n\\n#### Run the Pytorch version\\n\\nTo start training, just run:\\n\\n```bash\\npython3 run_ner.py --data_d...\"],[\"```\\n\\nIt must be saved with a `.json` extension and can be used by running `python3 run_ner.py config...\"],[\"```\\n\\n#### Run the Tensorflow 2 version\\n\\nTo start training, just run:\\n\\n```bash\\npython3 run_tf_ner.py ...\"],[\"```\\n\\nOn the test dataset the following results could be achieved:\\n```bash\\n           precision    re...\"],[\"```\\n\\n### Emerging and Rare Entities task: WNUTâ€™17 (English NER) dataset\\n\\nDescription of the WNUTâ€™17 ...\"],[\"```\\n\\nLet's define some variables that we need for further pre-processing steps:\\n\\n```bash\\nexport MAX_...\"],[\"```\\n\\n#### Run the Pytorch version\\n\\nFine-tuning with the PyTorch version can be started using the `ru...\"],[\"```\\n\\nOn the test dataset the following results could be achieved:\\n\\n```bash\\n05\\u002f29\\u002f2020 23:33:44 - INF...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nTake a look at an example now:\\n\\n```py\\n\\u003e\\u003e\\u003e minds[\\\"train\\\"][0]\\n{'audio': {'array': array([ 0.     ...\"],[\"```\\n\\nThe MInDS-14 dataset has a sampling rate of 8000khz (you can find this information in it's [dat...\"],[\"```\\n\\nNow create a preprocessing function that:\\n\\n1. Calls the `audio` column to load, and if necessar...\"],[\"```\\n\\n## Evaluate\\n\\nIncluding a metric during training is often helpful for evaluating your model's pe...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cTip\\u003e\\n\\nFor a more in-depth example of how to finetune a model for aud...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nGet the class with the highest probability, and use the model's `id2label` mapping to convert i...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoModel, AutoTokenizer\\n\\n\\u003e\\u003e\\u003e bertweet = Aut...\"],[\"```\\n\\n\\u003cTip\\u003e \\n\\nThis implementation is the same as BERT, except for tokenization method. Refer to [BERT...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The process of selecting output tokens to generate text is known as decoding, and you can customize ...\"],[\"```\\n\\nPrinting out the `model.generation_config` reveals only the values that are different from the ...\"],[\"```\\n\\nEven if the default decoding strategy mostly works for your task, you can still tweak a few thi...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoModelForCausalLM, GenerationConfig\\n\\n\\u003e\\u003e\\u003e model = AutoModel...\"],[\"```\\n\\nYou can also store several generation configurations in a single directory, making use of the `...\"],[\"\\u003e\\u003e\\u003e # You could then use the named generation config file to parameterize generation\\n\\u003e\\u003e\\u003e generation_...\"],[\"```\\n\\n## Streaming\\n\\nThe `generate()` supports streaming, through its `streamer` input. The `streamer`...\"],[\"```\\n\\n## Decoding strategies\\n\\nCertain combinations of the `generate()` parameters, and ultimately `ge...\"],[\"```\\n\\n### Contrastive search\\n\\nThe contrastive search decoding strategy was proposed in the 2022 paper...\"],[\"```\\n\\n### Multinomial sampling\\n\\nAs opposed to greedy search that always chooses a token with the high...\"],[\"```\\n\\n### Beam-search decoding\\n\\nUnlike greedy search, beam-search decoding keeps several hypotheses a...\"],[\"```\\n\\n### Beam-search multinomial sampling\\n\\nAs the name implies, this decoding strategy combines beam...\"],[\"```\\n\\n### Diverse beam search decoding\\n\\nThe diverse beam search decoding strategy is an extension of ...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(checkpoint)\\n\\u003e\\u003e\\u003e inputs = tokenizer(prompt, return_tens...\"],[\"```\\n\\nThis guide illustrates the main parameters that enable various decoding strategies. More advanc...\"],[\"```\\n\\nWhen using assisted decoding with sampling methods, you can use the `temperature` argument to c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you want to use a specific model from the [hub](https:\\u002f\\u002fhuggingface.co) you can ignore the t...\"],[\"```\\n\\nFor ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\np...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nHowever, this is not automatically a win for performance. It can be eithe...\"],[\"```\\n\\nExample where it's most a slowdown:\\n\\n```python\\nclass MyDataset(Dataset):\\n    def __len__(self):...\"],[\"```\\n------------------------------\\nStreaming no batching\\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ...\"],[\"```\\n\\nThere are no good (general) solutions for this problem, and your mileage may vary depending on ...\"],[\"```\\n\\nThis should be very transparent to your code because the pipelines are used in\\nthe same way.\\n\\nT...\"],[\"```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Imple...\"],[\"### ConversationalPipeline\\n\\n[[autodoc]] Conversation\\n\\n[[autodoc]] ConversationalPipeline\\n    - __cal...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image segmentation is usually addressed by training ...\"],[\"## Usage tips\\n\\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latt...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a new language representation model cal...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by ğŸŒ) resources to help you g...\"],[\"- A blog post on [BERT Text Classification in a different language](https:\\u002f\\u002fwww.philschmid.de\\u002fbert-t...\"],[\"- [`FlaxBertForSequenceClassification`] is supported by this [example script](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e...\"],[\"- A blog post on how to use [Hugging Face Transformers with Keras: Fine-tune a non-English BERT for ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`BertForMaskedLM`] is supported by this [example script](htt...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`BertForQuestionAnswering`] is supported by this [e...\"],[\"**Multiple choice**\\n- [`BertForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002fgithub...\"],[\"ğŸš€ **Deploy**\\n- A blog post on how to [Convert Transformers to ONNX with Hugging Face Optimum](https:...\"],[\"[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput\\n\\n[[autodoc]] models.bert.modeling_tf_...\"],[\"## TFBertForMultipleChoice\\n\\n[[autodoc]] TFBertForMultipleChoice\\n    - call\\n\\n## TFBertForTokenClassif...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Exporting a model requires two things:\\n\\n- model instantiation with the `torchscript` flag\\n- a forwar...\"],[\"```\\n`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension...\"],[\"```\\n\\nWe recommended you trace the model with a dummy input size at least as large as the\\nlargest inp...\"],[\"# Instantiating the model\\nmodel = BertModel(config)\\n\\n# The model needs to be in evaluation mode\\nmode...\"],[\"```\\n\\n### Loading a model\\n\\nNow you can load the previously saved `BertModel`, `traced_bert.pt`, from ...\"],[\"```\\n\\n## Deploy Hugging Face TorchScript models to AWS with the Neuron SDK\\n\\nAWS introduced the [Amazo...\"],[\"### Implications\\n\\nTransformers models based on the [BERT (Bidirectional Encoder Representations from...\"],[\"```python\\nfrom transformers import BertModel, BertTokenizer, BertConfig\\nimport torch\\nimport torch.ne...\"],[\"```\\n\\nYou only need to modify the following line:\\n\\n```diff\\n- torch.jit.trace(model, [tokens_tensor, s...\"]],\"hovertemplate\":\"source=transformers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"transformers, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"transformers, circle\",\"showlegend\":true,\"x\":[-3.7743423,0.31286585,-2.8584716,-3.079411,-2.4848566,-1.7534282,-1.8156812,-2.5167723,-3.974793,-3.8233027,-4.91691,0.76112366,-9.806279,-2.3951716,-10.295712,-10.069481,-0.5017477,-0.44445416,-0.08129914,-0.62532574,0.1526404,-0.4564885,0.14746316,-9.943618,-0.21654065,-9.926118,6.8179383,7.4568005,8.015871,7.8748364,8.062518,8.131334,7.928809,8.169195,8.343492,7.8837314,17.562016,17.562151,7.379245,7.9610224,-7.8820496,7.9984965,8.072713,7.737121,-2.9478862,7.8043404,-0.7376147,7.9424562,-2.863631,-2.8747702,8.392143,8.594844,8.116847,8.177269,8.06434,8.159368,7.654668,8.041621,1.1539344,1.1870728,7.4337564,8.034198,7.9668374,-12.838413,-12.859262,-13.20126,-12.927319,-12.838477,-12.895109,-12.917469,-12.598608,-12.672701,-12.788397,-12.742758,-12.999351,-12.641673,-12.555359,-13.307042,-12.667552,-12.812781,-12.9387665,-12.686148,-12.794473,-12.511031,-12.306725,-12.303609,-12.598646,-12.669815,-12.744349,-13.123709,-12.882916,-12.692207,-12.650539,-12.837331,-12.7953005,-12.377608,-12.919729,-12.8047695,-12.243218,-12.918961,-12.710296,-12.34524,-12.969265,-12.952169,-13.054837,-12.666737,-12.618303,-13.277045,-12.4896,-12.87717,-12.35399,-12.357961,-12.620912,-12.674262,-12.832606,-12.507656,-12.535648,-12.569023,-12.815356,-12.83608,-12.762157,-12.865051,-12.840593,5.052211,8.313771,8.154082,7.940748,-11.11709,2.7738643,3.3756893,3.6543171,-1.4459285,1.2178007,1.1237752,1.5514244,3.80864,4.1862793,-10.068657,-10.149895,-9.04069,-10.103062,-9.479463,-6.790007,-6.0301456,-2.3828568,-9.604425,-9.623177,-9.599452,-9.647283,-9.909194,-8.538214,-2.410987,-5.0437202,0.483832,6.709247,-6.2389708,-6.436672,-6.365136,-5.695553,-6.175628,-6.647714,-6.5680637,-6.697327,-6.6272597,-6.3076587,-6.322223,-11.143333,7.600469,9.1388,-9.552912,-9.847002,-6.839375,-10.032569,9.645574,-6.682723,-6.9848623,1.0954621,-6.4865737,-6.5248303,-1.2335025,-9.394517,-3.691002,-6.1127934,-6.69923,-6.459081,-6.6019773,-6.631526,-10.665729,7.7486715,-9.906658,-6.2884955,-3.8290112,-5.7357774,-5.760302,-6.5860605,7.2418523,-2.3150203,-1.6542382,-8.462957,-8.622659,-7.1528044,-2.3621054,-6.8028307,-4.7416635,-1.312348,-4.638317,-4.021142,-4.0631275,-2.649721,-1.255228,-3.4036918,-4.0917177,-3.0662162,1.118929,-2.7681336,0.2549039,-1.265872,-1.840512,-1.2437266,-3.035652,-1.7204283,-1.4582462,-2.7008998,3.819101,4.1453824,-3.0178785,-2.2437632,-1.1268219,-1.1942303,-1.188098,-0.8512889,8.374839,-1.2887602,-1.7451959,-1.4666979,-2.0447748,-1.9772598,-1.3546368,-2.6158152,-2.9539795,-1.9952401,-0.085241295,1.7818847,-3.4016855,-3.981904,-6.137141,-4.1262765,-0.22554077,6.912328,-6.3648205,-4.648794,-3.0414107,1.4271374,-6.4629493,-2.4254827,7.586689,-3.9117496,-6.354279,-7.228779,-7.175648,-2.4425733,-5.059108,-5.126023,-4.7898846,-2.5801494,-2.5018008,-3.6495173,-10.061744,0.8660551,2.3488793,3.4742658,1.0471368,-1.305057,-2.2524924,-1.7552677,0.12541817,0.23671071,0.7871587,0.76349545,-0.21262646,-2.6306224,-0.877011,0.58663875,-0.58585155,-9.625502,-1.1819528,-6.9299874,-6.7532306,-5.4562283,-6.7320504,-6.0198708,-5.581272,7.6339827,7.412754,8.57278,7.938827,8.27498,7.90896,8.155826,7.9099174,7.8810735,17.560783,17.562706,7.2971735,7.940974,8.267459,-2.8632755,8.66033,8.539398,1.164383,7.281916,8.22308,-12.808901,-12.861278,-13.226126,-12.875058,-12.821252,-12.81676,-12.830849,-12.733952,-12.72014,-13.036368,-12.884679,-12.932408,-12.56193,-12.676886,-13.078723,-12.691524,-12.543731,-13.090175,-12.917086,-12.807752,-12.3460865,-12.415757,-12.3706045,-12.658638,-12.530432,-12.723922,-13.143656,-12.860162,-12.885521,-12.651409,-12.86529,-12.8741,-12.542062,-12.385044,-12.711233,-12.762524,-12.7673235,-12.362106,-12.974335,-12.754413,-12.336894,-12.853173,-12.899174,-12.940756,-13.110426,-12.638786,-12.5711155,-13.345844,-12.520502,-12.807318,-12.803177,-12.279354,-12.635265,-12.737079,-12.579599,-12.850956,-12.6004095,-12.715184,-12.560915,-12.793381,-12.818432,-12.734695,-12.884003,-12.88361,-12.859225,8.482104,-11.187194,-5.1263685,-4.2928214,-4.1167564,-4.5602207,-4.0937915,-3.215011,-7.8587637,-5.023085,-3.9330394,-3.5375974,-3.533802,-4.259236,-4.048261,-3.7777257,-3.8276148,-3.791368,-3.873814,-3.8496194,-4.2251678,-3.9372802,-4.6736965,-4.8898425,-3.692605,-5.3573604,-4.17624,-4.315009,-4.3240914,-7.212831,-7.3134203,-3.6023166,-6.2785554,-2.9936028,-2.452402,-7.1041665,-6.6686664,-7.966031,-5.690621,-6.0365024,-4.792732,-4.270855,-2.3803566,-2.3682497,-9.79043,-4.4409895,-10.359603,-10.049403,-9.480192,4.980169,-7.3034186,-5.345637,-6.3114557,-6.2162113,-6.04767,-6.1649046,-5.390709,-2.4637945,-10.514927,-10.375923,-5.937845,-3.8937633,-4.008755,-4.113477,-0.0893915,-10.371466,-10.36599,-9.067553,-7.0269814,-8.619756,-8.415586,-8.449434,-9.103874,-8.878702,-8.837758,-7.079997,7.90998,-2.4213371,-2.4092894,-2.3894193,-2.3800952,-6.249998,-6.694148,-5.9590807,-10.256853,-10.256147,-9.091206,-7.2296553,-2.7918892,-6.736087,-6.325129,-6.1978636,-2.1471095,-2.439449,-2.2103512,-0.9223919,-1.1856229,-0.9948284,-10.472311,-10.383686,-2.5646846,-3.528619,-2.222937,-9.524026,-9.3011055,-3.1639285,-3.1587858,-5.7953115,-2.6420732,-3.2054594,-2.8353052,-3.1416123,-2.6726475,-1.1850265,-1.140006,-1.4492112,-1.5015068,-1.389128,-0.62759066,-1.9644945,-1.584683,-1.7605373,-7.124883,-3.4647377,-6.4583035,-3.93556,-1.5575981,-3.4441907,-2.6955357,-2.271882,-10.531757,-10.509071,6.8651156,-6.0709424,-6.32476,-1.7267256,-4.8687196,0.7814058,-2.794048,-5.7216196,-5.5483046,-5.7263727,-7.488861,-8.768148,-2.3736436,-7.4029603,-7.506932,-2.4439793,-4.7327075,-13.0622835,-10.373826,-9.528584,-5.5430484,-2.7649848,-4.665755,-5.2941732,-4.812375,-4.0402794,-3.6656125,-7.1578875,-7.4886026,-7.3758388,-10.65165,0.11202234,-0.09445879,0.061404757,-7.2927165,-7.2921596,-6.3334756,-6.343337,-1.6580602,-9.395869,-9.635898,-1.189633,-2.140511,-1.8588536,-1.9934908,-2.2971764,-2.2191865,-1.3783518,-6.2857175,-5.368734,-2.4221022,-10.489656,-10.286102,-11.082161,-10.205275,-9.676784,-9.54609,-8.551242,-2.360285,-7.3735476,-6.8644686,-5.993311,-6.37955,-2.72519,-2.9764254,-2.829491,-2.6514988,-4.661886,-3.814685,-4.596896,-4.099251,-4.058267,-2.4880471,-3.2149053,-1.5282866,-0.81364846,-3.737564,-10.408504,-10.555557,-9.711656,-9.438018,-9.579777,2.5040383,-0.50516653,-1.1397421,-1.045502,-1.6858993,-0.57020617,-0.99669486,-1.4814489,-9.523455,-1.8424045,-4.8056483,-1.3834773,-2.3025122,-3.4883285,-3.253658,-3.4996169,-3.6257184,-9.655062,-9.569458,-2.4354553,6.91964,8.04875,8.324147,8.175145,7.765302,8.143698,8.090481,7.9073305,17.561014,17.561022,7.2856765,7.799943,-3.3272817,1.4315715,8.425735,8.142776,8.718816,8.231304,8.265691,8.119423,1.1297004,1.1376303,7.392323,8.13729,-12.818176,-12.850177,-12.840545,-12.861953,-12.8321495,-12.7970085,-12.635013,-12.719831,-12.839453,-12.828853,-13.046833,-12.793257,-12.514868,-13.618654,-12.630093,-12.743681,-13.068565,-12.652053,-12.733436,-12.503816,-12.412478,-12.359557,-12.629671,-12.39182,-12.976905,-13.004152,-12.85546,-12.7865305,-12.660179,-12.805375,-12.796284,-12.489367,-12.263255,-12.765756,-12.767835,-12.807013,-12.230006,-12.824088,-12.744969,-12.554644,-12.544066,-12.807276,-12.730434,-12.88157,-13.133365,-12.486947,-12.480914,-13.303191,-12.53016,-12.667052,-12.836336,-12.398,-12.239108,-12.608439,-12.64273,-12.811324,-12.834422,-12.609724,-12.512514,-12.4654875,-12.840742,-12.850506,-12.8256035,-12.878655,-12.954282,-12.811782,7.923159,8.407404,8.306929,-5.146788,-11.115732,-2.6010513,-1.2831262,0.18369256,-2.2107456,-1.5807163,1.1731342,1.8847733,1.5752151,2.080584,2.045684,-0.23479122,-9.388419,-5.1398144,-1.3004608,-3.9627674,-6.805673,-2.5184286,-10.37929,-9.806094,-1.1901563,-9.388516,-9.326379,-8.01086,-3.0131595,-1.0867131,-3.0620584,-1.3997704,-2.9445477,-1.2755382,8.198816,-2.4863036,-9.498373,-2.781645,-9.404378,-2.5764167,-2.9493394,-3.6098545,-1.355004,-2.8391418,-5.043358,-2.7094822,6.8523307,8.023452,-6.0931625,-5.885222,8.216462,7.731148,7.0591173,-10.273593,-6.4081035,-3.4517128,-3.3923073,0.23805188,0.022205401,-2.2952397,-3.2598426,1.2646791,7.3227787,-3.0265148,-12.887887,-13.157489,-12.689328,-12.339778,-12.379413,-13.07971,-12.896749,-12.765073,-12.664222,-12.843009,-12.800959,-12.439867,-12.818087,-12.77811,-12.796557,-12.588624,-12.621574,-13.275641,-12.57994,-12.689627,-12.914826,-12.776239,-13.029621,-12.783674,-12.670267,-13.368416,-12.514803,-12.766199,-12.857524,-12.19164,-12.532408,-12.622212,-12.579464,-12.610321,-12.600875,-12.4964,-12.72339,-12.830507,-12.711158,-12.942109,-12.840384,7.575209,-2.3690503,-11.118876,-5.419802,2.8069477,1.3714294,-2.7952363,-3.4249651,0.016513662,0.34637442,0.01902832,-3.4987488,-2.8892229,2.363424,-1.4265877,-1.6258001,-1.9885502,-3.128547,-2.1827142,-2.3039465,-2.8698118,-2.1511497,-3.0943272,-2.4737449,-2.397429,-1.9733859,-3.6676073,-3.2120202,-3.1024501,-5.596889,9.033782,9.084855,9.074507,0.6525391,7.3277993,-1.6592549,0.7760748,-1.276249,0.15321808,-1.3480041,-0.73854864,-0.58191156,-3.3771672,-1.4919139,-8.444095,-8.501798,-2.4255629,6.7355275,-7.0608797,-6.2200413,-10.392354,-10.560811,-9.392298,-9.504407,-0.6461108,-2.8930044,-1.4732926,-2.5060904,-1.9231038,-2.7126143,-1.6302364,-10.659115,-10.532762,-9.321106,-2.4022892,-9.560415,-9.395569,1.5553397,-1.4348128,1.5633622,-1.1133311,-9.954257,-9.809992,-2.448486,-9.767897,-9.67726,-0.038080912,-0.41909808,-1.3789412,-9.479811,-9.664599,-9.75448,-9.327258,-2.4233713,-10.072704,-6.9282475,-4.0512013,-6.507198,6.7013693,6.8792124,6.9473815,-5.8161235,-1.3287383,-2.1328042,0.36526692,-0.04611618,-1.6415642,-1.5397948,-9.627759,-9.625358,-2.4593031,-9.812507,0.9353431,-3.3043005,-2.8416123,-3.4500475,-3.2095544,-2.9891994,-0.72465086,-2.607978,-2.7176101,-2.9968417,-3.10521,-3.096532,-2.428378,-1.5041652,-1.43138,-3.0296428,-2.5834792,-1.6795738,-2.6111417,-2.8850462,-1.3357188,-2.0457633,-2.68162,-3.9249516,-4.171385,-4.232494,6.6895833,-4.184628,-1.862398,6.366973,-4.170877,1.7174484,1.2296103,0.8426652,1.4061948,4.862028,4.6375504,4.946372,6.259147,-3.132854,-6.9323096,-7.2446346,-6.3136954,-2.4164698,-2.395796,5.85121,1.4336597,2.5897083,1.6603123,-1.9615781,-2.5344627,-2.1566994,-2.970912,-5.5935082,-5.090297,-5.7073383,-5.2019515,-5.2126985,-0.7828555,-4.612403,-4.551062,-4.883973,-6.3965497,-8.019864,-10.602336,-10.229859,-9.809121,-6.9129105,-7.01319,-6.9008193,-7.3388686,-9.780228,-9.85357,8.501517,-3.4784756,-1.440492,-0.8604251,-0.95256335,-1.046944,-0.94593304,-10.300863,-2.4575834,-6.337083,-1.1697854,-0.7173964,-2.1398005,-5.7221956,-6.9272027,-6.7680006,-6.9392557,-10.027105,-0.96945566,-10.323418,-10.452831,0.086395256,-3.5185063,-4.052897,-1.0297512,-3.2067323,-6.9293413,-6.4784284,-2.5313275,-9.186698,8.023897,-4.715636,-5.9723835,-3.0928144,-2.0113926,-5.2464976,-2.7506316,-4.4071765,-3.7734058,-6.265791,-8.057911,-6.006221,-9.690025,-2.5688455,-2.6040413,-2.6990716,-2.6744738,-2.8049767,-2.6114676,-2.6382995,-6.2396426,-6.7024584,-6.145821,-2.3831942,-2.3633957,6.605208,-6.2226515,-5.8542447,-2.7534444,-2.0386603,-5.900802,-5.320703,-3.7307053,0.032163534,-3.7746642,0.41068783,-2.743638,-9.7448635,-9.933869,-9.733604,-9.441014,-9.485079,-2.424512,-2.3800406,6.617826,5.0085406,1.5511109,-3.675726,4.182434,2.5491917,3.19901,4.1839714,4.11475,-3.4572754,0.8661819,0.9618359,3.5942388,-9.369311,-9.882668,-9.315874,-1.5199211,-1.5330232,-1.8686,-9.5226965,-7.1960244,-4.9839272,-4.3501897,-3.931179,-4.1640983,-4.597573,-7.4500694,-7.391322,-2.4498982,-6.9091253,-6.548207,-2.6503148,-6.4695663,-3.110909,-3.9546194,-4.000199,-3.8800964,-3.355297,-4.7419567,-4.4709334,-2.779555,-2.2739115,-2.5714197,-3.3623202,-3.6623197,-2.72266,-3.8291197,-3.863359,-4.6284685,0.0729584,-1.6908952,-6.3099394,-6.2303157,-3.1368983,1.3162034,-2.957934,-3.082874,-3.2186303,-4.5499625,-3.8627396,-9.401115,0.14054537,-0.7366966,-1.696942,-6.7576966,7.2126503,-5.887862,-5.115043,-2.3275938,-6.1229515,-2.3604708,-7.297626,-6.7755876,-1.9653143,-4.0336213,-6.1329436,-3.7441547,-5.3214703,6.8773236,6.8500786,-6.119286,-5.6561737,-5.6424894,-5.746098,-5.936992,-4.7593884,-3.9430594,-2.3863158,-3.0481799,4.131711,1.5464823,7.994444,7.814867,7.597538,7.5881824,7.5944276,7.5710583,2.6718452,1.4374111,-9.469923,-3.4320185,-5.0484824,-5.0388675,-0.6803947,-1.823106,-1.5792581,-2.1804113,-1.7645679,-1.7718087,-1.7176855,-1.9726291,7.997167,-8.4616375,7.434701,7.858628,8.090721,7.7081037,7.1921167,7.6843514,7.522569,7.8904004,0.14788404,7.9084153,-2.8079183,8.514137,7.801192,7.7259464,1.2997113,7.4027734,7.886276,-12.783178,-12.799974,-12.78127,-12.688033,-13.136773,-12.867208,-12.83984,-12.537995,-12.641717,-12.721353,-12.666672,-12.843069,-12.691613,-12.763325,-12.27745,-12.880137,-12.743307,-12.347053,-12.838349,-12.957003,-12.916068,-13.084455,-13.308445,-12.51826,-12.830994,-12.458589,-12.087442,-12.566678,-12.676901,-12.712318,-12.8429785,-12.517222,-12.500927,7.2545433,8.212051,8.113472,-11.140102,-6.5050364,-1.6384578,-3.7606332,-3.987417,-5.9232693,-3.378012,-3.563604,-3.3535776,-2.9950502,-3.4882936,-2.0087128,-1.9334298,-3.530561,-3.155014,-3.075335,-3.12031,-2.522992,-3.2123883,-3.1130319,-3.0451217,-2.4789085,-2.916297,-3.5304115,-3.537103,-3.2895608,-3.3174617,-3.3951707,-3.6350453,-3.5308447,-2.9409223,-1.7837924,-2.0354736,-4.461641,-2.3526545,-0.06340503,-9.704383,-9.868909,4.8474784,-7.0190053,-4.475773,-3.8729517,-7.156292,-6.6999984,-8.816563,-10.532645,-10.490055,-12.717797,-12.972074,-3.4682074,-3.1434066,-5.3559937,6.727592,-7.0045257,-0.7384802,-2.5223837,-0.59497184,-5.3794594,-4.2827463,-7.8039336,-7.3297076,-7.282798,-7.118626,-6.1822953,-6.163983,-6.1304545,-6.2605743,-7.088892,-2.4261754,-7.547991,-7.159605,-7.116346,-6.0137134,-6.119926,-5.7747,-2.4034922,-2.3664036,-1.2180464,-1.1941392,-1.224935,0.6822769,-1.8764389,-0.55976284,7.1673536,-6.4317145,-2.844889,-1.9286007,3.283022,3.9263663,7.381815,7.6297174,8.101354,8.073886,8.146732,7.954576,8.163222,8.274657,7.9341407,17.560102,17.562159,7.4564085,7.8763533,7.8234153,-2.9324682,8.4638605,8.126843,8.061295,8.069473,1.2961636,7.6835723,7.848944,-12.671549,-12.68736,-12.936488,-12.736853,-12.745785,-12.477743,-12.553322,-12.579963,-12.648524,-12.474559,-12.567436,-12.30836,-12.569058,-12.344601,-12.299863,-12.453953,-12.442581,-12.52656,-12.388557,-12.351751,-12.399619,-12.6021805,-12.385449,8.037108,-12.707996,-12.369413,-12.412134,-12.385709,-12.405279,-12.371316,-12.583861,-12.539911,-12.612823,-12.848983,-12.863034,-12.671357,-12.480527,-12.649544,-12.550461,-12.4922495,-12.592085,-12.35168,-12.452108,-12.662844,-12.622472,-12.564065,-12.226858,-12.619775,-12.585042,-12.636551,-12.391306,-12.655743,-12.760933,-12.52229,-12.621989,-12.494329,-12.77545,-12.381628,-12.504452,-13.144135,-12.558994,-12.173752,-12.47917,-12.449925,-12.327785,-12.367085,-12.436892,-12.454865,-12.490029,-12.711981,-12.4625635,-12.423351,-12.492291,-12.464845,-12.407182,-12.438302,-12.352899,-12.482531,-12.637907,-12.697442,-12.381332,-12.34029,7.8242483,8.07122,8.135523,-11.168989,-6.6679068,-6.344448,-2.3903522,-2.9671729,-2.5876625,-1.7887361,-1.9151942,-1.7987075,-1.9056858,-1.9583234,-2.1538541,-2.101781,-1.5868341,-1.6873574,-1.3841354,-1.452547,-1.7795614,-1.9037529,-1.1620568,-1.8754733,-1.8150623,-2.0900111,-2.2140317,-3.0750217,-2.2327633,-2.7574942,-2.319627,-1.7700133,-1.7679353,-2.6750314,-2.3171787,-2.3337226,-1.6602075,-1.6961638,-1.3881398,-1.725611,-2.2958353,-3.0521111,-2.8586807,-2.2299638,-1.8690362,-1.7938907,-1.9096776,-1.7945844,-1.9694343,-2.4263346,-1.8940423,-2.6214447,-1.8102679,-1.7002981,-1.5122899,-0.9433071,-1.3130311,-1.5060514,-1.5808402,-1.4059682,-1.8704076,-2.3033297,-2.9514894,-3.2623975,-2.8470817,-2.5836027,-2.452474,-2.0798798,-2.3918214,-2.1554308,-2.2491298,-1.2593864,-1.4526751,-2.2607691,-2.3022273,-2.2468946,-1.6741825,-2.3100817,-2.0350797,-2.7048764,-4.032498,7.733555,-7.1776824,-9.918315,-10.195217,-2.8044436,-6.0758014,-0.016400108,-6.5707583,-6.517709,-7.196398,-2.3930776,-7.0035143,-6.809053,-6.506924,-6.9665546,-6.8154716,-6.4139423,-2.386109,-3.7555974,-3.6473799,-7.051438,-6.7594585,-3.64528,-3.928079,-6.2771754,-0.44573882,-6.07094,-5.7671185,-5.572849,-10.167487,-10.15645,-10.071967,5.2397203,-12.954378,-6.5764465,-6.6311955,-7.2622814,-10.534164,-10.781846,-6.0715356,-9.448608,-9.374082,-9.594295,-9.574465,-0.30987176,-8.805925,-10.330142,1.7244658,3.475762,1.2642337,-1.1312592,-9.400187,-0.9093387,0.031655964,5.7510877,-7.838353,-7.1690626,-8.239274,-2.4154227,7.7133245,-2.0434303,-3.1876714,1.5748658,9.076818,7.780488,7.5331044,7.6251545,2.9565678,-1.2682518,-2.4474049,-1.1876436,-1.8283933,-2.17384,-0.9683348,-0.015688114,-0.76429373,2.387255,-3.9760768,-1.2004894,-1.2108767,-1.2100244,-1.4154739,-1.8049664,-2.9034796,-3.11978,0.17974663,0.34194472,-6.5150332,-6.707922,-2.3004143,-6.884412,-4.2614536,-6.4490113,-6.51086,-6.648785,-2.4358025,-9.626701,-1.3731376,-2.2900295,-10.585133,-10.736781,-9.981227,-2.416611,-5.8674855,2.6108615,-1.6530235,-4.0248194,-2.0385284,-1.8255095,-0.1667,-1.7175574,0.0027314883,-0.8691557,-1.4311403,-3.4613318,-7.2310886,-6.884584,-5.2642026,-3.4967113,-3.8294933,-7.150999,-7.303545,-2.4187696,-7.357807,-7.29687,-6.5310087,-6.42811,1.7549855,-6.4024982,-2.7724385,-6.6084356,-6.931428,-4.177548,-1.5011114,-3.5795777,-3.481287,-5.615035,-1.9765297,-1.5544175,-1.3781345,-6.903262,9.053953,9.046305,0.81287354,2.3957078,7.668904,-3.122618,-0.5376841,7.659758,-1.4799368,-1.9561967,-2.3265936,0.26304087,-0.60280395,-1.0947608,-2.387098,-2.8952649,-2.0180879,-2.0879366,-7.0860996,-6.1601143,-7.1464257,6.1474605,2.6731796,1.0501152,1.0517794,-0.4646813,1.4290092,1.2113034,1.3438005,1.3024391,1.1586754,-0.87102485,-1.7037412,0.5053568,0.23890394,0.9680816,1.1772362,1.2039993,0.9309563,1.167706,1.2407851,1.613505,1.1312829,1.0684152,1.0184803,0.7273618,1.0165955,1.2428293,1.1088402,1.1781636,3.6524177,2.1081839,2.0857124,-10.507343,-10.644319,-1.339567,-9.280675,-5.307654,-6.729077,-6.878114,-6.919879,-6.9124856,-7.025742,-6.939397,-6.9397297,-6.7157035,-6.6483455,-6.890558,-8.902114,-5.9200497,-6.9109235,-6.824789,-6.9129705,-7.074137,-6.788411,-6.672332,-6.7034597,-6.720832,-6.719064,-5.943949,7.9575067,7.8411913,7.862074,7.8975334,8.162618,7.740679,7.3476057,7.882772,7.761926,7.7051477,7.8350725,7.849343,-2.9257286,-2.7507539,7.818432,-0.29304639,1.1527944,7.460808,7.884407,-12.487489,-12.387201,-12.376809,-12.81452,-12.727086,-12.987105,-12.507871,-12.852635,7.808533,8.062451,-11.13348,-10.191253,-10.072724,-9.475928,-10.480608,-10.599303,-10.637592,-6.758118,8.545279,9.065877,9.063268,-6.6167197,2.4279783,-2.9597843,-0.5769677,-1.9167985,-1.9839579,0.2686994,-0.560388,-1.1251117,-2.9930902,-3.6228652,-3.584663,-7.339176,-7.384358,-5.925887,1.8986557,2.0192544,-1.2088022,-9.233585,-8.510655,-8.4745,-2.4163957,-7.131056,-6.579593,-6.3694587,-2.4532442,-9.922935,-10.011611,8.546238,-9.942636,-7.0429277,-6.8859663,-3.246269,-2.4297342,-3.028072,-7.3649187,-7.25573,-2.4193652,-6.283982,-6.7668543,-3.528337,-2.3583882,-5.5895224,-7.3661604,-7.279242,-7.605351,-7.635032,-9.118522,-7.2841635,-2.4018774,-5.0880737,-1.0158602,-5.2511106,-6.4148774,-2.3709552,-6.893595,8.253888,0.6853569,2.3175538,-2.1221962,0.64846003,0.43938187,-1.1798277,0.107251845,-0.5488825,-0.6567711,-2.8016906,-3.6717067,-3.2339897,-3.8114958,9.281323,9.297029,1.9629383,3.3217068,-3.105225,-3.3388102,-3.3418171,-0.3938802,-6.111085,-3.020485,7.616789,-9.554879,-2.4726133,-7.025831,-4.7412386,-7.021242,-4.505201,-5.2944193,-6.8797874,-6.4561796,-2.5473344,-7.483809,-8.840416,-7.999445,-8.89088,-3.3069322,-3.2754502,-2.1466992,-2.9839196,-2.322949,-5.9725866,-9.291366,-7.578477,-5.175716,-3.3515217,-3.3857698,-3.6381953,-9.576796,-9.364335,2.2215323,2.3929505,1.9647144,2.798394,-1.319713,-6.851239,-6.731021,-7.1893125,-3.966434,-5.2244563,-3.753183,-6.5075073,-6.13436,-6.523009,-6.5115294,-6.6330667,-6.590877,0.61630714,-1.5025615,-1.6453865,-5.1466045,-0.6831812,-6.6091523,-6.4730153,-6.798524,-7.4340825,-7.517341,-2.426051,-5.116318,-5.2740035,-5.198689,-5.0834746,-6.486928,-4.8650093,-5.321644,-1.5702845,-4.3698363,-1.2527864,-4.4351335,1.5354018,-4.77966,-4.4602776,-0.8798259,-1.1085782,0.31448078,-4.708523,-0.92525965,-0.49249423,-0.28650543,0.10043507,-4.434207,-1.5099756,-1.7717382,-2.555086,-2.2878501,-3.6373086,-0.33434388,-0.25171146,-3.9481542,-0.35874623,-4.1925855,-2.3969374,-5.327859,5.5447445,-5.4893126,9.02478,8.961299,2.51695,-1.7153966,-1.8994732,-1.1994547,-1.2170446,0.75504714,0.19302894,-2.729962,-0.56347483,-0.1286061,-1.3612958,-1.2713377,-4.9482884,-4.413617,-2.9301543,-3.5003872,-1.275052,-6.695353,-6.421428,-10.002042,-10.186305,-9.56928,-0.4230668,-8.769731,-8.553365,-10.35721,-10.63732,0.19380656,-10.58698,-10.611023,0.26807237,-10.378313,1.4650364,0.955481,1.4125333,1.7225124,3.2304037,0.067310326,0.2940581,0.039867114,0.30327958,0.8596411,0.6118736,0.563653,-1.0972342,-0.1236663,-0.4567712,-1.6025677,-0.272057,-0.2413349,0.13124515,0.08583927,-9.66684,-9.939351,-8.9557495,-1.2845467,-2.4541204,-3.23639,-1.8426799,-1.8681393,-0.9166105,-2.7703757,-1.9454027,-4.3159895,-5.835899,-5.8800726,-2.2748578,0.21709017,-2.2644129,-5.9755483,-4.379748,-2.7480805,-2.4668386,-2.7867959,-3.1413493,-2.8553405,-3.194786,-2.9761825,-2.9218884,-3.262473,-3.4354467,-2.8562553,-3.1253338,-3.3695636,-2.6078627,1.9377304,-3.1329575,13.045256,-3.3921661,-5.130367,-3.2128887,-2.7200027,-2.3024259,-8.0821495,-2.5669122,-1.0436379,-2.299149,-8.0678,-2.9434872,-3.1030345,-3.0860276,-8.204874,-2.408306,-2.7189262,-2.9304726,-8.134514,-3.0640635,-3.7709131,-2.6631272,-9.674142,-9.741267,-1.4010531,-0.16488375,-7.015359,-6.828199,-6.136681,-6.163332,-2.4128716,-2.3720636,-6.7000294,-3.7342894,-2.817536,1.0011632,1.5790471,0.90613604,1.288601,3.5143945,7.509451,-3.0199323,-2.3046885,-5.828058,-2.360372,-5.2784905,-5.7035575,-10.472117,-1.5907894,2.5134037,0.86949843,1.6690075,10.825736,-0.11714243,0.18103065,0.08799266,1.2097608,0.23535581,0.35520005,8.259565,-0.22269511,0.14268826,-1.4099951,-0.10853025,1.3095376,1.7860548,2.0919075,0.043797616,-0.0350716,0.5519291,-1.2185016,-0.47636887,11.05674,-7.004175,-7.049091,-6.7206516,-6.977448,-6.939094,-6.911643,-6.6693544,1.1264727,1.3251432,-6.7463217,1.3098209,1.7208469,-6.463955,-1.6812768,-0.49907702,1.2857788,4.2354207,-7.1525283,-0.045270517,-6.8496423,-6.1083145,-6.5059547,-5.869386,-6.7714014,-0.07389881,-7.118885,-7.2957115,-6.883553,-6.955614,-5.176671,-10.433373,-10.170641,-10.193428,0.10145724,-10.412575,7.164431,7.222403,7.3017917,7.476407,6.907161,7.2159076,7.205214,7.044808,7.1058187,2.0216935,-3.9136915,2.4568162,2.1900063,2.461854,2.3568077,1.8617798,2.0364916,1.9098018,2.1912904,2.023544,-1.9010385,8.601865,7.7808986,-5.636391,-1.7425855,-1.0022308,-1.7092086,-1.3922472,-1.879007,-1.8554391,-1.276989,-1.8752601,-1.858055,-1.7217672,-1.7620298,-1.8688239,-1.984136,-1.4984639,-2.3584197,-7.276492,7.978989,-7.3801827,-7.040855,7.9331985,-7.1617694,-6.3452015,-6.9861283,-7.1957393,-6.8304005,-7.1251507,-7.178092,-7.0339174,-6.5414376,3.7992857,3.6435306,3.3792262,1.759314,-2.70578,1.5346344,3.3941047,3.9835505,3.6305022,2.2216318,-5.9594264,-6.9737544,3.5493934,-7.1499662,7.5881114,-9.838463,-9.917646,-8.679407,-6.407496,1.1398255,-10.083707,-10.456683,-9.645641,5.3392143,-3.1998763,-9.789099,-9.829553,-9.534377,10.149891,10.164042,10.509103,-9.679844,-4.3022094,-7.197773,-2.5429466,7.121636,-4.0730305,-4.2297993,7.815158,0.67259157,-1.917685,-2.2244978,-5.6608534,-7.9859195,-4.195014,-2.458841,0.6391518,-4.060074,-3.032779,-3.6346743,-2.954684,-3.981266,-2.9829137,-3.3072145,-3.5999823,-3.52446,-3.6949217,-3.7816443,-3.6113229,-3.4059079,-3.6447861,-3.0427387,-2.8729365,-3.4362648,-3.3967915,-3.2949433,-2.633049,-3.3584678,-3.9850674,-6.883843,-6.6641126,-2.4241354,-9.474293,-3.302844,-7.0111237,-1.5778618,-1.547715,-3.5574572,-2.3436456,-4.9125834,-5.162697,-5.3086586,-5.0196614,8.153429,-4.511606,-4.931687,-5.8927145,-3.6617465,-4.459678,-2.8275883,-1.326251,-4.4629555,7.397116,6.927613,-5.4623775,-5.4468803,6.9258122,6.949856,-6.1035905,-3.9833374,-3.035465,-2.1647542,-7.1957498,-8.498285,-8.549308,-7.2034836,-7.2329082,-1.481038,2.404473,-6.3358107,-6.4809813,-6.3224764,-1.7482654,0.37751287,-0.8035809,0.2335598,-1.1070436,-6.2938375,-8.532755,-6.452873,-2.3678365,-7.4385233,-7.183679,-6.715026,-6.6018443,-2.167925,-6.258129,-3.445492,-3.092958,-2.7133496,0.89628005,-3.3460948,-2.6309822,-1.1179296,-1.5372893,-2.9051278,-2.7196639,-2.5617778,-2.119413,-6.7974668,-3.1126459,-9.241402,-2.3994648,-0.9489139,-10.08978,-9.691434,-1.2409018,-0.6879684,-9.90933,-1.2155911,-3.9052331,2.53323,-6.560932,-6.535365,-5.7436404,-5.9112873,-2.4456232,-2.0794728,-1.219999,-8.07787,7.134622,-8.173066,-1.5914987,-2.3871229,-9.425476,-2.494016,-1.978862,-2.16817,-1.2720966,-1.6547091,-1.350502,-10.9825735,-10.213208,-9.697582,-6.5268726,-6.8789287,-2.3625855,-5.389163,-6.0340924,-6.3093524,-2.455942,7.951161,7.70333,7.9566803,8.150685,8.167139,7.862325,17.560444,17.562325,7.141214,8.138474,8.050544,7.92216,8.027556,8.742187,8.083092,7.2165523,7.9829025,-12.504195,-12.961381,-12.33446,-12.872958,-12.767871,-12.276078,-12.573772,-12.630375,-12.620976,-12.861054,-12.567412,-12.403156,8.426162,-11.163422,-7.0847497,-5.5802746,-3.1114967,-2.3069918,-3.2643394,-2.4254172,-2.6640394,-2.5259213,-6.622078,-2.623734,-3.8228679,7.6870537,7.936163,8.278854,7.9291234,8.135263,7.8830647,7.26526,7.818655,8.415816,-2.8182433,8.516182,8.611245,1.1843987,7.369499,8.192165,-13.489178,-12.380996,-12.5617285,-12.356884,-12.253845,-12.968696,-12.638677,-13.275419,-12.486609,-12.789337,-12.576759,-12.60603,-12.83921,-12.5949745,-12.588029,-12.887861,-12.583572,8.4465475,-11.167727,-9.704657,-9.64124,-0.23451217,-1.3842703,-8.141336,-7.0147414,-1.5888913,-0.18359715,-6.631208,-10.160657,-10.157562,-9.29849,-6.1932344,-6.221527,-7.3312488,-6.6085095,-7.148583,-6.534407,-2.4365454,6.1855607,13.13678,-9.187193,0.5275703,-1.1726497,-1.5332396,-1.1224166,1.9007753,0.47619215,-7.313349,-6.956051,-1.3699119,-2.2922523,-2.4242196,-1.1189771,-2.5577607,-2.817352,-7.793037,-7.2862053,-3.173982,-7.883459,-7.8240895,7.816627,-7.291197,-2.9916494,-7.6067634,-7.7113276,-7.787827,-7.7491274,-7.333831,-3.1830995,-7.781653,-6.9061646,-6.6326427,0.6363072,-6.533794,-0.63499516,-1.5981847,-6.559859,-6.816553,-6.761662,-6.765077,-7.1791115,3.6118479,-0.42409268,-7.335725,-7.17167,-1.973587,-0.80176675,-2.302282,-2.730264,-2.577436,9.1142025,8.8383875,0.8211983,1.5871423,-10.356356,-10.525639,-10.411542,1.6906455,-5.928618,-3.142782,-0.5856605,-5.014207,-3.2549431,-0.9827018,-5.462384,-2.8873408,-3.0626526,-0.77133733,-5.693471,-3.240544,-0.46109578,-3.4286292,-1.0820992,-0.6163186,-1.6290399,-1.0173616,-6.0124106,-6.1123176,-2.413887,-7.3168006,-6.74463,-7.156705,-6.4978104,1.6940472,-6.9233375,-6.643973,-3.2875173,-6.559934,1.1770997,1.1345937,1.9149618,-6.8751955,3.4753323,3.239966,-6.644843,-0.57008225,-1.5120094,3.3923395,-0.9933668,-4.674504,1.17919,-6.9525476,-7.0061255,0.9473358,-1.2615504,-4.26375,-6.531312,-6.7503614,-6.728515,-7.0984616,-7.6485505,-7.599318,-7.3506827,-7.2176986,-7.3965654,-6.8831363,-2.626114,-5.7749443,-1.1875838,-3.2148218,-1.6605273,2.8169343,2.6311874,-10.263071,-10.292611,-9.539933,-6.9799824,-6.5391836,-5.7231193,-1.3158915,-2.9991229,-10.611153,-10.463611,5.4062986,-4.1271596,-4.2131577,-9.616618,-6.5480013,-5.0679398,-1.7380087,-5.4617653,0.19574586,-3.1754224,-0.61044234,8.49631,8.513167,5.0746107,5.2831216,4.851303,-1.3046634,1.7862821,0.1207544,-0.3943719,-1.6545542,4.670807,-6.266495,-0.1405281,1.303468,-4.6062255,-8.335636,-5.232101,-5.891053,-1.6137552,-0.6280264,-6.4891467,-5.4986434,-9.561602,0.080402456,-2.3392334,-2.467605,-7.2145114,-5.6704807,-4.0696206,-1.5573158,-2.5851843,-8.92566,-9.074279,-9.003427,-9.011116,-10.532337,-10.56593,-10.579765,-9.577194,-2.10995,-1.4628139,-0.93958527,-4.9052606,-4.0557866,-4.4737,-4.578224,-4.3049426,-4.496156,-4.475528,-4.46907,-4.787975,-4.348109,-4.589818,-4.354349,-4.7577343,-5.0903215,-4.7424397,-4.654275,-4.429811,7.4924245,7.9576063,6.565868,7.922669,7.77443,8.239112,7.9619656,7.8460793,17.561901,17.562471,7.4348993,7.7776217,7.7366824,7.6679816,7.9555774,-3.0140712,8.561017,8.644993,8.030844,1.1121559,7.2968946,7.999019,-12.845093,-12.863438,-13.223258,-12.901079,-12.899542,-12.950848,-12.919353,-12.686715,-12.623654,-13.054896,-12.829034,-13.023197,-12.651825,-12.72495,-13.235127,-12.775447,-12.534969,-12.883235,-12.89938,-12.657311,-12.820498,-12.361528,-12.425265,-12.349818,-12.575635,-12.519693,-12.976485,-13.192117,-12.873614,-12.839353,-12.668943,-12.837532,-12.872419,-12.564219,-12.459026,-12.879205,-12.738655,-12.758765,-12.39536,-12.906949,-12.745478,-12.350469,-12.789741,-12.8383,-12.931677,-13.120379,-12.643904,-12.56978,-13.293208,-12.459318,-12.826704,-12.838413,-12.246959,-12.578738,-12.650714,-12.606833,-12.869621,-12.589008,-12.672537,-12.489028,-12.753176,-12.8318205,-12.797667,-12.839885,-12.9022045,-12.861665,-12.459244,8.236892,8.326316,-11.159457,6.489894,-7.17294,-6.7179146,-3.856783,-1.9413813,-3.7514353,-6.7676067,-6.641193,-2.501819,-7.006841,-6.6746945,-2.407784,-9.716445,-9.908462,-2.444583,-6.5922623,-6.583939,-2.881258,-6.33563,-2.8111377,-3.5089245,-2.361644,-3.400839,-3.453655,-2.59703,-2.4276614,-1.3539363,-3.4400938,-4.0433083,-4.7841043,0.65715426,-3.794988,-0.05756162,-1.4159925,-1.7148345,-1.3086519,7.8278103,-3.7257524,-1.5164579,-1.2957385,-1.4194344,-2.7941089,4.0487885,-4.0204473,-2.8450592,-2.4917057,-1.5211006,-2.7631013,-1.1327618,-1.3523587,-1.5273011,8.391275,-1.2687492,-1.6634699,-1.280983,-2.0026178,-2.472121,-2.2591503,-3.0299613,-3.003818,-3.0252445,-2.4813714,-2.718991,-3.716009,-3.443737,2.7890356,-1.0639368,-1.5915377,-0.6107373,-1.1852812,-1.6416737,-6.7711816,-6.7525835,-6.8227844,-2.2910745,-2.8192966,-5.955893,0.71426284,-0.031190347,-2.9344034,-2.0991735,-2.2268114,-7.0072546,-6.6181793,-10.412825,-10.322211,-10.220773,0.14716531,-10.221362,5.450444,-10.531125,-10.734865,-10.320065,-10.384423,-6.5938325,-5.515339,-3.9068415,-3.774095,-6.5067983,-10.146246,-10.178958,-9.995925,-7.9151196,-9.724616,-2.3737297,8.446129,8.453799,-7.56939,-7.344811,-6.6246195,-2.6710365,-3.597534,-9.49274,-8.768712,-8.50203,-8.622232,-2.080366,-1.3337035,-9.604538,-2.8915546,0.10822765,-11.013233,-3.3806043,-10.966122,-11.279676,-11.435885,-11.3741865,-11.423006,-11.3223,-11.270705,-11.381067,-11.441223,-11.537385,-11.508476,-11.458096,-3.7115715,0.50566286,-1.0900315,-9.4877615,-8.52595,-1.2594926,-1.6225508,-7.572248,-7.4820642,-2.4044046,-9.935213,-10.4728565,-9.841624,5.298624,-7.309308,-7.150754,-1.3013439,-6.2857966,-6.1855426,-5.93015,-4.537987,-2.4946208,6.527183,5.601421,-2.1319284,-2.1966753,-1.1227536,-2.3045938,-2.1074433,-2.0661638,-9.302469,-9.604586,-4.55323,-3.3865666,-6.1450825,-4.8173056,-0.4793407,-2.0113611,-4.882833,-5.042549,-10.476902,-10.424976,-8.586458,-9.306276,-9.393271,-2.4006994,-10.581857,-10.5938,-10.581216,-1.3906696,-10.013042,-2.4022303,-5.578001,9.051558,9.088691,8.492676,2.9938126,-2.8443224,-2.365841,-1.4877304,-0.68218285,0.25397044,-1.9215837,-0.34629267,-4.193682,-3.4086728,-2.7897732,-7.3636975,-6.8606977,-1.8271177,-2.1960466,-6.2816405,-6.294263,-6.3776774,-6.2699814,-6.1647563,-6.6496553,-6.420116,-6.3753653,-4.221264,-3.970628,-1.732383,-2.1534884,-3.526042,-7.0942426,-6.67988,-2.4251287,-10.122492,-10.010167,-9.256704,-9.781389,1.0208251,-1.1501093,0.54813194,2.7964847,-3.5766382,-3.4161775,-3.6032753,2.0660498,2.6247728,2.0058684,2.0503201,-3.4988236,-7.379411,-6.4542055,-6.1255403,-4.443895,-6.759051,-10.047681,-10.215087,-9.43218,-9.431704,-9.656442,-9.986707,-5.979433,2.3151197,-1.8193924,-3.9067645,-3.22388,-2.0237725,-0.38574386,-1.5554632,-3.7238202,-4.7064075,-0.56095034,-1.7386734,-4.0552893,-1.5299034,1.2548336,-2.6175618,1.1688286,-6.2376785,-8.848097,-1.6760986,-2.831055,-8.025532,-2.1462467,-2.0594804,-1.0809435,-0.737283,-0.5732149,-1.0748386,-1.0127515,-0.2806862,-0.54713154,-0.43644837,-1.8268687,-1.4739,-1.5397781,-1.0384444,-1.0579845,-7.0690293,-2.70267,-1.9624032,-6.8841333,-1.0885174,-2.9536853,-1.0264761,-3.4744637,-5.695127,0.47194162,-0.55499095,7.675151,-5.986997,-6.235769,-6.3265843,-6.2491155,-8.157418,-6.119564,-6.2591515,-6.2478256,-6.994993,-6.1707773,-6.274111,-6.1596317,-5.917137,-6.086406,-5.39647,-6.6688447,-6.1418757,-6.25962,-6.8309503,7.6368475,-10.114108,-9.702551,-6.0378532,-6.274581,-10.244506,-6.35735,-6.486847,-6.3257227,8.943178,-2.703588,8.3928585,8.702952,8.893183,8.558023,8.571772,8.985754,8.66801,8.840478,8.77117,8.7673645,8.954559,8.7469425,8.693858,-3.168112,-3.1539469,-2.760656,-2.6359916,-2.3234446,-2.8044662,-1.5296824,-7.0250387,-7.4736743,-7.3951902,-7.3979774,-10.078542,-9.476269,-9.988921,-10.525348,-10.563562,-10.526997,-9.999122,-6.5344787,-6.1752806,-7.0789227,-6.7199473,-6.522714,-6.941171,-6.5891247,-2.3911586,-3.2134013,-2.0936823,-10.518532,-10.585693,-10.158405,-9.969591,-10.654902,-10.170409,-10.6456585,-1.2878727,-1.4347378,0.20674951,-10.555619,-1.3166528,-10.417042,-10.438883,-3.1168845,-1.386372,-1.0525001,-0.4844005,-1.1277297,-0.22602186,-0.7544269,-0.67468446,1.8099045,1.3611505,-5.9711013,-5.868119,-2.416675,-6.0793695,0.70777327,2.699573,7.185962,7.3498683,7.475782,7.075646,7.2416263,7.036941,-2.2112617,-1.9082139,0.406369,0.21836083,-0.53099453,-0.64457977,-2.7014885,-3.3989801,-3.4505951,-3.840608,-3.5043495,5.4282393,-6.3702254,-5.956238,-3.6225026,-2.1785116,-3.9620898,-4.8369436,-3.589709,-3.914982,-3.9164784,-3.4642794,-3.800148,-0.9741723,-9.202493,-9.468383,2.049561,2.3378356,2.7559228,-2.3068807,-7.1687126,-6.6544094,-2.9160411,-3.9762185,-6.6746836,-3.0857215,-3.9956913,-3.8064356,-7.1929197,-2.361267,-5.132056,-9.686155,-7.158502,-7.244859,-6.77645,-6.153284,-6.8865,-2.4843006,-0.59144896,-2.9870534,-5.1677623,-3.4532838,-3.5290961,-9.650623,1.6184797,1.5806308,-5.6541414,-3.158554,7.364397,-1.8392303,-1.3288808,-2.7706313,0.53609717,-1.8764447,-2.084584,-1.3406296,-0.8788386,-1.3973999,-0.9128782,-1.1645141,0.38355538,-7.141292,-6.668121,-4.1538644,5.885882,-5.8414307,-4.9571424,-5.99647,-5.8312683,-2.3789954,-5.1587987,-4.542109,-0.7079564,-2.0797162,7.4254227,0.32248768,0.40088797,0.4752142,-10.505404,6.6704016,-2.7373905,-1.529219,1.6533703,-1.7333335,-0.93145597,-1.4389687,-0.9689612,-1.7691903,-1.438133,-1.5789961,-1.5347703,-1.1249683,-0.8415211,-1.6191356,-5.775198,-5.1600122,-0.14132103,-5.818406,6.844432,-6.8420377,-6.707631,-6.6200285,-5.4093604,-2.5843098,-3.274537,-2.3629138,-2.4466133,-2.0870638,-4.0421896,-3.2465916,-3.5391142,-2.7522821,-5.39288,-3.7243614,-5.7865014,-7.0361404,-5.9967265,-4.8384895,-2.3661098,-7.18881,-6.679353,-3.743023,-3.7809732,-2.5581954,-2.654344,-1.5271481,-2.4182,-2.4923375,-2.4669595,-2.437569,-6.889665,-6.6431847,-6.359096,-2.4130006,-2.387459,-8.833055,-9.235351,1.9052985,-0.21573578,-9.133184,-0.52194184,-1.2536358,-2.0891125,7.023775,-0.981697,7.6653123,7.7408977,7.84366,7.6995077,-3.033014,-2.176695,-1.8431518,-0.022233818,-0.99723786,-4.7020736,0.22983372,0.50354517,-1.4879781,-6.247681,-5.6262746,-6.4775386,-6.2272544,-6.0573063,-5.744616,-5.899843,-4.8169255,-3.8784673,-2.3814905,-5.3589187,8.978823,9.06703,0.8588876,2.7043989,-3.1280491,-2.1536303,-1.642322,-0.94730383,0.4521416,0.5161858,-1.5153745,0.25986707,-1.7061576,-0.75892067,-0.1983232,-2.878095,-1.3208715,-1.145865,-4.120848,-5.3327355,-4.9682164,2.6919193,-1.4388765,6.6455393,-3.825861,-7.23877,-6.9761744,-7.094013,-3.30634,-0.21126574,-1.7232853,-1.0387776,-1.5765606,-9.616996,-9.607351,-2.171659,-9.499344,6.605491,-10.52273,-10.553525,-10.3711605,0.20295767,-9.993462,-9.799922,-10.450278,-7.343098,-7.3159556,-2.4522963,-0.9977533,-0.45741436,-0.6270031,-0.45741898,-3.6458118,-6.017788,-6.4590697,-5.9897695,-2.7901254,-2.5961645,-6.423937,-9.940508,-10.709541,8.194341,1.906242,0.099090196,-0.19500022,-1.4004618,0.28491566,0.6640449,-1.0130411,-0.2985059,-0.9079938,-0.46189073,-1.3002588,-0.31692648,-1.2905657,-9.59027,-1.2246028,-0.9136864,-7.246541,-6.4769554,-6.18825,4.4129806,-2.4285111,-2.3532438,-2.1107469,-1.6621059,-3.3599484,-3.1150925,-3.1322627,6.1256313,-7.0507064,-6.2891593,-7.172609,-4.0232916,-1.7648019,-1.196986,-2.4086316,-2.410863,-2.4129803,-2.4006023,-6.428397,-6.540706,-5.643533,-5.586035,-6.0242987,-5.052934,-2.388955,-7.28964,-6.380132,-2.4431694,-2.153435,-1.4314702,-0.50195235,1.6503704,-9.726254,-9.407275,-3.790014,-2.4320958,-9.454261,-8.396455,-2.418147,-10.493383,-10.60869,-0.91813403,-9.766981,-10.258519,-10.540143,-10.258076,-8.127793,-10.401852,-10.066482,-9.566603,-5.0934525,-1.5415399,-1.997491,-1.8043882,-1.9259022,-1.039396,-1.4189644,-1.5205102,3.0096278,-0.3841631,2.1855323,0.96809155,0.20015143,2.108993,0.9118028,-3.6741514,-3.1414344,-7.4889565,-7.2468514,-1.939398,-6.262217,-1.7482762,-10.553931,-10.52343,-8.97427,-9.891906,-9.925205,-9.685936,-8.667303,-5.794668,-4.6374316,-2.4739027,-1.8383427,0.99095386,-3.8931608,-3.4220605,-3.1210713,-2.8360932,-2.7041116,-2.7487116,-2.7538018,-2.186147,-1.9300649,-2.2880535,-0.22345269,-6.490365,-6.466839,-6.3235025,-6.482043,0.23746726,-0.14088994,0.17538501,0.24736844,0.13476998,-6.687845,-6.4691157,-6.4724464,-6.0318,-3.7888486,-3.304782,-5.2488465,-2.1861892,-4.2140555,-2.9749882,-3.6854305,-3.617898,-2.777113,-2.1058433,-5.854866,-2.900293,-5.8672695,-5.7317142,-2.900196,-4.572866,-2.6924167,-2.5562942,-2.701222,-2.815425,-2.7802808,-4.5222588,-3.732772,-6.1920886,-6.2514563,-6.363223,-7.448296,-10.393762,0.8159075,-0.97502166,-0.14170109,1.8508544,-4.334637,-1.5673511,-4.401816,0.4611205,-0.15618406,-0.29415777,0.64262927,0.49416226,-3.803914,-3.2641652,-3.0170171,-3.2531667,-3.097507,-2.8374546,-10.568838,-10.616999,-10.502875,-10.299331,-10.505654,-10.479587,-2.4231412,-9.404603,-9.613953,-9.031818,-5.5300913,-5.5707006,-5.2307296,-2.3974836,-7.0706544,-6.6574,-6.572653,-6.965794,-3.1102984,6.814871,-1.7046349,-1.2800479,-1.5662026,-0.65929514,-1.2410896,-0.76699805,-1.8383405,-1.876236,-1.4315447,-4.5966716,-3.1334083,0.95956004,-0.39425027,1.4048247,2.559725,-1.0588905,0.77493733,-0.39061764,0.52221197,-0.520833,-7.858696,-3.7636244,-10.414055,-10.488234,5.285692,-7.069875,-6.3311725,-6.2460647,-2.415873,-2.353953,-9.829295,-9.969299,-9.889441,-1.2603658,-9.498616,-4.5358257,-5.976215,-3.0088036,-2.6762338,-0.6694736,-1.4620057,-1.9488474,-2.1013067,-1.6041172,-3.588561,-3.5213044,-7.8576035,-9.33878,-1.209385,-2.0802708,0.3773146,-9.513452,-9.482102,-9.720314,0.9426728,6.5626807,-9.481814,-2.4523022,10.127326,-9.3207445,-5.7678404,-2.0916893,-2.105252,-1.9240019,-1.8957223,-2.1959143,-2.1094916,-2.0852938,-2.4257925,-5.344103,-1.8327111,-1.0862186,-7.171961,-6.753017,-2.5496507,-6.8588467,-6.3640604,-1.7399013,-3.387819,-7.353911,-7.2436256,-5.888952,-4.974372,-7.723707,-8.278722,-8.780748,-5.711969,-7.193377,-2.405378,-2.397685,-2.383899,-4.3896437,-3.551711,-9.774188,-9.509068,-2.4596903,6.864086,-5.366859,-5.2217283,-6.107778,-5.7733088,-5.684192,-6.0633173,-4.8457003,-4.525727,3.4271522,-2.2370713,-2.3944952,-2.3670368,-4.3251505,-6.281806,-7.864757,-4.93313,-4.527501,-3.4946837,-2.9926388,-4.84401,-5.2814484,-4.9949756,-8.257184,-8.201695,-3.0121083,-2.9160612,-7.7623725,-1.1713474,-6.8858833,-5.255695,-2.6443703,-7.74049,-7.867907,-8.021218,-5.995282,-8.298848,-7.365903,-5.811178,-2.411314,-9.157216,-9.402438,-1.0351738,-3.4162085,-6.954197,-0.657465,-4.375472,-3.7841654,-0.7306218,-3.9750717,-1.6923302,-1.6034361,-2.2000926,-0.9454,-6.2494683,-6.971454,-6.7390356,-2.9782627,-7.143354,5.7350197,1.2613065,1.4550067,-1.3925118,-0.42597365,-0.47445837,-0.73749423,-0.5170163,-4.261504,-2.2916436,-4.672507,-5.85328,-5.1004233,1.2036027,-3.2631657,-0.43563327,-2.9899704,-1.3802835,-2.8539224,-2.6783035,-3.3031332,3.8051195,4.186312,-3.0689304,-2.871641,-1.2866048,-1.7473472,-1.320114,-2.0531604,-1.9800059,-1.6285021,-2.9251266,-3.139348,-2.6499858,-1.4077759,-3.483702,6.745912,-6.6153555,-7.3461366,-2.0811443,-5.937783,-5.005217,-3.8412793,-2.805815,-5.6680217,-4.8304076,-2.4916997,-7.4719205,-7.3130283,-7.0468297,-10.153535,-10.34442,-9.925103,-8.121332,-3.989041,-6.370116,-9.337713,-9.70134,-2.421239,-2.9350548,-2.2050595,-2.385779,-2.4286976,-2.4362996,-2.452368,-2.4275403,-6.627233,-6.2896714,-2.3863525,-4.5480504,-3.1346078,-2.7982605,0.25830045,-3.569277,0.087088756,-5.3146806,-3.0600445,-2.3696885,-0.30031502,-7.15245,-6.2784796,2.2979221,-6.4530716,-6.619329,-6.5040364,0.87241346,0.21397197,-6.306905,-6.187512,-1.1101931,-6.179169,-2.5115132,-3.244651,-4.8228664,-3.7184582,-3.7793565,-4.0660377,-1.7511547,-1.9963466,-3.549807,-3.2131987,-3.9677734,-4.4909363,-3.7884672,-3.973773,-4.217014,-3.539832,-2.7313597,-3.9780967,-3.8339896,-4.7806926,-1.3079598,-3.1454177,-2.5106199,-0.17048289,-2.41658,-2.6447077,-1.7760618,-2.5890949,-2.5297678,-10.296768,-10.303727,-10.198037,-6.568153,-6.75696,5.3312345,-6.041813,-5.646455,-5.1111846,-5.5492315,-5.9704146,-4.887066,-4.972659,-5.018633,-2.4102044,-2.3735206,-1.5950173,-1.7834626,-1.2158841,-3.0656626,-2.4711487,-2.819315,2.6505008,-4.236841,-2.630811,-1.6090595],\"xaxis\":\"x\",\"y\":[0.40789914,0.5237282,-1.755641,-0.06212156,1.778205,3.03206,2.9551842,2.6642704,1.17943,0.0074873934,1.79895,0.18303438,0.35009953,-12.319393,-0.28811717,-0.7622015,-6.0772853,5.8308434,-6.0830207,-6.255552,-6.096553,-6.094869,-6.1540704,-0.72997993,-6.128023,-1.1706483,-0.5143653,-0.12168699,-0.3158286,0.05216504,0.18952659,0.12049113,3.5336971,3.5353959,2.4125834,3.4656303,-5.392719,-5.3922977,3.0941796,0.5316169,7.2645555,0.45949638,0.3082238,0.37517765,3.7804,0.2429825,5.0473695,0.0804319,3.4792864,3.2330742,2.285665,-0.16798432,0.446142,2.3510745,1.7961962,0.56752145,1.069728,0.6780111,0.86371964,0.7451628,-0.16998161,0.10567032,0.22725418,-18.683456,-18.066856,-18.24338,-18.114943,-18.13081,-18.313095,-18.491913,-18.455193,-17.590645,-18.065254,-18.503246,-18.332108,-18.360733,-18.418633,-18.224085,-18.70036,-18.486616,-18.039495,-17.286139,-18.1394,-18.366455,-17.841928,-17.845201,-17.39181,-18.656126,-18.24344,-18.30058,-18.236967,-17.480412,-17.040762,-18.427357,-18.372261,-18.478521,-18.174456,-18.6011,-18.460773,-18.304121,-18.374424,-18.444864,-18.167141,-18.210413,-18.266752,-18.577618,-18.366865,-18.24944,-18.596973,-19.274918,-18.475239,-18.358356,-18.13211,-18.306223,-19.061012,-18.554178,-18.394676,-18.563604,-19.220976,-19.21871,-18.44226,-18.639242,-19.096685,2.1147733,-0.17413354,-0.04867757,0.0864466,4.5140233,0.81443566,1.1348021,0.7349902,1.1241143,-0.3508541,-0.060299493,0.07734723,1.2143071,2.271571,0.4085429,-0.2382944,0.32569554,0.19879621,-0.7863903,2.6738236,2.8253994,-12.345198,2.9127645,2.963996,2.8290746,0.50724506,0.5359287,2.0419703,-12.123167,2.1864526,4.167647,-0.52402306,3.7470136,4.0165844,3.8380954,2.6261294,3.8829567,4.005109,4.0063925,3.9929335,4.0004463,3.727197,3.8290212,-1.0947648,2.7790875,1.7503555,-0.81608033,-0.6822517,3.9323468,-0.8358489,3.6361067,4.7069035,5.54351,2.219159,4.055204,3.8864524,0.6325165,2.7686667,-0.574016,3.643174,4.20773,3.963762,3.9506588,3.9488678,-0.8249209,2.8529732,-0.77493876,3.4137058,-0.06948093,1.9548633,1.9800506,3.8648083,2.281549,0.12074598,1.0808948,2.7342122,2.6811564,4.582882,-12.555846,2.3519762,-1.0475032,0.27669725,3.425083,1.7364347,1.3358186,1.4252224,1.1085174,1.264397,1.9015614,1.2961713,0.7979104,1.2510738,1.6526234,0.6076136,1.5149206,2.5078526,1.4530236,1.562872,2.242137,1.295803,0.9957637,1.056969,1.9980152,1.3753239,1.3193467,1.3235046,1.18296,3.324124,3.9343395,0.9915926,1.1125743,0.56409913,1.3833076,1.4927241,1.6166203,3.4151103,4.264425,2.3244426,2.119199,1.9046781,1.4930214,1.1862959,2.531855,3.9496646,-0.88320005,-0.8624399,1.621826,2.514886,5.821229,0.9664911,1.9328896,6.300736,3.4109945,4.156402,4.043591,6.3980923,6.4256616,-11.871388,3.2979372,3.3850832,3.331864,3.9835892,4.2253656,3.5114443,-0.2816817,0.52425647,1.455834,2.1375055,4.0781455,0.6030594,0.8494662,0.039142992,-6.228337,4.1928144,4.338298,4.1426487,-6.3983297,-0.20356105,0.04179132,5.0301313,-0.43383247,-0.88984793,2.5206356,2.5401416,2.6594126,2.6848094,2.8734844,3.5155528,3.7318003,0.44515017,-0.22092944,2.592787,-0.31745544,0.14870039,3.5046399,3.624969,3.2490773,3.428453,-5.396558,-5.394331,3.2563708,0.19635686,0.15524088,3.1878612,-0.13067171,0.5314011,0.7545245,-0.25079402,0.07700174,-18.653593,-17.984661,-18.214092,-18.057116,-18.160383,-18.425968,-18.36749,-18.84522,-17.878738,-18.351078,-18.174244,-18.438131,-18.4048,-18.594572,-18.29163,-18.51392,-18.583157,-18.296188,-17.901096,-18.327793,-18.451372,-18.024883,-17.872059,-17.407513,-18.468267,-18.234814,-18.239813,-18.255201,-18.219645,-17.052008,-18.177095,-19.104462,-18.46009,-18.504044,-18.44548,-17.928293,-18.570906,-18.357737,-18.341393,-18.411736,-18.36964,-18.002218,-18.222292,-18.179775,-18.237135,-18.574785,-18.47503,-18.321917,-18.613773,-19.193863,-19.115692,-18.374989,-18.158346,-18.073374,-18.440018,-19.199509,-18.492493,-18.335825,-18.46955,-19.177086,-19.118582,-18.56026,-18.761082,-18.782045,-19.039522,-0.29958326,4.559293,-1.548907,-0.6645222,-0.6929673,-0.81545025,-0.5359411,-0.12527978,1.2407966,-1.4318596,-0.6095483,-0.7391144,-0.55938596,-0.50500065,-0.8395429,-0.6167239,-0.59306246,-0.5741156,-0.9908475,-0.5520948,-1.2116886,-0.6101047,-1.1345906,-1.2887421,-5.201465,-0.9104192,-2.5739367,-2.5054064,-2.6946533,6.314355,6.5582433,3.7234743,7.0975504,4.363649,-11.775525,2.941654,3.0479856,0.18248239,3.5474298,3.3994865,3.5730774,2.2871354,-12.397113,-12.4875765,0.1497515,-5.32007,-0.02023613,-0.6172354,-0.5734692,2.0388725,7.1326294,-6.7262673,7.498073,7.5931573,7.637281,7.5968904,7.5136046,-11.675773,-0.41479328,-0.4383595,2.3972015,0.7939269,0.8848492,0.9359742,5.397026,-0.3618027,-0.2977498,-0.5446989,1.1834859,2.4154758,2.3626862,2.5084963,2.5261693,2.4661572,2.4473925,4.403353,-0.15083145,-12.028406,-12.118507,-12.231148,-12.378003,2.9786818,3.1360395,3.6392949,0.0349764,-0.1584606,-0.6641261,5.9319525,2.1156893,5.040998,6.433656,5.6005855,3.720925,-11.915338,-2.180272,-0.34342748,-1.1454009,-2.1377995,0.4337672,0.6841579,-11.612766,1.0386604,1.7462908,0.017521031,0.7495465,3.755662,3.862056,7.341234,3.1418579,3.6922781,3.2919087,5.102086,3.0266576,2.8319836,2.5687063,1.7132684,1.3477728,-0.17439188,3.9398203,0.84515023,1.3731358,3.522471,3.6375215,2.3406098,3.773745,3.5739005,1.1579216,3.0649579,3.808719,0.5131087,0.18334185,0.27026585,-1.0549957,0.12887675,0.5450656,0.23121317,-0.33939305,0.50787735,3.8360534,0.00073544536,0.062245663,-0.31012326,0.69495636,-0.8532891,-12.473533,6.4051065,6.426461,-11.903452,1.9282587,-1.4743259,-0.31648904,-0.7481289,2.3599284,-0.10776827,-0.19876829,-1.3549135,-1.096881,-0.7471133,-0.6617069,4.5049944,6.4011016,6.429257,-0.47019285,-6.120643,-6.2466626,-6.22574,6.5195208,6.632598,7.398351,7.679116,0.30073535,0.9613959,0.9857392,1.5424688,3.8369055,3.3789077,4.3993053,3.612942,3.7155316,0.30995566,2.5033832,-0.16862164,-12.060591,0.10668711,-0.13937889,-0.90589863,-0.20081262,0.58494204,-0.648611,-0.6007709,-12.563597,6.261346,6.4119363,7.387189,6.6510043,4.423451,4.563059,4.5985136,4.658368,1.7594236,-0.17704137,1.6723692,2.0130365,1.0535088,-0.8568871,-0.80877894,-0.96434057,-1.8286073,0.8178958,0.04165145,0.065886594,-0.5659212,0.57756406,0.45309427,1.1293302,4.424883,4.7348056,4.269209,3.6457644,4.198476,-0.09962396,2.5242999,0.48625615,2.8926845,1.8793159,0.21410865,-1.1393754,-0.42628422,-0.30385277,-1.6382617,-1.9673429,2.8464317,2.9419985,-11.914527,-1.2189618,-0.3315863,0.29210216,0.15128689,3.6048615,3.6567135,3.0197682,3.497708,-5.3918834,-5.393981,3.1224203,0.05943722,3.7917106,1.8398774,-0.22347671,2.9283516,-0.07008502,2.0502813,0.69156915,0.35527048,0.773362,1.0289973,-0.26255682,0.092473015,-18.626225,-18.086048,-17.951262,-18.281229,-18.298986,-18.48175,-18.677835,-17.608732,-18.13624,-18.442915,-18.301376,-18.230602,-18.290485,-18.30132,-18.624943,-18.724272,-18.066145,-17.461706,-17.815388,-18.31049,-18.074915,-17.84511,-17.248102,-18.333588,-18.171509,-18.247683,-18.163671,-18.256306,-17.094912,-18.191475,-19.298574,-18.521494,-18.676796,-18.271982,-17.756784,-18.729921,-18.456154,-18.288214,-18.434353,-18.3228,-18.061945,-17.967194,-18.27129,-18.16133,-18.348211,-18.488525,-18.543753,-18.242447,-18.638418,-18.798687,-19.21041,-18.464178,-18.404703,-18.16059,-18.23209,-19.090693,-19.204662,-18.622225,-18.33277,-18.435476,-19.155512,-19.248465,-18.355356,-18.625048,-18.794592,-19.159811,2.2733998,-0.274214,-0.24168245,2.2151477,4.5083957,-2.0613198,-0.8302038,-0.24013919,-1.7055148,-1.8891374,-0.0069281394,0.19854632,0.73921394,1.4929243,0.5089756,-0.6532672,0.5305216,2.4053502,2.4582703,-1.3116902,1.3048656,-11.422463,-0.08776108,0.029085094,1.9239625,-0.7208469,0.43694097,0.10867379,-0.48930687,-6.1218,4.0773463,-5.9062266,4.0900893,-6.182779,3.2261746,3.9651136,0.5497279,3.8121293,-0.6706219,3.9174519,4.193616,5.058132,-5.5122476,4.121334,2.1835732,4.0057316,-0.5904806,-0.3704126,1.2721695,3.3197188,3.6995149,3.5693154,3.1315234,-0.46120614,0.6517905,3.8010461,3.5809398,-5.9534307,-6.0388713,3.274909,0.9392343,0.9039818,-0.27049047,0.7745521,-18.079243,-18.229788,-18.546766,-17.9755,-17.92068,-18.20842,-18.19396,-18.235,-17.27173,-18.281048,-18.713324,-18.559887,-18.334547,-18.206417,-18.230457,-18.481058,-18.46169,-18.26599,-18.451982,-18.245031,-18.167912,-17.864273,-18.255486,-18.487455,-18.4954,-18.197706,-18.688831,-19.166292,-19.238031,-18.450897,-18.262468,-18.228407,-18.368866,-18.588438,-18.399689,-18.436892,-18.895128,-19.18634,-18.481005,-18.814796,-18.954353,0.084793895,1.4371529,4.5056987,1.3704162,2.2802134,3.8334715,3.0153768,2.6844006,0.272774,5.3615704,7.550551,-1.6394217,5.1922817,1.8398899,-1.7477928,-0.61885965,-0.18629085,-1.7776986,-1.2914364,-0.45286345,-1.5966158,-1.3159128,-1.2874434,-1.9884857,-2.040974,-1.2697345,-2.106761,-1.8907586,-2.5115168,2.9164433,0.4155028,0.4429018,0.42984012,0.7831175,3.157645,4.591598,6.5215874,0.82568413,-1.1328815,0.8119463,1.2936319,0.47368112,3.1108928,3.1192462,2.6157262,2.610318,-12.047826,-0.745243,2.8469558,4.00076,-0.31728458,-0.4186648,-0.6579313,-0.72693425,-1.8671277,0.6782496,0.24413083,-0.58669966,0.45885837,-0.3278971,0.6621672,0.30983448,0.006023707,-0.6983333,-12.216233,0.13412684,0.51899725,0.61873543,-4.143296,3.8645005,-2.9025073,0.29795498,-0.31233752,-11.896419,-0.5785311,-0.86007357,-5.939339,-6.2163696,2.7282655,1.0459095,1.1328107,1.1276788,0.7196911,-12.047495,0.21315327,1.7353376,3.4640048,3.7762196,1.5737681,1.5387795,1.6452272,2.8001719,0.22181466,1.3957766,4.61599,4.600666,-0.48032883,-1.6874561,2.920676,2.976534,-11.810496,0.17637958,2.279562,-2.1280243,-1.6553147,-1.9982835,-2.0277755,-1.8739086,-0.3817815,-1.6732613,-1.6048661,-1.6512246,-1.761903,-1.7833194,-1.6261452,-0.41418844,-0.24946372,-2.0207872,-2.4007087,-0.8091254,-1.2748238,-0.9308641,-0.93650234,-1.8168277,-1.2006496,-1.4129604,-1.9371048,-1.7395949,-0.60975593,2.0053792,4.781338,-0.5391453,2.6887243,1.4565178,2.020011,1.9919348,0.7114501,2.1885428,1.8841708,1.5152602,1.9437287,-2.5822055,2.1256804,2.8941977,2.0200331,-12.097374,-12.255603,-0.39285466,1.3772279,1.227603,1.8470409,1.6607851,2.069495,4.29032,2.713683,1.1776705,1.4415834,2.0179076,1.4331596,1.8185405,-0.7620549,1.3909634,1.259479,1.4464148,0.2683687,2.0159938,0.098151766,-0.07118463,0.13036467,2.765331,3.3220997,3.1321878,6.3764286,0.46855476,0.32862687,-11.795193,-1.5180023,-1.9507263,-2.0189223,-2.9307318,-2.1853473,-1.9691772,0.0972895,-11.802319,0.32393304,0.5376935,0.36494386,0.12299101,-0.6553211,1.7089936,7.2855935,6.470897,-0.6044103,3.7808275,-0.6964275,-0.79512006,-6.3020678,3.792498,4.308211,4.3181877,3.8513286,3.0438337,2.9075491,3.8654904,0.78365725,0.6038513,-0.64000034,1.3056035,-1.9650555,-0.36798483,2.6690197,-0.09268464,2.693984,3.8571596,3.0566971,2.5551054,2.5909011,0.99354005,3.3950515,3.5478652,3.874964,3.4706779,4.1199255,3.7821362,3.6823382,4.2634954,3.0554278,2.8286092,-12.391054,-12.542491,-0.8569856,1.5690722,1.9438707,3.353071,-0.0789966,-0.717568,2.8720694,2.339636,0.061825033,1.567422,-0.19033304,3.040046,0.14845634,0.30760983,-0.18739517,-0.11113709,0.055168502,-12.065441,-12.4196005,-0.38729778,0.2690249,0.92836183,1.1763458,0.6465417,1.0171138,1.111728,0.8659762,1.7407477,1.3915666,1.5048808,1.485195,1.0332098,0.30773064,0.41145363,0.14105737,3.1727333,3.3591104,2.3174846,0.19598225,7.1110835,-1.0945204,-0.6053808,0.21220669,-0.26778963,-1.1439637,6.8161025,6.731749,-11.861758,2.9365158,3.0205822,3.9660592,3.160246,3.6234062,4.544381,3.6772637,4.66433,1.2758307,3.3245263,3.4632685,2.540204,2.3854413,3.2090454,3.8322306,3.951672,3.8751388,3.0341325,3.608749,-0.99455786,3.9999201,3.9642687,0.0014280527,0.18919839,0.44628355,1.2973958,4.349761,4.403161,4.7355967,1.8051574,2.339061,-0.52203786,2.5534236,2.0496907,1.3553789,1.3192246,0.8936755,3.4082193,3.4764395,0.35109043,-0.5131049,-12.574735,2.80332,3.2175837,2.0565066,4.408983,3.9878943,2.4383376,0.6254936,-1.2069908,-0.9689399,2.5094788,2.713834,3.2832503,3.5097735,3.4687138,3.5300527,2.1443632,-12.35954,0.6771695,0.8166486,0.96752435,0.5342763,0.10056181,-0.060716957,-0.040855493,-0.032437537,-0.08975903,1.1183779,-0.024111684,0.5989495,0.9037604,3.406125,3.3256204,-1.090515,-1.4389637,-1.2289321,-1.5691586,-1.1026623,-1.2967646,-1.6215197,-0.87544906,-0.28482276,1.1252562,0.06683489,3.5151558,3.59699,2.7956207,3.1055796,0.36095238,0.1858051,0.13413945,-6.0337358,-0.115314044,3.3347292,-0.1746774,0.28067678,0.003986003,0.74605286,-0.30160773,-0.1473842,-18.334896,-18.41108,-18.580399,-18.437445,-18.28782,-18.26762,-18.532804,-18.372381,-18.474766,-18.306963,-17.231987,-18.324636,-18.134018,-18.5761,-18.432905,-18.279774,-18.305857,-18.358852,-18.051748,-18.135813,-18.05944,-18.280725,-18.16868,-18.76691,-19.249628,-18.537294,-18.443253,-18.090221,-18.279242,-18.656315,-19.219082,-18.542286,-18.576876,2.491047,-0.21711174,-0.12202824,4.515282,1.7484969,0.46696883,4.1143994,-2.916418,-0.37517822,-1.9693087,-1.9595491,-2.0285144,-1.9437091,-2.2794044,-1.2754129,-1.1985986,-2.2334461,-1.9059389,-1.7286173,-2.0564542,-2.0837047,-1.9137785,-2.622395,-2.425003,-1.4209795,-1.3233502,-1.8959585,-1.9227737,-1.8646516,-2.200642,-2.2446582,-2.1064618,-1.9081131,-1.2034993,-1.9791429,-1.5582255,2.0642667,1.7360512,-0.52942806,0.32813257,0.67031115,1.3383545,3.3063636,2.6794877,3.6598837,2.2049522,2.4363606,0.19165835,-0.28144035,-0.6237684,-0.873534,-0.99167925,5.5578876,5.6689878,2.445277,1.803653,6.459606,-1.2242936,-1.3448942,-0.91626775,5.1109858,-0.54246885,6.99691,5.990407,5.6629105,6.0657654,6.656395,6.834128,6.4044776,6.0753174,6.0146937,-11.86136,6.393684,6.4261966,6.246194,7.454862,7.3658795,6.9953704,-12.112634,-12.533169,6.8601365,6.749513,6.6353335,4.143594,3.6753974,5.2172794,0.19929437,2.9346693,-3.9759514,-2.7215388,0.5022451,2.298304,0.3419495,0.21929747,-0.35977918,0.36027268,0.05930864,3.4867368,3.6446266,1.2727497,3.3828712,-5.394199,-5.3926864,3.1674738,0.18109068,0.09175408,3.1042755,-0.024120655,1.0327616,1.0354345,0.35195428,0.77284455,0.0057180705,0.14902736,-18.20474,-17.792746,-17.85136,-17.841206,-17.951487,-17.887596,-17.9457,-17.831715,-18.325577,-18.294022,-17.582275,-17.392992,-17.787214,-17.352802,-17.282925,-17.628885,-17.568853,-18.226309,-17.451754,-17.451937,-18.313213,-17.863976,-17.465181,1.1190839,-17.751898,-17.425474,-17.898664,-17.497717,-17.518211,-17.78193,-17.090647,-17.637167,-17.894121,-18.162165,-18.212353,-18.02232,-17.801558,-17.223835,-17.726904,-18.310234,-17.646713,-18.263815,-17.468115,-17.925205,-17.486937,-18.29811,-18.329565,-17.92098,-18.040567,-18.12634,-18.23442,-17.826492,-17.800438,-17.71318,-17.543724,-17.883818,-18.067404,-17.45143,-17.720793,-18.079824,-18.066578,-18.486431,-17.833776,-17.700459,-17.961796,-18.006239,-17.659508,-17.78246,-18.23972,-18.391775,-17.988684,-17.581913,-17.906048,-18.134571,-17.893295,-17.895567,-17.3802,-17.91371,-18.204088,-18.05683,-17.404398,-17.424608,0.94352186,0.05867014,-0.0011568357,4.5411754,2.6259565,2.8117225,-12.297593,-2.2506382,-2.2009194,-1.2980149,-1.6841931,-1.9840447,-2.1960452,-2.255286,-2.332996,-2.3295827,-2.1967297,-2.0468671,-1.9269688,-2.1531522,-2.262267,-2.3737283,-2.0256243,-2.3638856,-2.1811178,-2.2423058,-2.6808972,-2.5078638,-2.646366,-2.5285463,-2.5599518,-2.6186702,-2.4203413,-2.4688125,-2.5264745,-2.564492,-2.5359914,-2.3808436,-0.014573952,-2.4490867,-2.7437587,-2.311184,-2.1468067,-2.188929,-1.6443416,-2.1950681,-2.353009,-1.967672,-2.125683,-1.2412008,-2.308684,-1.9252572,-2.2342868,-1.8497636,-1.2204304,-0.22887051,-0.4469005,-1.779919,-0.6157959,0.54490125,-2.0175345,-2.1617444,-2.3555367,-1.6026573,-2.4669976,-2.1883836,-2.2429478,-1.7633709,-2.308394,0.051200934,-1.8955137,0.23425812,0.6171972,-2.3535619,-2.11521,-2.083063,-2.0162024,-1.654893,-1.4824662,-2.4833012,2.119313,1.8512566,4.5398,0.11008146,-0.04064803,-10.795084,2.569414,0.19530436,2.4478655,2.657607,4.5260386,-12.254474,2.5756283,3.0038037,4.2595,2.7712994,3.1063077,3.0995238,-12.318356,5.492733,4.9560375,1.9419887,3.0908964,4.593356,4.445857,0.019720646,0.5177298,0.042173516,0.2613913,-0.14543778,-0.049568344,-0.048993394,-0.17239155,1.9395913,-18.022701,1.628358,1.5395869,4.5033736,0.0604936,-0.08953866,-0.026069233,-0.33381528,-0.60499203,2.7328403,2.9640036,4.200174,-0.44271705,-0.9199039,3.1157773,2.6351604,3.7231188,-0.83256054,-1.0836047,-2.3776877,-6.2813196,-0.17284028,2.2538476,2.8143604,2.4849565,-12.049521,-0.5926051,1.2954428,0.83955026,1.1491723,3.1887789,0.07329465,-0.16700484,-0.044001292,1.0462799,0.10050781,1.851908,-0.4684127,-1.3860248,-0.9720593,-0.1514834,3.5486445,-0.4618007,0.59337556,1.2370508,2.0777698,1.8515725,1.2360015,1.2585998,1.5259626,2.7743058,4.972751,-5.881469,-6.097417,7.285004,7.01599,3.0770278,2.7641852,4.100088,2.5448592,2.8197956,2.9252608,-11.992954,1.0170966,1.5257666,3.4314852,0.15985526,0.007952307,-0.5294412,-12.109693,2.8379238,2.3446982,-1.3994696,-0.9251436,0.3309265,-1.280334,5.4956617,-1.6556458,3.8052924,5.105705,-0.48205146,-1.5613333,2.8378887,3.1388733,0.85000193,4.422316,4.3936625,2.4371183,2.2570283,-12.083116,7.070582,7.1152215,7.470754,7.5969534,1.9594144,7.660258,3.0603445,7.308846,7.1241684,3.105283,4.0194964,1.7364101,5.4371953,6.7905784,1.4237127,0.88880193,1.3844991,2.9049575,0.3687918,0.38058934,0.76419264,1.3920959,3.6018326,4.3321476,4.6852274,3.592145,4.8525214,4.9009557,4.361268,-1.0437385,0.29098016,0.94625896,1.8799516,3.1863787,3.3334074,3.4454808,2.7017722,1.891977,5.623808,-0.4096919,1.1253507,1.3670721,1.3991003,1.874686,1.4953007,1.4286586,1.4482929,1.6707792,1.3670864,-0.85287195,-1.3439156,0.81525487,0.095491365,1.2186472,1.7613707,1.5277762,1.59845,1.5542243,1.7024624,1.4819907,1.5140553,1.3469355,1.5901623,1.2590501,1.2171829,2.010916,1.7875037,1.4404277,0.9123256,1.4087816,1.7457148,0.0026953632,-0.11573602,1.7806711,-0.58178365,1.2811543,0.823728,0.69660556,0.544306,0.5095329,0.2981768,0.7266095,0.4177726,-0.95238876,0.73131794,0.2541069,0.08859141,1.59485,0.63912773,0.47757238,0.5643037,0.40702674,0.64277756,0.5130527,1.0304551,0.7124957,0.44111866,0.045150463,-0.19200338,0.06924981,0.036214456,3.457976,3.58483,2.7702823,3.1046853,0.1977332,0.062996194,0.06946449,0.04839063,0.018220408,3.0958683,3.1976366,0.14921959,0.9899904,0.7367483,-0.20285542,-0.062247425,-18.44133,-17.85921,-17.902494,-18.37333,-18.309008,-18.214201,-18.436022,-19.185135,0.06413207,-0.09695952,4.505068,0.037141815,-0.16788837,-0.75295305,0.12617956,-0.011382005,-0.683409,3.0695295,2.1783445,0.3461505,0.3293896,6.420215,1.0711293,4.1558633,4.8843517,4.78558,4.3983817,-1.0093151,0.24138848,1.1036472,2.3282485,3.7647893,4.089921,6.356054,6.0966573,2.3541784,1.2353702,0.79492,-1.3368043,1.6670746,2.5552816,1.7187401,-12.099198,6.4492817,6.8739424,7.4447813,-11.797151,0.16806571,0.3798222,1.4684434,-0.15344721,2.7110484,2.8930879,2.06573,-11.914882,2.172543,2.6483855,2.7936873,-12.032446,3.151326,3.3029392,-0.33236945,3.1208198,3.531673,6.474953,6.412942,6.5504985,6.4182234,-0.47773948,6.572129,-12.195908,2.7185585,-2.0913033,1.5161618,0.71509844,-12.492158,3.0815055,1.7917732,0.84780717,0.98766196,4.5560603,6.2287827,6.898605,0.65520483,-1.0890784,0.23588003,0.717787,2.4151614,3.9639056,5.5849056,2.6141808,3.5764153,3.6472182,0.5627416,-0.7988549,3.0285873,3.0862753,3.1032836,1.9606239,1.717833,2.8610673,3.1802452,0.35192835,-11.849854,2.3546233,3.610421,4.4192724,2.2294948,-0.12210663,7.0058475,2.4230983,3.9048734,6.2306995,0.7049888,2.5679023,-0.35600764,4.114137,5.701812,2.1601875,4.4046597,4.4742646,1.9656707,0.059312124,6.29332,1.0530006,5.184982,5.1776967,3.3448455,-0.71936667,-0.8209475,2.6636648,3.0736384,4.157813,2.4039981,-2.0219285,6.5108967,6.768678,6.849608,6.1827197,6.7588716,6.077102,7.111906,7.143407,7.0474806,7.1343203,6.995901,7.1719904,3.9248576,4.1807623,4.247325,6.1075983,0.08002363,6.6089225,7.0367627,6.9199624,6.3476005,6.4853425,-11.988258,3.1807578,3.1136217,3.3843625,3.2278259,3.057514,3.3963916,2.9321785,1.1584119,3.1529682,1.2592161,3.068544,1.9917221,3.4331527,3.4534726,3.685708,4.2075453,4.2708883,3.5579498,3.492215,4.043288,4.2636695,4.0910754,3.424896,1.728804,0.9978912,1.246936,1.1966623,3.4388535,4.6793,5.3564754,3.4076362,4.6972747,3.5708609,-12.253267,2.0177617,3.218924,2.2845874,0.37343302,0.53446794,1.1643468,4.649518,4.376327,4.0035124,4.2031302,6.4784293,-1.1020979,1.1022874,1.4078193,0.38853392,1.128823,3.1069088,2.0790434,1.2641,0.26284042,0.9139315,-1.766721,2.8030615,3.1547642,0.070379704,0.029612858,-0.7103471,-6.315532,2.4210453,2.4633684,-0.6933424,-0.94170684,-5.9497557,-0.7744817,-0.8055843,2.9624267,-0.8853314,3.3222103,4.1414094,2.8318942,2.8134687,2.0135584,-6.3153706,-6.224758,-6.339888,-6.275003,6.429965,6.690909,6.707015,0.88807225,-1.0730933,0.30824232,2.0576682,0.7079657,-6.2367024,-6.2955074,-6.2518516,0.41766295,0.25815526,0.4528117,2.5755343,-11.809301,3.0536156,3.3641214,1.0382468,4.2332773,2.9680269,3.7088525,0.53089607,-0.5107626,-0.07733941,2.4701564,0.3328429,0.36001578,-0.39695555,2.070316,3.5369077,3.6190345,3.373574,3.241593,3.5072324,3.159869,2.9850903,3.299813,3.1841412,2.9059544,3.0736942,3.1281304,2.832377,3.1782415,0.54366845,3.2612972,3.8001933,2.8578866,-0.59267807,0.13006637,0.26613745,-0.52850616,1.3016566,-1.363434,2.5315766,-0.67490363,1.3045958,-1.0333085,-0.097108535,-0.6140559,1.4035301,-1.3001008,2.285911,-0.41333222,1.365443,-1.0465719,-0.21806742,-1.07875,2.8887324,2.9409075,0.22560023,3.8397179,2.4647336,2.3526382,3.7611887,3.644834,-12.118073,-12.473779,2.669686,1.3688309,1.04226,0.81064266,0.87592477,0.8441301,0.57189536,1.0297394,3.4779873,4.05378,-0.14187425,-0.33180776,-12.568575,1.9732503,2.0882916,-0.47940424,1.3571876,1.1629981,4.10949,3.1577163,3.915744,-5.7763653,-6.045115,-6.1904016,3.4049823,-6.2762604,-6.2947197,4.0898376,-6.271097,-6.117245,0.0077741067,-0.9147956,4.081212,3.6378484,3.2025635,-6.081773,5.3703175,7.446483,-0.14898463,-5.939788,4.1047463,6.007577,5.8367887,5.5119047,5.8991017,5.6763487,5.7144403,5.314373,1.8121848,2.1988938,6.0298543,0.8457172,0.95795995,6.827553,-1.5212771,-1.0589113,1.0518377,2.6157467,6.715576,7.9598045,6.522516,7.34383,6.781437,7.2647142,6.773803,5.0148573,6.272534,6.320669,5.6856236,2.0593305,0.14538166,-0.28616437,0.07501734,-0.46500045,-6.1093097,-0.45757535,2.0999396,2.2309577,2.2176216,2.1200078,2.0376194,2.1565475,2.0661926,2.0780675,2.0931728,3.1256166,-0.94809026,1.7569673,0.97593194,1.0340548,1.3972723,1.235654,0.9150991,1.1192847,1.0686024,1.4211504,0.54312694,0.66010267,0.4520117,2.514303,0.52620363,1.1491238,2.2009177,1.1341162,1.1079125,0.5715599,1.6060958,0.78130555,0.6687924,0.68953305,0.43251976,-0.19654353,0.91060203,0.0140940845,-0.36048082,-0.65189874,2.642617,-0.7101753,-0.5592358,2.570991,-0.49401173,1.4645718,-0.29931414,-0.42716366,0.3367905,-0.2603679,-0.34760872,-0.38041598,0.6835904,1.0592071,0.77824634,0.9696578,1.4070572,3.9106188,0.20058352,1.0989972,0.36177358,0.49639547,0.47072482,2.5285192,6.4141984,1.0828334,-2.7051215,2.2956207,0.3637297,0.44447136,-0.46126646,3.0841124,3.6063027,0.09129516,0.05344037,-0.35208052,1.9559656,0.9004629,0.05377755,0.17147228,-0.12526178,3.471006,3.503686,3.4628885,0.09678535,1.2389498,5.0193706,1.2178788,3.064812,-1.628824,0.7128186,0.21582639,0.30732173,-0.45676443,0.22133146,-0.5443355,1.8953706,0.7766497,3.6559713,0.3047483,-1.6432023,-0.56625926,-0.62065613,-1.1880085,-1.089904,0.44539997,0.28688598,6.109024,6.204267,6.0463524,6.270842,6.4388757,6.021629,6.5288754,7.034776,6.974366,6.5772076,6.665461,6.9653277,6.8203216,6.9589686,5.604498,2.3116996,2.596061,-12.042485,0.42060474,2.4673011,3.812503,-4.4726357,1.0741911,2.3811417,0.6333779,2.1181264,1.999236,1.7354075,1.0393807,4.322716,1.912971,2.0018084,2.2722473,3.3852518,2.4044232,1.0312068,-1.6816499,1.8532009,1.5996381,2.0709944,2.4599187,2.9476693,2.3954673,2.0609663,2.2709842,1.7133688,3.661941,0.36318406,4.5005064,2.7606828,2.7421863,4.4167147,6.387501,1.8398658,1.3310329,7.6408944,7.8206754,7.338708,4.3295555,6.3844438,1.0869576,-1.0775222,0.29453054,7.1195035,2.335604,2.607475,-12.520799,6.380253,6.436631,2.3242307,1.3652925,0.7989056,3.8613238,1.0767789,1.346657,1.0876683,0.9430428,1.5129695,1.6689909,1.4246452,1.2918767,1.579506,1.431159,1.421023,1.4463813,-1.6161222,1.3544174,-0.17212157,1.2315391,-2.690375,-0.20944083,-0.5187866,-1.5579234,-1.95433,-0.13641056,-2.6651065,-4.463339,2.5579255,2.8507903,3.069369,-0.36618257,0.5592316,-11.831892,-6.321799,-6.6310587,2.8441777,1.8032199,2.2479236,3.9341817,-12.394716,1.7111249,-11.745579,0.0152818505,0.18541895,-0.26944587,-0.1428996,0.13201565,-0.21744531,-0.56907046,-0.44756466,-0.16721418,0.39878565,-12.553783,-1.4361194,2.5117326,2.685574,-11.826656,-0.28980452,0.32782933,3.616032,3.667375,3.1854248,3.458557,-5.3906226,-5.3929534,3.119095,0.584849,0.40737277,0.25546396,0.043166175,-0.13491598,0.17581464,-0.15910268,0.097438104,-18.373123,-18.139704,-18.587069,-19.292309,-18.822714,-18.30132,-18.217112,-18.137712,-18.384865,-19.317656,-18.512522,-18.404465,-0.25388625,4.5289035,1.8899246,3.271991,4.1375194,4.140927,3.7299573,4.390258,4.557083,-11.2467,2.9387076,3.335988,5.425002,0.5772051,-0.35579628,0.15727578,3.6086411,3.6988552,3.4542143,3.1902204,0.26456618,0.10370208,3.1970205,-0.26280943,0.4304654,0.74712163,-0.22982855,0.10946192,-18.28378,-17.924452,-17.43365,-18.573387,-18.425125,-18.148441,-18.375853,-18.258787,-18.709873,-18.867393,-18.21561,-18.371918,-19.130384,-18.509356,-18.478964,-18.756313,-18.566109,-0.27382845,4.541398,0.95098835,0.98262,4.4172173,4.098237,0.4614543,0.7462133,0.44085357,1.0530634,6.5951314,-0.011544007,0.10371893,-0.65232813,1.0220051,0.8703151,2.0569224,2.6639922,4.4513955,0.26181033,-11.951119,-0.36753643,3.9072394,-0.6206586,0.8539277,1.7627726,1.6441438,1.9540105,-0.12315334,6.353456,6.413817,6.460299,-1.4625872,-1.3710481,-1.2334135,-1.2233868,-1.3227097,-1.1919928,6.953622,5.556536,-1.1585377,6.915504,6.9444647,2.4350846,6.8236365,-1.066232,6.889788,6.8822913,7.009485,6.933155,6.109936,-0.6664628,6.81343,6.274079,6.551177,-0.4120103,6.287461,-1.153829,2.6392345,6.4075375,6.1825876,6.3011074,6.311598,5.928097,1.3725789,-0.77418894,6.46572,6.265324,-1.4252025,-1.1258091,-1.5170242,-1.4518033,-1.239548,1.7402518,1.7064811,0.34585533,1.6974717,0.3082572,0.22388734,-0.39020783,0.9020912,2.3406358,5.1880307,-1.49885,1.8278716,5.663467,0.2740415,1.9244535,5.291193,5.5003834,0.47123334,2.1170835,5.0544634,-1.4648033,-0.48735553,-0.054360762,-1.5244979,-1.0624359,-1.3510882,2.3865674,2.5514,-12.078724,6.524595,6.6300654,6.7439833,7.0449214,4.433504,6.7723446,6.762389,6.1890783,6.60664,0.7865021,1.042071,1.7483715,5.9152317,0.55119884,0.47865993,6.609059,-0.66800654,-1.7079633,0.88729846,-1.0343026,-0.6599897,-0.18634334,6.1302185,6.740661,4.825714,2.254152,5.9406724,6.7996006,6.7240367,6.6444473,6.7356625,-1.1940829,-1.4982771,6.555471,0.60173106,6.238926,2.5689147,3.1377196,3.9904628,0.6634234,-1.1605322,-1.1850747,0.7020217,1.1850054,0.11288789,-0.060795475,-0.73963016,2.6706886,3.1121957,3.277252,2.3769224,4.961157,-0.7341495,-0.8122432,2.1201043,3.651189,3.4021764,0.16303536,2.8154652,3.9590847,-1.8297266,2.4992883,3.3722908,3.9848945,4.1663766,-11.830565,-11.7146,0.2135202,2.4257896,1.8021387,1.0231217,1.5784432,2.3152916,3.6145325,4.146683,1.6479497,7.317689,2.1531792,2.113896,2.7379665,0.929345,1.6316576,2.8296914,0.55356795,2.5154018,2.622691,2.7870073,1.145321,4.329731,3.9192445,-11.705837,2.9491677,3.864794,2.0750036,2.9282296,3.9636834,2.6496534,2.745861,2.797779,2.689447,-0.6717001,-0.5639682,-0.8780832,-0.48756677,-0.5991779,-1.6230838,-2.084311,3.4234815,3.9355364,3.982906,3.701453,3.559237,3.3967505,3.7141297,3.5315464,3.551191,3.9273129,3.78671,3.8063254,3.4874175,3.069308,3.7658157,3.561773,4.0187664,0.23925339,-0.3372472,0.6243849,0.37910864,3.6615536,3.7320075,2.9266703,3.4658656,-5.3949227,-5.3947096,3.1868834,0.81160647,0.18379784,0.023218974,0.014035724,3.30812,-0.12500022,1.6106579,0.35889223,0.71953934,-0.27915946,0.022742333,-18.574116,-18.043879,-18.211071,-18.056965,-18.11653,-18.351114,-18.342325,-18.650932,-18.030354,-18.246193,-18.21893,-18.312765,-18.356035,-18.535398,-18.26185,-18.392658,-18.439268,-18.29678,-18.133425,-17.407127,-18.07442,-18.446835,-17.912205,-17.847738,-17.158241,-18.108995,-18.187397,-18.267675,-18.242506,-18.269476,-17.119057,-18.165014,-19.117123,-18.421032,-18.490238,-18.323814,-17.815516,-18.475407,-18.395517,-18.33795,-18.296265,-18.3118,-17.900417,-18.1762,-18.103752,-18.246216,-18.494375,-18.357925,-18.208698,-18.65183,-19.047932,-19.159376,-18.36533,-18.21527,-18.27529,-18.297394,-19.102665,-18.561834,-18.391777,-18.392277,-19.040617,-19.128473,-18.94596,-18.5784,-18.626783,-18.912857,-18.401958,-0.006097479,-0.22239788,4.5343847,-0.48322046,6.3157096,6.7880983,-3.0660493,0.28426576,-3.008479,6.526027,6.720893,-11.179728,2.819284,2.8553908,-12.161641,0.45385775,0.54413515,-11.936062,2.9033585,3.0051665,3.6405008,0.8062491,3.3184724,1.5616691,-12.577825,1.0395564,1.3454938,1.3051745,1.801406,1.0386235,1.1291537,1.6488931,2.3288784,0.8871671,1.8472469,1.374892,0.5632499,1.7101443,2.4145176,3.9817488,1.7625729,1.5704782,2.4101605,1.4116611,1.2648548,0.9028555,1.7292702,1.9415331,1.5604745,1.2539616,1.621033,1.2436392,1.26595,2.1024916,3.9481125,0.92333674,1.0301976,0.16628751,1.5055078,1.687801,1.683991,2.485005,4.6675587,2.0348043,2.0575666,1.5276421,1.6003976,0.9443652,-2.3373148,-0.598776,-0.3266999,-0.32868034,-2.1537051,-0.24962996,5.8251157,6.69265,6.87207,-0.8502443,-2.3037617,6.5944195,3.904499,-6.067221,3.889885,3.4451954,-0.29633617,2.9136243,2.9596584,-0.25663626,-0.12981145,-0.32619715,-6.0787787,-0.64244133,0.6519664,-0.65024066,-0.57595426,-0.5520419,-0.6519987,2.246362,2.9638414,4.9041247,4.4636316,4.1272945,0.14785352,0.045332145,-0.32701543,0.4694868,-0.7398457,-12.48456,-11.764632,-11.944531,5.9691405,6.334675,3.107503,3.5452995,3.9092777,0.7749822,0.18082212,-0.13774061,0.21185546,3.5277236,0.2659733,-0.8000277,-4.09769,-6.0576944,-0.99320966,-3.822564,-1.377237,-1.3876659,-1.4298077,-1.3763801,-1.4386555,-1.3570257,-1.3617086,-1.3812228,-1.3979537,-1.467952,-1.4403584,-1.3957953,-1.9937685,0.29780835,-0.85183704,0.91671664,0.12157394,2.2594776,-0.15587296,6.4223456,6.394982,-12.177459,1.005167,0.30680922,0.115825534,1.9571807,5.4855056,4.5042315,0.7890427,6.8584995,6.49904,5.8271317,2.79773,-11.531897,-0.25164834,0.036414076,-1.6399499,-1.3428235,0.5590334,3.5668514,3.3257148,1.8024036,0.6311489,0.6693023,2.9357975,0.932253,2.938525,1.6530874,-2.3683145,5.42089,1.4598409,2.592069,0.038256463,-0.039236948,-0.21431267,-0.5561291,-0.7427887,-12.234564,-0.02777471,-0.031081475,-0.55992156,1.528162,-0.61820143,-12.182758,2.5130622,0.36918616,0.43591246,1.0166184,1.186762,3.8707674,4.7672615,4.8453827,4.2310333,-1.1084101,0.70496404,0.49495247,2.3666143,3.1418521,3.508677,6.1689987,6.1769075,2.2591965,0.91661173,7.0161014,6.814141,6.1358175,6.8800383,6.025841,6.7697473,6.9768047,6.374522,-1.5709873,-1.1104525,-0.44687778,-0.8663909,0.09008584,3.02588,3.0426536,-12.089262,-0.06285543,0.36964828,-0.0990342,0.1704149,2.657929,-4.2146344,0.79525393,1.0603698,6.041474,5.9664865,5.919848,-0.056897853,0.51828915,0.5581434,-0.011485562,-1.8793732,6.257321,6.5971503,7.3582816,5.137437,6.337755,0.7898576,0.5477495,-0.6685023,1.0849338,0.70443547,-0.046575293,2.7823427,2.8996558,-1.4344972,-1.0028763,0.12550414,-1.7632113,0.011792106,-0.6233844,-1.6243695,-1.2401675,-0.43286958,-1.8532287,-1.2003131,-0.8146143,0.7249228,-1.2056653,3.5699334,2.7120905,0.62592995,0.3694534,2.308131,0.26785806,0.4023602,-0.48031163,-0.17909622,-0.20262596,-0.5114328,-0.06534649,0.011070154,-0.0069488897,0.15227467,-0.052428238,-1.8854922,-2.1002135,-2.2624166,-2.1645207,-2.020197,6.5322967,-1.6068099,-1.8960196,6.4829617,-1.8666652,-1.82028,-1.7419474,-1.9308815,3.1359034,0.85886556,3.8324401,0.083305374,2.8931062,3.3079603,2.2155733,3.508568,-0.22143869,3.6768787,3.5274353,3.077327,0.109934025,3.2769828,3.695879,3.5166485,3.5167537,3.2288969,3.4728887,3.8157732,3.597957,3.9789636,5.0827837,3.4904425,-0.563001,-0.49434298,3.9049149,4.0007153,-0.47236958,3.6648033,1.6586512,0.2230036,2.1451774,0.7261562,1.315309,0.96776205,0.7624496,0.6830221,1.8216456,0.5113609,1.1342938,0.69066304,0.8541005,0.7124637,0.6014278,1.1836069,1.7610734,0.4588447,0.48447224,0.6080133,0.5276438,0.81297946,0.6038069,1.5316434,1.755029,6.398152,6.451578,6.449711,-0.35472426,0.06562256,-0.13012783,-0.383579,-0.49479648,-0.5137551,-0.11841423,2.863763,3.1293907,3.3972027,3.1934378,3.0854833,3.0363765,3.0141706,-12.288235,0.33654267,0.84895694,-0.45664996,-0.33384153,-0.111658946,0.5517543,-0.684595,-0.16268462,-0.62336373,0.62122506,0.5933285,5.0301175,-0.6744403,1.8571832,-0.5810083,-0.5551272,1.0960437,1.2627267,1.1468954,0.7834493,1.9235442,0.58982563,0.8373524,1.044004,0.12843429,0.31273946,2.339927,2.6039524,-12.06658,3.2638423,0.70696855,1.2217847,3.1230514,2.9311836,2.9053504,2.860524,2.8719423,2.873089,4.463126,4.2263246,6.4392023,-1.0771954,0.24071899,0.870823,2.2192507,3.940589,4.2357564,-1.5615515,-1.667689,2.500489,0.892935,1.3441393,-1.8520724,0.54835474,3.0815382,2.2041757,2.0941193,2.7279086,2.4714928,2.8935037,2.1058238,-1.9483352,-0.77404386,-0.80311507,2.7147992,2.9639769,2.3019333,-0.10717399,2.611951,3.1219816,3.6569703,4.659353,3.0048134,4.497451,4.42427,4.926977,4.4949436,-12.569215,1.5042064,2.8560522,6.4119487,6.185319,6.0797987,3.3211386,6.206445,-1.1867247,-0.71722686,-1.9469885,2.3163779,5.0239706,3.9752486,1.0118884,0.9310807,1.0516394,2.385688,0.42860812,3.1041458,4.839527,0.8051618,1.5502336,6.3118954,1.6553984,1.3040012,2.7193315,2.736693,0.56366414,-0.028149875,-0.6624501,5.7774796,2.4647691,3.321416,3.5623055,0.0003732243,3.1265216,1.9947534,3.3659523,3.4497626,-12.428074,2.4814935,2.37536,0.10663769,-1.2692356,3.0679717,4.452522,5.791727,1.4304062,-0.17435679,2.4428108,-1.7273201,-1.3046769,0.21281493,-1.6387517,0.12298099,1.5623597,1.7142687,0.9246138,1.5355866,1.5041935,2.1178305,1.2695922,0.9725404,0.9706706,0.9648499,-0.14210327,-0.46600336,2.452419,-0.95777893,2.8403249,3.0196333,2.4662755,3.1629934,3.7126782,3.7580087,3.8250153,4.086274,3.3843918,2.5923145,3.8504782,4.064024,3.7722192,1.4409968,3.786821,3.193301,4.391523,3.444289,3.4741693,-12.554264,6.5122914,6.001724,-0.7077529,3.5108511,3.5443828,3.4182422,3.210874,-12.03436,-11.491562,-11.729028,-11.910645,2.7025986,2.6138284,2.5541186,-12.068811,-12.275195,1.2319361,1.1263688,1.9151735,4.402933,1.1208253,-6.090202,4.3221636,4.571871,3.315529,4.5268674,3.384719,3.294237,3.3671997,3.6176252,5.466363,5.098711,5.0929103,3.7103083,4.1757736,2.5846043,-0.9360247,-0.18474254,2.7152488,2.3961477,1.7175187,2.630621,2.5897105,2.7174368,3.2859526,3.5833073,3.6572874,2.6196258,-12.376,3.096645,0.3995346,0.33643997,0.7945794,1.1790715,5.5449486,5.165608,4.872226,4.5056515,6.9251947,6.3116765,0.90391177,-1.0734288,0.7598435,1.2135506,0.36483145,3.0263562,3.1121209,3.3692873,-2.0161726,0.6205531,-0.5906631,1.7942042,-1.4085923,-0.55574054,0.7792231,6.6993113,7.151066,6.7277946,0.25652763,4.2540293,-0.21384531,-0.07365852,-0.17363752,0.79041994,0.9123425,1.8886508,0.1284355,2.5258281,-0.59111595,-0.38815507,-0.77195585,-6.3612065,-0.6180454,-0.7766227,-0.99969816,6.2429023,6.2900276,-11.869645,-6.2417054,-6.288219,-6.319989,-6.517488,6.0032444,2.9740124,2.9372633,3.11105,4.023056,3.640643,3.670888,-0.60516554,-0.3001364,0.65852624,1.6963133,-6.2725143,4.010975,0.9143531,-6.26911,6.4744277,0.8857963,-1.014721,-0.105799295,0.12592824,1.7546544,0.49696922,1.0797911,-0.8544515,2.6259344,3.3535035,6.3959103,6.574245,6.868531,0.4398263,-12.025036,-12.644327,0.07873469,1.5236807,-1.5578809,4.1891484,3.996999,0.6597536,0.21969774,3.666469,4.369246,2.225984,2.9671583,2.58647,-12.121129,-12.090971,-12.109372,-12.174059,2.5272171,2.7981832,3.0094428,3.5773156,3.4922,3.6251612,-12.288404,6.573923,7.296979,-11.8771925,0.085996516,0.97383153,-1.4062983,0.78400224,0.39258865,0.6616104,2.5227506,-11.938474,1.7181695,2.6950746,-12.085212,-0.77860177,-0.70776695,-6.6786256,-0.79427254,-0.11579415,-0.7918554,-0.8343939,0.6006928,0.060584426,-0.7203945,-0.7698228,2.4664876,0.3044964,-2.2983246,-2.2155201,-1.8556747,-0.5363866,-2.1997104,-2.3735085,-2.4649303,0.69793135,1.0026149,0.43365183,0.26520562,0.54784054,1.573955,-1.466671,1.3499112,6.413215,6.573733,-0.58095586,-0.20661107,0.59697527,-0.40905255,-0.32281202,-0.49178705,0.39800972,0.38710073,-0.0792782,-0.3448248,1.3861547,0.58257693,0.45672113,0.78987896,0.697273,1.4688348,4.467825,5.250674,4.958604,4.351928,4.9753184,4.826726,3.5436516,3.4006896,3.88578,4.5127807,7.652213,7.791026,7.612349,7.742501,-6.2029543,-6.236975,-6.306194,-6.2638774,-6.2373266,6.9181194,7.6131663,7.660538,1.6816286,3.8344905,3.8512957,2.8014376,2.091995,3.4547603,4.0316477,4.061783,4.1927423,3.9415472,4.252604,1.0272214,4.2652164,1.2828642,0.57290643,0.68152773,1.2994146,2.1356277,2.5721138,2.6021233,4.4616833,2.8558767,2.4597888,0.7627034,2.9030643,3.489311,3.226073,6.6664877,-0.93585646,3.6595664,-0.53670716,-0.691116,-0.22596772,1.2754745,-1.2217689,1.3986546,-0.8095282,-0.25239944,0.024319988,-0.34090552,0.39155373,-1.6288961,-2.3762717,-2.201259,-2.2951276,-1.5274016,-2.0093849,-0.74489707,-0.74602646,-0.7074779,-0.7561724,0.10707445,0.009665898,-11.699674,0.91222185,0.9378357,0.6046231,3.7688677,3.8028324,3.7425196,-12.234321,2.6840508,2.4402263,3.897024,2.9996638,3.1963925,2.4807632,1.7253561,2.8600278,-0.0974449,4.8294716,2.0103233,1.6200621,2.6276746,1.5234438,1.5204892,1.2955428,0.14037465,1.0689224,-0.33133233,0.070466146,-0.46979916,-6.396028,4.2486343,-6.217451,5.659852,-1.2311276,1.8081422,-1.8005744,-0.13593519,-0.3569584,1.9636528,2.5417745,2.9265378,2.7909977,-12.082173,-12.60094,0.24285543,0.3761285,-0.117845036,1.7015355,0.41995448,-5.366465,1.2914417,1.1253514,1.700573,1.7220598,1.41203,0.96945435,1.6391494,1.6620176,-0.7591192,-7.7877526,1.6579443,0.47873005,-6.467631,3.4543998,0.7798708,0.9719678,1.0595454,1.1310204,0.6911819,2.2217894,0.6244929,4.1912146,3.1129007,-0.041137543,3.671395,3.340989,3.8667185,3.6808426,3.8054206,3.9975522,3.8105652,3.7054138,-11.873321,2.3657343,0.080779195,-1.6716285,1.8522065,3.0352046,-11.216938,2.90626,2.9754097,1.9676433,3.6149719,6.370647,6.357893,2.3925216,1.8921862,1.2019081,2.6405985,2.3549674,3.57961,4.5206275,-12.15852,-12.1995945,0.5739438,1.1629102,0.546738,0.45808387,0.15805966,-11.729666,-0.9308314,2.1741552,3.037907,3.3691301,3.3633063,3.7386394,3.3926563,3.6326935,1.6195338,-1.2468901,0.48643404,-12.264519,-12.520182,2.0435874,0.3336361,1.5956107,-1.6171998,-1.8014259,-2.224387,-0.78884846,-1.5951393,-1.4994962,-1.5052234,2.0432339,1.825997,-1.6369737,-2.5704606,1.658454,3.7075193,2.2219446,2.6939578,3.611132,1.9976871,1.7442919,1.7263433,-0.3785933,2.7179565,1.8462268,2.286354,-12.101726,0.37692323,0.56538194,2.0927134,-4.4659395,6.2871456,-0.9629804,5.9821973,6.0321965,-1.0828872,6.119016,-1.7052441,-2.0452163,-2.1245642,-1.3216891,6.8266735,2.6665547,3.1320674,3.0721824,4.440868,-0.3201474,2.2340086,2.2534952,0.25545138,0.49537864,-0.15875393,-0.16035676,-0.117962874,1.1056467,1.4212327,2.3934958,1.454663,1.6643949,0.79302657,1.375727,1.4664127,2.2332716,2.153944,1.4801352,1.8360534,1.2044199,1.0162398,0.87157047,2.208257,1.9159154,1.099153,1.221602,0.32937828,1.4275244,1.6175271,1.5240717,2.3518405,4.387657,2.3740354,1.6761473,1.3949804,-0.8249696,0.7496031,0.6568805,0.07681849,-0.57783365,1.5117444,2.34672,2.9566207,2.6869817,2.972432,-11.569747,7.0767922,6.8754587,6.8531227,0.80121523,0.5322307,-0.5407026,0.20829195,-5.0711126,0.72221696,0.28456104,0.3803958,-11.914572,1.4317138,1.6078265,-12.341015,-12.013498,-11.978926,-11.848159,-11.994605,2.8589678,3.182754,-12.30856,3.3726842,3.815441,0.107636884,4.7946177,0.6100493,6.439664,3.1195822,3.9376507,-0.4051981,5.8429346,6.6506696,6.799616,1.2043269,7.500951,7.7684193,7.690865,6.501133,-1.1595325,7.2408786,7.040057,3.1738527,2.6584141,3.0622473,3.7954125,3.417162,3.8793404,3.8992565,4.270072,1.6104038,2.224956,3.617182,3.6338384,4.1926904,4.3090425,3.9963307,4.4656053,4.4423575,4.2248955,3.3821146,4.2949967,4.248317,3.534668,3.916388,3.7805345,-2.4251218,3.972189,-1.9174179,-1.6219898,3.2978394,-9.792986,-11.413108,-0.411877,-0.37647718,-0.48115575,2.8143525,2.7207804,0.2425553,2.794003,3.1605644,3.9498618,2.2054665,3.5699725,3.6467192,1.9887528,1.6077495,-12.133519,-12.46857,0.24683979,1.2985637,2.186493,2.5804017,1.896049,1.8858529,-2.3553245,2.351186,2.3300488,1.806066],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Git over SSH\\n\\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shel...\"],[\"```\\n$ ssh-add ~\\u002f.ssh\\u002fid_ed25519\\n```\\n\\nIf you chose a different location than the default to store you...\"],[\"Using Flair at Hugging Face\\n\\n[Flair](https:\\u002f\\u002fgithub.com\\u002fflairNLP\\u002fflair) is a very simple framework f...\"],[\"```\\n\\nIt outputs the following:\\n\\n```text\\nSentence[6]: \\\"George Washington ging nach Washington.\\\" â†’ [\\\"G...\"],[\"Widget Examples\\n\\nNote that each widget example can also optionally describe the corresponding model ...\"],[\"```\\n\\n### Summarization\\n\\n```yaml\\nwidget:\\n- text: \\\"The tower is 324 metres (1,063 ft) tall, about the ...\"],[\"```\\n\\n### Text Generation\\n\\n```yaml\\nwidget:\\n- text: \\\"My name is Julien and I like to\\\"\\n  example_title:...\"],[\"```\\n\\n### Feature Extraction\\n\\n```yaml\\nwidget:\\n- text: \\\"My name is Sylvain and I live in Paris\\\"\\n  exam...\"],[\"```\\n\\n## Computer Vision\\n\\n### Image Classification\\n\\n```yaml\\nwidget:\\n- src: https:\\u002f\\u002fhuggingface.co\\u002fdat...\"],[\"```\\n\\n### Document Question Answering\\n\\n```yaml\\nwidget:\\n- text: \\\"What is the invoice number?\\\"\\n  src: \\\"...\"],[\"```\\n\\n## Other\\n\\n### Structured Data Classification\\n\\n```yaml\\nwidget:\\n- structured_data:\\n    fixed_acid...\"],[\"Configure the Dataset Viewer\\n\\nThe Dataset Viewer supports many [data files formats](.\\u002fdatasets-addin...\"],[\"Adding a Sign-In with HF button to your Space\\n\\nYou can enable a built-in sign-in flow in your Space ...\"],[\"```\\n\\nYou can check out the [configuration reference docs](.\\u002fspaces-config-reference) for more inform...\"],[\"Those scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\\n\\n-...\"],[\"Basically, you need to:\\n\\n- Redirect the user to `https:\\u002f\\u002fhuggingface.co\\u002foauth\\u002fauthorize?redirect_uri...\"],[\"User access tokens\\n\\n## What are User Access Tokens?\\n\\nUser Access Tokens are the preferred way to aut...\"],[\"If you are a member of an organization with read\\u002fwrite\\u002fadmin role, then your User Access Tokens will...\"],[\"## How to use User Access Tokens?\\n\\nThere are plenty of ways to use a User Access Token to access the...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\nTry not to leak your token! Though you can always rotate it, anyone will b...\"],[\"ZenML on Spaces\\n\\n[ZenML](https:\\u002f\\u002fgithub.com\\u002fzenml-io\\u002fzenml) is an extensible, open-source MLOps fram...\"],[\"Visit [the ZenML documentation](https:\\u002f\\u002fdocs.zenml.io\\u002f) to learn more about its\\nfeatures and how to ...\"],[\"To personalize your Space's appearance, such as the title, emojis, and colors,\\nnavigate to \\\"Files an...\"],[\"```\\n\\nYou can also use the Direct URL in your browser to use the ZenML dashboard as a\\nfullscreen appl...\"],[\"\\u003cTip warning={true}\\u003e\\nIf you wish to use a cloud secrets backend together with ZenML for secrets\\nmana...\"],[\"## ğŸ¤— Feedback and support\\n\\nIf you are having trouble with your ZenML server on HuggingFace Spaces, y...\"],[\"Using Spaces for Organization Cards\\n\\nOrganization cards are a way to describe your organization to o...\"],[\"Repository Settings \\n\\n## Private repositories\\n\\nYou can choose a repository's visibility when you cre...\"],[\"## Disabling Discussions \\u002f Pull Requests\\n\\nYou can disable all discussions and Pull Requests. Once di...\"],[\"--\\n# Example metadata to be added to a dataset card.  \\n# Full dataset card template at https:\\u002f\\u002fgithu...\"],[\"task_categories:  # Full list at https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface.js\\u002fblob\\u002fmain\\u002fpackages\\u002fta...\"],[\"# Optional. This part can be used to store the feature types and size of the dataset to be used in p...\"],[\"```\\n\\n# Optional. If you want your dataset to be protected behind a gate that users have to accept to...\"],[\"Valid license identifiers can be found in [our docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002frepositories-li...\"],[\"Repository limitations and recommendations\\n\\nThere are some limitations to be aware of when dealing w...\"],[\"Under the hood, the Hub uses Git to version the data, which has structural implications on what you ...\"],[\"- **Repository size**: The total size of the data you're planning to upload. There is no hard limit ...\"],[\"improve the service, but one must always remember that a git repository is not meant to work as a da...\"],[\"Dask\\n\\n[Dask](https:\\u002f\\u002fgithub.com\\u002fdask\\u002fdask) is a parallel and distributed computing library that scal...\"],[\"```\\n\\nThis creates a dataset repository `username\\u002fmy_dataset` containing your Dask dataset in Parquet...\"],[\"Access control in organizations\\n\\n\\u003cTip\\u003e\\n\\nYou can set up [Single Sign-On (SSO)](.\\u002fsecurity-sso) to be ...\"],[\"Billing\\n\\nAt Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), an...\"],[\"Any feedback or support request related to billing is welcome at billing@huggingface.co.\\n\\n## Invoici...\"],[\"Streamlit Spaces\\n\\n**Streamlit** gives users freedom to build a full-featured web app with Python in ...\"],[\"```\\n\\nYou can edit the `sdk_version`, but note that issues may occur when you use an unsupported Stre...\"],[\"## Add the dependencies\\n\\nFor the **Hot Dog Classifier** we'll be using a [ğŸ¤— Transformers pipeline](h...\"],[\"```\\ntransformers\\ntorch\\n```\\n\\nThe Spaces runtime will handle installing the dependencies!\\n\\n## Create t...\"],[\"```\\n\\nThis Python script uses a [ğŸ¤— Transformers pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fpi...\"],[\"```\\n\\n\\u003c!-- The height of this iframe has been calculated as 236 + 64 * 2. 236 is the inner content he...\"],[\"- `id` is set to `\\u003ciframe \\u002f\\u003e` that is used to specify the auto-resize target.\\n- The `iFrame Resizer`...\"],[\"```\\n\\nAdditionally, you can checkout [our documentation](.\\u002fspaces-embed)....\"],[\"Next Steps\\n\\nThese next sections highlight features and additional information that you may find usef...\"],[\"Beyond making it easy to identify important commits in your repo's history, using Git tags also allo...\"],[\"A duplicate of a repository with the commit history preserved is called a *fork*. You may choose to ...\"],[\"```\\ngit clone git@hf.co:me\\u002fmyfork\\n```\\n\\n3. Fetch non-LFS files:\\n\\n```\\ncd myfork\\ngit lfs install --skip...\"],[\"Run with Docker\\n\\nYou can use Docker to run most Spaces locally.\\nTo view instructions to download and...\"],[\"Advanced Topics\\n\\n## Contents\\n\\n- [Using OpenCV in Spaces](.\\u002fspaces-using-opencv)\\n- [More ways to crea...\"],[\"Using spaCy at Hugging Face\\n\\n`spaCy` is a popular library for advanced Natural Language Processing u...\"],[\"```\\n\\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open th...\"],[\"```\\n\\nYou can then check if the command has been registered successfully\\n\\n```bash\\npython -m spacy hug...\"],[\"```\\n\\n| Argument             | Type         | Description                                            ...\"],[\"```bash\\nhuggingface-cli login\\npython -m spacy package .\\u002fen_ner_fashion .\\u002foutput --build wheel\\ncd .\\u002fo...\"],[\"```\\n\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browse...\"],[\"Audit Logs\\n\\n\\u003cTip warning={true}\\u003e\\nThis feature is part of the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fenterpr...\"],[\"Spaces Overview\\n\\nHugging Face Spaces make it easy for you to create and deploy ML-powered demos in m...\"],[\"Under the hood, Spaces stores your code inside a git repository, just like the model and dataset rep...\"],[\"| **Hardware**        \\t| **GPU Memory** \\t| **CPU** \\t| **Memory** \\t| **Disk** \\t| **Hourly Price** \\t|\\n...\"],[\"| Ephemeral (default) \\t| 50GB                \\t| No               \\t| Free!                \\t|\\n| Small ...\"],[\"Note: Find more detailed and comprehensive pricing information on [our pricing page](https:\\u002f\\u002fhugging...\"],[\"Variables are publicly accessible and viewable and will be automatically added to Spaces duplicated ...\"],[\"Some Spaces might have environment variables that you may need to set up. In these cases, the duplic...\"],[\"In case [OAuth](.\\u002fspaces-oauth) is enabled for your Space, the following variables will also be avai...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\ntitle: My lovely space\\nemoji: ğŸ¤—\\ncolorFrom: blue\\ncolorTo: green\\nsdk: docker\\npinned: false\\nmodels:...\"],[\"Search\\n\\nYou can now easily search anything on the Hub with **Full-text search**. We index model card...\"],[\"Moreover, one can copy & share the URL from one's browser's address bar, which should contain the fi...\"],[\"[paddlenlp-banner](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub...\"],[\"## Installation\\n\\nTo get started, you can follow [PaddlePaddle Quick Start](https:\\u002f\\u002fwww.paddlepaddle....\"],[\"```\\npip install -U paddlenlp\\n```\\n\\n## Using existing models\\n\\nSimilar to `transformer` models, the `pa...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in paddlenlp` and you will ...\"],[\"Reference\\n\\n## Deep Learning Container\\n\\nBelow you can find a version table of currently available Hug...\"],[\"**Example 1: PyTorch Training:**\\n`763104351884.dkr.ecr.us-west-2.amazonaws.com\\u002fhuggingface-pytorch-t...\"],[\"| ğŸ¤— Transformers version | ğŸ¤— Datasets version | PyTorch\\u002fTensorFlow version | type     | device | Pyt...\"],[\"| 4.11.0                  | 1.12.1              | PyTorch 1.9.0              | training | GPU    | 3...\"],[\"## Inference DLC Overview\\n\\nThe Inference DLC overview includes all released and available Hugging Fa...\"],[\"| ğŸ¤— Transformers version | PyTorch\\u002fTensorFlow version | type      | device | Python Version |\\n| ----...\"],[\"| 4.11.0                  | PyTorch 1.9.0              | inference | CPU    | 3.8            |\\n| 4.1...\"],[\"## Hugging Face Transformers Amazon SageMaker Examples\\n\\nExample Jupyter notebooks that demonstrate h...\"],[\"| Notebook                                                                                          ...\"],[\"| [03 Distributed Training: Data Parallelism](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsag...\"],[\"| [08 Distributed Training: Summarization with T5\\u002fBART](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblo...\"],[\"| [14 Fine-tune and push to Hub](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsagemaker\\u002f14_tra...\"],[\"## Inference Toolkit API\\n\\nThe Inference Toolkit accepts inputs in the `inputs` key, and supports add...\"],[\"```\\n\\n**`sentiment-analysis`**\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"Don't waste your time.  We had two different p...\"],[\"```\\n\\n**`parameterized-request`**\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"Hugging Face, the winner of VentureBeatâ€™s I...\"],[\"```\\n\\n**`HF_API_TOKEN`**\\n\\n`HF_API_TOKEN` defines your Hugging Face authorization token. The `HF_API_T...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fgithub.com\\u002fpandas-dev\\u002fpandas) is a widely used Python data analysis toolkit...\"],[\"```\\n\\nThis creates a dataset repository `username\\u002fmy_dataset` containing your Pandas dataset in Parqu...\"],[\"Datasets without language challenge\\n\\nRelated to https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fissues\\u002f986.\\n...\"],[\"```\\n\\nHaving this field filled in is essential for users to find datasets in their language and give ...\"],[\"1. Find a dataset that doesn't have the `language` field filled in. You can find a list of datasets ...\"],[\"4. Once you've identified the language(s) of the dataset, you can add the language tag(s) to the dat...\"],[\"## F.A.Q.\\n\\n### Does it make sense to add language metadata to all datasets?\\n\\nNo! This is why we have...\"],[\"```\\n\\n## Datasets without language field filled in...\"],[\"| status | pr_url                                                                                   ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMatthijs\\u002fcmu-arctic-xvectors\\u002fdiscussions\\u002f4)       ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fiamtarun\\u002fpython_code_instructions_18k_alpaca\\u002fdiscu...\"],[\"|        |                                                                                          ...\"],[\"|        |                                                                                          ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fajaykarthick\\u002fimdb-movie-reviews\\u002fdiscussions\\u002f1)    ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvhtran\\u002fid-en\\u002fdiscussions\\u002f1#651ababdc4fdc1c93efb0f2...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdandrade\\u002fes-en\\u002fdiscussions\\u002f1#651ac2720047dc5f7aae8...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ffathyshalab\\u002fDialogsum-german\\u002fdiscussions\\u002f1)       ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmanu\\u002fwmt-en-fr\\u002fdiscussions\\u002f1#651ab850e3558015826cd...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdmayhem93\\u002fagieval-logiqa-en\\u002fdiscussions\\u002f1#651ab8cd...\"],[\"|        |                                                                                          ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpszemraj\\u002fsimplepile-lite\\u002fdiscussions\\u002f1)           ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgenerative-newsai\\u002fnews-unmasked\\u002fdiscussions\\u002f1)    ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fTigerResearch\\u002ftigerbot-wiki-qa-bart-en-10k\\u002fdiscuss...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkmkarakaya\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#65...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthomasavare\\u002fitalian-dataset-deepl2\\u002fdiscussions\\u002f2) ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmasoudjs\\u002fc4-en-html-with-metadata-ppl-clean\\u002fdiscus...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthomasavare\\u002fitalian-dataset-helsinki\\u002fdiscussions\\u002f1...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthesistranslation\\u002fdistilled-ccmatrix-fr-en\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fFreedomIntelligence\\u002falpaca-gpt4-italian\\u002fdiscussion...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-ente-da-sys-test\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-enhi-da-sys-test\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-enmr-da-test\\u002fdiscussions...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-engu-da-test\\u002fdiscussions...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enmr-da-sys-test\\u002fdi...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-ente-da-test\\u002fdiscus...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enhi-da-test\\u002fdiscus...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-ente-sys-test\\u002fdiscu...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-engu-sys-test\\u002fdiscu...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enmr-test\\u002fdiscussio...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002faimona\\u002fstripchat-fixed-grammar-eng\\u002fdiscussions\\u002f1#6...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fVFiona\\u002fcovid-19-synthetic-it-en-5000\\u002fdiscussions\\u002f1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpvduy\\u002foasst-h4-en\\u002fdiscussions\\u002f2#651ac64ada7605b213...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-iu-en\\u002fdiscussions\\u002f1#651ac567c4fdc...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-pl-en\\u002fdiscussions\\u002f1#651ac51f11f56...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-ru-en\\u002fdiscussions\\u002f1#651ac4cd9c406...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-en-km\\u002fdiscussions\\u002f1#651ac46a88af1...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2023-en-ru-train-val-split-0.2...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2023-en-vi-train-val-split-0.2...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2023-en-vi-train-val-split-0.1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2022-en-es\\u002fdiscussions\\u002f1#651ac...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvocab-transformers\\u002fwiki-en-passages-20210101\\u002fdiscu...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMakxxx\\u002ffrench_CEFR\\u002fdiscussions\\u002f1)                 ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgollumeo\\u002ffrench-litterature\\u002fdiscussions\\u002f1)        ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ffathyshalab\\u002fgermanquad_qaeval_dataset\\u002fdiscussions\\u002f...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgermank\\u002fhh-rlhf_with_features_flan_t5_large-no_eos...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fHarsit\\u002fxnli2.0_train_german\\u002fdiscussions\\u002f1)        ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002forhanxakarsu\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002forhanxakarsu\\u002fturkishPoe-generation-1\\u002fdiscussions\\u002f1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ferkanxyzalaca\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002framazank2000\\u002fturkishReviews-ds-mini1\\u002fdiscussions\\u002f1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkaaniince\\u002fturkishReviews-ds-textGeneration\\u002fdiscuss...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMursel\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#651aec...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMemis\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#651aec9...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fAnanthZeke\\u002ftamil_sentences_sample\\u002fdiscussions\\u002f1#65...\"],[\"Advanced Topics\\n\\n## Contents\\n\\n- [Integrate your library with the Hub](.\\u002fmodels-adding-libraries)\\n- [...\"],[\"Storage Regions on the Hub\\n\\nRegions let you decide where your org's models and datasets will be stor...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub\\u002fstorage-region...\"],[\"Managing Spaces with Github Actions\\n\\nYou can keep your app in sync with your GitHub repository with ...\"],[\"```\\n\\nFinally, create an Action that automatically checks the file size of any new pull request:\\n\\n\\n``...\"],[\"Webhook guide: Setup an automatic metadata quality review for models and datasets \\n\\n\\u003cTip\\u003e\\n\\nWebhooks ...\"],[\"```\\n\\nThis metadata contains essential information about your model or dataset for potential users. T...\"],[\"```python\\nfrom huggingface_hub import DatasetCard, ModelCard\\nfrom huggingface_hub.utils import Entry...\"],[\"```\\n\\nThis function will return a Python dictionary containing the metadata associated with the repos...\"],[\"```\\n\\nOnce we have this dictionary, we can create our metadata report. In the interest of brevity, we...\"],[\"```\\n\\n\\u003cTip\\u003e\\n    `:=` is the Python Syntax for an assignment expression operator added to the Python l...\"],[\"\\u003cTip\\u003e\\n\\tWhen creating a bot that will interact with other users on the Hub, we ask that you clearly l...\"],[\"```\\n\\nThe above function will receive Webhook events and creates or updates the metadata review repor...\"],[\"If you build a metadata quality app using Webhooks, please tag me @davanstrien; I would love to know...\"],[\"ğŸŸ§ Label Studio on Spaces\\n\\n[Label Studio](https:\\u002f\\u002flabelstud.io) is an [open-source data labeling\\nplat...\"],[\"After launching Label Studio, you will be presented with the standard login\\nscreen. You can start by...\"],[\"### Enable Configuration Persistence\\n\\nBy default, this Space stores all project configuration and da...\"],[\"* `STORAGE_AWS_SECRET_ACCESS_KEY`: `\\u003cYOUR_SECRET_ACCESS_KEY\\u003e`\\n\\n* `STORAGE_AWS_BUCKET_NAME`: `\\u003cYOUR_B...\"],[\"Webhooks\\n\\n\\u003cTip\\u003e\\n\\nWebhooks are now publicly available!\\n\\n\\u003c\\u002fTip\\u003e\\n\\nWebhooks are a foundation for MLOps-r...\"],[\"```json\\n{\\n  \\\"event\\\": {\\n    \\\"action\\\": \\\"create\\\",\\n    \\\"scope\\\": \\\"discussion\\\"\\n  },\\n  \\\"repo\\\": {\\n    \\\"type\\\"...\"],[\"```\\n\\n### Event\\n\\nThe top-level properties `event` is always specified and used to determine the natur...\"],[\"### Repo\\n\\nIn the current version of webhooks, the top-level property `repo` is always specified, as ...\"],[\"```\\n\\n`repo.headSha` is the sha of the latest commit on the repo's `main` branch. It is only sent whe...\"],[\"```\\n\\n## Webhook secret\\n\\nSetting a Webhook secret is useful to make sure payloads sent to your Webhoo...\"],[\"## Debugging Webhooks\\n\\nYou can easily find recently generated events for your webhooks. Open the act...\"],[\"Dataset viewer\\n\\nThe dataset page includes a table with the contents of the dataset, arranged by page...\"],[\"## Access the parquet files\\n\\nTo power the dataset viewer, every dataset is auto-converted to the Par...\"],[\"To have a properly working Dataset Viewer for your dataset, make sure your dataset is in a supported...\"],[\"The Model Hub\\n\\n## What is the Model Hub?\\n\\nThe Model Hub is where the members of the Hugging Face com...\"],[\"Signing commits with GPG\\n\\n`git` has an authentication layer to control who can push commits to a rep...\"],[\"For a more in-depth explanation of how git and GPG interact, please visit the the [git documentation...\"],[\"```\\n\\nGPG will then guide you through the process of creating a GPG key pair.\\n\\nMake sure you specify ...\"],[\"Spaces Changelog\\n\\n## [2023-07-28] - Upstream Streamlit frontend for `\\u003e=1.23.0`\\n\\n- Streamlit SDK uses...\"],[\"- Read more doc here: [Spaces sleep time](.\\u002fspaces-gpus#sleep-time)\\n\\n## [2022-12-07] - Add support f...\"],[\"## [2022-03-02] - Gradio version pinning\\n\\n- The `sdk_version` configuration field now works with the...\"],[\"- [Streamlit changelog](https:\\u002f\\u002fgithub.com\\u002fstreamlit\\u002fstreamlit\\u002freleases\\u002ftag\\u002f0.82.0)\\n\\n## [2021-08-01]...\"],[\"Licenses\\n\\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let o...\"],[\"\\u003c!-- region licenses --\\u003e\\nFullname | License identifier (to use in model card)\\n--- | ---\\nApache licen...\"],[\"Creative Commons Attribution No Derivatives 4.0 | `cc-by-nd-4.0`\\nCreative Commons Attribution Non Co...\"],[\"LaTeX Project Public License v1.3c | `lppl-1.3c`\\nMicrosoft Public License | `ms-pl`\\nMozilla Public L...\"],[\"In case of `license: other` please add the license's text to a `LICENSE` file inside your repo (or c...\"],[\"Hugging Face Hub documentation\\n\\nThe Hugging Face Hub is a platform with over 350k models, 75k datase...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-orange-100 bg-gradient-to-br from...\"],[\"\\u003cdiv class=\\\"flex items-center py-0.5 text-lg font-semibold text-orange-600 dark:text-gray-400 mb-1\\\"\\u003e...\"],[\"\\u003ca class=\\\"transform !no-underline transition-colors hover:translate-x-px hover:text-gray-700\\\" href=\\\"...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-indigo-100 bg-gradient-to-br from...\"],[\"\\u003cdiv class=\\\"flex items-center py-0.5 text-lg font-semibold text-indigo-600 dark:text-gray-400 mb-1\\\"\\u003e...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fm...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fm...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-red-100 bg-gradient-to-br from-re...\"],[\"\\u003csvg class=\\\"shrink-0 mr-1.5 text-red-400\\\" xmlns=\\\"http:\\u002f\\u002fwww.w3.org\\u002f2000\\u002fsvg\\\" xmlns:xlink=\\\"http:\\u002f\\u002fwww...\"],[\"13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003c\\u002fsvg\\u003e Datasets\\u003c\\u002f...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fd...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fd...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-blue-100 bg-gradient-to-br from-b...\"],[\"\\u003csvg class=\\\"shrink-0 mr-1.5 text-blue-500\\\" xmlns=\\\"http:\\u002f\\u002fwww.w3.org\\u002f2000\\u002fsvg\\\" xmlns:xlink=\\\"http:\\u002f\\u002fww...\"],[\"2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003cpath opacity=\\\".25\\\" d=\\\"M16....\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fs...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fs...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-green-100 bg-gradient-to-br from-...\"],[\"\\u003cdiv class=\\\"flex items-center py-0.5 text-lg font-semibold text-green-600 dark:text-gray-400 mb-1\\\"\\u003e\\n...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fo...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\"htt...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n## What's the Hugging Face Hub?\\n\\nWe are helping the community work together towards the goal...\"],[\"## Models\\n\\nYou can discover and use dozens of thousands of open-source ML models shared by the commu...\"],[\"The [ğŸ¤— `datasets`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002findex) library allows you to programmaticall...\"],[\"## Organizations\\n\\nCompanies, universities and non-profits are an essential part of the Hugging Face ...\"],[\"Using ESPnet at Hugging Face\\n\\n`espnet` is an end-to-end toolkit for speech processing, including aut...\"],[\"Here is an inference example:\\n\\n```py\\nimport soundfile\\nfrom espnet2.bin.tts_inference import Text2Spe...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be ...\"],[\"Model Card components\\n\\n**Model Card Components** are special elements that you can inject directly i...\"],[\"Annotated Model Card Template\\n\\n\\n## Template\\n\\n[modelcard_template.md file](https:\\u002f\\u002fgithub.com\\u002fhugging...\"],[\"_Instructions are provided below, in italics._\\n\\nTemplate variable names appear in `monospace`.\\n\\n# Mo...\"],[\"## Model Sources [optional]\\n\\n* **Repository:** `repo`\\n* **Paper [optional]:** `paper`\\n* **Demo [opti...\"],[\"## Recommendations\\n\\n\\n`bias_recommendations`\\n\\n_What are recommendations with respect to the foreseeab...\"],[\"### Metrics\\n\\n`testing_metrics`\\n\\n_What metrics will be used for evaluation in light of tradeoffs betw...\"],[\"# Glossary [optional]\\n\\n\\n**Section Overview:** This section defines common terms and how metrics are ...\"],[\"Organizations\\n\\nThe Hugging Face Hub offers **Organizations**, which can be used to group accounts an...\"],[\"Using ğŸ¤— Datasets\\n\\nOnce you've found an interesting dataset on the Hugging Face Hub, you can load the...\"],[\"Appendix\\n\\n## Appendix A: User Study\\n_Full text responses to key questions_\\n\\n### How would you define...\"],[\"### What do you like about model cards?\\n\\n* They are interesting to teach people about new models\\n* A...\"],[\"### Other key new insights\\n\\n* Model cards are best filled out when done by people with different rol...\"],[\"* Google Cloud: [Face Detection](https:\\u002f\\u002fmodelcards.withgoogle.com\\u002fface-detection), [Object Detectio...\"],[\"* [Duke PULSE Model Card](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2003.03808.pdf)\\n* [Stanford Dynasent](https:\\u002f\\u002fgithub...\"],[\"### MODEL CARDS FOR LARGE LANGUAGE MODELS\\nLarge language models are often released with associated d...\"],[\"### MODEL CARD GENERATION TOOLS\\nTools for programmatically or interactively generating model cards i...\"],[\"Notifications\\n\\nNotifications allow you to know when new activities (Pull Requests or discussions) ha...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"## Mute notifications for a specific repository\\n\\nIt's possible to mute notifications for a particula...\"],[\"How to configure SAML SSO with Azure\\n\\nIn this guide, we will use Azure as the SSO provider and with ...\"],[\"### Step 2: Configure your application on Azure\\n\\nOpen a new tab\\u002fwindow in your browser and navigate ...\"],[\"Then under \\\"SAML Certificates\\\", verify that \\\"Signin Option\\\" is set to \\\"Sign SAML response and assert...\"],[\"```\\n-----BEGIN CERTIFICATE-----\\n{certificate}\\n-----END CERTIFICATE-----\\n```\\n\\n\\u003cdiv class=\\\"flex justif...\"],[\"Gradio Spaces\\n\\n**Gradio** provides an easy and intuitive interface for running a model from a list o...\"],[\"## Create a new Gradio Space\\n\\nWe'll start by [creating a brand new Space](https:\\u002f\\u002fhuggingface.co\\u002fnew...\"],[\"```\\ntransformers\\ntorch\\n```\\n\\nThe Spaces runtime will handle installing the dependencies!\\n\\n## Create t...\"],[\"```\\n\\nThis Python script uses a [ğŸ¤— Transformers pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fpi...\"],[\"Cookie limitations in Spaces\\n\\nIn Hugging Face Spaces, applications have certain limitations when usi...\"],[\"Argilla on Spaces\\n\\n**Argilla** is an open-source, data labelling tool, for highly efficient human-in...\"],[\"\\u003cTip\\u003e\\n**IMPORTANT NOTE ABOUT DATA PERSISTENCE:**\\nYou can use the Argilla Quickstart Space as is for ...\"],[\"The Space template provides a way to set up different **optional settings** focusing on securing you...\"],[\"The combination of these secret variables gives you the following setup options:\\n\\n1. *I want to avoi...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentatio...\"],[\"```\\n\\nThird, you need to read the dataset using the `datasets` library. For reading other file types,...\"],[\"```\\n\\nTo train a SetFit model with this dataset:\\n\\n```python\\nfrom sentence_transformers.losses import ...\"],[\"Using Stable-Baselines3 at Hugging Face\\n\\n`stable-baselines3` is a set of reliable implementations of...\"],[\"```\\n\\nYou need to define seven parameters:\\n- `--model`: your trained model.\\n- `--model_architecture`:...\"],[\"File names and splits\\n\\nTo host and share your dataset, create a dataset repository on the Hugging Fa...\"],[\"```\\nmy_dataset_repository\\u002f\\nâ”œâ”€â”€ README.md\\nâ”œâ”€â”€ train.csv\\nâ”œâ”€â”€ test.csv\\nâ””â”€â”€ validation.csv\\n```\\n\\nIf you d...\"],[\"```\\n\\nMake sure all the files of your `train` set have *train* in their names (same for test and vali...\"],[\"Integrate your library with the Hub\\n\\nThe Hugging Face Hub aims to facilitate sharing machine learnin...\"],[\"```\\n\\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging F...\"],[\"```\\n\\n   `notebook_login` will launch a widget in your notebook from which you can enter your Hugging...\"],[\"```\\n\\nUse the `cache_dir` parameter to change where a file is stored:\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface...\"],[\"```\\n\\nDoing so will also add a tag to your model so users can quickly identify models from your libra...\"],[\"```\\n\\nWhen you check your Hugging Face account, you should now see a `test-model` repository under yo...\"],[\"```\\n\\nIf you need to upload more than one file, look at the [utilities offered by the `Repository` cl...\"],[\"```\\n\\n    * For each task your library supports, modify the `app\\u002fpipelines\\u002ftask_name.py` files accord...\"],[\"```\\n\\n### Register your libraries supported tasks on the hub\\n\\nTo register the tasks supported by your...\"],[\"Aim on Spaces\\n\\n**Aim** is an easy-to-use & supercharged open-source experiment tracker. Aim logs you...\"],[\"```python\\nfrom aim import Run\\nfrom aim.pytorch import track_gradients_dists, track_params_dists\\n\\n# I...\"],[\"```\\n\\nThe experiments tracked by Aim are stored in the `.aim` folder. **To display the logs with the ...\"],[\"```\\n\\nThatâ€™s it! Now open the App section of your Space and the Aim UI is available with your logs.\\nH...\"],[\"# Model `license:other` challenge\\n\\nRelated to https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fissues\\u002f985.\\n\\n#...\"],[\"```\\n\\nwhich display on the Hub as\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n## How to contribute?\\n\\nHow to do it in practice? That's simple! We have listed models below tha...\"],[\"For each model, the workflow looks like this:\\n1. Choose a model in the list below. We suggest focusi...\"],[\"## F.A.Q.\\n\\n### What if the model has 2 licenses?\\n\\nThis use case can happen when a model is finetuned...\"],[\"|status|pr_url                                                                      |model_id|nb_dow...\"],[\"|------|----------------------------------------------------------------------------|--------|------...\"],[\"-------------------------------------------------------------------------------|--------------------...\"],[\"|Opened|[Hub PR](https:\\u002f\\u002fhuggingface.co\\u002fdecapoda-research\\u002fllama-7b-hf\\u002fdiscussions\\u002f130)|[decapoda-res...\"],[\"|Opened|[hub_pr](https:\\u002f\\u002fhuggingface.co\\u002felinas\\u002fchronos-13b-v2\\u002fdiscussions\\u002f3) |[elinas\\u002fchronos-13b-v2...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ\\u002fdiscu...\"],[\"|      |                                                                            |[aipicasso\\u002fpica...\"],[\"|      |                                                                            |[sambanovasyste...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fhuggyllama\\u002fllama-30b\\u002fdiscussions\\u002f1)                         ...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[georgesung\\u002flla...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[shibing624\\u002fchi...\"],[\"|      |                                                                            |[shibing624\\u002fchi...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenA...\"],[\"|      |                                                                            |[Mitsua\\u002fmitsua-...\"],[\"|      |                                                                            |[aipicasso\\u002fcool...\"],[\"|      |                                                                            |[aipicasso\\u002fcool...\"],[\"|      |                                                                            |[aipicasso\\u002fmang...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[shalomma\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[valurank\\u002ffinet...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenC...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[TheBloke\\u002fZaraf...\"],[\"|      |                                                                            |[Mitsua\\u002fvroid-d...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[coqui\\u002fXTTS-v1]...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenA...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002fCodeF...\"],[\"|      |                                                                            |[deerslab\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002fCodeF...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[yswill\\u002fllama-1...\"],[\"|      |                                                                            |[nonlinearshima...\"],[\"|      |                                                                            |[TheBloke\\u002fChron...\"],[\"|      |                                                                            |[4bit\\u002fllama-13b...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[pankaj-munde\\u002fl...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[TheBloke\\u002fTulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenA...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002frober...\"],[\"|      |                                                                            |[nonlinearshima...\"],[\"|      |                                                                            |[TheBloke\\u002fKuchi...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[Cartinoe5930\\u002fo...\"],[\"|      |                                                                            |[nenkoru\\u002fllama-...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fTulu-...\"],[\"|      |                                                                            |[agonh\\u002fLlama-2-...\"],[\"|      |                                                                            |[AI-Engine\\u002fMyth...\"],[\"|      |                                                                            |[Amerbarhoush\\u002fO...\"],[\"|      |                                                                            |[baffo32\\u002fdecapo...\"],[\"|      |                                                                            |[bluefoxcreatio...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[cekal\\u002fLLaMA-7B...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[Enoch\\u002fllama-13...\"],[\"|      |                                                                            |[Jamessjunk\\u002fAKG...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fdisti...\"],[\"|      |                                                                            |[khachdallak\\u002fll...\"],[\"|      |                                                                            |[Mithilss\\u002fllama...\"],[\"|      |                                                                            |[nonlinearshima...\"],[\"|      |                                                                            |[prodm93\\u002fllama_...\"],[\"|      |                                                                            |[ruibin-wang\\u002fll...\"],[\"|      |                                                                            |[shekharchatter...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fAirob...\"],[\"|      |                                                                            |[TheBloke\\u002fHerme...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[Thireus\\u002fVicuna...\"],[\"|      |                                                                            |[usamakenway\\u002fll...\"],[\"|      |                                                                            |[Zhejian\\u002fllama-...\"],[\"ChatUI on Spaces\\n\\n**Hugging Chat** is an open-source interface enabling everyone to try open-source ...\"],[\"\\u003ca href=\\\"Parameters\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"Model Cards\\n\\n\\u003cTip\\u003e\\n\\n[New! Try our experimental Model Card Creator App](https:\\u002f\\u002fhuggingface.co\\u002fspaces...\"],[\"### Adding metadata to your model card\\n\\nThere are a few different ways to add metadata to your model...\"],[\"#### Editing the YAML section of the `README.md` file\\n\\nYou can also directly edit the YAML section o...\"],[\"```\\n\\nYou can find the detailed model card metadata specification \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhugging...\"],[\"```\\n\\nThis metadata will be used to display the base model on the model page. Users can also use this...\"],[\"```\\n\\n### Specifying a task (`pipeline_tag`)\\n\\nYou can specify the `pipeline_tag` in the model card me...\"],[\"```\\n\\nIf the license is not available via a URL you can link to a LICENSE stored in the model repo.\\n\\n...\"],[\"```yaml\\n---\\nmodel-index:\\n  - name: Yi-34B\\n    results:\\n      - task:\\n          type: text-generation...\"],[\"```\\n\\nFor more details on how to format this data, check out the [Model Card specifications](https:\\u002f\\u002f...\"],[\"### How can I indicate that my model is not suitable for all audiences\\n\\nYou can add a `not-for-all-a...\"],[\"Uploading datasets\\n\\nThe [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets) is home to an extensive collection of...\"],[\"2. Drag and drop your dataset files.\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggin...\"],[\"3. Write your dataset documentation in the Dataset Card to introduce your dataset to the community a...\"],[\"## File formats\\n\\nThe Hub natively supports multiple file formats:\\n\\n- CSV (.csv, .tsv)\\n- JSON Lines, ...\"],[\"WebDataset\\n\\n[WebDataset](https:\\u002f\\u002fgithub.com\\u002fwebdataset\\u002fwebdataset) is a library to write I\\u002fO pipelin...\"],[\"Pull requests and Discussions\\n\\nHub Pull requests and Discussions allow users to do community contrib...\"],[\"## Editing a Discussion \\u002f Pull request title\\n\\nIf you opened a PR or discussion, are the author of th...\"],[\"## Lock a Discussion \\u002f Pull Request\\n\\nIf you have write access to a repository, you can lock discussi...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"You can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its co...\"],[\"```\\n\\n### Draft mode\\n\\nDraft mode is the default status when opening a new Pull request from scratch i...\"],[\"Using fastai at Hugging Face\\n\\n`fastai` is an open-source Deep Learning library that leverages PyTorc...\"],[\"```\\n\\n\\nIf you want to see how to load a specific model, you can click `Use in fastai` and you will be...\"],[\"Using SpeechBrain at Hugging Face\\n\\n`speechbrain` is an open-source and all-in-one conversational too...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in speechbrain` and you wil...\"],[\"Model Card Guidebook \\n\\nModel cards are an important documentation and transparency framework for mac...\"],[\"Our work presents a view of where we think model cards stand right now and where they could go in th...\"],[\"Models Frequently Asked Questions\\n\\n## How can I see what dataset was used to train the model?\\n\\nIt's ...\"],[\"The Hugging Face Hub is also home to Spaces, which are interactive demos used to showcase models. If...\"],[\"## Can I link my model to a paper on arXiv?\\n\\nIf the model card includes a link to a paper on arXiv, ...\"],[\"Using OpenCLIP at Hugging Face\\n\\n[OpenCLIP](https:\\u002f\\u002fgithub.com\\u002fmlfoundations\\u002fopen_clip) is an open-so...\"],[\"```\\n\\nOnce loaded, you can encode the image and text to do [zero-shot image classification](https:\\u002f\\u002fh...\"],[\"```\\n\\nIt outputs the probability of each possible class:\\n\\n```text\\nLabel probs: tensor([[0.0020, 0.003...\"],[\"Libraries\\n\\nThe Datasets Hub has support for several libraries in the Open Source ecosystem.\\nThanks t...\"],[\"The table below summarizes the supported libraries and their level of integration.\\n\\n| Library       ...\"],[\"Using Stanza at Hugging Face\\n\\n`stanza` is a collection of accurate and efficient tools for the lingu...\"],[\"Panel on Spaces\\n\\n[Panel](https:\\u002f\\u002fpanel.holoviz.org\\u002f) is an open-source Python library that lets you ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub\\u002fspaces-p...\"],[\"## ğŸŒ Join Our Community\\nThe Panel community is vibrant and supportive, with experienced developers a...\"],[\"User Studies\\n## Model Card Audiences and Use Cases\\n\\nDuring our investigation into the landscape of m...\"],[\"Participants ranked the different sections of model cards in the perspective of one reading a model ...\"],[\"## User Study Details\\n\\nWe selected people from a variety of different backgrounds relevant to machin...\"],[\"**Insights:**\\n\\n* Several respondents found this format overwhelming, but they generally found it les...\"],[\"Docker Spaces Examples\\n\\nWe gathered some example demos in the [Spaces Examples](https:\\u002f\\u002fhuggingface....\"],[\"Datasets\\n\\nThe Hugging Face Hub is home to a growing collection of datasets that span a variety of do...\"],[\"Using GPU Spaces\\n\\nYou can upgrade your Space to use a GPU accelerator using the _Settings_ button in...\"],[\"## Hardware Specs\\n\\nIn the following table, you can see the Specs for the different upgrade options.\\n...\"],[\"### PyTorch\\n\\nYou'll need to install a version of PyTorch compatible with the built-in CUDA drivers. ...\"],[\"```\\n--extra-index-url https:\\u002f\\u002fdownload.pytorch.org\\u002fwhl\\u002fcu113\\ntorch\\n```\\n\\nYou can verify whether the i...\"],[\"```\\n\\n### Tensorflow\\n\\nThe default `tensorflow` installation should recognize the CUDA device. Just ad...\"],[\"```\\n\\n## Billing\\n\\nBilling on Spaces is based on hardware usage and is computed by the minute: you get...\"],[\"The following interface will then be available in your Spaces hardware settings:\\n\\n\\u003cdiv class=\\\"flex j...\"],[\"How to Add a Space to ArXiv\\n\\nDemos on Hugging Face Spaces allow a wide audience to try out state-of-...\"],[\"And that's it! Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes ...\"],[\"```\\n\\n    *Note*: Here's an [overview on building demos on Hugging Face Spaces](.\\u002fspaces-overview) an...\"],[\"Datasets Overview\\n\\n## Datasets on the Hub\\n\\nThe Hugging Face Hub hosts a [large number of community-c...\"],[\"Using AllenNLP at Hugging Face\\n\\n`allennlp` is a NLP library for developing state-of-the-art models o...\"],[\"```\\n\\nTo get a snippet such as this, you can click `Use in AllenNLP` at the top right,\\n\\n\\u003cdiv class=\\\"f...\"],[\"```\\n\\n| Argument                    \\t| Type         \\t| Description                                   ...\"],[\"### From a Python script\\n\\nThe `push_to_hf` function has the same parameters as the bash script.\\n\\n```...\"],[\"```\\n\\nIn just a minute, you can get your model in the Hub, try it out directly in the browser, and sh...\"],[\"Webhook guide: Setup an automatic system to re-train a model when a dataset changes\\n\\n\\u003cTip\\u003e\\n\\nWebhooks...\"],[\"Your Webhook will look like this:\\n\\n![webhook-creation](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fd...\"],[\"```\\n\\n2.  2. This route checks that the `X-Webhook-Secret` header is present and that its value is th...\"],[\"```\\n\\n3. The event's payload is encoded as JSON. Here, we'll be using pydantic models to parse the ev...\"],[\"```\\n\\n4. If the payload is valid, the next step is to create a project on AutoTrain, schedule a fine-...\"],[\"```\\n\\nVisit the link inside the comment to review the training cost estimate, and start fine-tuning t...\"],[\"```\\n\\n## Configure your Webhook to send events to your Space\\n\\nLast but not least, you'll need to conf...\"],[\"Downloading models\\n\\n## Integrated libraries\\n\\nIf a model on the Hub is tied to a [supported library](...\"],[\"```\\n\\n## Using Git\\n\\nSince all models on the Model Hub are Git repositories, you can clone the models ...\"],[\"Getting Started with Repositories\\n\\nThis beginner-friendly guide will help you get the basic skills y...\"],[\"```\\n\\n**The content in the Getting Started section of this document is also available as a video!**\\n\\n...\"],[\"After creating your model repository, you should see a page like this:\\n\\n\\u003cdiv class=\\\"flex justify-cen...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nYou can clone over SSH with the following command:\\n```bash\\ngit clone git@hf.co:\\u003cyour-username\\u003e\\u002f...\"],[\"```\\n\\nAnd you're done! You can check your repository on Hugging Face with all the recently added file...\"],[\"You can click on an individual commit to see what changes that commit introduced:\\n\\n\\u003cdiv class=\\\"flex ...\"],[\"Embed your Space in another website\\n\\nOnce your Space is up and running you might wish to embed it in...\"],[\"```\\n\\nFor instance using the [NimaBoscarino\\u002fhotdog-gradio](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fNimaBoscarin...\"],[\"Tabby on Spaces\\n\\n[Tabby](https:\\u002f\\u002ftabby.tabbyml.com) is an open-source, self-hosted AI coding assista...\"],[\"If you want to customize the title, emojis, and colors of your space, go to \\\"Files and Versions\\\" and...\"],[\"![Code Completion](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub...\"],[\"Data files Configuration\\n\\nThere are no constraints on how to structure dataset repositories.\\n\\nHoweve...\"],[\"Single Sign-On (SSO)\\n\\n\\u003cTip warning={true}\\u003e\\nThis feature is part of the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface....\"],[\"Models\\n\\nThe Hugging Face Hub hosts many models for a [variety of machine learning tasks](https:\\u002f\\u002fhug...\"],[\"Using Asteroid at Hugging Face\\n\\n`asteroid` is a Pytorch toolkit for audio source separation. It enab...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in Adapter Transformers` an...\"],[\"Models Download Stats\\n\\n## How are download stats generated for models?\\n\\nCounting the number of downl...\"],[\"```json\\n{\\n    \\\"adapter-transformers\\\": {\\n        filter: [\\n            {\\n                term: { path...\"],[\"},\\n    \\\"timm\\\": {\\n        filter: [\\n            {\\n                terms: { path: [\\\"pytorch_model.bin\\\"...\"],[\"Using RL-Baselines3-Zoo at Hugging Face\\n\\n`rl-baselines3-zoo` is a training framework for Reinforceme...\"],[\"```\\n\\nYou can define three parameters:\\n- `--repo-name`: The name of the repo.\\n- `-orga`: Your Hugging...\"],[\"Using OpenCV in Spaces\\n\\nIn order to use OpenCV in your Gradio or Streamlit Spaces, you'll need to ma...\"],[\"Uploading models\\n\\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](ht...\"],[\"2. From there, select a file from your computer to upload and leave a helpful commit message to know...\"],[\"5. Add metadata\\n\\nYou can add metadata to your model card. You can specify:\\n* the type of task this m...\"],[\"## Using the `huggingface_hub` client library\\n\\nThe rich feature set in the `huggingface_hub` library...\"],[\"Digital Object Identifier (DOI)\\n\\nThe Hugging Face Hub offers the possibility to generate DOI for you...\"],[\"After you agree to those terms, your model or dataset will get a DOI assigned, and a new tag should ...\"],[\"## Further Reading\\n\\n- [Introducing DOI: the Digital Object Identifier to Datasets and Models](https:...\"],[\"Secrets Scanning\\n\\nIt is important to manage [your secrets (env variables) properly](.\\u002fspaces-overvie...\"],[\"Downloading datasets\\n\\n## Integrated libraries\\n\\nIf a dataset on the Hub is tied to a [supported libra...\"],[\"```py\\nfrom huggingface_hub import hf_hub_download\\nimport pandas as pd\\n\\nREPO_ID = \\\"YOUR_REPO_ID\\\"\\nFILE...\"],[\"```\\n\\n## Using Git\\n\\nSince all datasets on the Hub are Git repositories, you can clone the datasets lo...\"],[\"Handling Spaces Dependencies\\n\\n## Default dependencies\\n\\nThe default Spaces environment comes with sev...\"],[\"Inference API\\n\\nPlease refer to [Inference API Documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fapi-inferen...\"],[\"## Can I send large volumes of requests? Can I get accelerated APIs?\\n\\nIf you are interested in accel...\"],[\"Enterprise Hub\\n\\nEnterprise Hub adds advanced capabilities to organizations, enabling safe, compliant...\"],[\"How to configure SAML SSO with Okta\\n\\nIn this guide, we will use Okta as the SSO provider and with th...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"In the SSO section of your organization's settings, copy-paste these values from Okta:\\n\\n- Sign-on UR...\"],[\"```\\n-----BEGIN CERTIFICATE-----\\n{certificate}\\n-----END CERTIFICATE-----\\n```\\n\\n\\u003cdiv class=\\\"flex justif...\"],[\"Using ğŸ§¨ `diffusers` at Hugging Face\\n\\nDiffusers is the go-to library for state-of-the-art pretrained ...\"],[\"## Using existing pipelines\\n\\nAll `diffusers` pipelines are a line away from being used! To run gener...\"],[\"```\\n\\nIf you want to load a specific pipeline component such as the UNet, you can do so by:\\n\\n```py\\nfr...\"],[\"Security\\n\\nThe Hugging Face Hub offers several security features to ensure that your code and data ar...\"],[\"Using SpanMarker at Hugging Face\\n\\n[SpanMarker](https:\\u002f\\u002fgithub.com\\u002ftomaarsen\\u002fSpanMarkerNER) is a fram...\"],[\"```\\n```json\\n[\\n    {\\\"span\\\": \\\"Amelia Earhart\\\", \\\"label\\\": \\\"person-other\\\", \\\"score\\\": 0.7629689574241638, \\\"...\"],[\"```\\n\\nIf you want to load a specific SpanMarker model, you can click `Use in SpanMarker` and you will...\"],[\"Disk usage on Spaces\\n\\nEvery Space comes with a small amount of disk storage. This disk space is ephe...\"],[\"\\u003cTip warning={true}\\u003e\\n\\tWARNING: all data stored in the storage is lost when you delete it.\\n\\u003c\\u002fTip\\u003e\\n\\n##...\"],[\"Visit the [`datasets` library](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002findex) documentation and the [`h...\"],[\"Hugging Face on Amazon SageMaker\\n\\n![cover](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation...\"],[\"- Cost-effective: Training instances are only live for the duration of your job. Once your job is co...\"],[\"Hugging Face Deep DLCs make it easier than ever to train Transformer models in SageMaker. Here is wh...\"],[\"---\\n\\n## Resources, Documentation & Samples ğŸ“„\\n\\nTake a look at our published blog posts, videos, docum...\"],[\"### Documentation\\n\\n- [Run training on Amazon SageMaker](\\u002fdocs\\u002fsagemaker\\u002ftrain)\\n- [Deploy models to A...\"],[\"- [All notebooks](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002ftree\\u002fmaster\\u002fsagemaker)\\n- [Getting Started...\"],[\"- [Image Classification with Vision Transformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002f...\"],[\"Hub API Endpoints\\n\\nWe have open endpoints that you can use to retrieve information from the Hub as w...\"],[\"### GET \\u002fapi\\u002fmodels\\n\\nGet information from all models in the Hub. The response is paginated, use the ...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_models()`.\\n\\n### GET \\u002fapi\\u002fmodels\\u002f{repo_id} or \\u002fapi\\u002fm...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_datasets()`.\\n\\n### GET \\u002fapi\\u002fdatasets\\u002f{repo_id} or \\u002fa...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.dataset_info(repo_id, revision)`.\\n\\n### GET \\u002fapi\\u002fdatasets...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_spaces()`.\\n\\n### GET \\u002fapi\\u002fspaces\\u002f{repo_id} or \\u002fapi\\u002fs...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.update_repo_visibility()`.\\n\\n### POST \\u002fapi\\u002frepos\\u002fmove\\n\\nMo...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.create_collection()`.\\n\\n### GET \\u002fapi\\u002fcollections\\u002f{namespa...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_collections()`.\\n\\n### PATCH \\u002fapi\\u002fcollections\\u002f{namesp...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.add_collection_item()`.\\n\\n### PATCH \\u002fapi\\u002fcollections\\u002f{nam...\"],[\"Displaying carbon emissions for your model\\n\\n## Why is it beneficial to calculate the carbon emission...\"],[\"```\\n\\n## How is the carbon footprint of my model calculated? ğŸŒ\\n\\nConsidering the computing hardware, l...\"],[\"Train and deploy Hugging Face on Amazon SageMaker\\n\\nThe get started guide will show you how to quickl...\"],[\"```\\n\\nIf you want to run this example in [SageMaker Studio](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemaker\\u002flat...\"],[\"```\\n\\n## Preprocess\\n\\nThe ğŸ¤— Datasets library makes it easy to download and preprocess a dataset for tr...\"],[\"```\\n\\n## Upload dataset to S3 bucket\\n\\nNext, upload the preprocessed dataset to your S3 session bucket...\"],[\"```\\n\\n## Start a training job\\n\\nCreate a Hugging Face Estimator to handle end-to-end SageMaker trainin...\"],[\"```\\n\\nBegin training with one line of code:\\n\\n```python\\nhuggingface_estimator.fit({\\\"train\\\": training_i...\"],[\"Dataset Cards\\n\\n## What are Dataset Cards?\\n\\nEach dataset may be documented by the `README.md` file in...\"],[\"```\\n\\nThe metadata that you add to the dataset card enables certain interactions on the Hub. For exam...\"],[\"* Visit the Paper page\\n* Filter for other models on the Hub that cite the same paper.\\n\\n\\u003cdiv class=\\\"f...\"],[\"Using sample-factory at Hugging Face\\n\\n[`sample-factory`](https:\\u002f\\u002fgithub.com\\u002falex-petrenko\\u002fsample-fac...\"],[\"```\\ngit clone git@hf.co:\\u003cName of HuggingFace Repo\\u003e # example: git clone git@hf.co:bigscience\\u002fbloom\\n`...\"],[\"```\\npython -m sample_factory.huggingface.push_to_hub -r \\u003chf_username\\u003e\\u002f\\u003chf_repo_name\\u003e -d \\u003cexperiment_...\"],[\"```\\npython -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=\\u003cre...\"],[\"hub-docs\\n\\nThis repository regroups documentation and information that is hosted on the Hugging Face ...\"],[\"Datasets Download Stats\\n\\n## How are download stats generated for datasets?\\n\\nThe Hub provides downloa...\"],[\"Using TensorBoard\\n\\nTensorBoard provides tooling for tracking and visualizing metrics as well as visu...\"],[\"Organizations, Security, and the Hub API\\n\\n## Contents\\n\\n- [Organizations](.\\u002forganizations)\\n  - [Manag...\"],[\"Using timm at Hugging Face\\n\\n`timm`, also known as [pytorch-image-models](https:\\u002f\\u002fgithub.com\\u002frwightma...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click **Use in timm** and you will be ...\"],[\"# Get the labels from the model config\\nlabels = model.pretrained_cfg['labels']\\ntop_k = min(len(label...\"],[\"```\\n\\nThis should leave you with a list of predictions, like this:\\n\\n```py\\n[\\n    {'label': 'american_p...\"],[\"```\\n\\nThen, push your model using the `push_to_hf_hub` method:\\n\\n```py\\nimport timm\\n\\n# Build or load a ...\"],[\"```\\n\\n## Inference Widget and API\\n\\nAll `timm` models on the Hub are automatically equipped with an [i...\"],[\"```\\n\\n## Additional resources\\n\\n* timm (pytorch-image-models) [GitHub Repo](https:\\u002f\\u002fgithub.com\\u002frwightm...\"],[\"Image Dataset\\n\\nThis guide will show you how to configure your dataset repository with image files. Y...\"],[\"```\\nmy_dataset_repository\\u002f\\nâ””â”€â”€ train\\n    â”œâ”€â”€ 1.jpg\\n    â”œâ”€â”€ 2.jpg\\n    â”œâ”€â”€ 3.jpg\\n    â”œâ”€â”€ 4.jpg\\n    â””â”€â”€...\"],[\"```\\n\\nMetadata file cannot be put in subdirectories of a directory with the images.\\n\\n## Image classif...\"],[\"Deploy models to Amazon SageMaker\\n\\nDeploying a ğŸ¤— Transformers models in SageMaker for inference is a...\"],[\"```\\n\\nThis guide will show you how to deploy models with zero-code using the [Inference Toolkit](http...\"],[\"```\\n\\n**SageMaker environment**\\n\\nSetup your SageMaker environment as shown below:\\n\\n```python\\nimport s...\"],[\"```\\n\\n## Deploy a ğŸ¤— Transformers model trained in SageMaker\\n\\n\\u003ciframe width=\\\"700\\\" height=\\\"394\\\" src=\\\"ht...\"],[\"# deploy model to SageMaker Inference\\npredictor = hf_estimator.deploy(initial_instance_count=1, inst...\"],[\"```\\n\\nAfter you run your request you can delete the endpoint as shown:\\n\\n```python\\n# delete endpoint\\np...\"],[\"```\\n\\nCreate your own `model.tar.gz` from a model from the ğŸ¤— Hub:\\n\\n1. Download a model:\\n\\n```bash\\ngit ...\"],[\"```\\n\\nNow you can provide the S3 URI to the `model_data` argument to deploy your model later.\\n\\n## Dep...\"],[\"# deploy model to SageMaker Inference\\npredictor = huggingface_model.deploy(\\n   initial_instance_coun...\"],[\"```\\n\\nAfter you run our request, you can delete the endpoint again with:\\n\\n```python\\n# delete endpoint...\"],[\"```\\n\\nğŸ““ Open the [deploy_transformer_model_from_hf_hub.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"```python\\nbatch_job = huggingface_estimator.transformer(\\n    instance_count=1,\\n    instance_type='ml...\"],[\"```\\n\\nIf you want to run your batch transform job later or with a model from the ğŸ¤— Hub, create a `Hug...\"],[\"```\\n\\nğŸ““ Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"```\\n\\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file ...\"],[\"```python\\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\\n\\ndef model_fn(model_di...\"],[\"```\\n\\nCustomize your inference module with only `model_fn` and `transform_fn`:   \\n\\n```python\\nfrom sag...\"],[\"Single Sign-On (SSO)\\n\\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-...\"],[\"## How to configure OIDC\\u002fSAML provider in the Hub\\n\\nWe have some guides available to help with config...\"],[\"Spaces Configuration Reference\\n\\nSpaces are configured through the `YAML` block at the top of the **R...\"],[\"**`suggested_storage`** : _string_  \\nSpecify the suggested [permanent storage](https:\\u002f\\u002fhuggingface.c...\"],[\"**`pinned`** : _boolean_  \\nWhether the Space stays on top of your profile. Can be useful if you have...\"],[\"**`custom_headers`** : _Dict[string, string]_  \\nSet custom HTTP headers that will be added to all HT...\"],[\"```\\n\\n*Note:* all headers and values must be lowercase.\\n\\n**`preload_from_hub`**: _List[string]_\\nSpeci...\"],[\"Spaces Settings\\n\\nYou can configure your Space's appearance and other settings inside the `YAML` bloc...\"],[\"Your First Docker Space: Text Generation with T5\\n\\nIn the following sections, you'll learn the basics...\"],[\"```\\n\\n## Add the dependencies\\n\\nFor the **Text Generation** Space, we'll be building a FastAPI app tha...\"],[\"```\\n\\nWhen the changes are saved, the Space will rebuild and your demo should be up after a couple of...\"],[\"```\\n\\nLet's go through all the steps to make this working. We'll skip some of the details of the CSS ...\"],[\"```\\n\\n3. In the `app.py` file, mount the static files and show the html file in the root route\\n\\n```py...\"],[\"```\\n\\n5. Grant permissions to the right directories\\n\\nAs discussed in the [Permissions Section](.\\u002fspac...\"],[\"```\\n\\nSuccess! Your app should be working now! Check out [DockerTemplates\\u002ffastapi_t5](https:\\u002f\\u002fhugging...\"],[\"On the **Container** tab, you will see the application status, in this case, `Uvicorn running on htt...\"],[\"Using Sentence Transformers at Hugging Face\\n\\n`sentence-transformers` is a library that provides easy...\"],[\"## Using existing models\\n\\nThe pre-trained models on the Hub can be loaded with a single line of code...\"],[\"```\\n\\nHere is an example that encodes sentences and then computes the distance between them for doing...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in sentence-transformers` a...\"],[\"How to configure OIDC SSO with Okta\\n\\nIn this guide, we will use Okta as the SSO provider and with th...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"In the SSO section of your organization's settings on Hugging Face, copy-paste these values from Okt...\"],[\"Repositories\\n\\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](...\"],[\"Malware Scanning\\n\\nWe run every file of your repositories through a [malware scanner](https:\\u002f\\u002fwww.cla...\"],[\"If at least one file has a been scanned as unsafe, a message will warn the users:\\n\\n\\u003cdiv class=\\\"flex ...\"],[\"Managing Spaces with CircleCI Workflows\\n\\nYou can keep your app in sync with your GitHub repository w...\"],[\"```\\n\\nThen force push to sync everything for the first time:\\n\\n```bash\\ngit push --force space main\\n```...\"],[\"Docker Spaces\\n\\nSpaces accommodate custom [Docker containers](https:\\u002f\\u002fdocs.docker.com\\u002fget-started\\u002f) f...\"],[\"```\\n\\nInternally you could have as many open ports as you want. For instance, you can install Elastic...\"],[\"```\\n\\n```Dockerfile\\n# Expose the secret SECRET_EXAMPLE at buildtime and use its value as a Bearer tok...\"],[\"```\\n\\n\\u003cTip warning=\\\"{true}\\\"\\u003e\\nAlways specify the `--chown=user` with `ADD` and `COPY` to ensure the ne...\"],[\"```\\n(same goes for `ADD` command)\\n\\n\\n## Data Persistence\\n\\nThe data written on disk is lost whenever y...\"],[\"\\u003cTip warning=\\\"{true}\\\"\\u003e\\nDuring Docker buildtime, you don't have access to a GPU hardware. Therefore, ...\"],[\"Webhook guide: build a Discussion bot based on BLOOM\\n\\n\\u003cTip\\u003e\\n\\nWebhooks are now publicly available!\\n\\n\\u003c...\"],[\"The Space's code is [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdiscussion-bot\\u002fwebhook\\u002ftree\\u002fmain). \\n\\nWe use...\"],[\"```\\n\\nHere, we listen to POST requests made to `\\u002f`, and then we check that the `X-Webhook-Secret` hea...\"],[\"```\\n\\nThis is the coolest part: we call the Inference API for the BLOOM model, prompting it with `PRO...\"],[\"Sign in with Hugging Face\\n\\nYou can use the HF OAuth \\u002f OpenID connect flow to create a **\\\"Sign in wit...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Currently supported scopes\\n\\nThe currently supported scopes are:\\n\\n- `openid`: Get the ID t...\"],[\"Check out [our badges](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fbadges#sign-in-with-hugging-face)...\"],[\"[![Sign in with Hugging Face](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fbadges\\u002fresolve\\u002fmain\\u002fsign-i...\"],[\"Using `Transformers.js` at Hugging Face\\n\\nTransformers.js is a JavaScript library for running ğŸ¤— Trans...\"],[\"```\\n\\n\\u003c\\u002ftd\\u003e\\n\\u003c\\u002ftr\\u003e\\n\\u003c\\u002ftable\\u003e\\n\\n\\nYou can also use a different model by specifying the model id or path as...\"],[\"Using SetFit with Hugging Face\\n\\nSetFit is an efficient and prompt-free framework for few-shot fine-t...\"],[\"```\\npip install -U setfit\\n```\\n\\n## Using existing models\\n\\nAll `setfit` models can easily be loaded fr...\"],[\"Spaces\\n\\n[Hugging Face Spaces](https:\\u002f\\u002fhuggingface.co\\u002fspaces) offer a simple way to host ML demo apps...\"],[\"You'll also be able to upgrade your Space to run [on a GPU or other accelerated hardware](.\\u002fspaces-g...\"],[\"Using Adapter Transformers at Hugging Face\\n\\n`adapter-transformers` is a library that extends ğŸ¤— `tran...\"],[\"```\\n\\nYou can also use `list_adapters` to find all Adapter Models programmatically\\n\\n```py\\nfrom transf...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in Adapter Transformers` an...\"],[\"```\\n\\nThis command creates a repository with an automatically generated model card and all necessary ...\"],[\"Using PEFT at Hugging Face\\n\\nğŸ¤— [Parameter-Efficient Fine-Tuning (PEFT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fp...\"],[\"```\\n\\nOnce loaded, you can pass your inputs to the tokenizer to prepare them, and call `model.generat...\"],[\"```\\n\\nIf you want to load a specific PEFT model, you can click `Use in PEFT` in the model card and yo...\"],[\"Organization cards\\n\\nYou can create an organization card to help users learn more about what your org...\"],[\"Libraries\\n\\nThe Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the `...\"],[\"| Library                                                                     | Description         ...\"],[\"| [docTR](https:\\u002f\\u002fgithub.com\\u002fmindee\\u002fdoctr)                                    | Models and datasets ...\"],[\"| [OpenCLIP](https:\\u002f\\u002fgithub.com\\u002fmlfoundations\\u002fopen_clip)                      | Library for open-sou...\"],[\"| [SetFit](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fsetfit)                             | Efficient few-shot t...\"],[\"### How can I add a new library to the Inference API?\\n\\nIf you're interested in adding your library, ...\"],[\"Gated models\\n\\nTo give more control over how models are used, the Hub allows model authors to enable ...\"],[\"By default, access to the model is automatically granted to the user when requesting it. This is ref...\"],[\"Once access requests are enabled, you have full control of who can access your model or not, whether...\"],[\"| Method | URI | Description | Headers | Payload\\n| ------ | --- | ----------- | -------  | -------  ...\"],[\"### Download access report\\n\\nYou can download a report of all access requests for a gated model with ...\"],[\"Here is an example of customized request form where the user is asked to provide their company name ...\"],[\"```\\n\\n\\nIn some cases, you might also want to modify the text in the gate heading and the text in the ...\"],[\"```\\n\\n### Example use cases of programmatically managing access requests\\n\\nHere are a few interesting ...\"],[\"Requesting access can only be done from your browser. Go to the model on the Hub and you will be pro...\"],[\"```\\n\\nAlternatively, you can programmatically login using `login()` in a notebook or a script:\\n\\n```py...\"],[\"Gated datasets\\n\\nTo give more control over how datasets are used, the Hub allows datasets authors to ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdat...\"],[\"Once access requests are enabled, you have full control of who can access your dataset or not, wheth...\"],[\"| Method | URI | Description | Headers | Payload\\n| ------ | --- | ----------- | -------  | -------  ...\"],[\"### Download access report\\n\\nYou can download a report of all access requests for a gated datasets wi...\"],[\"Here is an example of customized request form where the user is asked to provide their company name ...\"],[\"```\\n\\n## Access gated datasets as a user\\n\\n\\nAs a user, if you want to use a gated dataset, you will ne...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Download files\\n\\nTo download files from a gated dataset you'll need to be authenticated. ...\"],[\"Custom Python Spaces\\n\\n\\u003cTip\\u003e\\n\\nSpaces now support arbitrary Dockerfiles so you can host any Python app...\"],[\"Paper Pages\\n\\nPaper pages allow people to find artifacts related to a paper such as models, datasets ...\"],[\"## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based ...\"],[\"### Can I have a Paper page even if I have no model\\u002fdataset\\u002fSpace?\\n\\nYes. You can go to [the main Pap...\"],[\"DuckDB\\n\\n[DuckDB](https:\\u002f\\u002fgithub.com\\u002fduckdb\\u002fduckdb) is an in-process SQL [OLAP](https:\\u002f\\u002fen.wikipedia....\"],[\"```\\n\\nThis creates a file `data.parquet` in the dataset repository `username\\u002fmy_dataset` containing y...\"],[\"More ways to create Spaces\\n\\n## Duplicating a Space\\n\\nYou can duplicate a Space by clicking the three ...\"],[\"Shiny on Spaces\\n\\n[Shiny](https:\\u002f\\u002fshiny.posit.co\\u002f) is an open-source framework for building simple, b...\"],[\"_Dockerfile_\\n\\nThe Dockerfile for a Shiny for Python app is very minimal because the library doesn't ...\"],[\"_app.R_\\nThis file contains all of your application logic. If you prefer, you can break this file up ...\"],[\"Using ML-Agents at Hugging Face\\n\\n`ml-agents` is an open-source toolkit that enables games and simula...\"],[\"```\\nmlagents-load-from-hf --repo-id=\\\"ThomasSimonini\\u002fMLAgents-Pyramids\\\" --local-dir=\\\".\\u002fdownloads\\\"\\n```...\"],[\"--\\n# Example metadata to be added to a model card.  \\n# Full model card template at https:\\u002f\\u002fgithub.co...\"],[\"# Optional. Add this if you want to encode your eval results in a structured way.\\nmodel-index:\\n- nam...\"],[\"value: {metric_value}       # Required. Example: 20.90\\n        name: {metric_name}         # Optiona...\"],[\"This markdown file contains the spec for the modelcard metadata regarding evaluation parameters. Whe...\"],[\"Using ğŸ¤— `transformers` at Hugging Face\\n\\nğŸ¤— `transformers` is a library maintained by Hugging Face and...\"],[\"You can find models for many different tasks:\\n\\n* Extracting the answer from a context ([question-ans...\"],[\"You can try out the models directly in the browser if you want to test them out without downloading ...\"],[\"```\\n\\nYou can also load a model from a specific version (based on commit hash, tag name, or branch) a...\"],[\"```\\n\\nThere is much more you can do, so we suggest to review the [Share a model](https:\\u002f\\u002fhuggingface....\"],[\"THE LANDSCAPE OF ML DOCUMENTATION TOOLS\\nThe development of the model cards framework in 2018 was ins...\"],[\"| **Stage of ML System Lifecycle** \\t|  **Tool**                                                     ...\"],[\"|:--------------------------------:\\t|---------------------------------------------------------------...\"],[\"----------------------------------------------------------------------------------------------------...\"],[\"| DATA                             \\t| ***Datasheets*** [(Gebru et al., 2018)](https:\\u002f\\u002fwww.fatml.org\\u002f...\"],[\"| DATA                             \\t| ***Data Cards for NLP*** [(McMillan-Major et al., 2021)](https...\"],[\"| DATA                             \\t| ***CrowdWorkSheets***  [(DÃ­az et al., 2022)](https:\\u002f\\u002fhuggingfa...\"],[\"| MODELS AND METHODS               \\t| ***Method Cards*** [Adkins et al. (2022)](https:\\u002f\\u002fdl.acm.org\\u002fd...\"],[\"| SYSTEMS                          \\t| ***System Cards***  [Procope et al. (2022)](https:\\u002f\\u002fai.faceboo...\"],[\"| SYSTEMS                          \\t| ***ABOUT ML***  [Raji and Yang, (2019)](https:\\u002f\\u002fhuggingface.co...\"],[\"### DATA-FOCUSED DOCUMENTATION TOOLS\\n\\nSeveral proposed documentation tools focus on datasets used in...\"],[\"* Extending the concept of datasheets in the electronics industry, [Gebru et al. (2018)](https:\\u002f\\u002fwww...\"],[\"* [Hutchinson et al. (2021)](https:\\u002f\\u002fdl.acm.org\\u002fdoi\\u002fpdf\\u002f10.1145\\u002f3442188.3445918) describe the need f...\"],[\"### MODEL-AND-METHOD-FOCUSED DOCUMENTATION TOOLS\\n\\nAnother set of documentation tools can be thought ...\"],[\"* They envision the relationship between model cards and method cards, in part, by stating: â€œThe sec...\"],[\"Rather than focusing on particular models, datasets, or methods, system-focused documentation tools ...\"],[\"## THE EVOLUTION OF MODEL CARDS\\n\\nSince the proposal for model cards by Mitchell et al. in 2018, mode...\"],[\"1) How many of these models have model cards?\\n   \\n2) What percent of downloads had an associated mod...\"],[\"Using Keras at Hugging Face\\n\\n`keras` is an open-source machine learning library that uses a consiste...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click **Use in keras** and you will be...\"],[\"```py\\nfrom huggingface_hub import push_to_hub_keras\\n\\npush_to_hub_keras(model,\\n    \\\"your-username\\u002fyou...\"],[\"```\\nThe repository will host your TensorBoard traces like below.\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n...\"],[\"Widgets\\n\\n## What's a widget?\\n\\nMany model repos have a widget that allows anyone to run inferences di...\"],[\"For some libraries, such as ğŸ¤—  `Transformers`, the model type should be inferred automatically based...\"],[\"```\\n\\nYou can provide more than one example input. In the examples dropdown menu of the widget, they ...\"],[\"```\\n\\nNote that you can also include example files in your model repository and use\\nthem as:\\n\\n```yaml...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" width=\\\"450\\\" src=\\\"https:\\u002f\\u002fhuggi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" width=\\\"450\\\" src=\\\"https:\\u002f\\u002fhuggi...\"],[\"Here are some links to examples:\\n\\n- `text-classification`, for instance [`roberta-large-mnli`](https...\"],[\"## How can I control my model's widget Inference API parameters?\\n\\nGenerally, the Inference API for a...\"],[\"```\\n\\nOr if you'd like to change the temperature for a summarization task in the widget:\\n\\n```yaml\\ninf...\"],[\"No-license models challenge\\n\\n## Context\\n\\nThe Hugging Face Hub hosts hundreds of thousands of public ...\"],[\"```\\n\\nOtherwise, the license is considered as `other`. In that case, we can set a custom name and a U...\"],[\"```\\n\\nThis challenge aims to improve the completeness of this metadata on the Hub, which will ultimat...\"],[\"For each model, the workflow looks like this:\\n1. Choose a model in the list below. We suggest focusi...\"],[\"## F.A.Q.\\n\\n### What if the model has 2 licenses?\\n\\nThis use case can happen when a model is finetuned...\"],[\"|status|pr_url|model_id                                                                             ...\"],[\"|------|------|-------------------------------------------------------------------------------------...\"],[\"-------------------------------------------------------|-------------|...\"],[\"|opened|[here](https:\\u002f\\u002fhuggingface.co\\u002fNousResearch\\u002fLlama-2-13b-hf\\u002fdiscussions\\u002f5)|[NousResearch\\u002fLlama...\"],[\"|opened|[here](https:\\u002f\\u002fhuggingface.co\\u002fNousResearch\\u002fLlama-2-7b-hf\\u002fdiscussions\\u002f4)|[NousResearch\\u002fLlama-...\"],[\"|      |      |[meta-llama\\u002fLlama-2-13b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fmeta-llama\\u002fLlama-2-13b-chat-h...\"],[\"|      |      |[meta-llama\\u002fLlama-2-70b-hf](https:\\u002f\\u002fhuggingface.co\\u002fmeta-llama\\u002fLlama-2-70b-hf)        ...\"],[\"|      |      |[elinas\\u002fchronos-13b-v2](https:\\u002f\\u002fhuggingface.co\\u002felinas\\u002fchronos-13b-v2)                ...\"],[\"|      |      |[TheBloke\\u002fOpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBl...\"],[\"|      |      |[decapoda-research\\u002fllama-13b-hf](https:\\u002f\\u002fhuggingface.co\\u002fdecapoda-research\\u002fllama-13b-h...\"],[\"|      |      |[TheBloke\\u002fLlama-2-7B-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-7B-fp16)          ...\"],[\"|      |      |[NousResearch\\u002fLlama-2-13b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fNousResearch\\u002fLlama-2-13b-ch...\"],[\"|      |      |[kfkas\\u002fLlama-2-ko-7b-Chat](https:\\u002f\\u002fhuggingface.co\\u002fkfkas\\u002fLlama-2-ko-7b-Chat)          ...\"],[\"|      |      |[TheBloke\\u002fLlama-2-7b-chat-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-7b-chat-fp16)...\"],[\"|      |      |[TheBloke\\u002fllama2_7b_chat_uncensored-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama2_7b_c...\"],[\"|      |      |[TheBloke\\u002fllama-2-70b-Guanaco-QLoRA-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama-2-70b...\"],[\"|      |      |[jondurbin\\u002fairoboros-l2-7b-gpt4-1.4.1](https:\\u002f\\u002fhuggingface.co\\u002fjondurbin\\u002fairoboros-l2-...\"],[\"|      |      |[shibing624\\u002fchinese-alpaca-plus-7b-hf](https:\\u002f\\u002fhuggingface.co\\u002fshibing624\\u002fchinese-alpa...\"],[\"|      |      |[TheBloke\\u002ftulu-13B-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002ftulu-13B-fp16)              ...\"],[\"|      |      |[THUDM\\u002fvisualglm-6b](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fvisualglm-6b)                      ...\"],[\"|      |      |[llange\\u002fxlm-roberta-large-english-clinical](https:\\u002f\\u002fhuggingface.co\\u002fllange\\u002fxlm-roberta...\"],[\"|      |      |[PeanutJar\\u002fLLaMa-2-PeanutButter_v19_R8-7B](https:\\u002f\\u002fhuggingface.co\\u002fPeanutJar\\u002fLLaMa-2-P...\"],[\"|      |      |[TheBloke\\u002fOpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-13B-gpt4-1.4.1-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros...\"],[\"|      |      |[THUDM\\u002fchatglm-6b-int4-qe](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fchatglm-6b-int4-qe)          ...\"],[\"|      |      |[liuhaotian\\u002fllava-llama-2-7b-chat-lightning-lora-preview](https:\\u002f\\u002fhuggingface.co\\u002fliuh...\"],[\"|      |      |[aipicasso\\u002fmanga-diffusion-poc](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fmanga-diffusion-poc)...\"],[\"|      |      |[4bit\\u002fLlama-2-70b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002f4bit\\u002fLlama-2-70b-chat-hf)          ...\"],[\"|      |      |[aipicasso\\u002fcool-japan-diffusion-2-1-1-beta](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fcool-jap...\"],[\"|      |      |[allenai\\u002fopen-instruct-stanford-alpaca-7b](https:\\u002f\\u002fhuggingface.co\\u002fallenai\\u002fopen-instru...\"],[\"|      |      |[THUDM\\u002fWebGLM-2B](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fWebGLM-2B)                            ...\"],[\"|      |      |[TheBloke\\u002fLlama-2-70B-Chat-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-70B-Chat-GGM...\"],[\"|      |      |[TheBloke\\u002fAiroboros-L2-70B-GPT4-m2.0-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fAiroboros-...\"],[\"|      |      |[TheBloke\\u002fOpenChat_v3.2-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenChat_v3.2-GPTQ)    ...\"],[\"|      |      |[valurank\\u002fdistilroberta-bias](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fdistilroberta-bias)    ...\"],[\"|      |      |[TheBloke\\u002fKuchiki-L2-7B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fKuchiki-L2-7B-GPTQ)    ...\"],[\"|      |      |[sschet\\u002fbert-base-uncased_clinical-ner](https:\\u002f\\u002fhuggingface.co\\u002fsschet\\u002fbert-base-uncas...\"],[\"|      |      |[allenai\\u002ftulu-7b](https:\\u002f\\u002fhuggingface.co\\u002fallenai\\u002ftulu-7b)                            ...\"],[\"|      |      |[silver\\u002fchatglm-6b-int4-slim](https:\\u002f\\u002fhuggingface.co\\u002fsilver\\u002fchatglm-6b-int4-slim)    ...\"],[\"|      |      |[TheBloke\\u002fLlama2-22B-Daydreamer-v3-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama2-22B-D...\"],[\"|      |      |[coqui\\u002fXTTS-v1](https:\\u002f\\u002fhuggingface.co\\u002fcoqui\\u002fXTTS-v1)                                ...\"],[\"|      |      |[nenkoru\\u002fllama-7b-onnx-merged-fp16](https:\\u002f\\u002fhuggingface.co\\u002fnenkoru\\u002fllama-7b-onnx-merg...\"],[\"|      |      |[localmodels\\u002fLlama-2-13B-Chat-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002flocalmodels\\u002fLlama-2-13B-Ch...\"],[\"|      |      |[TheBloke\\u002fOpenAssistant-Llama2-13B-Orca-8K-3319-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke...\"],[\"|      |      |[arogov\\u002fllama2_13b_chat_uncensored](https:\\u002f\\u002fhuggingface.co\\u002farogov\\u002fllama2_13b_chat_unc...\"],[\"|      |      |[silver\\u002fchatglm-6b-slim](https:\\u002f\\u002fhuggingface.co\\u002fsilver\\u002fchatglm-6b-slim)              ...\"],[\"|      |      |[TheBloke\\u002fLLaMA-30b-AWQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLLaMA-30b-AWQ)              ...\"],[\"|      |      |[TheBloke\\u002fqCammel-13-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fqCammel-13-GPTQ)          ...\"],[\"|      |      |[FreedomIntelligence\\u002fAceGPT-7b-chat-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fFreedomIntelligence\\u002f...\"],[\"|      |      |[TheBloke\\u002fNewHope-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fNewHope-GPTQ)                ...\"],[\"|      |      |[khuranagarvit019\\u002fMentalHealthChatbot](https:\\u002f\\u002fhuggingface.co\\u002fkhuranagarvit019\\u002fMental...\"],[\"|      |      |[symanto\\u002fsn-mpnet-base-snli-mnli](https:\\u002f\\u002fhuggingface.co\\u002fsymanto\\u002fsn-mpnet-base-snli-m...\"],[\"|      |      |[sschet\\u002fbert-large-uncased_med-ner](https:\\u002f\\u002fhuggingface.co\\u002fsschet\\u002fbert-large-uncased_...\"],[\"|      |      |[michaelfeil\\u002fct2fast-Llama-2-7b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fmichaelfeil\\u002fct2fast-L...\"],[\"|      |      |[TheBloke\\u002fllama2-22B-daydreamer-v2-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama2-22B-d...\"],[\"|      |      |[luodian\\u002fllama-13b-hf](https:\\u002f\\u002fhuggingface.co\\u002fluodian\\u002fllama-13b-hf)                  ...\"],[\"|      |      |[nonlinearshimada\\u002fllama-13b](https:\\u002f\\u002fhuggingface.co\\u002fnonlinearshimada\\u002fllama-13b)      ...\"],[\"|      |      |[valurank\\u002ft5-paraphraser](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002ft5-paraphraser)            ...\"],[\"|      |      |[4bit\\u002fllama-13b-4bit-hf](https:\\u002f\\u002fhuggingface.co\\u002f4bit\\u002fllama-13b-4bit-hf)              ...\"],[\"|      |      |[TheBloke\\u002fLlama2-13B-MegaCode2-OASST-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama2-13B...\"],[\"|      |      |[gsaivinay\\u002fLlama-2-7b-Chat-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fgsaivinay\\u002fLlama-2-7b-Chat-GPT...\"],[\"|      |      |[valurank\\u002fxsum_headline_generator](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fxsum_headline_gene...\"],[\"|      |      |[TheBloke\\u002fKimiko-7B-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fKimiko-7B-fp16)            ...\"],[\"|      |      |[TheBloke\\u002fOpenChat_v3.2-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenChat_v3.2-GGML)    ...\"],[\"|      |      |[nenkoru\\u002falpaca-lora-7b-onnx-fp16-with-past](https:\\u002f\\u002fhuggingface.co\\u002fnenkoru\\u002falpaca-lo...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-13B-gpt4-1.4.1-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros...\"],[\"|      |      |[TheBloke\\u002fllama2-22B-daydreamer-v2-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama2-22B-d...\"],[\"|      |      |[TheBloke\\u002fTulu-30B-SuperHOT-8K-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fTulu-30B-SuperHO...\"],[\"|      |      |[AllanFrostin\\u002fanalise-morfossintatica-ptbr](https:\\u002f\\u002fhuggingface.co\\u002fAllanFrostin\\u002fanali...\"],[\"|      |      |[TheBloke\\u002fAiroboros-L2-70B-GPT4-m2.0-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fAiroboros-...\"],[\"|      |      |[usamakenway\\u002fllama2_7b_chat_uncensored-AutoGPTQ_Wizard_Vicuna](https:\\u002f\\u002fhuggingface.co...\"],[\"Manual Configuration\\n\\nThis guide will show you how to configure a custom structure for your dataset ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nNote that `config_name` field is required even if you have a single confi...\"],[\"Static HTML Spaces\\n\\nSpaces also accommodate custom HTML for your app instead of using Streamlit or G...\"],[\"Tasks\\n\\n## What's a task?\\n\\nTasks, or pipeline types, describe the \\\"shape\\\" of each model's API (inputs...\"],[\"Finally, you can add a couple of UI elements, such as the task icon and the widget, that complete th...\"],[\"**Adding a new task is relatively straightforward and requires 2 PRs:**\\n* PR 1: Add the new task to ...\"],[\"* [Add text-classification to spaCy](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fcommit\\u002f6926fd9be...\"],[\"### Adding Community Inference API for a quick prototype\\n\\n**My model is not supported by any library...\"],[\"If you would be interested in contributing with a widget, you can look at the [implementation](https...\"],[\"Collections\\n\\nUse Collections to group repositories from the Hub (Models, Datasets, Spaces and Papers...\"],[\"It's possible to add external repositories to your collections, not just your own.\\n\\n## Collaborating...\"],[\"![Collection delete](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fc...\"],[\"![Collection image viewer](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002f...\"],[\"Run training on Amazon SageMaker\\n\\n\\u003ciframe width=\\\"700\\\" height=\\\"394\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembe...\"],[\"To start training locally, you need to setup an appropriate [IAM role](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fs...\"],[\"```\\n\\n## Prepare a ğŸ¤— Transformers fine-tuning script\\n\\nOur training script is very similar to a traini...\"],[\"# data, model, and output directories\\n    parser.add_argument(\\\"--model-dir\\\", type=str, default=os.en...\"],[\"```\\n\\n_Note that SageMaker doesnâ€™t support argparse actions. For example, if you want to use a boolea...\"],[\"In addition to the options already mentioned above, there is another option to save the training art...\"],[\"The following code sample shows how to train with a custom script `train.py` with three hyperparamet...\"],[\"```\\n\\nIf you are running a `TrainingJob` locally, define `instance_type='local'` or `instance_type='l...\"],[\"```\\n\\n## Access trained model\\n\\nOnce training is complete, you can access your model through the [AWS ...\"],[\"```\\n\\nğŸ““ Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"```\\n\\nğŸ““ Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"# Training seconds: 874\\n# Billable seconds: 262\\n# Managed Spot Training savings: 70.0%...\"],[\"```\\n\\nğŸ““ Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"```\\n\\n## SageMaker metrics\\n\\n[SageMaker metrics](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemaker\\u002flatest\\u002fdg\\u002ftrain...\"],[\"Managing organizations\\n\\n## Creating an organization\\n\\nVisit the [New Organization](https:\\u002f\\u002fhf.co\\u002forga...\"],[\"Pickle Scanning\\n\\nPickle is a widely used serialization format in ML. Most notably, it is the default...\"],[\"```\\n\\nWhen you run this, it will create a pickle file and print the following instructions in your te...\"],[\"```\\n\\nWhen we run this script we get the `payload.pkl` again. When we check the fileâ€™s contents:\\n\\n```...\"],[\"```\\n\\nHere weâ€™re using the [fickling](https:\\u002f\\u002fgithub.com\\u002ftrailofbits\\u002ffickling) library for simplicity...\"],[\"```\\n\\nBasically, this is whatâ€™s happening when you unpickle:\\n\\n```python\\n# ...\\nopcodes_stack = [exec_f...\"],[\"```\\n\\nThe instructions that pose a threat are `STACK_GLOBAL`, `GLOBAL` and `REDUCE`.\\n\\n`REDUCE` is wha...\"],[\"```\\n\\n### Use your own serialization format\\n\\n- [MsgPack](https:\\u002f\\u002fmsgpack.org\\u002findex.html)\\n- [Protobuf]...\"],[\"On the hub the list of imports will be displayed next to each file containing imports. If any import...\"],[\"Thankfully, there is always a trace of the `eval` import, so reading the opcodes directly should all...\"],[\"[CTFtime.org \\u002f Balsn CTF 2019 \\u002f pyshv1 \\u002f Writeup](https:\\u002f\\u002fctftime.org\\u002fwriteup\\u002f16723)\\n\\n[Rehabilitatin...\"],[\"Moderation\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [Code of Conduct](https:\\u002f\\u002fhuggingface.co\\u002fcode-of-conduct) and the [...\"],[\"Livebook on Spaces\\n\\n**Livebook** is an open-source tool for writing interactive code notebooks in [E...\"],[\"Then:\\n\\n1. Give your Space a name\\n2. Set the password of your Livebook\\n3. Set its visibility to publi...\"],[\"Here's a quick video showing how to do that:\\n\\n\\u003cYoutube id=\\\"IcR60pVKeGY\\\"\\u002f\\u003e\\n\\n## How to configure Liveb...\"]],\"hovertemplate\":\"source=hub-docs\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hub-docs, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hub-docs, circle\",\"showlegend\":true,\"x\":[4.181682,4.313088,-6.0019135,6.6803823,7.4730453,7.9054236,7.8105397,-7.117624,-9.705148,7.5878525,0.086748175,2.6040485,5.5167503,5.7030735,5.310464,4.7627983,4.647088,4.6468296,3.999183,4.58777,6.003661,5.840983,5.9479933,5.831491,4.746251,5.8476367,5.5185204,4.7346935,5.4950137,3.7451155,3.6241288,1.5027093,1.7513195,6.5316067,3.535436,3.696754,3.571268,3.592369,3.1652484,3.0561817,4.9513206,4.3563595,5.2892227,6.0299435,6.3045473,4.3936543,0.9986044,6.145649,9.662979,10.022127,7.0952077,4.2311826,4.1611366,3.9091017,3.822264,5.9549985,6.747797,-6.163222,3.2387586,3.5745447,3.2945282,4.0843563,2.9162476,5.2999578,6.1340566,5.6371408,-3.4056878,7.8411045,5.919352,5.4800777,5.781207,5.69524,4.312364,6.6514425,4.272818,6.443611,-6.783824,1.5298084,1.8533605,2.787789,2.8921678,1.9851443,-2.9124372,-3.107394,2.5364783,-3.1674466,-3.0490506,2.683472,-6.235124,2.4925432,2.610222,2.693507,-6.6928477,-4.6362214,3.3092635,4.6453786,3.2153616,2.9258375,3.504977,3.6231508,3.4683568,4.389192,3.5241015,2.523843,6.8536124,9.291833,9.293932,9.294235,9.292274,9.300091,9.297585,9.295428,9.292121,9.293614,9.293049,9.292727,9.298155,9.294072,9.296004,9.304722,9.29434,9.291313,9.294881,9.291639,9.292423,9.295526,9.296788,9.295618,9.2942295,9.306569,9.307938,9.309338,9.30558,9.310516,9.320269,9.310997,9.31152,9.319458,9.320312,9.30987,9.290795,9.2977915,9.295337,9.295067,9.294873,9.294384,9.293381,9.294409,9.296216,9.294709,9.292975,9.29364,9.294905,9.294032,9.295941,9.294921,9.2925,9.290203,9.290118,9.291874,9.290173,9.292606,9.291658,9.291594,9.292326,3.3579295,4.702252,4.8922763,5.1202097,3.6034687,3.6777718,3.6833053,2.9463856,3.0090814,3.8658845,4.527263,5.0710373,4.699633,4.838431,5.9799614,5.387745,5.109318,4.783247,4.995643,4.374333,5.1568074,4.9162607,4.9659166,5.2618146,5.26027,3.2372637,3.3155437,2.6647,3.5404553,4.024157,3.983348,4.118561,6.540565,16.535843,16.405218,5.966527,6.089906,6.707424,6.591041,6.6445503,6.6181874,3.9557817,9.112733,8.833479,6.02506,9.267893,8.928664,-2.0956354,8.606777,9.226691,8.823165,8.530387,7.7725706,8.976539,9.35556,8.790886,8.566344,12.49151,8.56333,9.219698,8.932097,5.97094,6.1848607,4.00665,3.9576464,4.188588,4.773153,-6.926774,-6.184444,3.1097164,4.209628,4.538939,-6.319696,2.6785805,-3.6034994,1.6907551,4.137713,4.8129005,2.987019,4.5020742,4.5608225,4.5307245,-9.407697,4.106741,-6.231552,4.5043964,5.5017333,5.530806,5.3100634,5.034356,5.226935,5.222839,5.1924534,12.767124,12.688867,12.81284,12.831958,6.1831107,5.801005,5.653344,5.391209,3.4551835,2.8551395,0.12550464,-3.5352623,8.102661,2.4348638,2.9995828,2.5198848,1.9036708,3.8524706,4.123385,3.8467255,3.508483,3.5024657,3.8563304,3.5348406,3.08703,3.3834321,5.950169,-0.31008008,5.276611,5.902331,6.392256,6.49934,6.368727,6.4283485,6.445618,6.829453,9.127237,8.921006,15.790928,15.821022,15.786418,15.587939,15.779912,15.776483,15.780988,15.772852,15.767888,15.773207,15.778702,15.778518,15.773524,15.608026,15.569532,15.570987,15.508124,15.779161,15.581205,15.77684,15.774139,15.77223,15.776328,15.776334,15.541161,15.773518,15.76539,15.773151,15.758525,15.766974,15.77452,15.758012,15.625664,15.784005,15.7745075,15.770891,15.76574,15.77281,15.777452,15.779719,15.773136,15.765752,15.775855,15.775111,15.778017,15.773421,15.776587,15.774667,15.770211,15.769471,15.775478,15.781182,15.776322,15.764652,15.75639,15.765838,15.752429,15.771468,15.775413,15.764849,15.768417,15.7769785,15.775678,15.7685,15.76952,15.709795,15.772191,15.744394,15.774099,15.775561,15.777129,15.776019,15.776398,15.775678,15.773582,15.774851,15.803309,15.768723,15.756849,15.759407,15.757022,15.754188,15.774419,15.770307,15.770256,15.772959,15.7816725,15.75808,15.77333,15.77503,15.775289,15.773183,15.7721,15.771519,15.776735,15.761858,15.76554,15.747329,6.026662,6.0236444,4.23695,4.1101723,3.9562616,3.791789,4.2100706,5.981841,6.3530235,-0.015465654,4.4451675,5.651113,3.6289246,3.612325,3.5499923,3.1842585,2.8488917,5.046314,5.2691355,5.4340982,5.941845,5.7284517,4.054181,2.1069663,2.7296233,-7.027635,2.774031,4.6300154,4.578639,3.5999587,4.5306745,4.662629,2.4792092,-0.8129514,-0.91227716,3.768241,3.0231936,-6.1995587,6.106483,6.055384,7.3597264,4.7158585,4.6005344,4.647075,4.7170315,5.897109,3.423375,6.1277566,-4.0859785,0.014818935,-0.35298744,-1.287995,5.9844136,6.005985,6.1180687,5.3814054,6.6767693,3.6964822,-5.644977,3.8889225,2.86002,1.6751906,3.174917,3.188387,5.2502027,4.8979115,4.57098,2.5099273,-4.6403694,5.4717493,2.7382078,3.5111017,4.0534444,3.8308651,4.0421643,4.1322374,3.7424428,4.318142,4.9060063,6.527699,12.546984,6.0526495,6.218596,6.2894163,2.5992448,5.238658,3.7576237,3.4791002,2.9459808,1.9842325,4.8421574,-3.0622811,8.266049,3.3229568,13.0842495,3.8587189,4.4756384,3.1127899,3.79655,4.715177,4.744184,3.7754474,4.5764623,3.1768606,3.181115,3.6624384,5.8156195,3.4055722,3.8056505,4.9906244,5.214732,5.2346816,5.235539,5.270013,-4.0047035,-2.824797,-2.847988,4.967752,-6.179078,3.7054737,7.239693,5.6095085,5.102816,3.5527945,3.0450404,3.0948393,2.937726,2.9657109,2.7799199,1.7272912,2.7682743,4.3819165,4.1661763,4.0738854,3.44692,3.7619236,4.487845,4.2377887,4.306305,4.3940196,4.463543,2.6726704,2.9235024,2.6889453,2.3101356,-0.06965906,2.3385546,1.9204175,2.0242567,3.6164045,3.8229485,4.7359056,8.107125,3.3799293,2.0617647,1.850502,4.429492,3.0852897,0.9567179,4.8838625,-11.418575,2.617581,-0.99921185,1.4763367,0.72810525,2.7659287,-11.536024,2.6519928,2.2613442,2.2800648,2.5808496,2.5900722,2.3232832,2.5511303,2.227598,2.1995568,2.889029,2.4426699,2.353748,2.6500566,2.6624284,0.9737716,2.3066635,2.339119,0.99920285,1.870012,1.7358929,5.0855184,5.05995,6.024292,5.8165183,5.9155054,6.2407317,3.8261464,6.184525,5.978293,1.781255,5.734523,9.90585,10.416536,4.3384275,5.940158,5.880173,-5.7967496,-2.51628,-2.4282696,2.71034,5.2797604,5.2966433,5.2679644,4.2630777,4.6382365,5.0803614,5.1868267,4.1843853,6.172068,4.449332,3.8197656,3.838434,5.187088,3.6397173,5.3168077,5.723863,5.0793123,4.9009585,5.084804,4.9702077,5.35855,5.5163183,-5.4407663,-2.1075814,-5.844853,1.2187743,5.9915156,6.1536307,-4.476361,-2.1574163,2.7986932,3.5614324,-2.6807163,-2.5698395,3.2341747,4.7289333,3.8986251,-6.3531938,-6.9076357,-6.679388,-6.5453587,3.303473,4.477844,4.47999,4.6401453,4.462368,4.675017,6.247728,6.7449985,4.549223,3.931065,3.6286585,4.2300606,4.7382207,4.446252,4.2480855,4.47159,4.354115,4.3519664,3.5823748,6.0281277,4.8150654,5.3266172,6.1325073,3.1584446,3.1900175,6.4052477,5.8727717,5.8326325,4.4745803,8.234455,8.209798,3.9452164,0.67973137,0.61638963,6.2821493,-5.1268167,-4.8184557,2.584587,2.681344,-3.4849138,4.222864,-3.0847173,9.16102,9.0318365,3.3049116,3.6395068,4.5188394,4.420877,-6.9587555,4.061522,3.8901,3.4505663,3.555255,4.4366345,4.401595,4.525463,4.472105,4.4483576,2.1369848,2.7762764,1.9831835,1.7695047,-5.4867535,-1.4415886,9.105579,2.2794611,5.484724,5.204694,-5.8380737,3.5646877,3.3644712,6.46792,6.4671946,6.4879913,6.3281827,6.5375443,6.734913,9.156306,8.753263,15.791509,15.788514,15.777339,15.775454,15.761691,15.578256,15.773102,15.771665,15.811339,15.774647,15.777515,15.772975,15.770932,15.773777,15.775652,15.782722,15.751309,15.761227,15.784009,15.773294,15.775144,15.604853,15.724342,15.582087,15.5566435,15.572055,15.772526,15.573524,15.587702,15.773573,15.772565,15.815788,15.773252,15.768876,15.773378,15.762571,15.7715435,15.7601595,15.764534,15.757627,15.772619,15.775288,15.722302,15.776405,15.7740965,15.769322,15.768113,15.76425,15.765489,15.763678,15.766639,15.77149,15.794359,15.79317,15.771931,15.776361,15.771041,15.773899,15.76881,15.776837,15.771437,15.771892,15.776912,15.772847,15.775454,15.756311,15.774897,15.771469,2.691383,2.4764411,6.4114957,3.3729818,3.2765222,3.3426163,-5.847156,3.7173111,5.792939,4.2859826,4.557413,4.7568398,5.0968213,2.2317257,2.2072463,1.7698611,0.4865638,1.2006127,2.0249696,1.3539219,1.9300518,1.9602079,1.7497348,1.7402269,-3.6978905,1.9228803,-0.13680272,4.7794037,0.8690916,1.1490622,1.249708,1.126414,0.7906154,0.98187447,1.0282266,1.9652992,1.148651,0.5898156,5.678805,6.073144,5.7664614,6.0093412],\"xaxis\":\"x\",\"y\":[0.8388831,0.537878,1.9521837,2.514735,3.4642813,3.568829,3.4228601,6.5807385,-0.56468546,3.5568829,4.924772,3.7429984,1.0137312,1.4401851,0.960608,0.40086454,0.43299133,0.56866217,0.29813355,1.0419388,1.6995602,1.6388793,1.6151801,1.5864942,1.1470101,1.6438572,2.026744,1.2802807,1.5578458,3.6437736,1.6931177,4.075178,3.8695922,2.8937325,1.8035635,1.4237603,1.7617663,1.4795626,3.2544944,3.1834137,0.6900805,-1.0573566,-0.37107265,1.8301497,1.8423934,0.35106817,0.69248664,1.9286284,2.9601712,3.2596295,2.7096791,0.8544198,1.022789,1.0832746,1.0716245,1.7067261,2.091291,1.080014,1.039227,0.7739391,1.6484461,0.21638441,0.6199237,0.8798985,1.7405373,1.5331979,-2.2886539,2.5620565,1.555324,1.5029391,1.5731666,1.4307023,1.8707972,2.2304194,1.8875904,1.9589114,0.6304165,0.65315205,0.121763356,0.47340956,-2.2555346,-2.2615972,-1.0046829,-0.61205566,-2.3722289,-0.91246784,-0.5683535,-2.2776854,2.9518974,-2.3267775,-2.382721,-2.289505,5.4834213,4.2217426,-2.1391866,0.16637643,3.274888,3.3390672,5.868003,5.7939367,5.7862673,3.749651,5.725265,5.5842204,1.9470052,-20.00098,-19.997118,-19.998907,-20.001501,-19.998117,-19.998474,-19.997847,-20.00111,-20.00077,-20.001928,-20.000498,-19.996384,-20.00024,-20.001848,-19.998934,-20.00266,-20.000065,-19.998638,-20.00113,-20.001698,-20.001143,-19.997648,-20.000446,-20.001066,-19.993546,-19.994543,-19.993864,-19.996006,-19.992895,-19.98881,-19.993755,-19.9939,-19.985935,-19.987036,-19.993654,-20.002304,-19.998463,-20.001047,-20.004532,-20.003075,-20.00464,-20.00448,-20.001472,-20.00242,-20.000574,-20.002691,-20.001726,-19.999895,-20.00028,-20.000082,-19.998722,-20.000193,-20.00373,-20.004358,-20.001543,-20.002117,-20.004013,-20.00338,-20.003746,-20.002121,0.5881421,0.97071505,1.1120465,1.1313,1.3988944,3.1794777,2.7019842,1.8757888,2.6577878,2.3778472,1.8548148,1.679441,1.8363988,2.076578,1.5298964,1.4549392,1.5735552,1.6831151,1.7720885,3.8813946,1.6670184,1.9057816,1.6998522,1.7074081,1.7757794,3.5164385,3.8884885,3.7183077,0.901805,0.81704205,0.80580574,0.73389065,1.8589563,2.842812,2.8098414,1.949823,2.36824,2.8097808,2.969563,2.9545124,2.8488445,0.96419156,2.929223,2.995906,1.9598144,2.8801432,2.9771914,0.041694548,2.4799724,2.8804598,3.0001822,3.6538482,3.2008307,2.9928215,2.9474251,3.0777957,3.871905,3.154639,2.5499835,2.9104955,3.0541563,1.7253217,0.75471485,1.07028,2.2299378,2.667301,0.61891645,6.217081,7.041719,0.78126526,2.5999105,3.5249965,0.6574147,2.5259452,-0.6184819,5.132951,3.8969944,0.6489987,2.7369857,3.9196944,4.0100684,3.9607468,-0.49269286,3.3129907,0.49996623,3.721764,1.3567886,1.495624,1.4532025,-0.14478661,-0.012117299,-0.032743134,0.0064110477,4.0445786,3.4063797,4.2630005,3.756014,1.7208567,1.7126523,1.5693442,1.5377328,2.6948414,3.1916904,3.9695256,2.585283,-9.997064,0.115059786,3.5310457,3.686814,3.6145022,0.90299076,0.4138823,0.57942265,1.1762137,0.9563719,0.6676789,-0.16525716,-0.23670752,-0.54918855,1.6818565,0.42698088,2.0477395,1.6465262,2.8486469,2.5672913,2.886635,2.9089348,2.8639097,2.8357491,1.7459701,1.7996798,-16.027771,-16.00443,-16.033623,-16.193474,-16.037052,-16.038477,-16.034822,-16.041218,-16.043575,-16.041426,-16.037289,-16.035553,-16.03681,-16.176455,-16.211601,-16.206875,-16.260647,-16.039074,-16.200668,-16.03949,-16.035955,-16.041943,-16.032516,-16.037779,-16.225864,-16.036118,-16.04638,-16.037088,-16.04613,-16.044964,-16.037983,-16.049656,-16.162916,-16.030968,-16.034946,-16.039536,-16.042997,-16.038212,-16.036682,-16.035221,-16.037514,-16.046417,-16.03755,-16.037298,-16.037104,-16.03814,-16.034983,-16.03641,-16.041945,-16.041246,-16.039669,-16.033215,-16.037382,-16.039185,-16.043663,-16.04255,-16.0555,-16.038866,-16.037062,-16.042702,-16.043226,-16.038254,-16.034588,-16.039907,-16.041494,-16.080666,-16.039152,-16.06361,-16.042452,-16.038286,-16.040947,-16.041527,-16.040012,-16.039232,-16.038591,-16.03689,-16.017094,-16.03951,-16.04253,-16.039541,-16.039673,-16.045628,-16.039627,-16.038483,-16.040356,-16.03796,-16.035307,-16.04968,-16.04053,-16.039179,-16.036589,-16.038061,-16.038694,-16.03763,-16.03706,-16.047007,-16.044289,-16.057722,1.6591469,1.684736,3.11513,2.4281654,2.5680714,2.5321896,1.8700367,3.038546,2.762909,5.9566135,2.4563265,2.8970037,2.948677,3.2311888,3.3076618,3.0176504,3.2823584,1.4527168,1.4935857,1.5258213,1.6977626,1.6291604,1.0476497,0.26502928,0.54986745,6.531407,0.65750545,3.6985388,3.7339022,1.768239,1.6309762,2.0476494,-0.13838822,2.6261578,2.1959956,2.2704525,3.983613,1.843827,1.795244,1.8145565,2.4589243,3.8178544,3.7867203,3.857103,4.0517592,1.9044267,3.102483,1.5748812,-1.9946848,-0.21258023,-0.17561473,-0.43278086,1.5748975,1.6531409,1.8561499,1.9912585,1.9317319,3.0494428,1.7763325,1.1417689,1.3614571,1.730108,1.1759378,2.0505555,1.7250888,1.5241852,1.9539318,2.1291206,0.030719986,1.7830135,0.6626678,0.89667624,0.949963,1.251825,1.2340479,1.1581482,0.90169996,1.256743,1.2877746,1.8961827,3.312338,1.7367116,1.8710117,2.033462,3.7412112,0.52414906,1.6019468,0.7624788,0.60124576,1.6220956,3.9162824,-8.271199,-9.985707,0.8375276,4.2019258,0.99611455,1.3136011,2.042616,0.7070879,2.2830055,2.301894,4.165496,1.4436148,3.297929,3.1073735,1.3888551,1.6028166,-0.97974396,-1.3193938,0.6085502,-0.0054354467,0.054170046,0.15262905,0.02857743,-7.3324533,-7.7354574,-8.254792,0.63550454,1.6687655,4.4577794,2.7436757,1.431364,1.5611495,3.0211816,-2.4340527,-2.4045393,-2.3865225,-2.4246986,-2.367261,-2.144162,-2.344009,2.0453348,2.3423266,2.4683287,3.6247392,3.1008046,1.8778197,1.6746075,2.5825357,2.4376302,2.389722,3.21811,3.407797,-2.313606,-1.5572894,3.9059982,2.742366,-2.0281625,-1.8630283,3.417166,3.29559,1.9169945,-9.875068,0.83185124,0.0060055517,0.5763435,0.47076496,3.1257596,0.94401604,1.0241041,-1.3865825,0.57250273,2.8235888,-0.06741798,0.057677314,0.33910543,-1.2901742,3.5592284,3.6872501,3.6670053,-2.0987322,-2.1591942,-1.273186,-2.1884623,-1.6755891,-1.6505182,0.86585206,-1.8575785,-1.8301768,-0.22615463,-2.2242372,3.486589,-1.826834,-1.9555621,-0.32684362,-1.2527913,-1.1448545,0.2158077,0.1990306,1.7257831,1.6328555,1.5462782,1.6704088,1.0962716,1.7783811,1.6773053,0.42769557,1.6181744,3.2639732,3.429152,1.3041043,1.7780969,1.6240284,2.6800401,2.1846416,4.1302423,0.5916235,0.0140600465,0.0121769095,0.09605762,1.0922756,1.1962492,1.0396825,1.504237,0.8422495,1.7964228,1.5165336,1.064007,1.4563305,1.4028908,0.9199827,1.6183546,1.5530317,1.5674511,0.6392995,0.406111,0.6144415,0.30508313,0.030871648,1.4713494,1.7669269,1.8948778,0.13393377,1.7800908,1.6028094,1.2005206,2.012084,0.60682845,2.3397284,-1.7550973,3.1097178,0.99145263,1.8290459,1.1562575,1.3249612,1.4978849,1.5411681,1.7246897,-0.59991175,1.720117,1.5602019,1.8924556,1.3170701,1.8698889,2.919459,2.6503582,1.5520992,0.6796945,0.30484325,2.2626252,1.9177368,1.976059,2.28851,2.2621272,2.755955,1.8149694,1.1622655,1.7642438,1.9311144,1.6680295,2.4629242,3.311988,3.589925,1.8307652,1.7202165,1.8274671,1.7028414,-9.931847,-9.964144,2.806232,3.830166,6.4437137,2.8293364,1.1530646,2.3138406,0.49481162,0.5696867,1.2160268,4.499633,-9.449164,1.8252963,1.7679701,5.072895,4.7884927,4.503722,4.2955046,-0.7282111,4.6746483,4.886411,4.9193344,4.6756268,4.243125,4.1372724,4.474368,3.4826126,3.1855786,0.24383657,0.5834909,0.13892229,1.0192977,2.0003972,1.6817349,3.5566926,1.3050665,2.3162386,2.1574194,2.9297981,-0.8794199,-0.7382979,2.7695181,2.9303956,2.8091762,2.8937297,2.8772614,2.8979642,1.7595186,1.8121682,-16.03122,-16.03151,-16.03757,-16.037863,-16.045746,-16.204279,-16.041245,-16.039612,-16.013039,-16.036913,-16.036175,-16.04147,-16.041521,-16.039131,-16.038542,-16.032131,-16.055447,-16.047707,-16.0326,-16.038328,-16.04012,-16.18068,-16.080315,-16.200079,-16.218714,-16.20819,-16.039389,-16.207476,-16.192757,-16.040518,-16.03891,-15.999745,-16.037066,-16.040237,-16.038874,-16.048315,-16.039598,-16.04907,-16.044985,-16.052055,-16.039675,-16.03727,-16.077143,-16.034786,-16.038958,-16.039757,-16.041943,-16.044237,-16.047989,-16.043816,-16.043325,-16.041758,-16.019705,-16.022213,-16.04017,-16.036,-16.041798,-16.038649,-16.041273,-16.036297,-16.040878,-16.039242,-16.038996,-16.037935,-16.03956,-16.042994,-16.038471,-16.040346,3.8006861,3.8469005,1.8633649,-0.69804883,-0.49510294,-0.75833267,3.1221879,-0.59560597,1.0569475,1.9569312,2.019744,1.9100847,1.6810108,-2.0546923,-1.5476524,-1.898007,0.9741919,-1.7914616,-2.0808933,-1.7111417,-1.9665476,-2.0920668,-1.9465331,-2.0506337,-0.3300126,-1.7733343,0.18591179,0.6004633,1.6474814,2.0153282,1.9937403,2.177322,2.5787585,1.5534843,1.4959106,1.66918,1.8436836,2.5643158,1.2875849,1.786269,1.6418091,1.5306629],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, fil...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the ...\"],[\"```...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"- Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - R...\"],[\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale ...\"],[\"- Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Train...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Cla...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural archit...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. Yo...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversaria...\"],[\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) that aim...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance G...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Resid...\"],[\"ID: tresnet_m\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-tresnet\\u002ftresnet_m_...\"],[\"File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Con...\"],[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnext50`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Back...\"],[\"RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RexNet\\n  Paper:\\n    Title: 'ReXNet: Diminishing Represen...\"],[\"Training Techniques:\\n    - Label Smoothing\\n    - Linear Warmup With Cosine Annealing\\n    - Nesterov ...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 1.0e...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81....\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tens...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n  ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnety_006...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    We...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID:...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnety_120...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various typ...\"],[\"```\\nOutput:\\n```text\\nUnpooled shape: torch.Size([2, 2048, 7, 7])\\n```\\n\\n#### Remove it later\\n```python ...\"],[\"```\\nOutput:\\n```text\\nOriginal shape: torch.Size([2, 1000])\\nPooled shape: torch.Size([2, 1024])\\n```\\n\\n\\n...\"],[\"```\\n\\n### Query the feature information\\n\\nAfter a feature backbone has been created, it can be queried...\"],[\"```\\n\\n### Select specific feature levels or limit the stride\\n\\nThere are two additional creation argum...\"],[\"Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Wide ResNet\\n  Paper:\\n    Title: Wide Residual Networks\\n ...\"],[\"- Softmax\\n    - Wide Residual Block\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - I...\"],[\"Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https:\\u002f\\u002fpaperswithcod...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2NeXt\\n  Paper:\\n    Title: 'Res2Net: A New Multi-...\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nTo load and preprocess the image:\\n\\n```py\\n\\u003e\\u003e\\u003e import urllib\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from t...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, fil...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I...\"],[\"Archived Changes\\n\\n### Nov 22, 2021\\n* A number of updated weights anew new model defs\\n  * `eca_halone...\"],[\"### Oct 19, 2021\\n* ResNet strikes back (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.00476) weights added, plus any ex...\"],[\"* freeze\\u002funfreeze helpers by [Alexander Soare](https:\\u002f\\u002fgithub.com\\u002falexander-soare)...\"],[\"### Aug 18, 2021\\n* Optimizer bonanza!\\n  * Add LAMB and LARS optimizers, incl trust ratio clipping op...\"],[\"### July 5-9, 2021\\n* Add `efficientnetv2_rw_t` weights, a custom 'tiny' 13.6M param variant that is ...\"],[\"### June 20, 2021\\n* Release Vision Transformer 'AugReg' weights from [How to train your ViT? Data, A...\"],[\"* NFNets and ResNetV2-BiT models work w\\u002f Pytorch XLA now\\n  * weight standardization uses F.batch_nor...\"],[\"### June 8, 2021\\n* Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w\\u002f my XLA branch. 24 b...\"],[\"### May 5, 2021\\n* Add MLP-Mixer models and port pretrained weights from [Google JAX impl](https:\\u002f\\u002fgi...\"],[\"### April 13, 2021\\n* Add Swin Transformer models and weights from https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fSwin-...\"],[\"### April 1, 2021\\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and...\"],[\"### Feb 18, 2021\\n* Add pretrained weights and model variants for NFNet-F* models from [DeepMind Haik...\"],[\"### Feb 16, 2021\\n* Add Adaptive Gradient Clipping (AGC) as per https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.06171. Int...\"],[\"### Feb 8, 2021\\n* Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320...\"],[\"### Jan 25, 2021\\n* Add ResNetV2 Big Transfer (BiT) models w\\u002f ImageNet-1k and 21k weights from https:...\"],[\"### Dec 18, 2020\\n* Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256\\n  * 256x2...\"],[\"### Oct 21, 2020\\n* Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79...\"],[\"### Aug 12, 2020\\n* New\\u002fupdated weights from training experiments\\n  * EfficientNet-B3 - 82.1 top-1 (v...\"],[\"### Aug 5, 2020\\nUniversal feature extraction, new models, new weights, new test sets.\\n* All models s...\"],[\"### June 11, 2020\\nBunch of changes:\\n* DenseNet models updated with memory efficient addition from to...\"],[\"### May 1, 2020\\n* Merged a number of execellent contributions in the ResNet model family over the pa...\"],[\"### April 5, 2020\\n* Add some newly trained MobileNet-V2 models trained with latest h-params, rand au...\"],[\"### Feb 18, 2020\\n* Big refactor of model layers and addition of several attention mechanisms. Severa...\"],[\"### Jan 31, 2020\\n* Update ResNet50 weights with a new 79.038 result from further JSD \\u002f AugMix experi...\"],[\"### Dec 28, 2019\\n* Add new model weights and training hparams (see Training Hparams section)\\n  * `ef...\"],[\"### Nov 29, 2019\\n* Brought EfficientNet and MobileNetV3 up to date with my https:\\u002f\\u002fgithub.com\\u002frwight...\"],[\"(Tensorflow) EfficientNet CondConv\\n\\n**EfficientNet** is a convolutional neural network architecture ...\"],[\"```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_cc_b0_4e`. You c...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet CondConv\\n  Paper:\\n    Title: 'CondConv: ...\"],[\"In Collection: TF EfficientNet CondConv\\n  Metadata:\\n    FLOPs: 224158524\\n    Parameters: 24010000\\n  ...\"],[\"- Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Clas...\"],[\"SWSL ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-blo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnext101_32x16d`. You can...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-supe...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average...\"],[\"LR: 0.0015\\n    Epochs: 30\\n    Layers: 101\\n    Crop Pct: '0.875'\\n    Batch Size: 1536\\n    Image Size:...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_weakly_supervised_resne...\"],[\"Model Summaries\\n\\nThe model architectures included come from a wide variety of sources. Sources, incl...\"],[\"## DenseNet [[densenet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels...\"],[\"## HRNet [[hrnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fhrnet...\"],[\"## Inception-ResNet-V2 [[inception_resnet_v2.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fb...\"],[\"* Papers:\\n  * EfficientNet NoisyStudent (B0-B7, L2) - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.04252\\n  * Efficient...\"],[\"## RegNet [[regnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002freg...\"],[\"* ResNet (V1B)\\n  * Paper: `Deep Residual Learning for Image Recognition` - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f151...\"],[\"* Code: Added to ResNet base, this is current version going forward, old `senet.py` is being depreca...\"],[\"## Res2Net [[res2net.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fr...\"],[\"## SelecSLS [[selecsls.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels...\"],[\"## Vision Transformer [[vision_transformer.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblo...\"],[\"## Xception (Modified Aligned, Gluon) [[gluon_xception.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-imag...\"],[\"(Tensorflow) MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mobilenetv3_large_075`. You c...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MobileNet V3\\n  Paper:\\n    Title: Searching for Mobile...\"],[\"File Size: 22076649\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolut...\"],[\"Training Data:\\n    - ImageNet\\n    Training Resources: 4x4 TPU Pod\\n    ID: tf_mobilenetv3_large_minim...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 4096\\n    Image Size: '224'\\n    Weight Decay: 4.0...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_mobilen...\"],[\"Sharing and Loading Models From the Hugging Face Hub\\n\\nThe `timm` library has a built-in integration ...\"],[\"```\\n\\nRunning the above would push the model to `\\u003cyour-username\\u003e\\u002fresnet18-random` on the Hub. You can...\"],[\"Big Transfer (BiT)\\n\\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can fi...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Big Transfer\\n  Paper:\\n    Title: 'Big Transfer (BiT): Ge...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - Group Normalization\\n    - Max Pooling\\n    - ReLU\\n  ...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    - JFT-300M\\n    ID: resnetv2_152x2_bitm\\n    Crop...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 84....\"],[\"- Residual Connection\\n    - Softmax\\n    - Weight Standardization\\n    Tasks:\\n    - Image Classificati...\"],[\"MobileNet v2\\n\\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V2\\n  Paper:\\n    Title: 'MobileNetV2: Inverted ...\"],[\"- Depthwise Separable Convolution\\n    - Dropout\\n    - Inverted Residual Block\\n    - Max Pooling\\n    ...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1536\\n    Image Size: '224'\\n    Weight Decay: 4.0...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"EfficientNet (Knapsack Pruned)\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `efficientnet_b1_pruned`. You can...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet Pruned\\n  Paper:\\n    Title: Knapsack Pruning...\"],[\"- Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: efficientnet...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"(Tensorflow) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF Inception v3\\n  Paper:\\n    Title: Rethinking the ...\"],[\"DenseNet\\n\\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet...\"],[\"```\\n\\nTo get the model predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e with torch.no_grad():\\n...     out = m...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `densenet121`. You can find the I...\"],[\"```\\n\\n```\\n@misc{rw2019timm,\\n  author = {Ross Wightman},\\n  title = {PyTorch Image Models},\\n  year = {2...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DenseNet\\n  Paper:\\n    Title: Densely Connected Convoluti...\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Kaiming Initializatio...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fdensenet169-b2777c0a.pth\\n  Results:\\n  - Task: Image Cla...\"],[\"- ReLU\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID:...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"Training Examples\\n\\n## EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5\\nThese params are for...\"],[\"## SE-ResNeXt-26-D and SE-ResNeXt-26-T\\nThese hparams (or similar) work well for a wide range of ResN...\"],[\"`.\\u002fdistributed_train.sh 2 \\u002fimagenet\\u002f --model efficientnet_b0 -b 384 --sched step --epochs 450 --deca...\"],[\"`.\\u002fdistributed_train.sh 8 \\u002fimagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay...\"],[\"`.\\u002fdistributed_train.sh 8 \\u002fimagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 ...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MNASNet\\n  Paper:\\n    Title: 'MnasNet: Platform-Aware Neu...\"],[\"- Max Pooling\\n    - ReLU\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n...\"],[\"SK-ResNet\\n\\n**SK ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNet\\n  Paper:\\n    Title: Selective Kernel Networks\\n ...\"],[\"Epochs: 100\\n    Layers: 34\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Siz...\"],[\"Inception v4\\n\\n**Inception-v4** is a convolutional neural network architecture that builds on previou...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_v4`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v4\\n  Paper:\\n    Title: Inception-v4, Ince...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mnasnet_100`. You can find the I...\"],[\"NASNet\\n\\n**NASNet** is a type of convolutional neural network discovered through neural architecture ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: NASNet\\n  Paper:\\n    Title: Learning Transferable Ar...\"],[\"SelecSLS\\n\\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SelecSLS\\n  Paper:\\n    Title: 'XNect: Real-time Multi-Per...\"],[\"- Image Classification\\n    Training Techniques:\\n    - Cosine Annealing\\n    - Random Erasing\\n    Trai...\"],[\"MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mixnet_l`. You can find the IDs ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise Con...\"],[\"- Swish\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - MNAS\\n    Training Data:...\"],[\"Top 5 Accuracy: 92.79%\\n- Name: mixnet_xl\\n  In Collection: MixNet\\n  Metadata:\\n    FLOPs: 1195880424\\n ...\"],[\"DenseNet\\n\\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"SK-ResNeXt\\n\\n**SK ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnext...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `skresnext50_32x4d`. You can find...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNeXt\\n  Paper:\\n    Title: Selective Kernel Netw...\"],[\"Recent Changes\\n\\n### Aug 29, 2022\\n* MaxVit window size scales with img_size by default. Add new RelPo...\"],[\"### Aug 26, 2022\\n* CoAtNet (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.04803) and MaxVit (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2204...\"],[\"* EfficientFormer (adapted from https:\\u002f\\u002fgithub.com\\u002fsnap-research\\u002fEfficientFormer)\\n* PyramidVisionTra...\"],[\"### Aug 15, 2022\\n* ConvNeXt atto weights added\\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\\n  * `con...\"],[\"### July 27, 2022\\n* All runtime benchmark and validation result csv files are finally up-to-date!\\n* ...\"],[\"### July 8, 2022\\nMore models, more fixes\\n* Official research models (w\\u002f weights) added:\\n  * EdgeNeXt...\"],[\"* `vit_srelpos_medium_patch16_224` - 82.3 @ 224, 83.1 @ 320\\n  * `vit_relpos_small_patch16_cls_224` -...\"],[\"### May 13, 2022\\n* Official Swin-V2 models and weights added from (https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fSwin...\"],[\"### May 2, 2022\\n* Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (`visi...\"],[\"### March 23, 2022\\n* Add `ParallelBlock` and `LayerScale` option to base vit models to support model...\"],[\"### March 21, 2022\\n* Merge `norm_norm_norm`. **IMPORTANT** this update for a coming 0.6.x release wi...\"],[\"* `xception41p` - 82 @ 299   (timm pre-act)\\n  * `xception65` -  83.17 @ 299\\n  * `xception65p` -  83....\"],[\"* Gradient checkpointing support added to many models\\n* `forward_head(x, pre_logits=False)` fn added...\"],[\"### Feb 2, 2022\\n* [Chris Hughes](https:\\u002f\\u002fgithub.com\\u002fChris-hughes10) posted an exhaustive run through...\"],[\"### Jan 14, 2022\\n* Version 0.5.4 w\\u002f release to be pushed to pypi. It's been a while since last pypi ...\"],[\"### Dec 23, 2022 ğŸ„â˜ƒ\\n* Add FlexiViT models and weights from https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fbig_vi...\"],[\"| model                                     | top1 | param_count |  gmac | macts | hub              ...\"],[\"| model                                    |   top1 |   param_count |   gmac |   macts | hub        ...\"],[\"### Dec 5, 2022\\n\\n* Pre-release (`0.8.0dev0`) of multi-weight support (`model_arch.pretrained_tag`). ...\"],[\"| model                                            |   top1 |   param_count |   gmac |   macts | hub...\"],[\"| vit_large_patch14_clip_336.laion2b_ft_in12k_in1k |   88.2 |         304.5 |  191.1 |   270.2 | [li...\"],[\"| vit_large_patch14_clip_224.laion2b_ft_in1k       |   87.3 |         304.2 |   81.1 |    88.8 | [li...\"],[\"| vit_base_patch16_clip_224.openai_ft_in12k_in1k   |   85.9 |          86.6 |   17.6 |    23.9 | [li...\"],[\"| vit_base_patch32_clip_224.laion2b_ft_in12k_in1k  |   83.3 |          88.2 |    4.4 |     5   | [li...\"],[\"* Port of MaxViT Tensorflow Weights from official impl at https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fmaxvit\\n...\"],[\"| model                              |   top1 |   param_count |   gmac |   macts | hub              ...\"],[\"| maxvit_large_tf_384.in21k_ft_in1k  |   88   |         212   |  132.6 |   445.8 | [link](https:\\u002f\\u002fhu...\"],[\"| maxvit_tiny_tf_512.in1k            |   85.7 |          31   |   33.5 |   257.6 | [link](https:\\u002f\\u002fhu...\"],[\"### Oct 15, 2022\\n* Train and validation script enhancements\\n* Non-GPU (ie CPU) device support\\n* SLUR...\"],[\"### Oct 10, 2022\\n* More weights in `maxxvit` series, incl first ConvNeXt block based `coatnext` and ...\"],[\"### Sept 7, 2022\\n* Hugging Face [`timm` docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002ftimm) home now exists,...\"],[\"### Aug 26, 2022\\n* CoAtNet (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.04803) and MaxVit (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2204...\"],[\"### Aug 15, 2022\\n* ConvNeXt atto weights added\\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\\n  * `con...\"],[\"### July 27, 2022\\n* All runtime benchmark and validation result csv files are up-to-date!\\n* A few mo...\"],[\"* `vit_srelpos_medium_patch16_224` - 82.3 @ 224, 83.1 @ 320\\n  * `vit_relpos_small_patch16_cls_224` -...\"],[\"### Jan 14, 2022\\n* Version 0.5.4 w\\u002f release to be pushed to pypi. It's been a while since last pypi ...\"],[\"Hugging Face Timm Docs\\n\\n## Getting Started\\n\\n```\\npip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdoc-b...\"],[\"ResNeSt\\n\\nA **ResNeSt** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet), which i...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnest101e`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeSt\\n  Paper:\\n    Title: 'ResNeSt: Split-Attention Ne...\"],[\"- Dense Connections\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Connect...\"],[\"Layers: 200\\n    Dropout: 0.2\\n    Crop Pct: '0.909'\\n    Momentum: 0.9\\n    Batch Size: 2048\\n    Image ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-resnest\\u002fresnest269...\"],[\"Parameters: 27480000\\n    File Size: 110273258\\n    Architecture:\\n    - 1x1 Convolution\\n    - Convolut...\"],[\"LR: 0.1\\n    Epochs: 270\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-resnest\\u002fresnest50_...\"],[\"FBNet\\n\\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https:\\u002f\\u002fp...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `fbnetc_100`. You can find the ID...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: FBNet\\n  Paper:\\n    Title: 'FBNet: Hardware-Aware Ef...\"],[\"HRNet\\n\\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ...\"],[\"```\\n\\nTo get the model predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e with torch.no_grad():\\n...     out = m...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `hrnet_w18`. You can find the IDs...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: HRNet\\n  Paper:\\n    Title: Deep High-Resolution Represent...\"],[\"Epochs: 100\\n    Layers: 18\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Siz...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"ID: hrnet_w32\\n    Epochs: 100\\n    Layers: 32\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size:...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"ID: hrnet_w48\\n    Epochs: 100\\n    Layers: 48\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size:...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Learning Rate Schedulers\\n\\nThis page contains the API reference documentation for learning rate sched...\"],[\"MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv3_large_100`. You can ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V3\\n  Paper:\\n    Title: Searching for MobileNet...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Depthwise Separable Convolutio...\"],[\"ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet\\n  Paper:\\n    Title: Deep Residual Learning for Im...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet26-9...\"],[\"- ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classific...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Epochs: 90\\n    Crop Pct: '0.875'\\n    LR Gamma: 0.1\\n    Momentum: 0.9\\n    Batch Size: 32\\n    Image Si...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 73....\"],[\"SK-ResNet\\n\\n**SK ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `skresnet18`. You can find the ID...\"],[\"SelecSLS\\n\\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"PNASNet\\n\\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: PNASNet\\n  Paper:\\n    Title: Progressive Neural Arch...\"],[\"(Tensorflow) EfficientNet CondConv\\n\\n**EfficientNet** is a convolutional neural network architecture ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_cc_b0_4e`. You c...\"],[\"ECA-ResNet\\n\\nAn **ECA ResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ecaresnet101d`. You can find the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ECAResNet\\n  Paper:\\n    Title: 'ECA-Net: Efficient Channe...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - ...\"],[\"LR: 0.1\\n    Epochs: 100\\n    Layers: 50\\n    Crop Pct: '0.875'\\n    Batch Size: 256\\n    Image Size: '22...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Validation and Benchmark Results\\n\\nThis folder contains validation and benchmark results for the mode...\"],[\"An ImageNet test set of 10,000 images sampled from new images roughly 10 years after the original. C...\"],[\"For clean validation with same 200 classes, see [`results-imagenet-r-clean.csv`](results-imagenet-r-...\"],[\"Results\\n\\nCSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation resul...\"],[\"|Model | Acc@1 (Err) | Acc@5 (Err) | Param # (M) | Interpolation | Image Size |\\n|---|---|---|---|---...\"],[\"| skresnext50d_32x4d | 80.156 (19.844) | 94.642 (5.358) | 27.5 | bicubic | 224 |\\n| cspdarknet53 | 80...\"],[\"| efficientnet_es | 78.066 (21.934) | 93.926 (6.074) | 5.44 | bicubic | 224 |\\n| seresnext26t_32x4d |...\"],[\"| resnet26d | 76.68 (23.32) | 93.166 (6.834) | 16 | bicubic | 224 |\\n| densenetblur121d | 76.576 (23....\"],[\"| mnasnet_b1 | 74.658 (25.342) | 92.114 (7.886) | 4.38 | bicubic | 224 |\\n| spnasnet_100 | 74.084 (25...\"],[\"## Ported and Other Weights\\n\\nFor weights ported from other deep learning frameworks (Tensorflow, MXN...\"],[\"Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Inception fam...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v3\\n  Paper:\\n    Title: Rethinking the Inc...\"],[\"CSP-DarkNet\\n\\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspdarknet53`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP DarkNet\\n  Paper:\\n    Title: 'YOLOv4: Optimal Sp...\"],[\"SPNASNet\\n\\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SPNASNet\\n  Paper:\\n    Title: 'Single-Path NAS: Desi...\"],[\"Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find ...\"],[\"SSL ResNeXT\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-bloc...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ssl_resnext101_32x16d`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-super...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average...\"],[\"LR: 0.0015\\n    Epochs: 30\\n    Layers: 101\\n    Crop Pct: '0.875'\\n    Batch Size: 1536\\n    Image Size:...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_supervised_resnext50_32...\"],[\"RegNetX\\n\\n**RegNetX** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnetx_002`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetX\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnetx_004\\n    Ep...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnetx_006...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnetx_032...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnetx_080...\"],[\"- Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    Tasks:\\n   ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnetx_320...\"],[\"Installation\\n\\nBefore you start, you'll need to setup your environment and install the appropriate pa...\"],[\"```\\n\\n## From Source\\n\\nBuilding `timm` from source lets you make changes to the code base. To install ...\"],[\"(Tensorflow) MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mobilenetv3_large_075`. You c...\"],[\"Big Transfer (BiT)\\n\\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can fi...\"],[\"Instagram ResNeXt WSL\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fre...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ig_resnext101_32x16d`. You can f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: IG ResNeXt\\n  Paper:\\n    Title: Exploring the Limits of W...\"],[\"- 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Gro...\"],[\"Epochs: 100\\n    Layers: 101\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 8064\\n    Image S...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fig_resnext101_32x8-c38310e5.pth\\n  Results:\\n  - Task: Im...\"],[\"(Gluon) Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Xception\\n  Paper:\\n    Title: 'Xception: Deep ...\"],[\"This guideline is very much a work-in-progress.*\\n\\nContributions to `timm` for code, documentation, t...\"],[\"```\\nblack --skip-string-normalization --line-length 120 \\u003cpath-to-file\\u003e\\n```\\n\\nAvoid formatting code th...\"],[\"```\\npytest -k \\\"substring-to-match\\\" -n 4 tests\\u002f\\n```\\n\\n## Building documentation\\n\\nPlease refer to [this...\"],[\"Deep Layer Aggregation\\n\\nExtending  â€œshallowâ€ skip connections, **Dense Layer Aggregation (DLA)** inc...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DLA\\n  Paper:\\n    Title: Deep Layer Aggregation\\n    URL: ...\"],[\"- SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x ...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla102x2-262837b6.pth\\n  Results:\\n  - Task: Image Classi...\"],[\"- Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    T...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla46_c-2bfd52c3.pth\\n  Results:\\n  - Task: Image Classif...\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n   ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"LR: 0.1\\n    Epochs: 120\\n    Layers: 60\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n ...\"],[\"Data\\n\\n[[autodoc]] timm.data.create_dataset\\n\\n[[autodoc]] timm.data.create_loader\\n\\n[[autodoc]] timm.da...\"],[\"CSP-ResNet\\n\\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnet50`. You can find the I...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNet\\n  Paper:\\n    Title: 'CSPNet: A New Backb...\"],[\"Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Inception fam...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_v3`. You can find the ...\"],[\"Getting Started\\n\\n## Welcome\\n\\nWelcome to the `timm` documentation, a lean set of docs that covers the...\"],[\"```\\n\\n## List Models with Pretrained Weights\\n```python\\nimport timm\\nfrom pprint import pprint\\nmodel_na...\"],[\"Model Summaries\\n\\nThe model architectures included come from a wide variety of sources. Sources, incl...\"],[\"## DenseNet\\n\\n* Implementation: [densenet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002f...\"],[\"## HRNet\\n\\n* Implementation: [hrnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster...\"],[\"## Inception-ResNet-V2\\n\\n* Implementation: [inception_resnet_v2.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpyto...\"],[\"## EfficientNet\\n\\n* Implementation: [efficientnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"## RegNet\\n\\n* Implementation: [regnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmast...\"],[\"## Res2Net\\n\\n* Implementation: [res2net.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fma...\"],[\"## SelecSLS\\n\\n* Implementation: [selecsls.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002f...\"],[\"## VGG\\n\\n* Implementation: [vgg.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftim...\"],[\"## Xception (Modified Aligned, Gluon)\\n\\n* Implementation: [gluon_xception.py](https:\\u002f\\u002fgithub.com\\u002frwig...\"],[\"Deep Layer Aggregation\\n\\nExtending  â€œshallowâ€ skip connections, **Dense Layer Aggregation (DLA)** inc...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `dla102`. You can find the IDs in...\"],[\"Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various typ...\"],[\"```\\n\\nOutput:\\n\\n```text\\nUnpooled shape: torch.Size([2, 2048, 7, 7])\\n```\\n\\n#### Remove it later\\n\\n```py\\n\\u003e...\"],[\"```\\n\\nOutput:\\n\\n```text\\nPooled shape: torch.Size([2, 2048])\\n```\\n\\n#### Remove it later\\n\\n```py\\n\\u003e\\u003e\\u003e impor...\"],[\"```\\n\\nOutput:\\n\\n```text\\ntorch.Size([2, 64, 112, 112])\\ntorch.Size([2, 256, 56, 56])\\ntorch.Size([2, 512,...\"],[\"```\\n\\n### Select specific feature levels or limit the stride\\n\\nThere are two additional creation argum...\"],[\"(Gluon) ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnet101_v1b`. You can fi...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNet\\n  Paper:\\n    Title: Deep Residual Learning ...\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 70....\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"Adversarial Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the I...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `adv_inception_v3`. You can find ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Adversarial Inception v3\\n  Paper:\\n    Title: Advers...\"],[\"ResNet-D\\n\\n**ResNet-D** is a modification on the [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) a...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnet101d`. You can find the ID...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet-D\\n  Paper:\\n    Title: Bag of Tricks for Image Cla...\"],[\"- Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    T...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 72....\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet26d-...\"],[\"- ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classific...\"],[\"(Tensorflow) EfficientNet Lite\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_lite0`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet Lite\\n  Paper:\\n    Title: 'EfficientNet: ...\"],[\"- RELU6\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficient...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"- RELU6\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficient...\"],[\"CSP-DarkNet\\n\\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scaling method tha...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `efficientnet_b0`. You can find t...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethinki...\"],[\"- Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.38%\\n      Top 5 Accuracy: 95.08%\\n- Name: eff...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"- Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: efficientnet...\"],[\"Metrics:\\n      Top 1 Accuracy: 78.09%\\n      Top 5 Accuracy: 93.93%\\n- Name: efficientnet_lite0\\n  In C...\"],[\"timm\\n\\n\\u003cimg class=\\\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\\\" src=\\\"ht...\"],[\"Read the [quick start guide](quickstart) to get up and running with the `timm` library. You will lea...\"],[\"Scripts\\nA train, validation, inference, and checkpoint cleaning script included in the github root f...\"],[\"`python validate.py \\u002fimagenet\\u002fvalidation\\u002f --model seresnext26_32x4d --pretrained`\\n\\nTo run inference ...\"],[\"(Legacy) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmetho...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_seresnext101_32x4d`. You ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excita...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average...\"],[\"ID: legacy_seresnext50_32x4d\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pc...\"],[\"RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `rexnet_100`. You can find the ID...\"],[\"MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"(Gluon) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_seresnext101_32x4d`. You c...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitati...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n  ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-pretrained-gluonresnet\\u002freleases\\u002fdownload\\u002fv0.1\\u002fgluon_se...\"],[\"RegNetX\\n\\n**RegNetX** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnetx_002`. You can find the I...\"],[\"(Tensorflow) EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scali...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0`. You can fin...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethi...\"],[\"Metadata:\\n    FLOPs: 883633200\\n    Parameters: 7790000\\n    File Size: 31512534\\n    Architecture:\\n   ...\"],[\"Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_b2\\n    LR: 0.256\\n    Epochs: 350\\n    Crop Pct:...\"],[\"Batch Size: 2048\\n    Image Size: '300'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSP...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"In Collection: TF EfficientNet\\n  Metadata:\\n    FLOPs: 24180518488\\n    Parameters: 43040000\\n    File ...\"],[\"- Stochastic Depth\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_b7\\n ...\"],[\"Batch Size: 2048\\n    Image Size: '672'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSP...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"- Swish\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficient...\"],[\"Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n   ...\"],[\"(Legacy) SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fr...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_seresnet101`. You can fin...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitat...\"],[\"- Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling...\"],[\"LR: 0.6\\n    Epochs: 100\\n    Layers: 18\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fseresnet34...\"],[\"Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_resnet_v2`. You can fi...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception ResNet v2\\n  Paper:\\n    Title: Inception-v...\"],[\"(Gluon) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Incep...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_inception_v3`. You can fin...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Inception v3\\n  Paper:\\n    Title: Rethinking t...\"],[\"(Legacy) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_senet154`. You can find t...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SENet\\n  Paper:\\n    Title: Squeeze-and-Excita...\"],[\"Quickstart\\n\\nThis quickstart is intended for developers who are ready to dive into the code and see a...\"],[\"```\\n\\nYou can also list models with a specific pattern in their name.\\n\\n```py\\n\\u003e\\u003e\\u003e import timm\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n\\n## Image Augmentation\\n\\nTo transform images into valid inputs for a model, you can use [`timm.da...\"],[\"```\\n\\nWe can then resolve only the data related configuration by using [`timm.data.resolve_data_confi...\"],[\"```\\n\\n\\u003cTip\\u003e\\n    Note: Here, the pretrained model's config happens to be the same as the generic confi...\"],[\"```\\n\\nNow we can pass that image to the model to get the predictions. We use `unsqueeze(0)` in this c...\"],[\"```\\n\\nIf we check the imagenet labels for the top index, we can see what the model predicted...\\n\\n```p...\"],[\"Optimization\\n\\nThis page contains the API reference documentation for learning rate optimizers includ...\"],[\"SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresneXt...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnext26d_32x4d`. You can fin...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average...\"],[\"ID: seresnext50_32x4d\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pct: '0.8...\"],[\"SWSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnet18`. You can find the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-super...\"],[\"- ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classific...\"],[\"FBNet\\n\\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https:\\u002f\\u002fp...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"Dual Path Network (DPN)\\n\\nA **Dual Path Network (DPN)** is a convolutional neural network which prese...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DPN\\n  Paper:\\n    Title: Dual Path Networks\\n    URL: http...\"],[\"Training Resources: 40x K80 GPUs\\n    ID: dpn131\\n    LR: 0.316\\n    Layers: 131\\n    Crop Pct: '0.875'\\n...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76....\"],[\"ID: dpn92\\n    LR: 0.316\\n    Layers: 92\\n    Crop Pct: '0.875'\\n    Batch Size: 1280\\n    Image Size: '2...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"(Tensorflow) MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise ...\"],[\"- Swish\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - MNAS\\n    Training Data:...\"],[\"Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"(Gluon) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_seresnext101_32x4d`. You c...\"],[\"(Tensorflow) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_inception_v3`. You can find t...\"],[\"SK-ResNeXt\\n\\n**SK ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnext...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `skresnext50_32x4d`. You can find...\"],[\"HRNet\\n\\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"(Legacy) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmetho...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_seresnext101_32x4d`. You ...\"],[\"SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresneXt...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnext26d_32x4d`. You can fin...\"],[\"(Legacy) SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fr...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scaling method tha...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https:\\u002f\\u002fpaperswithcod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2next50`. You can find the ID...\"],[\"ECA-ResNet\\n\\nAn **ECA ResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) t...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"Instagram ResNeXt WSL\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fre...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ig_resnext101_32x16d`. You can f...\"],[\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find...\"],[\"Scripts\\n\\nA train, validation, inference, and checkpoint cleaning script included in the github root ...\"],[\"```\\n\\nTo run inference from a checkpoint:\\n\\n```bash\\npython inference.py \\u002fimagenet\\u002fvalidation\\u002f --model ...\"],[\"```\\n\\n### SE-ResNeXt-26-D and SE-ResNeXt-26-T\\n\\nThese hparams (or similar) work well for a wide range ...\"],[\"```\\n### EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5\\n\\nThe training of this model starte...\"],[\"```\\n### ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5\\n\\nTrai...\"],[\"```\\n### MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5\\n\\n```bash\\n.\\u002fdistributed_train.sh 2 \\u002fimagen...\"],[\"ResNeSt\\n\\nA **ResNeSt** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet), which i...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnest101e`. You can find the I...\"],[\"ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnet18`. You can find the IDs ...\"],[\"NASNet\\n\\n**NASNet** is a type of convolutional neural network discovered through neural architecture ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `nasnetalarge`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: NASNet\\n  Paper:\\n    Title: Learning Transferable Ar...\"],[\"(Gluon) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Incep...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"(Gluon) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SENet\\n  Paper:\\n    Title: Squeeze-and-Excitat...\"],[\"Vision Transformer (ViT)\\n\\nThe **Vision Transformer** is a model for image classification that employ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `vit_base_patch16_224`. You can f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Vision Transformer\\n  Paper:\\n    Title: 'An Image is Wort...\"],[\"Parameters: 86860000\\n    File Size: 347460194\\n    Architecture:\\n    - Attention Dropout\\n    - Convol...\"],[\"ID: vit_base_patch32_384\\n    Crop Pct: '1.0'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-vitjx\\u002fjx_vit_base_...\"],[\"Metadata:\\n    FLOPs: 174702764032\\n    Parameters: 304720000\\n    File Size: 1218907013\\n    Architectu...\"],[\"- ImageNet\\n    - JFT-300M\\n    Training Resources: TPUv3\\n    ID: vit_small_patch16_224\\n    Crop Pct: ...\"],[\"ResNet-D\\n\\n**ResNet-D** is a modification on the [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) a...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"PNASNet\\n\\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `pnasnet5large`. You can find the...\"],[\"AdvProp (EfficientNet)\\n\\n**AdvProp** is an adversarial training scheme which treats adversarial examp...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ap`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: AdvProp\\n  Paper:\\n    Title: Adversarial Examples Improve...\"],[\"Parameters: 7790000\\n    File Size: 31515350\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Po...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_b2_ap\\n    LR: 0.256\\n    Epo...\"],[\"Batch Size: 2048\\n    Image Size: '300'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSP...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"- Name: tf_efficientnet_b6_ap\\n  In Collection: AdvProp\\n  Metadata:\\n    FLOPs: 24180518488\\n    Parame...\"],[\"- Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Techni...\"],[\"Momentum: 0.9\\n    Batch Size: 2048\\n    Image Size: '672'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"MobileNet v2\\n\\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv2_100`. You can find t...\"],[\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) that aim...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tresnet_l`. You can find the IDs...\"],[\"ESE-VoVNet\\n\\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https:\\u002f\\u002fpaper...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ese_vovnet19b_dw`. You can find ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ESE VovNet\\n  Paper:\\n    Title: 'CenterMask : Real-Time A...\"],[\"- ReLU\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: ese_vovnet39b...\"],[\"CSP-ResNet\\n\\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"AdvProp (EfficientNet)\\n\\n**AdvProp** is an adversarial training scheme which treats adversarial examp...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ap`. You can ...\"],[\"(Legacy) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"(Gluon) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_senet154`. You can find th...\"],[\"SWSL ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-blo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnext101_32x16d`. You can...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-block) t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnext101_32x8d`. You can find ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Transfor...\"],[\"- Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - I...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"PyTorch Image Models\\n- [What's New](#whats-new)\\n- [Introduction](#introduction)\\n- [Models](#models)\\n...\"],[\"â—Updates after Oct 10, 2022 are available in version \\u003e= 0.9â—\\n* Many changes since the last 0.6.x sta...\"],[\"* The Hugging Face Hub (https:\\u002f\\u002fhuggingface.co\\u002ftimm) is now the primary source for `timm` weights. M...\"],[\"### Nov 23, 2023\\n* Added EfficientViT-Large models, thanks [SeeFun](https:\\u002f\\u002fgithub.com\\u002fseefun)\\n* Fix...\"],[\"### Oct 20, 2023\\n* [SigLIP](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2303.15343) image tower weights supported ...\"],[\"### Aug 28, 2023\\n* Add dynamic img size support to models in `vision_transformer.py`, `vision_transf...\"],[\"### Aug 25, 2023\\n* Many new models since last release\\n  * FastViT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2303.14189...\"],[\"### Aug 11, 2023\\n* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image\\u002fwindow size on c...\"],[\"### May 11, 2023\\n* `timm` 0.9 released, transition from 0.8.xdev releases\\n\\n### May 10, 2023\\n* Huggin...\"],[\"### April 21, 2023\\n* Gradient accumulation support added to train script and tested (`--grad-accum-s...\"],[\"### April 5, 2023\\n* ALL ResNet models pushed to Hugging Face Hub with multi-weight support\\n  * All p...\"],[\"| model                                                                                             ...\"],[\"* Add EVA-02 MIM pretrained and fine-tuned weights, push to HF hub and update model cards for all EV...\"],[\"| model                                              |top1  |top5  |param_count|img_size|\\n|---------...\"],[\"| eva_large_patch14_336.in22k_ft_in22k_in1k          |89.214|98.854|304.53     |336     |\\n| eva_gian...\"],[\"* Multi-weight and HF hub for DeiT and MLP-Mixer based models\\n\\n### March 22, 2023\\n* More weights pus...\"],[\"### Feb 26, 2023\\n* Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features...\"],[\"### Feb 7, 2023\\n* New inference benchmark numbers added in [results](results\\u002f) folder.\\n* Add convnex...\"],[\"* Add ImageNetInfo \\u002f DatasetInfo classes to provide labelling for various ImageNet classifier layout...\"],[\"### Jan 20, 2023\\n* Add two convnext 12k -\\u003e 1k fine-tunes at 384x384\\n  * `convnext_tiny.in12k_ft_in1k...\"],[\"|model                                                                                              ...\"],[\"|[maxvit_large_tf_512.in21k_ft_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_large_tf_512.in21k_ft_in1k) ...\"],[\"|[coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_rmlp_2_rw_384.sw_in12k...\"],[\"|[coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_rmlp_2_rw_224.sw_in12k...\"],[\"|[maxvit_tiny_tf_384.in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_tiny_tf_384.in1k)                     ...\"],[\"|[maxvit_rmlp_small_rw_224.sw_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_rmlp_small_rw_224.sw_in1k)   ...\"],[\"|[coatnet_rmlp_1_rw_224.sw_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_rmlp_1_rw_224.sw_in1k)         ...\"],[\"|[coatnet_0_rw_224.sw_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_0_rw_224.sw_in1k)                   ...\"],[\"### Jan 11, 2023\\n* Update ConvNeXt ImageNet-12k pretrain series w\\u002f two new fine-tuned weights (and p...\"],[\"| model                                     | top1 | param_count |  gmac | macts | hub              ...\"],[\"### Oct 15, 2022\\n* Train and validation script enhancements\\n* Non-GPU (ie CPU) device support\\n* SLUR...\"],[\"### Sept 23, 2022\\n* LAION-2B CLIP image towers supported as pretrained backbones for fine-tune or fe...\"],[\"* Aggregating Nested Transformers - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2105.12723\\n* BEiT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f...\"],[\"* EfficientFormer - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2206.01191\\n* EfficientNet (MBConvNet Family)\\n    * Efficie...\"],[\"* FlexiViT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2212.08013\\n* FocalNet (Focal Modulation Networks) - https:\\u002f\\u002farxiv...\"],[\"* FBNet-V3 - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2006.02049\\n  * HardCoRe-NAS - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.11646\\n  ...\"],[\"* RepGhostNet - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.06088\\n* RepViT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2307.09283\\n* ResML...\"],[\"* Swin S3 (AutoFormerV2) - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2111.14725\\n* Swin Transformer - https:\\u002f\\u002farxiv.org\\u002fa...\"],[\"## Features\\n\\nSeveral (less common) features that I often utilize in my projects are included. Many o...\"],[\"* All models have a common default configuration interface and API for\\n    * accessing\\u002fchanging the ...\"],[\"* A 'Test Time Pool' wrapper that can wrap any of the included models and usually provides improved ...\"],[\"* `fused\\u003cname\\u003e` optimizers by name with [NVIDIA Apex](https:\\u002f\\u002fgithub.com\\u002fNVIDIA\\u002fapex\\u002ftree\\u002fmaster\\u002fape...\"],[\"* SplitBachNorm - allows splitting batch norm layers between clean and augmented (auxiliary batch no...\"],[\"* Squeeze-and-Excitation (SE) - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1709.01507\\n    * Selective Kernel (SK) - (http...\"],[\"## Results\\n\\nModel validation results can be found in the [results tables](results\\u002fREADME.md)\\n\\n## Get...\"],[\"### Computer Vision \\u002f Image Augmentation\\n* Albumentations - https:\\u002f\\u002fgithub.com\\u002falbumentations-team\\u002fa...\"],[\"#### Pretrained on more than ImageNet\\nSeveral weights included or references here were pretrained wi...\"],[\"```\\n\\n### Latest DOI\\n\\n[![DOI](https:\\u002f\\u002fzenodo.org\\u002fbadge\\u002f168799526.svg)](https:\\u002f\\u002fzenodo.org\\u002fbadge\\u002flates...\"],[\"ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-block) t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnext101_32x8d`. You can find ...\"],[\"Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `xception`. You can find the IDs ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Xception\\n  Paper:\\n    Title: 'Xception: Deep Learning wi...\"],[\"- ReLU\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Da...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.55%\\n      T...\"],[\"Dual Path Network (DPN)\\n\\nA **Dual Path Network (DPN)** is a convolutional neural network which prese...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `dpn107`. You can find the IDs in...\"],[\"(Gluon) ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnext101_32x4d`. You can...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Tr...\"],[\"- Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n    - ResNeXt Block\\n    - Residual Connection\\n   ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-pretrained-gluonresnet\\u002freleases\\u002fdownload\\u002fv0.1\\u002fgluon_re...\"],[\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural archit...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. Yo...\"],[\"(Gluon) ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnext101_32x4d`. You can...\"],[\"SSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-superv...\"],[\"- ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classific...\"],[\"Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"(Gluon) ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnet101_v1b`. You can fi...\"],[\"(Tensorflow) EfficientNet Lite\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_lite0`. You can ...\"],[\"Models\\n\\n[[autodoc]] timm.create_model\\n\\n[[autodoc]] timm.list_models...\"],[\"SPNASNet\\n\\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `spnasnet_100`. You can find the ...\"],[\"MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"EfficientNet (Knapsack Pruned)\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"Inception v4\\n\\n**Inception-v4** is a convolutional neural network architecture that builds on previou...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"Noisy Student (EfficientNet)\\n\\n**Noisy Student Training** is a semi-supervised learning approach. It ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ns`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Noisy Student\\n  Paper:\\n    Title: Self-training with Noi...\"],[\"- Name: tf_efficientnet_b1_ns\\n  In Collection: Noisy Student\\n  Metadata:\\n    FLOPs: 883633200\\n    Pa...\"],[\"- Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n ...\"],[\"Training Data:\\n    - ImageNet\\n    - JFT-300M\\n    Training Resources: Cloud TPU v3 Pod\\n    ID: tf_eff...\"],[\"Momentum: 0.9\\n    Batch Size: 2048\\n    Image Size: '380'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n   ...\"],[\"Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n   ...\"],[\"Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n   ...\"],[\"Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n   ...\"],[\"Noisy Student (EfficientNet)\\n\\n**Noisy Student Training** is a semi-supervised learning approach. It ...\"],[\"```\\n\\nTo load and preprocess the image:\\n\\n```py \\n\\u003e\\u003e\\u003e import urllib\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ns`. You can ...\"],[\"(Tensorflow) MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mixnet_l`. You can find the I...\"],[\"SSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ssl_resnet18`. You can find the ...\"],[\"Adversarial Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the I...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `adv_inception_v3`. You can find ...\"],[\"SWSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"(Tensorflow) EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scali...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"ESE-VoVNet\\n\\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https:\\u002f\\u002fpaper...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ese_vovnet19b_dw`. You can find ...\"],[\"(Gluon) Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_xception65`. You can find ...\"]],\"hovertemplate\":\"source=pytorch-image-models\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"pytorch-image-models, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"pytorch-image-models, circle\",\"showlegend\":true,\"x\":[-15.09673,-0.9835363,-21.741343,7.232281,-18.27105,-18.897938,-14.860945,-21.741806,-18.402363,-18.82737,-18.762926,-18.805313,-18.501602,-14.470616,-0.9314863,-21.744097,-17.420708,-17.896523,-14.916279,-13.613869,-17.418867,-18.168707,-18.812511,-18.868616,-18.73068,-18.857063,-15.1429205,-13.613667,-17.419146,-14.888309,-21.74022,-18.136578,-14.730409,-13.61358,-17.419443,-18.373568,-18.741522,-18.841913,-18.487738,-14.461034,-0.9845831,-21.738659,-18.40174,-18.93693,-18.775774,-18.831747,-18.844612,-18.698444,-18.843588,-18.737259,-18.901121,-0.75881743,-0.7342775,-0.67497253,-0.68667436,-0.69481254,-14.923585,-13.61259,-17.418798,-18.368668,-18.690462,-14.9543,-13.614045,-17.420166,-17.92225,-14.115827,-15.02349,-0.9037292,-21.73877,-12.422581,-12.582801,7.6619735,-12.262951,-12.273226,-12.28309,-12.458992,-12.405317,-12.368397,-12.23565,-12.334209,-12.481172,-12.047585,-12.494182,-12.414212,-12.392102,-12.364909,-12.462576,-12.31423,-12.392103,-12.479748,-12.574329,-12.466151,-12.583749,-12.559311,-12.43491,-13.308328,-15.041304,-21.743181,-17.419853,-18.277514,-18.468529,-18.69382,-15.013475,-21.742893,-17.418657,-18.367416,-18.770256,-18.849073,-18.474558,-12.91213,-13.787756,-13.593499,-13.682176,-13.45777,-13.66605,-13.508573,-13.613765,-13.720438,-13.653385,-12.629701,-14.921565,-14.838969,-21.741982,-17.419626,-18.474861,-18.559029,-18.688517,-18.797998,-18.674805,2.5977721,1.1001543,-14.983564,-21.74277,-18.406061,-18.79357,-18.802565,-18.67111,-18.726072,-14.868907,-13.613377,-17.420525,-18.390087,-18.756414,-18.824425,-18.48205,-13.224205,-21.74136,-18.037052,-18.546267,-18.484913,-14.450128,-13.612067,-17.419134,-17.885153,-14.201013,-1.0463543,-21.742496,-11.333223,-18.410645,-18.73471,-18.621363,-18.769436,-18.483355,-3.5490797,-12.642727,-12.760845,-12.819477,-12.618628,-14.864428,-13.612941,-17.42089,-18.326988,-18.6533,-14.987118,-13.61204,-17.41891,-18.304268,-18.830946,-14.51959,-21.745052,-17.799784,-14.910979,-13.613632,-17.418377,-14.843741,-21.740091,-14.733673,-13.613617,-17.418133,-17.979338,-14.974988,-21.74204,-17.76498,-18.615337,-14.859943,-21.742311,-18.19056,-18.512766,-18.515615,-14.589873,-13.61364,-17.420712,-15.032993,-21.740746,-18.05462,-11.910541,-12.466488,-5.1908298,-12.247279,-12.338654,-12.249552,-12.313961,-12.233035,-12.058823,-12.292264,-12.381669,-12.316346,-9.68652,-12.012406,-12.367077,-12.254791,8.296048,8.324153,-12.175825,8.328129,8.363224,8.337162,8.319079,8.332793,-12.312039,8.287007,8.309313,8.359416,-12.258044,-12.1399145,-11.440422,-12.457213,-12.234295,-12.235414,-12.308118,-12.296822,4.569988,-14.937214,-21.741972,-18.284866,-18.860332,-18.912212,-18.779007,-18.811756,-18.903183,-18.508507,-14.824821,-21.742119,-18.0638,-14.459553,-0.95713586,-21.741285,-18.282637,-18.877924,-18.667295,-18.839397,-18.685053,-18.834545,-18.503233,-2.9789827,-14.893837,-21.741205,-17.418861,-18.337788,-18.633018,-14.922336,-13.613424,-17.418978,-18.399717,-18.723465,-18.653772,-18.735748,-18.872883,-18.730675,-14.960788,-21.740934,-14.930049,-13.613657,-17.418049,-14.46163,-13.613861,-17.419176,-18.028818,-13.252792,-21.74304,-15.111883,-21.741001,-18.366419,-18.6802,-18.789118,-18.635418,-12.788958,-12.612987,-12.649175,-12.418598,8.278652,8.323438,8.287085,8.306028,8.284436,-13.0081,-14.5021515,-13.61207,-17.837936,-14.826775,-21.740614,-18.088238,-14.683827,-13.612841,-17.420675,-17.859917,-14.919663,-21.742998,-14.997112,-21.742481,-18.448227,-18.816166,-18.871433,-18.417185,-14.59406,-21.737307,-18.39225,-18.870087,-18.767836,-18.878775,-18.683315,-18.843555,-18.794397,-18.787106,-18.45507,1.1757706,0.7895224,-14.867573,-21.742857,-14.995711,-21.74163,-14.97594,-21.741003,-18.366316,-18.839846,-18.896866,-18.501034,-15.14258,-13.61258,-17.418242,-16.464437,0.69341916,0.8981494,0.20455147,-14.684819,-13.6139765,-17.418755,-18.465975,-18.823069,-18.640965,-18.737932,-18.594912,-18.736101,-18.686348,-18.830034,2.0650344,-14.849969,-21.740911,-17.88402,-14.511817,-21.744026,0.2805606,-1.0000561,-12.895882,-13.700578,-13.641451,-13.817529,-13.416952,-13.871133,-13.731346,-13.603998,-11.408522,-15.082564,-14.373222,-21.739851,-0.6765572,-0.69376314,-0.7142466,-0.92754316,-0.7635134,-14.906146,-21.7455,-18.496576,-18.68729,-18.680824,-18.69265,-18.689974,-18.69183,-18.726753,-18.75107,-18.689634,-18.69388,-14.5298395,-21.744226,-17.420174,-17.81785,-14.991967,-21.74148,-18.155504,-18.654308,-18.687977,-18.697384,-18.697128,-13.257701,-21.74246,-18.255611,-18.641193,-18.683392,-18.617079,-14.864801,-13.613725,-17.41867,-13.245929,-21.739697,-18.189714,-18.47852,-18.585583,-18.634302,-18.574997,-18.61117,-11.146405,8.591112,-1.5027972,-1.7699345,-15.092445,-21.74387,-18.23008,-18.813072,-18.71786,-14.846145,-21.740776,-14.794898,-13.614319,-17.41752,-15.146484,-21.744183,-18.149048,-18.67032,-18.490952,-14.290998,-21.739887,-13.350904,-21.741735,-18.345179,-18.670637,-18.749374,-18.86708,-18.636433,-18.446062,-18.789623,-18.818813,-18.639832,-18.560411,-18.83733,-15.085863,-21.74115,-18.237785,-18.818624,-18.831264,-18.71894,-14.584225,-21.745108,-17.876463,-14.517393,-21.742535,-17.768162,-14.886214,-21.740404,-18.038853,-0.54210675,-21.72907,0.25445038,-1.0509776,-14.930608,-0.9841655,-0.93836474,-2.821011,-15.086927,-21.742565,-18.292135,-18.81373,-18.745352,-14.822716,-21.741106,-18.454144,-18.714092,-14.853318,-13.614159,-17.42098,-14.474255,-13.613888,-17.418804,-18.361588,-18.760048,-18.676931,-18.762749,-18.474726,-14.899432,-13.613247,-18.149565,-18.543854,-14.458993,-13.612747,-17.418514,-15.124843,-21.744043,-14.496637,-21.744608,-14.983218,-21.742655,-14.423987,-13.613521,-17.419062,-15.077092,-21.742958,-15.101266,-21.7438,-15.076402,-13.612978,-13.371202,-13.613139,-17.418924,-14.967785,-21.742266,-15.069756,-13.613182,-17.42014,-14.941669,-21.74129,-14.8548,-21.745224,-1.324137,-1.7560834,-12.619507,-12.689639,-12.720947,-12.69444,-14.728725,-21.743065,-14.909047,-21.742653,-14.773365,-21.73973,-17.889248,-14.47648,-13.611109,-14.982776,-13.611373,-17.868467,-14.867893,-21.738434,-18.455776,-18.677748,-18.748997,-18.723797,-18.644737,-18.664371,-14.988583,-13.61344,-17.420418,-14.645482,-21.74088,-15.0155525,-21.742382,-18.29053,-18.640265,-18.778841,-18.826616,-18.682066,-18.521666,-18.658188,-18.840324,-14.934974,-21.739527,-14.906864,-21.742298,-14.815138,-21.739132,-17.998283,-18.636375,-14.88816,-13.6135645,-14.964461,-21.741499,-14.889757,-13.613634,-14.930691,-21.74171,-14.970319,-21.7422,-17.418423,-15.00022,-21.741394,-17.421926,-18.275118,-18.69703,-18.689646,-11.739094,-12.162996,-11.74823,-12.216933,-12.243923,-12.023696,-12.350718,-12.328729,-11.94893,-12.31463,-12.152681,8.324005,-4.912901,8.324299,8.308033,-12.246294,-12.208494,-12.126658,1.9537387,-12.082711,8.286391,8.372309,8.336756,8.376207,8.373142,8.356943,8.373793,8.347882,-12.268538,8.30535,-12.296482,-12.047101,-10.419014,-12.0073395,-11.309379,-11.135528,-12.941732,-10.54193,0.68346643,-5.007918,-3.7654524,-3.3496454,-7.2159505,8.265307,-10.987167,-12.068632,-12.327329,7.375992,-14.991289,-21.743837,-15.096898,-21.742886,-17.55689,-18.663761,-18.32918,-14.321762,-21.740463,-15.020751,-21.743288,-18.371817,-18.702473,-18.449287,-14.37868,-21.74502,-14.980091,-21.744576,-14.924384,-13.613567,-18.489155,-18.825335,-15.166326,-13.612685,-17.417397,-14.927932,-21.743895,-13.359564,-21.741673,-2.428695,-14.736783,-21.741121,-14.875052,-13.611906,-13.38592,-13.612898,-17.420061,-14.496671,-13.613089,-8.855299,-21.741709,-18.289726,-18.49519,-18.690287,-18.671652,-18.841015,-18.842958,-18.839073,-18.837095,-18.839424,-6.798273,-15.091673,-21.74176,-14.906873,-21.742413,-14.905243,-21.741335,-14.462618,-21.744164,-14.900411,-13.612968,-13.335807,-13.612174,-14.868375,-21.740664,-15.137093,-21.74294],\"xaxis\":\"x\",\"y\":[-2.1728911,3.2404704,8.547104,2.9856446,-4.2000875,-4.374819,-2.1015,8.547349,-4.2696185,-4.353298,-4.273654,-4.3435173,-3.7978578,-2.1720214,3.3138142,8.549384,14.799302,-4.0323315,-2.0742664,19.436842,14.800111,-4.156048,-4.3362875,-4.3670483,-4.2495556,-4.3767567,-2.1724672,19.436844,14.799291,-2.0793433,8.548835,-4.15092,-2.006033,19.436548,14.801071,-4.2076287,-4.321308,-4.3608685,-3.5723188,-1.9322891,3.133478,8.548545,-4.183468,-4.4615097,-4.28645,-4.324836,-4.348728,-3.7340307,-4.3522835,-4.280814,-4.3483133,2.1833398,2.1546974,2.224743,2.2492337,2.2679307,-2.1117344,19.438711,14.799883,-4.210856,-4.1787534,-2.1288013,19.436106,14.800386,-4.0936785,-1.829263,-2.2240522,3.3443894,8.548075,-1.5955858,-1.6119207,2.6443076,-1.6263344,-1.5391167,-1.5251004,-1.6352097,-1.5827956,-1.5514766,-1.4343343,-1.6015953,-1.4794759,-1.6277469,-1.5866177,-1.5589962,-1.5786936,-1.566278,-1.637554,-1.534188,-1.6018822,-1.58247,-1.5898262,-1.5613565,-1.6285157,-1.6296017,-1.5540586,-1.0557457,-2.2151365,8.549714,14.799295,-4.195821,-4.200753,-4.288931,-2.1419005,8.550357,14.800099,-4.231897,-4.323965,-4.3494234,-3.7193615,-1.6474577,-1.8369843,-1.8074265,-1.8396499,-1.7271959,-1.8068424,-1.8141112,-1.7836688,-1.8131025,-1.8017324,-1.4791789,-2.3012664,-1.901685,8.548975,14.799096,-4.2369556,-4.268672,-4.2875686,-4.367717,-4.2307806,0.33789057,0.00036682808,-2.1708977,8.548014,-4.2540827,-4.2869925,-4.3202734,-3.8708582,-4.29129,-1.9872395,19.437727,14.798562,-4.2222757,-4.310111,-4.344591,-3.7956786,-0.94334275,8.548896,-4.0736465,-4.20206,-3.5723815,-2.2452319,19.439043,14.799655,-4.0289283,-1.6687059,3.1571546,8.548346,-1.0966668,-4.2265916,-4.265457,-4.163023,-4.152121,-3.4975402,-1.6656666,-1.6203591,-1.6375619,-1.6714877,-1.5827608,-1.9661342,19.437082,14.79992,-4.202221,-4.1511307,-2.1184683,19.43628,14.800622,-4.224122,-4.3207426,-2.234247,8.549774,-4.0096917,-2.0853765,19.438057,14.799296,-1.9159418,8.547847,-1.9804498,19.437325,14.800131,-4.0844502,-2.1499238,8.548793,-3.9407616,-4.181144,-2.0376291,8.550089,-4.145166,-4.190086,-4.1165333,-1.9371942,19.437777,14.800475,-2.142627,8.548773,-4.110752,-1.4031123,-1.5947762,-1.3326963,-1.5453527,-1.6220591,-1.5596076,-1.5965781,-1.4807211,-1.3929193,-1.5639087,-1.6280898,-1.5903953,-0.57389385,-1.5005288,-1.5821238,-1.5515056,5.3220143,5.3911853,-1.5464606,5.410125,5.2532125,5.20892,5.1540594,5.08481,-1.5905362,5.3340893,5.2730737,5.3943233,-1.6205355,-1.5172416,-1.4777156,-1.5853338,-1.5325545,-1.5682583,-1.5806459,-1.5769851,0.32206112,-2.065903,8.547756,-4.2537293,-4.348778,-4.4368143,-4.282239,-4.3341436,-4.3561587,-3.679953,-2.018414,8.549135,-4.1144943,-1.9132198,3.1312735,8.5489435,-4.173845,-4.3792686,-3.6807382,-4.293392,-3.6801898,-4.3017654,-3.5792546,-9.297024,-1.973074,8.549597,14.798788,-4.2125034,-4.2550297,-2.1149793,19.438208,14.799408,-4.2154307,-4.2317734,-4.202182,-3.7116375,-4.396972,-3.6813836,-2.1048036,8.548114,-2.1326106,19.437952,14.800424,-1.8895015,19.43717,14.798948,-4.101066,-1.0191934,8.5498295,-2.129271,8.548007,-4.2220507,-4.262845,-4.300155,-3.530013,-1.5979084,-1.4953154,-1.5823417,-1.529889,4.924812,4.4349103,4.442536,4.624117,4.405762,-1.736029,-2.2461426,19.44037,-4.0038376,-2.0413706,8.548791,-4.126365,-1.9978882,19.437712,14.799704,-3.993568,-2.1120236,8.54745,-2.130828,8.549261,-4.235377,-4.3358016,-4.3558826,-3.5857904,-1.9684315,8.54821,-4.2183433,-4.372741,-4.3303123,-4.369131,-4.259082,-4.3556905,-4.4121685,-4.330791,-3.736553,0.6533947,0.8145037,-1.8838546,8.550027,-2.1796787,8.547577,-2.1226907,8.548972,-4.2331333,-4.346927,-4.3587446,-3.7953563,-2.2882721,19.436949,14.800942,-3.117099,3.0593977,2.133212,3.1725793,-2.0130205,19.43625,14.800338,-4.2030935,-4.3443108,-4.155707,-4.2415695,-4.099364,-4.225438,-3.8429873,-4.329643,3.538925,-2.0431712,8.547842,-4.0509405,-2.2506201,8.548591,0.6013996,0.94995,-1.6388258,-1.7846758,-1.7970331,-1.8762897,-1.6550214,-1.8230904,-1.7973952,-1.821078,-0.9529387,-2.3512852,-1.8835609,8.548305,2.2397814,2.1845763,2.1754637,2.421106,2.209449,-2.1034088,8.54845,-4.1983585,-4.1815534,-3.6337988,-4.180246,-3.7017047,-4.17884,-3.6668894,-4.184638,-3.7202528,-4.148049,-2.2938917,8.549868,14.798408,-3.9840546,-2.1190877,8.547379,-4.174298,-4.2502565,-3.7007203,-4.231961,-4.182166,-0.9758142,8.54875,-4.1821237,-4.097374,-3.6484182,-3.992251,-2.0577056,19.436146,14.799031,-1.0172172,8.549121,-4.1683116,-4.182777,-3.9890842,-4.1503534,-4.0911756,-4.0732045,-1.2609673,2.724229,-2.599129,-3.6986864,-2.161434,8.547086,-4.2684817,-4.3363366,-4.193006,-2.056473,8.54733,-2.0094059,19.438513,14.801731,-2.1874404,8.547132,-4.176834,-4.1730294,-3.7968566,-1.8788636,8.548424,-1.0793552,8.55003,-4.206989,-4.2774587,-4.3246098,-4.3824368,-4.2023473,-4.190364,-4.331258,-4.348423,-3.7179928,-4.21139,-4.3620005,-2.1482778,8.548064,-4.193463,-4.3372855,-4.3296666,-4.1798496,-2.214262,8.548083,-4.032079,-2.2509773,8.547713,-3.9474206,-2.0566247,8.547285,-4.1282277,0.65701663,8.545175,-6.167286,2.3547819,-2.170955,2.9057996,3.3864067,-0.6985342,-2.156996,8.547432,-4.2061157,-4.336769,-4.249636,-2.0771306,8.54748,-4.234593,-4.299633,-2.0265274,19.438496,14.798148,-1.9259926,19.437183,14.800096,-4.2065425,-4.305915,-3.6990504,-4.24591,-3.5016818,-2.0551882,19.43912,-4.167104,-4.2017756,-2.1660907,19.439854,14.799689,-2.1804528,8.547109,-2.2425177,8.550294,-2.1311486,8.547331,-1.8858106,19.436792,14.800417,-2.1524053,8.546597,-2.164042,8.546944,-2.1485214,19.437841,-1.085184,19.436098,14.801108,-2.1391587,8.54744,-2.1403236,19.437862,14.802203,-2.0993512,8.54892,-2.1138453,8.548983,-2.2924447,-3.5865092,-1.5883794,-1.6475247,-1.6237347,-1.6379579,-2.021878,8.546805,-2.1171446,8.546965,-1.9954393,8.548651,-4.0931168,-2.265987,19.439983,-2.0844245,19.440695,-4.0117,-2.1067383,8.54884,-4.189647,-4.2591357,-4.2590303,-4.121586,-4.3029404,-4.1941295,-2.1219003,19.437183,14.799671,-1.9496304,8.548241,-2.1425302,8.549653,-4.2453494,-4.3052974,-4.3317766,-4.35869,-4.2279797,-4.230122,-4.280459,-4.3625283,-1.9575561,8.548703,-2.0761693,8.548023,-2.0172224,8.549075,-4.072078,-4.062214,-2.0603318,19.436602,-2.136137,8.54976,-2.0578482,19.436018,-2.0911152,8.54785,-2.1140623,8.549983,14.800009,-2.129506,8.548132,14.796928,-4.191772,-4.187597,-3.7022953,-1.3157457,-1.579587,-1.4668977,-1.6319817,-1.5420197,-1.5157351,-1.6273493,-1.5672154,-1.5614386,-1.5749425,-1.5619456,5.3993506,0.030428626,5.372459,4.984824,-1.5387019,-1.5325594,-1.5011741,1.8390944,-1.4370563,5.2602835,5.301414,5.2901006,5.3387294,5.310096,5.337269,5.355973,5.311761,-1.5434234,5.2789574,-1.6054784,-1.4909278,-0.22967272,-1.1221274,-0.8632588,-0.8165471,-1.5628397,-0.15552153,1.7727911,-0.17926177,-1.252506,-1.6794068,-0.13608682,2.6920445,-1.0959625,-1.4162184,-1.4213374,3.013926,-2.1247783,8.547062,-2.2514865,8.549731,-3.7758944,-4.1249337,-3.8715167,-1.8794838,8.547237,-2.132968,8.55029,-4.2128897,-4.1320767,-3.73158,-2.1801002,8.548776,-2.1182597,8.547652,-2.1184428,19.436909,-4.269155,-4.382013,-2.3064435,19.439068,14.800375,-2.1406763,8.549385,-1.010399,8.550536,-11.902233,-2.0132575,8.547466,-1.9685426,19.437302,-1.0832019,19.437826,14.798895,-2.235527,19.438356,-0.08472233,8.550167,-4.207009,-4.274622,-4.288085,-4.2906,-4.364874,-4.3652225,-4.368139,-4.3627563,-4.3609266,0.44274306,-2.2291794,8.550195,-2.069676,8.549997,-2.1152132,8.547871,-2.2876298,8.549747,-2.1066263,19.43757,-1.0198457,19.437326,-2.047783,8.548102,-2.2689977,8.548879],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"--\\ntitle: \\\"Large Language Models: A New Moore's Law?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f33_large_language_mode...\"],[\"### Deep Learning, Deep Pockets?\\n\\nAs you would expect, training a 530-billion parameter model on hum...\"],[\"I'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? ...\"],[\"Downsizing efforts are also under way in the Natural Language Processing community, using transfer l...\"],[\"In other words: save time, save money, save hardware resources, save the world! \\n\\nIf you need a tuto...\"],[\"However, the Machine Learning community is still struggling with this topic, and for good reason. Op...\"],[\"--\\ntitle: \\\"Why weâ€™re switching to Hugging Face Inference Endpoints, and maybe you should too\\\"\\nthumbn...\"],[\"Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served...\"],[\"The following table shows latency (ms Â± standard deviation and time to complete test in seconds) for...\"],[\"```\\nWhat we see from these results is pretty encouraging. The application that will consume these en...\"],[\"```\\n\\nWe can say a couple of things about this. Firstly, we want a managed solution to deployment, we...\"],[\"```bash\\nhugie endpoint create example\\u002fdevelopment.json...\"],[\"```\\n\\nFor me, whatâ€™s lacking is a [custom terraform provider](https:\\u002f\\u002fwww.hashicorp.com\\u002fblog\\u002fwriting-...\"],[\"--\\ntitle: \\\"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\\\" \\nthumbnail: \\u002fblog\\u002fassets...\"],[\"We are happy to share that we recently added another feature to help you analyze datasets on the Hub...\"],[\"```\\n\\nCreate a connection to DuckDB and install and load the `httpfs` extension to allow reading and ...\"],[\"```\\n\\nTo learn more, check out the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets-server\\u002fparque...\"],[\"--\\ntitle: \\\"Building an AI WebTV\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f156_ai_webtv\\u002fthumbnail.gif\\nauthors:\\n- user:...\"],[\"Additionally, a base theme and idea (written by a human) are passed through a LLM (in this case, Cha...\"],[\"Other spaces deployed by the community can also be found if you [search for Zeroscope on the Hub](ht...\"],[\"```\\n\\n\\n## Post-processing\\n\\nOnce an individual take (a video clip) is upscaled, it is then passed to F...\"],[\"let playlist = 'ffconcat version 1.0\\\\n'\\nallFilePaths.forEach(filePath =\\u003e {\\n  playlist += `file '${fi...\"],[\"```\\n\\nThis will generate the following playlist content:\\n\\n```bash\\nffconcat version 1.0\\nfile 'video1.m...\"],[\"```\\n\\nThere are many different configuration options for FFmpeg, for more information in the [officia...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo5....\"],[\"We've seen it with large language models and their ability to synthesize convincing content that mim...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo18...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo2....\"],[\"**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here...\"],[\"**Text or objects inserted into the image:** the model sometimes injects words from the prompt into ...\"],[\"ğŸ’¡ This will also improve the quality of the image since the prompt is used for the upscaling part wi...\"],[\"--\\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"Over the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or...\"],[\"The ability to search through large collections of images using text queries is an immensely powerfu...\"],[\"In addition, we used the [UCM Dataset](https:\\u002f\\u002fmega.nz\\u002ffolder\\u002fwCpSzSoS#RXzIlrv--TDt3ENZdKN8JA) and t...\"],[\"#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size o...\"],[\"The `baseline` model represents the pre-trained `openai\\u002fclip-vit-base-path32` CLIP model. This model...\"],[\"| Model-name                               | k=1   | k=3   | k=5   | k=10  |\\n| ---------------------...\"],[\"| bs128x8-lr5e-5-wd02\\u002fckpt-4               | 0.820 | 0.946 | 0.965 | 0.990 |\\n| [bs128x8-lr5e-6-adam\\u002f...\"],[\"_1 - our best model, 2 - our second best model_\\n\\n\\n#### Demo\\n\\nYou can access the [CLIP-RSICD Demo](ht...\"],[\"--\\ntitle: \\\"Multivariate Probabilistic Time Series Forecasting with Informer\\\" \\nthumbnail: \\u002fblog\\u002fasset...\"],[\"##  Multivariate Probabilistic Time Series Forecasting\\n\\nAs far as the modeling aspect of probabilist...\"],[\"1. **Quadratic computation of canonical self-attention:** The vanilla Transformer has a computationa...\"],[\"### ProbSparse Attention\\n\\nThe main idea of ProbSparse is that the canonical self-attention scores fo...\"],[\"$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nWhere \\\\\\\\(Q\\\\in \\\\math...\"],[\"This is good! But how can we select the \\\\\\\\(u\\\\\\\\) \\\"active\\\" queries to create \\\\\\\\(Q_{reduce}\\\\\\\\)? Let's d...\"],[\"But how can we calculate the term \\\\\\\\(q_ik_j^T\\\\\\\\) in non-quadratic time? Recall that most of the dot-...\"],[\"# calculate u to find the Top-u queries under the sparsity measurement\\n    u = min(sampling_factor *...\"],[\"```\\nNote that in the implementation, \\\\\\\\(U_{part}\\\\\\\\) contain \\\\\\\\(L_Q\\\\\\\\) in the calculation, for stabil...\"],[\"Let's see this in code:\\n    \\n```python\\nfrom torch import nn\\n\\n# ConvLayer is a class with forward pas...\"],[\"```\\n    \\nBy reducing the input of each layer by two, we get a memory usage of \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\...\"],[\"```\\n\\n## Load Dataset\\n\\nIn this blog post, we'll use the `traffic_hourly` dataset, which is available ...\"],[\"```\\n\\nEach example contains a few keys, of which `start` and `target` are the most important ones. Le...\"],[\"```\\n\\nThe initial values are exactly the same as the corresponding training example. However, this ex...\"],[\"```\\n\\nWe now use `datasets`' [`set_transform`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.7.0\\u002fen\\u002fpackage...\"],[\"```\\n\\n## Define the Model\\n\\nNext, let's instantiate a model. The model will be trained from scratch, h...\"],[\"```\\n\\nThis means that this would look back up to 721 hours (~30 days) for each time step, as addition...\"],[\"```\\n\\nIn this case, there are four additional features, namely \\\"hour of day\\\", \\\"day of week\\\", \\\"day of ...\"],[\"```\\n\\nNote that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more inf...\"],[\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for ...\"],[\"```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level...\"],[\"return Chain(\\n        # step 1: remove static\\u002fdynamic fields if not specified\\n        [RemoveFields(...\"],[\"log_scale=True,\\n            ),\\n            # step 6: vertically stack all the temporal features into...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"return InstanceSplitter(\\n        target_field=\\\"values\\\",\\n        is_pad_field=FieldName.IS_PAD,\\n     ...\"],[\"```\\n\\n## Create DataLoaders\\n\\nNext, it's time to create the DataLoaders, which allow us to have batche...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n  ...\"],[\"# We create a test Instance splitter to sample the very last\\n    # context window from the dataset p...\"],[\"```\\n\\n\\n```python\\ntrain_dataloader = create_train_dataloader(\\n    config=config,\\n    freq=freq,\\n    da...\"],[\"```\\n\\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the ...\"],[\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts t...\"],[\"model, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_datalo...\"],[\"```\\n\\n```python\\n# view training\\nloss_history = np.array(loss_history).reshape(-1)\\nx = range(loss_hist...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i...\"],[\"```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values pr...\"],[\"```\\n\\n\\n```python\\nplt.scatter(mase_metrics, smape_metrics, alpha=0.2)\\nplt.xlabel(\\\"MASE\\\")\\nplt.ylabel(\\\"s...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"So the vanilla Transformer still performs best here! In the future, we hope to better benchmark thes...\"],[\"--\\ntitle: \\\"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\\"...\"],[\"For example, the [IMDB dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fimdb) specifies `en` in the YAML met...\"],[\"However, there is a major caveat to this. Most datasets (around 87%) do not specify the language use...\"],[\"### Predicting the Languages of Datasets Using Machine Learning\\n\\nWeâ€™ve already seen that many of the...\"],[\"```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole datase...\"],[\"We pass 20 examples to the model representing rows from a dataset. This results in 20 individual lan...\"],[\"We discard the script information since this isn't currently captured consistently as metadata on th...\"],[\"#### Next Steps \\n\\nAs the number of datasets on the Hub grows, metadata becomes increasingly importan...\"],[\"--\\ntitle: \\\"Generating Human-level Text with Contrastive Search in Transformers ğŸ¤—\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"**[Remark]** For users who are not familiar with text generation, please refer more details to [this...\"],[\"```\\n\\n****\\n\\n\\u003cspan id='problems_of_decoding_methods'\\u002f\\u003e\\n\\n### 4. Problems of Existing Decoding Methods:\\n...\"],[\"```\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---------------------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** From the result generated by greedy search, we can see obvious pattern ...\"],[\"```\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---------------------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** While nucleus sampling can generate text free of repetitions, the seman...\"],[\"\\u003ccenter class=\\\"half\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f115_introducing_contrastive_search\\u002fformulation.png\\\" width...\"],[\"\\u003cspan id='contrastive_generation'\\u002f\\u003e\\n\\n#### 5.2. Generating Text with Contrastive Search:\\n\\nBelow, we u...\"],[\"```\\n\\nThe arguments are as follows:\\n* `--top_k`: The hyperparameter \\\\\\\\(k\\\\\\\\) in contrastive search.\\n* ...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"\\\"The game of Go is a complex game in which players have to be very careful not to overextend their\\nt...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** We see that the generated text is of exceptionally high quality. The en...\"],[\"\\u003cspan id='more_examples'\\u002f\\u003e\\n\\n### 6. More Generated Examples:\\n\\nIn this section, we provide more genera...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cspan id='gpt2_greedy_example_one'\\u002f\\u003e\\n\\n##### 6.1.1. Generating Text with Greedy Searc...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\nare...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cspan id='gpt2_nucleus_example_one'\\u002f\\u003e\\n\\n##### 6.1.2. Generating Text with Nucleus Sam...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='gpt2_contrastive_example_one'\\u002f\\u003e\\n\\n##### 6.1.3. Generating Text with Contra...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"While the discovery is exciting, it's not the first time scientists have discovered an animal that s...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='opt_example_two'\\u002f\\u003e\\n\\n#### 6.2. Example Two - OPT:\\n\\nIn this part, we use th...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output: [click to expand]\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='opt_greedy_example_two'\\u002f\\u003e\\n\\n##### 6.2.2. Generating Text with Nucleus Samp...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\n\\u003cspan id='opt_contrastive_example_two'\\u002f\\u003e\\n\\n##### 6.2.3. Generating Text with Contra...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n----------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n****\\n\\n\\u003cspan id='resources'\\u002f\\u003e\\n\\n### 7. Resources:\\n\\nFor more details of contrastive sea...\"],[\"```\\n\\n\\n\\n****\\n\\n\\u003cspan id='references'\\u002f\\u003e\\n\\n## Reference:\\n\\u003e [1] Su et al., 2022 [\\\"A Contrastive Framework ...\"],[\"--\\ntitle: \\\"AMD + ğŸ¤—: Large Language Models Out-of-the-Box Acceleration with AMD GPU\\\"\\nthumbnail: \\u002fblog...\"],[\"```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\nmodel_id = \\\"01-...\"],[\"```\\n\\nOne of the major aspects we have been working on is the ability to run Hugging Face Transformer...\"],[\"* Flash Attention v2 from AMD Open Source efforts in [ROCmSoftwarePlatform\\u002fflash-attention](https:\\u002f\\u002f...\"],[\"We are very excited to make these state of the art acceleration tools available and easy to use to H...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg alt=\\\"\\\" src=\\\"assets\\u002foptimum_amd\\u002ftrans...\"],[\"Performance-wise, we spent a lot of time benchmarking Text Generation Inference on AMD Instinct GPUs...\"],[\"Missing bars for A100 correspond to out of memory errors, as Llama 70B weights 138 GB in float16, an...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Lewis Tunstall\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f60_lewis_tunstall_inte...\"],[\"*Note: Transcription has been slightly modified\\u002freformatted to deliver the highest-quality reading e...\"],[\"This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been he...\"],[\"So what I've been working on for the last few months on the transformers library is providing the fu...\"],[\"### That's incredible. I remember when they released those examples for GPT-2, and one of my favorit...\"],[\"**Lewis:** So one that's kind of fun is in the course we had an event last year where we got people ...\"],[\"So this accelerates the whole field in a really powerful way. And I can imagine these applications u...\"],[\"### That is super interesting and powerful.\\n\\n**Lewis:** Maybe one thing to mention is that the whole...\"],[\"And then I think the second big lesson Iâ€™ve learned from building a lot of projects is that you can ...\"],[\"So that would have been maybe a way to shortcut my path to doing this full-time. \\n\\n### I love the id...\"],[\"**Lewis:** Sorry, that's a bit dark.\\n\\n### No, that was great. The next question is a follow-up on yo...\"],[\"It gives you a very interesting insight into the Deep Mind researchers and their backstory as well.\\n...\"],[\"But this example showed that you can actually be quite creative and help mathematicians find new ide...\"],[\"I think the idea or the joke he had was that there's a lot of discussion in the community about this...\"],[\"### So glad you got that one in there. Well done! Look forward to many more in the next edition. Tha...\"],[\"--\\ntitle: 'Welcome fastai to the Hugging Face Hub'\\nthumbnail: \\u002fblog\\u002fassets\\u002f64_fastai\\u002ffastai_hf_blog....\"],[\"Because of all this, and more (the writer of this post started his journey thanks to the fast.ai cou...\"],[\"![Fastai Models in the Hub](assets\\u002f64_fastai\\u002fhf_hub_fastai.png)\\n\\nIn addition to free model hosting a...\"],[\"```\\n\\n## Creating a fastai `Learner`\\n\\nHere we train the [first model in the fastbook](https:\\u002f\\u002fgithub....\"],[\"```\\n\\n3. Use the `token` argument of the `push_to_hub_fastai` function.\\n\\nYou can input `push_to_hub_f...\"],[\"```\\n\\nThe `Learner` is now in the Hub in the repo named [`espejelomar\\u002fidentify-my-cat`](https:\\u002f\\u002fhuggi...\"],[\"![Fastai Model Card](assets\\u002f64_fastai\\u002fcat.jpeg)\\n\\nNow let's load the `Learner` we just shared in the ...\"],[\"```\\nIt works ğŸ‘‡!\\n\\n```py\\n_,_,probs = learner.predict(img)\\nprint(f\\\"Probability it's a cat: {100*probs[1...\"],[\"```\\n\\n```python\\nimport torch\\nimport transformers\\nfrom fastai.text.all import *\\n\\nfrom blurr.text.data....\"],[\"```\\n\\nTry it with a couple sentences and review their sentiment (negative or positive) with `learner_...\"],[\"```\\nAgain, it works!\\n\\n```python\\nProbability that sentence 'This integration is amazing!' is negative...\"],[\"--\\ntitle: \\\"StackLLaMA: A hands-on guide to train LLaMA with RLHF\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f138_stack...\"],[\"By combining these approaches, we are releasing the StackLLaMA model. This model is available on the...\"],[\"## Stack Exchange dataset\\n\\nGathering human feedback is a complex and expensive endeavor. In order to...\"],[\"## Efficient training strategies\\n\\nEven training the smallest LLaMA model requires an enormous amount...\"],[\"In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters (depending on the...\"],[\"```\\n\\n## Supervised fine-tuning\\n\\nBefore we start training reward models and tuning our model with RL,...\"],[\"```python\\n# load model in 8bit\\nmodel = AutoModelForCausalLM.from_pretrained(\\n        args.model_path...\"],[\"```\\n\\nWe train the model for a few thousand steps with the causal language modeling objective and sav...\"],[\"This can be translated into the following loss function:\\n\\n\\n\\\\\\\\( \\\\operatorname{loss}(\\\\theta)=- E_{\\\\lef...\"],[\"```\\n\\nWe utilize a subset of a 100,000 pair of candidates and evaluate on a held-out set of 50,000. W...\"],[\"```\\n\\nThe same template was used for SFT, RM and RLHF stages.\\n\\nA common issue with training the langu...\"],[\"# Run PPO step\\n    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\\n\\t# Log stat...\"],[\"```\\n\\nWe train for 20 hours on 3x8 A100-80GB GPUs, using the ğŸ¤— research cluster, but you can also get...\"],[\"In general in RL, you want to achieve the highest reward. In RLHF we use a Reward Model, which is im...\"],[\"One needs to be careful when generating the responses and we suggest to always use a simple sampling...\"],[\"## Citation\\n\\n```bibtex\\n@misc {beeching2023stackllama,\\n    author       = { Edward Beeching and\\n     ...\"],[\"```\\n\\n## Acknowledgements\\n\\nWe thank Philipp Schmid for sharing his wonderful [demo](https:\\u002f\\u002fhuggingfa...\"],[\"--\\ntitle:  Deploy LLMs with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f155_inference_e...\"],[\"Before we start, let's refresh our knowledge about Inference Endpoints. \\n\\n## What is Hugging Face In...\"],[\"## 1. How to deploy Falcon 40B instruct\\n\\nTo get started, you need to be logged in with a User or Org...\"],[\"You can then deploy your model with a click on â€œCreate Endpointâ€. After 10 minutes, the Endpoint sho...\"],[\"```\\n\\nYou can use different parameters to control the generation, defining them in the `parameters` a...\"],[\"## 3. Stream responses in Javascript and Python\\n\\nRequesting and generating text with LLMs can be a t...\"],[\"```\\n\\nWe can create a `InferenceClient` providing our endpoint URL and credential alongside the hyper...\"],[\"```\\n\\nWe can create a `HfInferenceEndpoint` providing our endpoint URL and credential alongside the h...\"],[\"```\\n\\nReplace the `process.stdout` call with the `yield` or with a function you want to stream the to...\"],[\"--\\ntitle: \\\"2D Asset Generation: AI for Game Development #4\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games...\"],[\"Requirements:\\n- Your preferred image-editing software, such as [Photoshop](https:\\u002f\\u002fwww.adobe.com\\u002fpro...\"],[\"### Example: Corn\\n\\nIn this section, I'll walk through how I generated a corn icon for the farming ga...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"[Dreambooth](https:\\u002f\\u002fdreambooth.github.io\\u002f), [textual inversion](https:\\u002f\\u002ftextual-inversion.github.io...\"],[\"--\\ntitle: \\\"Supercharged Customer Service with Machine Learning\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_superchar...\"],[\"To filter out unsatisfied messages in an automated way, we plan on applying natural language process...\"],[\"Let's take a look at all available Datasets on the [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"Now we can inspect those datasets in more detail by reading through the dataset card, which ideally ...\"],[\"Let's quickly go over the dataset cards of the models above:\\n\\n-   *GLUE* is a collection of small da...\"],[\"In addition, the Hugging Face Hub offers:\\n\\n-   [A dataset viewer for every dataset](https:\\u002f\\u002fhuggingf...\"],[\"Alright, the next step now is to find a suitable pretrained model to be used for fine-tuning. This i...\"],[\"At the time of writing this blog post, the best performing models are very large models containing m...\"],[\"```\\n\\nAlso, we install the ğŸ¤— Transformers and ğŸ¤— Datasets libraries to run this notebook. Since we wil...\"],[\"```\\n\\n**Output:**\\n```\\n    Downloading and preparing dataset amazon_reviews_multi\\u002fen (download: 82.11 ...\"],[\"```\\n\\n\\n\\nWe have 200,000 training examples as well as 5000 validation and test examples. This sounds r...\"],[\"```\\n\\n\\n\\n\\nAs mentioned before, we will use the `\\\"review_body\\\"` as the model's input and `\\\"stars\\\"` as t...\"],[\"```\\n\\n\\nTo apply this function to all data samples in our dataset, we use the [`map`](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\n**Output:**\\n```\\n    Input IDS: [1, 329, 714, 2044, 3567, 5127, 265, 312, 1158, 260, 273, 286, 4...\"],[\"```\\n\\n```\\n    Some weights of the model checkpoint at microsoft\\u002fdeberta-v3-base were not used when in...\"],[\"```\\n\\n\\nDuring training, it is important to monitor the performance of the model on a held-out validat...\"],[\"```\\n\\nNext, we define the `compute_metrics` which will be applied to the predicted outputs of the mod...\"],[\"```\\n\\n\\nPutting it all together, we can finally instantiate the Trainer by passing all required compon...\"],[\"**Output:**\\n\\u003cdiv\\u003e\\n\\u003ctable\\u003e\\u003cp\\u003e\\n  \\u003ctbody\\u003e\\n \\u003ctr style=\\\"text-align: left;\\\"\\u003e\\n  \\u003ctd\\u003eStep\\u003c\\u002ftd\\u003e\\n  \\u003ctd\\u003eTrainin...\"],[\"\\u003ctd\\u003e0.820500\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e0.904160\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e0.615800\\u003c\\u002ftd\\u003e\\n  \\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e45000\\u003c\\u002ftd\\u003e\\n    ...\"],[\"**Output:**...\"],[\"```\\n    ***** Running Evaluation *****\\n      Num examples = 5000\\n      Batch size = 8\\n    Saving mod...\"],[\"```\\n\\n### Evaluate \\u002f Analyse the model\\n\\nNow that we have fine-tuned the model we need to be very care...\"],[\"```\\n\\n**Output:**\\n```\\n    ***** Running Prediction *****\\n      Num examples = 5000\\n      Batch size =...\"],[\"```\\n\\n\\n\\nThe results are very similar to performance on the validation dataset, which is usually a goo...\"],[\"# Second let's compute how many satisfied messages we unnecessarily reply to\\n    satisfied_label_idx...\"],[\"```\\n\\n\\nWe again instantiate the `Trainer` to easily run the evaluation.\\n\\n\\n```python\\ntrainer = Trainer...\"],[\"```\\n\\n\\nand again upload everything on the Hub.\\n\\n\\n```python\\ntrainer.push_to_hub()\\n```\\n\\n**Output:**\\n```...\"],[\"```\\n\\n\\n\\nThe data is now saved [here](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten\\u002fdeberta_amazon_reviews_v...\"],[\"Moreover, if you are searching for **support for your custom use cases**, Hugging Face's team of exp...\"],[\"--\\ntitle: \\\"Accelerating Document AI\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f112_document-ai\\u002fthumbnail.png\\nauthors:...\"],[\"Turning typed, handwritten, or printed text into machine-encoded text is known as Optical Character ...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"![png](assets\\u002f112_document-ai\\u002fdoc_class.png)\\n\\nThat's where models like [LayoutLM](https:\\u002f\\u002fhuggingfac...\"],[\"Document layout analysis is the task of determining the physical structure of a document, i.e., iden...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"LayoutLMv1 now has many successors. [Donut](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdonut...\"],[\"Documents often contain tables, and most OCR tools don't work incredibly well out-of-the-box on tabu...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"DocVQA is typically evaluated using the Average Normalized Levenshtein Similarity (ANLS) metric. For...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"Data preparation for Document AI is critical and challenging. It's crucial to have properly annotate...\"],[\"The flexibility of building your models leads to many options for data scientists. Our strong recomm...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\n### Next Steps\\n\\nAre you seeing the possibilities of Document AI? ...\"],[\"| model | paper | license | checkpoints |\\n| --- | --- | --- | --- |\\n| [Donut](https:\\u002f\\u002fhuggingface.co...\"],[\"| [LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002flayoutlmv3) | [arxiv](http...\"],[\"| [LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002flilt) | [arxiv](https:\\u002f\\u002farxiv.or...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n \\u003c\\u002fhtml\\u003e...\"],[\"--\\ntitle: \\\"How we sped up transformer inference 100x for ğŸ¤— API customers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f09...\"],[\"-| Naive version                                                                                    ...\"],[\"## Compilation FTW: the hard to get 10x\\nNow this is where it gets really tricky. In order to get the...\"],[\"If you want to feel the speed on our infrastructure, start a [free trial](https:\\u002f\\u002fhuggingface.co\\u002fpri...\"],[\"--\\ntitle: \\\"Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chine...\"],[\"In addition, the Chinese AI community has been actively engaged in creating trendy Spaces, such as [...\"],[\"## Beyond Boundaries: Embracing a Diverse AI Community\\n\\nAs we embark on this new chapter, our collab...\"],[\"--\\ntitle: \\\"How to generate text: using different decoding methods for language generation with Trans...\"],[\"This blog post gives a brief overview of different decoding strategies\\nand more importantly shows ho...\"],[\"```\\n\\n``` python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\n\\ntorch_dev...\"],[\"```\\n\\n\\n## Greedy Search\\n\\nGreedy search is the simplest decoding method.\\nIt selects the word with the ...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nAlright\\\\! We have generated our first short text with GPT2 ğŸ˜Š. The\\ngenerated words following t...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002fbeam_search.png\\\" alt=\\\"beam search\\\" style=\\\"margin: auto; di...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\nAs can be seen, the five beam hypotheses are only marginally different\\nto each other - which s...\"],[\"$$ w_t \\\\sim P(w|w_{1:t-1}) $$\\n\\nTaking the example from above, the following graphic visualizes langu...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nInteresting\\\\! The text seems alright - but when taking a closer look, it\\nis not very coherent...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nOK. There are less weird n-grams and the output is a bit more coherent\\nnow\\\\! While applying t...\"],[\"# set top_k to 50\\nsample_output = model.generate(\\n    **model_inputs,\\n    max_new_tokens=40,\\n    do_...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nNot bad at all\\\\! The text is arguably the most *human-sounding* text so\\nfar. One concern thou...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002ftop_p_sampling.png\\\" alt=\\\"Top p sampling\\\" style=\\\"margin: au...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\nCool, now you should have all the tools to let your model write your\\nstories with `transformer...\"],[\"## Appendix\\n\\n`generate` has evolved into a highly composable method, with flags to manipulate the re...\"],[\"--\\ntitle: \\\"Accelerate your models with ğŸ¤— Optimum Intel and OpenVINO\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f113_ope...\"],[\"â€‹Let us show you how to get started in minutes!â€‹\\n\\n## Quantizing a Vision Transformer with Optimum In...\"],[\"```\\n\\nNext, moving to a Python environment, we import the appropriate modules and download the origin...\"],[\"```\\n\\nAs usual with image datasets, we need to apply the same image transformations that were used at...\"],[\"```\\n\\nWe're now ready to quantize the model. The `OVQuantizer.quantize()` method quantizes the model ...\"],[\"```\\n\\nâ€‹To verify that quantization did not have a negative impact on accuracy, we applied an evaluati...\"],[\"```\\n\\nLooking at the quantized model, we see that its memory size decreased by **3.8x** from 344MB to...\"],[\"--\\ntitle: Guiding Text Generation with Constrained Beam Search in ğŸ¤— Transformers\\nthumbnail: \\u002fblog\\u002fas...\"],[\"$$ S_{expected} = \\\\{ s_1, s_2, ..., s_k, t_1, t_2, s_{k+1}, ..., s_n \\\\} $$\\n\\nThe problem is that beam...\"],[\"The above examples are actually very reasonable use-cases, as it will be shown below, and the new co...\"],[\"```\\n!pip install -q git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n```\\n\\n\\n```python\\nfrom transfo...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"force_word = \\\"scared\\\"\\nforce_flexible = [\\\"scream\\\", \\\"screams\\\", \\\"screaming\\\", \\\"screamed\\\"]\\n\\nforce_words_i...\"],[\"```\\n\\n    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\\n\\n\\n    Output:\\n    -...\"],[\"Instead of only choosing `\\\"The dog\\\"` like what a greedy search would do, a beam search would allow *...\"],[\"![Constrained Beam Search Step 1](https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fblog\\u002fmain\\u002fassets\\u002f53_...\"],[\"Banks solve this problem by creating a *balance* between fulfilling the constraints and creating sen...\"],[\"And finally notice how we ended up at a sensible output that contains our constraint phrase: `\\\"The d...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"```\\n\\nor:\\n```python\\nstarting_text = \\\"The woman\\\"\\ntemplate = [\\\"the\\\", \\\"\\\", \\\"\\\", \\\"University\\\", \\\"\\\", \\\"in\\\"]\\n\\np...\"],[\"*Thumbnail of this post uses an icon with the attribution: \\u003ca href=\\\"https:\\u002f\\u002fwww.flaticon.com\\u002ffree-ic...\"],[\"--\\ntitle: \\\"Making a web app generator with open ML models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f153_text_to_webap...\"],[\"Some of those techniques are now available as ready-to-use NPM libraries:\\n\\n- Using AI\\u002fML libraries s...\"],[\"## Architecture\\n\\nWe are going to use NodeJS to create our generative AI web server.\\n\\nThe model will ...\"],[\"```\\n\\nThen, we can install the Hugging Face Inference client:\\n\\n```html\\nnpm install @huggingface\\u002finfer...\"],[\"```\\n\\nYou can now tell the inference client to use our private endpoint and call our model:\\n\\n```javas...\"],[\"```\\n\\nStart your web server:\\n\\n```bash\\nnpm run start\\n```\\n\\nand open `https:\\u002f\\u002flocalhost:3000?prompt=some...\"],[\"```\\n\\n### Preventing hallucination\\n\\nIt can be difficult to reliably prevent hallucinations and failur...\"],[\"```\\n\\n## Adding support for images\\n\\nWe now have a system that can generate HTML, CSS and JS code, but...\"],[\"```\\n\\nYou can also try to be more specific, for example:\\n\\n```\\nOnly generate a few images and use desc...\"],[\"```\\n\\nTo make this work, you will have to make some changes:\\n\\n```javascript\\n...\\n\\n\\u002f\\u002f going to localhos...\"],[\"```\\n\\n## Going further\\n\\nThe final demo Space includes a [more complete example](https:\\u002f\\u002fhuggingface.c...\"],[\"--\\ntitle: 'Liftoff! How to get started with your first ML project ğŸš€'\\nthumbnail: \\u002fblog\\u002fassets\\u002f84_firs...\"],[\"\\u003e Compute dense vector representations for sentences, paragraphs, and images\\n\\nIn a nutshell, Sentenc...\"],[\"Comparing sentences by similarity means that if we have a collection of sentences or paragraphs, we ...\"],[\"Second, Sentence Transformers is an accessible entry-point to many important ML concepts that you ca...\"],[\"Third, embeddings are key for several industrial applications. Google searches use embeddings to [ma...\"],[\"1. **Do a brain dump of everything you know the toolâ€™s capable of**: For Sentence Transformers this ...\"],[\"4. **Ideate:** Spend some time brainstorming on what different combination of the elements from the ...\"],[\"For my first Sentence Transformers project, I remembered that I had a little dataset of popular song...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"---\\n\\nOnce youâ€™ve gone through your first project, youâ€™ll find that youâ€™ll have even more ideas for t...\"],[\"--\\ntitle: \\\"Fit More and Train Faster With ZeRO via DeepSpeed and FairScale\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"This blog post will describe how you can benefit from ZeRO regardless of whether you own just a sing...\"],[\"```\\nexport BS=16\\npython -m torch.distributed.launch --nproc_per_node=2 .\\u002ffinetune_trainer.py \\\\\\n--mod...\"],[\"```\\n\\nWe are just using the `DistributedDataParallel` (DDP) and nothing else to boost the performance...\"],[\"Let's look at the results of these six test runs:\\n\\n| Method                    | max BS |  train tim...\"],[\"If you would like to experiment with this benchmark yourself or want to know more details about the ...\"],[\"```\\nexport BS=1\\nCUDA_VISIBLE_DEVICES=0 .\\u002ffinetune_trainer.py \\\\\\n--model_name_or_path t5-3b --n_train ...\"],[\"```\\net voila! We get a batch size of 20 trained just fine. I could probably push it even further. Th...\"],[\"```\\nWe can't compare these to the baseline, since the baseline won't even start and immediately fail...\"],[\"As of this writing FairScale and DeepSpeed only perform Partitioning (Sharding) for the optimizer st...\"],[\"```\\nRuntimeError: CUDA out of memory. Tried to allocate 1.48 GiB (GPU 0; 23.65 GiB total capacity;\\n1...\"],[\"```\\nThe program wants to allocate ~1.5GB and the GPU still has some 6-7GBs of unused memory, but it ...\"],[\"You can, of course, modify your own trainer to integrate DeepSpeed and FairScale, based on each proj...\"],[\"* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https:\\u002f\\u002farxiv.org\\u002fab...\"],[\"from the FairScale team and:\\n\\n* Jeff Rasley [@jeffra](https:\\u002f\\u002fgithub.com\\u002fjeffra)\\n* Olatunji Ruwase [...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #1\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f103_ethics-soc-1\\u002fthumbnail.png...\"],[\"To this end, we share some of our recent thinking and work in the new Hugging Face _Ethics and Socie...\"],[\"- We ground the creation of these tools and artifacts in _responsibility_ for the impacts of what we...\"],[\"Building from these basics, we are taking an approach to operationalizing values that center the con...\"],[\"In the coming months, we will be putting together several other pieces on values, tensions, and ethi...\"],[\"--\\ntitle: \\\"Open LLM Leaderboard: DROP deep dive\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fevaluating-mmlu-leaderboard...\"],[\"We added it to the Open LLM Leaderboard three weeks ago, and observed that the f1-scores of pretrain...\"],[\"Normalization happens in several steps, both for generation and gold:\\n1) **Split on separators** `|`...\"],[\"## Diving into the results\\nExtending our investigations, our friends at [Zeno](https:\\u002f\\u002fzenoml.com) j...\"],[\"We hypothesized that both these problems could be fixed by using `\\\\n` instead of `.` as an end of ge...\"],[\"In 10% of the cases, the gold answer is a floating number (for example `12.25`) and model prediction...\"],[\"--\\ntitle: \\\"Evaluating Language Model Bias with ğŸ¤— Evaluate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f112_evaluating-ll...\"],[\"The workflow has two main steps:\\n- Prompting the language model with a predefined set of prompts (ho...\"],[\"```\\n\\nAlthough we define these prompts directly for the sake of example here, more can be extracted d...\"],[\"```\\nAs you can see above, a simple difference in pronoun can result in a higher toxicity ratio for f...\"],[\"```\\n\\nAnd as before, we use GPT-2 to generate completions:\\n```python\\n\\u003e\\u003e\\u003e profession1_completions = [\\\"...\"],[\"```\\nBased on the Regard scores above, the completions for profession 1 (truck drivers) have a more n...\"],[\"```\\n\\nHigher HONEST scores mean more hurtful completions. Based on the model completions above, we ha...\"],[\"*- Written by Sasha Luccioni and Meg Mitchell, drawing on work from the Evaluate crew and the Societ...\"],[\"--\\ntitle: \\\"Can foundation models label data like humans?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fllm-leaderboard\\u002fle...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fllm-leaderboa...\"],[\"## Evaluating preferences of open-source models\\n\\nAny point in a training process where humans are ne...\"],[\"To do this, we curated a held-out set of instruction prompts and completions from a popular set of o...\"],[\"### Human Elo results\\n\\nWe partnered with Scale AI to collect high-quality human annotations for a ha...\"],[\"Given the Likert scale, it is also debatable whether a score of 4 or 5 should constitute a win, so w...\"],[\"For the rest of this post, you will see similar analyses with different data generation criteria.\\n\\n#...\"],[\"```\\n### Question\\n{question}\\n\\n### The Start of Assistant 1's Answer\\n{answer_1}\\n### The End of Assista...\"],[\"```\\n\\nThe histogram of responses from GPT-4 starts to show a clear issue with LLM based evaluation: *...\"],[\"## Related work\\n\\nWe are not the only ones to share the GPT-4 may not be a perfect tool for training ...\"],[\"---\\n\\n**Question:**\\nIndicate the genre of the book to which it belongs.\\\\n Input: Love in the Time of ...\"],[\"**Human response:**\\n\\nIâ€™m excited beyond words to share with you my decision to accept the role of Ma...\"],[\"**Human response:**\\n\\nrelevant\\n\\n**GPT-4 rating: 5** (model slightly better)\\n\\nBoth assistants provided...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"```\\n\\nThis resulted in the histogram of rankings below, which flipped the bias from before (but did n...\"],[\"## Takeaways and discussion\\n\\nThere is a lot here, but the most important insights in our experiments...\"],[\"Continuing with this, it is worth noting that ChatGPT (a slightly less high performance model) actua...\"],[\"- **Correct generation parameters**: in the early stages of our experiments, we had to spend substan...\"],[\"### Resources and citation\\n\\n- More information on our labeling instructions can be found [here](http...\"],[\"```\\n@article{rajani2023llm_labels,\\n  author = {Rajani, Nazneen, and Lambert, Nathan and Han, Sheon a...\"],[\"--\\ntitle: \\\"Student Ambassador Programâ€™s call for applications is open!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f67_a...\"],[\"ğŸ§‘ğŸ»â€ğŸ’» Workshops and support from the Hugging Face team! \\n\\nğŸ¤— Insight into the latest projects, feature...\"],[\"--\\ntitle: \\\"Training a language model with ğŸ¤—Â Transformers using TensorFlow and TPUs\\\"\\nthumbnail: \\u002fblog...\"],[\"Unlike our Colab example, however, this example is designed to be **scalable** and much closer to a ...\"],[\"## What to expect\\n\\nWeâ€™re going to train a [RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_d...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"We tokenize the samples individually. We then take a batch of samples, concatenate them together, an...\"],[\"```\\n\\nBut since weâ€™re in the TPU territory, we need to perform this initialization under a strategy s...\"],[\"```\\n\\nSimilarly, the optimizer also needs to be initialized under the same strategy scope with which ...\"],[\"```\\n\\nIf `args.dataset` contains the `gs:\\u002f\\u002f` identifier, TensorFlow will understand that it needs to ...\"],[\"[{'score': 0.1003185287117958,\\n  'token': 52,\\n  'token_str': 'be',\\n  'sequence': 'Goal of my life is...\"],[\"```\\n\\n## Conclusion\\n\\nIf thereâ€™s one thing we want to emphasize with this example, itâ€™s that TPU train...\"],[\"--\\ntitle: \\\"A Dive into Vision-Language Models\\\"\\nthumbnail: \\u002fblog\\u002f\\u002fassets\\u002f128_vision_language_pretrain...\"],[\"## Introduction\\n\\nWhat does it mean to call a model a â€œvision-languageâ€ model? A model that combines ...\"],[\"A vision-language model typically consists of 3 key elements: an image encoder, a text encoder, and ...\"],[\"### 1) Contrastive Learning\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggin...\"],[\"For CLIP, the distance is simply the cosine distance between the text and image embeddings, whereas ...\"],[\"Letâ€™s break this down and see how this works. Language models with a prefix objective predict the ne...\"],[\"#### Frozen PrefixLM\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fd...\"],[\"A nifty advantage of the Frozen PrefixLM pre-training objective is it enables training with limited ...\"],[\"Models such as VisualGPT use a visual encoder to embed images and feed the visual embeddings to the ...\"],[\"Letâ€™s break down what MLM and ITM objectives mean. Given a partially masked caption, the MLM objecti...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"### Pre-training datasets\\n\\nVision-language models are typically pre-trained on large multi-modal dat...\"],[\"Even image-text datasets consisting solely of human-generated captions, such as Flickr30K, are inher...\"],[\"Models fine-tuned on the question-answering downstream task, such as [ViLT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f21...\"],[\"Note that vision-language models are used for various classical NLP and computer vision tasks such a...\"],[\"* [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)\\n* [FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"* [`VisionTextDualEncoder`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002fvision-text-d...\"],[\"While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide j...\"],[\"### ViLT for VQA\\n\\nLetâ€™s start with ViLT and download a model pre-trained on the VQA dataset. We can ...\"],[\"```\\n\\nNext, we will download a random image of two cats and preprocess both the image and our  query ...\"],[\"```\\n\\nStraight-forward, right? Letâ€™s do another demonstration with CLIPSeg and see how we can perform...\"],[\"```\\n\\nSimilar to ViLT, itâ€™s important to refer to the [original work](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2112.1000...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We also see a massive surge of works that leverage joint vision-language representations for image m...\"],[\"While robotics research hasnâ€™t leveraged vision-language models on a wide scale yet, we see works su...\"],[\"We are continuing to integrate the most impactful computer vision and multi-modal models and would l...\"],[\"--\\ntitle: \\\"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Dis...\"],[\"- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tw...\"],[\"- [Trainer Setup](#trainer-setup)\\n            - [RoBERTa](#roberta)\\n            - [Mistral-7B](#mist...\"],[\"\\u003c!-- \\u002fTOC --\\u003e\\n\\n\\n\\n## Introduction \\n\\nIn the fast-moving world of Natural Language Processing (NLP), we...\"],[\"## Dependencies\\n\\n```bash\\ndatasets\\nevaluate\\npeft\\nscikit-learn\\ntorch\\ntransformers\\nwandb...\"],[\"```\\nNote: For reproducing the reported results, please check the pinned versions in the [wandb repor...\"],[\"### [Mistral 7B](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2310.06825)\\n\\nMistral 7B v0.1, with 7.3 billion parameters, is...\"],[\"```\\n\\n## Data preparation\\n### Data loading\\n\\nWe will load the dataset from Hugging Face:\\n```python\\nfro...\"],[\"```\\n\\n- Train dataset\\n\\n```\\u003cclass 'pandas.core.frame.DataFrame'\\u003e\\nRangeIndex: 7613 entries, 0 to 7612\\nD...\"],[\"```\\n\\nThe final weights are: \\n```\\nPOS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)\\n`...\"],[\"```\\n**Note:** The RoBERTa tokenizer has been trained to treat spaces as part of the token. As a resu...\"],[\"```\\n\\n**Note:** we deleted the undesired columns from our data: id, keyword, location and text. We ha...\"],[\"```\\n\\n\\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models: \\n\\n**No...\"],[\"```\\n\\n- Llama 2:\\n```python\\n# Load Llama 2 Tokenizer\\nfrom transformers import AutoTokenizer, DataColla...\"],[\"```\\n\\n\\n####  LoRA setup for RoBERTa classifier\\n\\nWe import LoRa configuration and set some parameters ...\"],[\"```\\nFor Mistral 7B, we have to add the padding token id as it is not defined by default.\\n\\n```python\\n...\"],[\"```\\n\\n#### LoRa setup for Llama 2 classifier\\n\\nWe define LoRa for Llama 2 with the same parameters as ...\"],[\"```\\ntrainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\\n```\\n\\n...\"],[\"```\\n\\n### Custom Trainer for Weighted Loss \\nAs mentioned at the beginning of this post, we have an im...\"],[\"```\\nIt will print the following: \\n```\\ndevice(type='cuda', index=0)\\n```\\n\\nThen, we set the training ar...\"],[\"```\\n\\n#### Mistral-7B\\n\\nSimilar to RoBERTa, we initialize the `WeightedCELossTrainer` as follows: \\n\\n``...\"],[\"```\\n\\n**Note** that we needed to enable half-precision training by setting `fp16` to `True`. The main...\"],[\"```\\n\\n\\n\\n\\n## Hyperparameter Tuning\\n\\nWe have used Wandb Sweep API to run hyperparameter tunning with Ba...\"],[\"For more information, you can check the Wandb experiment report in the [resources sections](#resourc...\"],[\"## Resources \\n\\n1. You can find the code script in the following [Github project](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"--\\ntitle: \\\"Introducing DOI: the Digital Object Identifier to Datasets and Models\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg alt=\\\"Cite DOI\\\" src=\\\"assets\\u002f107_launching_doi\\u002fcite-modal.jpeg\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nIf ever thereâ€™s ...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 1\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"In addition, Xeon CPUs are generally more affordable and widely available compared to specialized ha...\"],[\"## Building a Cluster of Sapphire Rapids CPUs\\n\\nAt the time of writing, the simplest way to get your ...\"],[\"Let's get to work and build the master node of the cluster.\\n\\n## Setting Up the Master Node\\n\\nWe first...\"],[\"```\\namx_bf16 amx_tile amx_int8\\n```\\n\\nThen, we install native and Python dependencies.\\n\\n```\\nsudo apt-g...\"],[\"```\\n\\nNext, we create a new ssh key pair called 'cluster' with `ssh-keygen` and store it at the defau...\"],[\"```\\nsource ~\\u002fcluster_env\\u002fbin\\u002factivate\\ncd ~\\u002ftransformers\\u002fexamples\\u002fpytorch\\u002fquestion-answering\\npip3 ins...\"],[\"```\\n\\nNo need to let the job run to completion, We just run for a minute to make sure that all depend...\"],[\"```\\n\\nNow, we launch the distributed training job.\\n\\n```\\n# Launch distributed training\\nmpirun -f ~\\u002fhos...\"],[\"```\\n\\nOne epoch now takes **7 minutes and 30 seconds**. \\n\\nHere's what the job looks like. The master ...\"],[\"--\\ntitle: Introducing our new pricing\\nthumbnail: \\u002fblog\\u002fassets\\u002f114_pricing-update\\u002fthumbnail.png\\nautho...\"],[\"--\\ntitle: Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac\\nthumbnail: \\u002fblog\\u002fassets\\u002f149_...\"],[\"## New Core ML Optimizations\\n\\nCore ML is a mature framework that allows machine learning models to r...\"],[\"\\u003cimg style=\\\"border:none;\\\" alt=\\\"Illustration of 2-bit palettization. Image credit: Apple WWDCâ€™23 Sess...\"],[\"## Using Quantized and Optimized Stable Diffusion Models\\n\\n[Last December](https:\\u002f\\u002fhuggingface.co\\u002fblo...\"],[\"In order to make it easy for everyone to take advantage of these improvements, we have converted the...\"],[\"\\u003cbr\\u003e\\n\\u003cdiv style=\\\"background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-radius...\"],[\"```Python\\nfrom huggingface_hub import snapshot_download\\nfrom pathlib import Path\\n\\nrepo_id = \\\"apple\\u002fc...\"],[\"```\\n\\n## Converting and Optimizing Custom Models\\n\\nIf you want to use a personalized Stable Diffusion ...\"],[\"```bash\\npython -m python_coreml_stable_diffusion.torch2coreml \\\\\\n    --model-version prompthero\\u002fopenj...\"],[\"```\\n\\n\\u003cbr\\u003e\\n\\u003cdiv style=\\\"background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-r...\"],[\"```\\n\\n4. Test the converted models on the desired hardware. As a rule of thumb, the `ORIGINAL` versio...\"],[\"We have plans to evaluate this method soon, and canâ€™t wait to see how 4-bit optimized models work an...\"],[\"his notebook shows how to deploy a vision model from ğŸ¤— Transformers (written in TensorFlow) to [Vert...\"],[\"```\\n\\n\\n```python\\nimport transformers\\n\\nprint(tf.__version__)\\nprint(transformers.__version__)\\n```\\n\\n## S...\"],[\"```\\n\\n\\n```python\\ndef normalize_img(img, mean=processor.image_mean, std=processor.image_std):\\n    # Sc...\"],[\"predictions = m_call(**images)\\n        indices = tf.argmax(predictions.logits, axis=1)\\n        pred_...\"],[\"```\\n\\n\\n```python\\n# To deploy the model on Vertex AI we must have the model in a storage bucket.\\ntf.sa...\"],[\"```\\n\\n\\n```python\\n# Upload the model to Vertex AI. \\ntf28_gpu_model_dict = {\\n    \\\"display_name\\\": \\\"ViT B...\"],[\"```\\n\\n## Make a prediction request\\n\\n\\n```python\\n# Generate sample data. \\nimport base64\\n\\nimage_path = t...\"],[\"```\\n\\n## Cleaning up of resources\\n\\n\\n```python\\ndef cleanup(endpoint, model_name, deployed_model_id):\\n ...\"],[\"--\\ntitle: \\\"Retrieval Augmented Generation with Huggingface Transformers and Ray\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"[RAG](https:\\u002f\\u002fai.facebook.com\\u002fblog\\u002fretrieval-augmented-generation-streamlining-the-creation-of-intel...\"],[\"Instead, a framework-agnostic and a more flexible implementation for ad-hoc concurrent programming i...\"],[\"And as you can see below, using the [Ray](https:\\u002f\\u002fdocs.ray.io\\u002fen\\u002fmaster\\u002f) based implementation leads...\"],[\"### How do I use it?\\n\\n[Huggingface](https:\\u002f\\u002fhuggingface.co\\u002f) provides a [PyTorch Lightning](https:\\u002f\\u002f...\"],[\"```\\n\\n\\nThen, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](h...\"],[\"```\\n\\n## Whatâ€™s next?\\n\\nUsing RAG with [Huggingface transformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftrans...\"],[\"If you plan to try RAG+Ray integration out, please feel free to share your experiences on the [Ray D...\"],[\"--\\ntitle: Introducing Pull Requests and Discussions ğŸ¥³\\nthumbnail: \\u002fblog\\u002fassets\\u002f76_community_update\\u002fth...\"],[\"## Pull requests\\n\\n![Pull requests on the Hugging Face Hub](assets\\u002f76_community_update\\u002fnew-pr.png)\\n\\n[...\"],[\"--\\ntitle: \\\"Introducing Agents.js: Give tools to your LLMs using JavaScript\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"```\\n\\nThen the code can be evaluated as such:\\n\\n```ts\\nconst messages = await agent.evaluateCode(code);...\"],[\"```\\n\\n### Usage warning\\n\\nCurrently using this library will mean evaluating arbitrary code in the brow...\"],[\"```\\n\\n## Custom Tools ğŸ› ï¸\\n\\nAgents.js was designed to be easily expanded with custom tools & examples. ...\"],[\"```\\n\\n## Passing input files to the agent ğŸ–¼ï¸\\n\\nThe agent can also take input files to pass along to th...\"],[\"--\\ntitle: \\\"Using Machine Learning to Aid Survivors and Race through Time\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fu...\"],[\"![organization](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisast...\"],[\"![NER](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisaster-assets...\"],[\"In the end, we decided to fine-tune our own model as it would take roughly three minutes to fine-tun...\"],[\"![active_learning](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdis...\"],[\"To address these issues and create open source tools that can be leveraged in the future, we started...\"],[\"![output_satellite](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdi...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"Cost is also an important factor to consider. GPUs can be expensive, and using a CPU may be a more c...\"],[\"```\\nsudo apt-get update\\n\\n# Add libtcmalloc for extra performance\\nsudo apt install libgoogle-perftool...\"],[\"```\\nsentence_short = \\\"This is a really nice pair of shoes, I am completely satisfied with my purchas...\"],[\"```\\n\\nOn the c6i (Ice Lake) instance, we only use a vanilla Transformers pipeline. \\n\\n```\\nfrom transfo...\"],[\"```\\n\\nFor the sake of brevity, we'll just look at the p99 results for [distilbert-base-uncased](https...\"],[\"--\\ntitle: Getting Started with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f109_inferenc...\"],[\"Starting from my [model page](https:\\u002f\\u002fhuggingface.co\\u002fjuliensimon\\u002fautotrain-food101-1471154053), I cl...\"],[\"Let's first deploy a protected endpoint, and then we'll deploy a private one.\\n\\n### Deploying a Prote...\"],[\"```\\nimport requests, json\\n\\nAPI_URL = \\\"https:\\u002f\\u002foncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.c...\"],[\"```\\n5c7fbb4485cd8w7 2022-10-10T08:19:04.915Z 2022-10-10 08:19:04,915 | INFO | POST \\u002f | Duration: 142...\"],[\"```\\n\\nNow, let's increase our security level and deploy a private endpoint.\\n \\n### Deploying a Private...\"],[\"```\\ncurl https:\\u002f\\u002foncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.cloud \\\\\\n-X POST --data-binary ...\"],[\"--\\ntitle: \\\"Non-engineers guide: Train a LLaMA 2 chatbot\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_ml_director_insi...\"],[\"## Introduction to Spaces\\n\\nSpaces from Hugging Face is a service that provides easy to use GUI for b...\"],[\"1.3 In order to deploy the AutoTrain app from the Docker Template in your deployed space select Dock...\"],[\"2.2 Choose the LLM you want to train from the â€œModel Choiceâ€ field, you can select a model from the ...\"],[\"2.5 Optional: You can upload â€œValidation Dataâ€ to test your newly trained model against, but this is...\"],[\"3.1 Follow the same process of setting up a new Space as in steps 1.1 \\u003e 1.3, but select the ChatUI d...\"],[\"_If youâ€™re feeling inspired, but still need technical support to get started, feel free to reach out...\"],[\"--\\ntitle: \\\"Ethical Guidelines for developing the Diffusers library\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fethics-...\"],[\"# Safety features and mechanisms\\n\\nIn addition, we provide a non-exhaustive - and hopefully continuou...\"],[\"--\\ntitle: \\\"Introducing BERTopic Integration with the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_b...\"],[\"## What is BERTopic?\\n\\nBERTopic is a state-of-the-art Python library that simplifies the topic modell...\"],[\"## BERTopic Model Management with the Hugging Face Hub\\n\\nWith the latest integration, BERTopic users ...\"],[\"```\\nYou can then load this model in two lines and use it to predict against new data.\\n\\n```python\\nfro...\"],[\"```\\n\\nBy leveraging the power of the Hugging Face Hub, BERTopic users can effortlessly share, version...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eClick here for an overview of all topics.\\u003c\\u002fsummary\\u003e\\n  \\n  | Topic ID | Topic Key...\"],[\"| 12 | ner - named - named entity - entity - named entity recognition | 376 | 12_ner_named_named ent...\"],[\"| 27 | argument - arguments - argumentation - argumentative - mining | 160 | 27_argument_arguments_a...\"],[\"| 43 | discourse - discourse relation - discourse relations - rst - discourse parsing | 117 | 43_dis...\"],[\"| 58 | agreement - syntactic - verb - grammatical - subject verb | 85 | 58_agreement_syntactic_verb_...\"],[\"| 74 | biased - biases - spurious - nlp - debiasing | 57 | 74_biased_biases_spurious_nlp | \\n| 75 | v...\"],[\"| 89 | reviews - summaries - summarization - review - opinion | 36 | 89_reviews_summaries_summarizat...\"],[\"Due to the improved saving procedure, training on large datasets generates small model sizes. In the...\"],[\"BERTopic offers one way of getting a better understanding of the topics in this dataset. In this cas...\"],[\"```\\n\\nWe can predict on a single example text: \\n\\n```python\\nexample = \\\"Stalemate is a drawn position. ...\"],[\"```\\n\\nWe can then compare the distribution of topics across both datasets. We can see here that there...\"],[\"Some examples of BERTopic models already on the hub:\\n- [MaartenGr\\u002fBERTopic_ArXiv](https:\\u002f\\u002fhuggingfac...\"],[\"--\\ntitle: \\\"OpenRAIL: Towards open and responsible AI licensing frameworks\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"However, empirical evidence is also telling us that a rigid approach to open sourcing [and\\u002for](https...\"],[\"Same concerns are rising in commercial and government ML licensing practices. In the words of [Bowe ...\"],[\"- **Open:** these licenses allow royalty free access and flexible downstream use and re-distribution...\"],[\"## **OpenRAIL could be for good machine learning what open software licensing is to code**\\n\\nThree ex...\"],[\"The licenses are BigScience's reaction to 2 partially addressed challenges in the licensing space: (...\"],[\"Let's invest in a healthy open and responsible AI licensing culture, the future of AI innovation and...\"],[\"--\\ntitle: Using LoRA for Efficient Stable Diffusion Fine-Tuning\\nthumbnail: \\u002fblog\\u002fassets\\u002flora\\u002fthumbna...\"],[\"![Latent Diffusion Architecture](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"- Training is much faster, as already discussed.\\n- Compute requirements are lower. We could create a...\"],[\"Diffusers now provides a [LoRA fine-tuning script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmai...\"],[\"```\\n\\nOne thing of notice is that the learning rate is `1e-4`, much larger than the usual learning ra...\"],[\"```Python\\nfrom huggingface_hub import model_info\\n\\n# LoRA weights ~3 MB\\nmodel_path = \\\"sayakpaul\\u002fsd-mo...\"],[\"```\\n\\nThis snippet will print the model he used for fine-tuning, which is `CompVis\\u002fstable-diffusion-v...\"],[\"```\\n\\n## Dreamboothing with LoRA\\n\\nDreambooth allows you to \\\"teach\\\" new concepts to a Stable Diffusion...\"],[\"## Other Methods\\n\\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual in...\"],[\"--\\ntitle: \\\"Graph Classification with Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"## Requirements\\nTo follow this tutorial, you need to have installed `datasets` and `transformers` (v...\"],[\"```\\n\\nThis dataset already has three splits, `train`, `validation`, and `test`, and all these splits ...\"],[\"```\\n\\n### Format\\nOn the Hub, graph datasets are mostly stored as lists of graphs (using the `jsonl` f...\"],[\"A single graph is a dictionary, and here is the expected format for our graph classification dataset...\"],[\"- `edge_attr` contains the available attributes (if present) for each edge of the graph, following t...\"],[\"### Preprocessing\\nGraph transformer frameworks usually apply specific preprocessing to their dataset...\"],[\"```\\n\\nIt is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (b...\"],[\"```\\nLet's look at this in more detail. \\n\\nCalling the `from_pretrained` method on our model downloads...\"],[\"```\\nIn the `Trainer` for graph classification, it is important to pass the specific data collator fo...\"],[\"--\\ntitle: Fine-Tune a Semantic Segmentation Model with a Custom Dataset\\nthumbnail: \\u002fblog\\u002fassets\\u002f56_f...\"],[\"Because semantic segmentation is a type of classification, the network architectures used for image ...\"],[\"```\\n\\n# 1. Create\\u002fchoose a dataset\\n\\nThe first step in any ML project is assembling a good dataset. In...\"],[\"We went ahead and captured a thousand images of sidewalks in Belgium. Collecting and labeling such a...\"],[\"### Label the images\\n\\nNow that the raw data is loaded, go to [segments.ai\\u002fhome](https:\\u002f\\u002fsegments.ai\\u002f...\"],[\"Note that creating the release can take a few seconds. You can check the releases tab on Segments.ai...\"],[\"```\\n\\nIf we inspect the features of the new dataset, we can see the image column and the correspondin...\"],[\"```\\n\\nYou can also rewrite the `convert_segmentation_bitmap` function to use batches and pass `batche...\"],[\"```\\n\\n# 2. Load and prepare the Hugging Face dataset for training\\n\\nNow that we've created a new datas...\"],[\"```\\n\\n## Image processor & data augmentation\\n\\nA SegFormer model expects the input to be of a certain ...\"],[\"```\\n\\n# 3. Fine-tune a SegFormer model\\n\\n## Load the model to fine-tune\\n\\nThe SegFormer authors define ...\"],[\"```\\n\\n## Set up the Trainer\\n\\nTo fine-tune the model on our data, we'll use Hugging Face's [Trainer AP...\"],[\"```\\n\\nNext, we'll define a function that computes the evaluation metric we want to work with. Because...\"],[\"pred_labels = logits_tensor.detach().cpu().numpy()\\n    # currently using _compute instead of compute...\"],[\"```\\n\\nFinally, we can instantiate a `Trainer` object.\\n\\n\\n```python\\nfrom transformers import Trainer\\n\\nt...\"],[\"```\\n\\n# 4. Inference\\n\\nNow comes the exciting part, using our fine-tuned model! In this section, we'll...\"],[\"```python\\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\n\\nproces...\"],[\"```\\n\\nNext, we'll load an image from our test dataset.\\n\\n\\n```python\\nimage = test_ds[0]['pixel_values']...\"],[\"```\\n\\nNow it's time to display the result. We'll display the result next to the ground-truth mask.\\n\\n\\u003c...\"],[\"--\\ntitle: \\\"Efficient Controllable Generation for SDXL with T2I-Adapters\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002ft2i...\"],[\"| **Model Type** | **Model Parameters** | **Storage (fp16)** |\\n| --- | --- | --- |\\n| [ControlNet-SDX...\"],[\"Compared to previous versions of T2I-Adapter (SD-1.4\\u002f1.5), [T2I-Adapter-SDXL](https:\\u002f\\u002fgithub.com\\u002fTen...\"],[\"```\\n\\nThe generation process of the T2I-Adapter-SDXL mainly consists of the following two steps:\\n\\n1. ...\"],[\"# load pipeline\\nmodel_id = \\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\neuler_a = EulerAncestralDiscre...\"],[\"```\\n\\nThen, load an image to detect lineart:\\n\\n```python\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadap...\"],[\"```\\n\\n![Lineart Generated Dragon](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"\\u003cscript type=\\\"module\\\" src=\\\"https:\\u002f\\u002fgradio.s3-us-west-2.amazonaws.com\\u002f3.43.1\\u002fgradio.js\\\"\\u003e\\u003c\\u002fscript\\u003e\\n\\u003cgr...\"],[\"### Depth Guided\\n\\n![Depth guided results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"--\\ntitle: \\\"Introduction to Graph Machine Learning\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"### What are graphs used for?\\n\\nLet's look at a panel of possible tasks we can do on graphs.\\n\\nAt the ...\"],[\"### How do we represent graphs?\\n\\nThe common ways to represent a graph to process and operate it are ...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhugging...\"],[\"### Pre-neural approaches\\n\\n#### Simply using engineered features\\n\\nBefore neural networks, graphs and...\"],[\"**Edge-level** features complement the representation with more detailed information about the conne...\"],[\"## Graph Neural Networks\\n\\nNeural networks can generalise to unseen data. Given the representation co...\"],[\"### Aggregation and message passing\\n\\nThere are many ways to aggregate messages from neighbour nodes,...\"],[\"This can be solved by :\\n\\n- scaling the GNN to have a layer number small enough to not approximate ea...\"],[\"- [*Graph Transformer for Graph-to-Sequence Learning*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.07470) (Cai and La...\"],[\"The most recent approach is [*Pure Transformers are Powerful Graph Learners*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f...\"],[\"# Further resources\\n\\nIf you want to delve deeper, you can look at some of these courses:\\n\\n- Academic...\"],[\"If you need quality benchmarks you can check out:\\n\\n- [OGB, the Open Graph Benchmark](https:\\u002f\\u002fogb.sta...\"],[\"### External images attribution\\nEmojis in the thumbnail come from Openmoji (CC-BY-SA 4.0), the Graph...\"],[\"--\\ntitle: \\\"Transformer-based Encoder-Decoder Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f05_encoder_decoder\\u002fthum...\"],[\"```\\n\\nThe *transformer-based* encoder-decoder model was introduced by Vaswani\\net al. in the famous [A...\"],[\"At the time of writing this notebook, ğŸ¤—Transformers comprises the\\nencoder-decoder models *T5*, *Bart...\"],[\"$$ f: \\\\mathbf{X}_{1:n} \\\\to \\\\mathbf{Y}_{1:m}. $$\\n\\n[Sutskever et al. (2014)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f140...\"],[\"During inference, the encoder RNN encodes an input sequence\\n\\\\\\\\(\\\\mathbf{X}_{1:n}\\\\\\\\) by successively u...\"],[\"then it can model the distribution of any target vector sequence given\\nthe hidden state \\\\\\\\(\\\\mathbf{c...\"],[\"For more detail on the logit vector and the resulting probability\\ndistribution, please see footnote ...\"],[\"Given such a decoding method, during inference, the next input vector\\n\\\\\\\\(\\\\mathbf{y}_i\\\\\\\\) can then be...\"],[\"![](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder_decoder\\u002frnn_...\"],[\"The English sentence \\\\\\\"I want to buy a car\\\\\\\", represented by\\n\\\\\\\\(\\\\mathbf{x}_1 = \\\\text{I}\\\\\\\\), \\\\\\\\(\\\\math...\"],[\"\\\\\\\\(\\\\text{car}\\\\\\\\), \\\\\\\\(\\\\text{EOS}\\\\\\\\) in the same fashion, updating its hidden\\nstate at each step until...\"],[\"To generate the first target vector, the decoder is fed the \\\\\\\\(\\\\text{BOS}\\\\\\\\)\\nvector, illustrated as ...\"],[\"$$ p_{\\\\theta_{\\\\text{enc}}, \\\\theta_{\\\\text{dec}}}(\\\\mathbf{Y}_{1:m} | \\\\mathbf{X}_{1:n}) = \\\\prod_{i=1}^{...\"],[\"Nevertheless, RNN-based encoder-decoder models have two pitfalls. First,\\nRNNs suffer from the vanish...\"],[\"\\\\\\\\({}^4\\\\\\\\) A neural network can define a probability distribution over all\\nwords, *i.e.* \\\\\\\\(p(\\\\mathb...\"],[\"\\\\\\\\({}^6\\\\\\\\) [Sutskever et al. (2014)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1409.3215)\\nreverses the order of the inpu...\"],[\"As a reminder, to solve a *sequence-to-sequence* problem, we need to\\nfind a mapping of an input sequ...\"],[\"$$ p_{\\\\theta_{dec}}(\\\\mathbf{Y}_{1:n} | \\\\mathbf{\\\\overline{X}}_{1:n}).$$\\n\\nBy Bayes\\\\' rule, this distri...\"],[\"The transformer-based decoder hereby maps the sequence of encoded hidden\\nstates \\\\\\\\(\\\\mathbf{\\\\overline...\"],[\"Let\\\\'s visualize the complete process of *auto-regressive* generation of\\n*transformer-based* encoder...\"],[\"To begin with, the encoder processes the complete input sequence\\n\\\\\\\\(\\\\mathbf{X}_{1:7}\\\\\\\\) = \\\\\\\"I want t...\"],[\"Next, the first target vector \\\\\\\\(\\\\mathbf{y}_1\\\\\\\\) = \\\\\\\\(\\\\text{Ich}\\\\\\\\) is sampled\\nfrom the distribution...\"],[\"As can be seen, only in step \\\\\\\\(i=1\\\\\\\\) do we have to encode \\\\\\\"I want to buy\\na car EOS\\\\\\\" to \\\\\\\\(\\\\mathb...\"],[\"```\\n\\n_Output:_\\n\\n```\\n    \\u003cpad\\u003e Ich will ein Auto kaufen...\"],[\"```\\n\\nCalling `.generate()` does many things under-the-hood. First, it passes\\nthe `input_ids` to the ...\"],[\"Great, now that we have gotten a general overview of how\\n*transformer-based* encoder-decoder models ...\"],[\"Taking a closer look at the architecture, the transformer-based encoder\\nis a stack of residual _enco...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"Let\\\\'s take a deeper look at how bi-directional self-attention works.\\nEach input vector \\\\\\\\(\\\\mathbf{x...\"],[\"Note, that the **same** weight matrices are applied to each input vector\\n\\\\\\\\(\\\\mathbf{x}_i, \\\\forall i ...\"],[\"Alright, this sounds quite complicated. Let\\\\'s illustrate the\\nbi-directional self-attention layer fo...\"],[\"On the left, the previously illustrated second encoder block is shown\\nagain and on the right, an in ...\"],[\"\\\\\\\"EOS\\\\\\\" so that the self-attention weights mirror the importance each of\\nthe other input vector repr...\"],[\"To further understand the implications of the bi-directional\\nself-attention layer, let\\\\'s assume the...\"],[\"$$\\\\mathbf{X''}_{1:n} = \\\\mathbf{V}_{1:n} \\\\text{Softmax}(\\\\mathbf{Q}_{1:n}^\\\\intercal \\\\mathbf{K}_{1:n}) ...\"],[\"\\\\\\\\({}^1\\\\\\\\) An in-detail explanation of the role the feed-forward layers play\\nin transformer-based mo...\"],[\"# pass input_ids to encoder\\nencoder_hidden_states = model.base_model.encoder(input_ids, return_dict=...\"],[\"```\\n\\n_Outputs:_\\n```\\n    Length of input embeddings 7. Length of encoder_hidden_states 7\\n    Is encod...\"],[\"```\\n\\nWe compare the length of the input word embeddings, *i.e.*\\n`embeddings(input_ids)` correspondin...\"],[\"On a side-note, _autoencoding_ models, such as BERT, have the exact same\\narchitecture as _transforme...\"],[\"Let\\\\'s first understand how the transformer-based decoder defines a\\nprobability distribution. The tr...\"],[\"respectively. The \\\\\\\"LM head\\\\\\\" is often tied to the transpose of the word\\nembedding matrix, *i.e.*\\n\\\\\\\\...\"],[\"Putting it all together, in order to model the conditional distribution\\nof a target vector sequence ...\"],[\"In contrast to transformer-based encoders, in transformer-based\\ndecoders, the encoded output vector ...\"],[\"![](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder_decoder\\u002fenco...\"],[\"can therefore be computed as the following product:\\n\\n$$ p_{\\\\theta_{dec}}(\\\\text{Ich} | \\\\text{BOS}, \\\\m...\"],[\"As in bi-directional self-attention, in uni-directional self-attention,\\nthe query vectors \\\\\\\\(\\\\mathbf...\"],[\"Note that the index range of the key and value vectors is \\\\\\\\(0:i\\\\\\\\) instead\\nof \\\\\\\\(0: m-1\\\\\\\\) which wo...\"],[\"So why is it important that we use uni-directional self-attention in the\\ndecoder instead of bi-direc...\"],[\"This is obviously disadvantageous as the transformer-based decoder would\\nnever learn to predict the ...\"],[\"Great! Now we can move to the layer that connects the encoder and\\ndecoder - the *cross-attention* me...\"],[\"Note that the index range of the key and value vectors is \\\\\\\\(1:n\\\\\\\\)\\ncorresponding to the number of c...\"],[\"So intuitively, what happens here exactly? Each output vector\\n\\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\) is a weighted su...\"],[\"Cool! Now we can see how this architecture nicely conditions each output\\nvector \\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\...\"],[\"To verify our theoretical understanding, let\\\\'s continue our code\\nexample from the encoder section a...\"],[\"# create token ids for encoder input\\ninput_ids = tokenizer(\\\"I want to buy a car\\\", return_tensors=\\\"pt...\"],[\"# compare values of word embedding of \\\"I\\\" for input_ids and perturbed input_ids\\nprint(\\\"Is encoding f...\"],[\"```\\n\\n_Output:_\\n\\n```\\n    Shape of decoder input vectors torch.Size([1, 5, 512]). Shape of decoder log...\"],[\"```\\n\\nWe compare the output shape of the decoder input word embeddings, *i.e.*\\n`embeddings(decoder_in...\"],[\"On a final side-note, _auto-regressive_ models, such as GPT2, have the\\nsame architecture as _transfo...\"],[\"# create ids of encoded input vectors\\ninput_ids = tokenizer(\\\"I want to buy a car\\\", return_tensors=\\\"p...\"],[\"# concat again\\ndecoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\\n\\n...\"],[\"```\\n\\n_Outputs:_\\n\\n```\\n    Generated so far: Ich will ein\\n```\\n\\nIn this code example, we show exactly w...\"],[\"--\\ntitle: Block Sparse Matrices for Smaller and Faster Language Models\\nthumbnail: \\u002fblog\\u002fassets\\u002f04_py...\"],[\"## Usage\\nThe provided `BlockSparseLinear` module is a drop in replacement for `torch.nn.Linear`, and...\"],[\"```\\n\\nThe extension also provides a `BlockSparseModelPatcher` that allows to modify an existing model...\"],[\"The memory savings are even more significant: for **75% sparsity**, memory consumption is reduced by...\"],[\"--\\ntitle: \\\"Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)\\\"\\nthumbnail: \\u002fb...\"],[\"Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series...\"],[\"|      Dataset      | Autoformer (uni.) MASE | DLinear  MASE |\\n|:-----------------:|:---------------...\"],[\"####  Decomposition of Time Series\\nIn time series analysis, [decomposition](https:\\u002f\\u002fen.wikipedia.org...\"],[\"Autoformer incorporates a decomposition block as an inner operation of the model, as presented in th...\"],[\"def forward(self, x):\\n        \\\"\\\"\\\"Input shape: Batch x Time x EMBED_DIM\\\"\\\"\\\"\\n        # padding on the b...\"],[\"```\\n\\nAs you can see, the implementation is quite simple and can be used in other models, as we will ...\"],[\"In theory, given a time lag \\\\\\\\(\\\\tau\\\\\\\\), _autocorrelation_ for a single discrete variable \\\\\\\\(y\\\\\\\\) is ...\"],[\"Now, we are ready to see the code in PyTorch: \\n\\n```python\\nimport torch \\n\\ndef autocorrelation(query_s...\"],[\"```\\n\\nQuite simple! ğŸ˜ Please be aware that this is only a partial implementation of `autocorrelation(...\"],[\"It can be summarized with the following equations:\\n\\n$$\\n\\\\tau_1, \\\\tau_2, ... \\\\tau_k = \\\\textrm{arg Top-...\"],[\"Now, we are ready to see the final code:\\n\\n```python\\nimport torch\\nimport math\\n\\ndef time_delay_aggrega...\"],[\"# apply softmax on the channel dim\\n    top_k_autocorrelations = torch.softmax(top_k_autocorrelations...\"],[\"```\\n\\nWe did it! The Autoformer model is [now available](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmai...\"],[\"```\\n\\nIn the probabilistic setting one can project the context length arrays to  `prediction-length *...\"],[\"```\\n\\nThe transformers models are all relatively small with:\\n\\n```python\\nencoder_layers=2\\ndecoder_laye...\"],[\"```\\n\\nLet's visualize a time series in the dataset and plot the train\\u002ftest split:\\n\\n```python\\nimport m...\"],[\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for ...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"```\\n\\n## Create PyTorch DataLoaders\\n\\nNext, it's time to create PyTorch DataLoaders, which allow us to...\"],[\"return as_stacked_batches(\\n        training_instances,\\n        batch_size=batch_size,\\n        shuffl...\"],[\"if config.num_static_real_features \\u003e 0:\\n        PREDICTION_INPUT_NAMES.append(\\\"static_real_features\\\"...\"],[\"```\\n\\n## Evaluate on Autoformer\\n\\nWe have already pre-trained an Autoformer model on this dataset, so ...\"],[\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i...\"],[\"```\\n\\nSo the result for the Autoformer model is:\\n\\n```python\\nprint(f\\\"Autoformer univariate MASE: {np.m...\"],[\"```\\n\\nFor example, for time-series in the test set with index `4`:\\n\\n```python\\nplot(4)\\n```\\n\\n![png](htt...\"],[\"```\\n\\nTrain the model:\\n\\n```python\\npredictor = estimator.train(\\n    training_data=train_dataset, \\n    ...\"],[\"```\\n\\nAs before, we plot the predictions from our trained DLinear model via this helper:\\n\\n```python\\nd...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f148_a...\"],[\"As one can observe, the [vanilla Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftim...\"],[\"## Acknowledgements\\nWe express our appreciation to [Lysandre Debut](https:\\u002f\\u002fgithub.com\\u002fLysandreJik) ...\"],[\"--\\ntitle: \\\"Image search with ğŸ¤— datasets\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f54_image_search_datasets\\u002fspaces_ima...\"],[\"First, we'll install `datasets`. Since we're going to be working with images, we'll also install [`p...\"],[\"```\\n\\nTo start, let's take a look at the image feature. We can use the wonderful [rich](https:\\u002f\\u002frich....\"],[\"\\u003cpre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #00ffff; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e                               ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e                               ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e  \\u003cspan style=\\\"color: #808000; ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003eâ”‚\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #808000; t...\"],[\"We can see there a few different ways in which we can pass in our images. We'll come back to this in...\"],[\"There have also been projects to tag the dataset [using machine learning](https:\\u002f\\u002fblogs.bl.uk\\u002fdigita...\"],[\"```\\n\\nLet's see what we get back.\\n\\n```python\\ndataset\\n```\\n\\n```\\nDatasetDict({\\n    train: Dataset({\\n    ...\"],[\"```\\n```python\\n\\u002froot\\u002f.cache\\u002fhuggingface\\u002fdatasets\\u002fdownloads\\u002fextracted\\u002ff324a87ed7bf3a6b83b8a353096fbd95...\"],[\"```\\n\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_search_datasets\\u002fdataset_image.jpg\\\" alt=\\\"An example image from our da...\"],[\"```\\n\\n\\n``` python\\ndataset.push_to_hub('davanstrien\\u002fembellishments-sample', private=True)\\n```\\n\\n\\n\\u003e **No...\"],[\"```\\n## Creating embeddings ğŸ•¸ \\nWe now have a dataset with a bunch of images in it. To begin creating ...\"],[\"```\\n\\nWe now have a new column which contains the embeddings for our images. We could manually search...\"],[\"```\\n\\nWe can index into the first example this retrieves:\\n\\n``` python\\nretrieved_examples['image'][0]\\n...\"],[\"```\\n\\n``` python\\nget_image_from_text(\\\"An illustration of the sun behind a mountain\\\")\\n```\\n\\u003cimg src=\\\"as...\"],[\"```\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_search_datasets\\u002fmusical_instrument.jpg\\\"\\u003e\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_s...\"],[\"This is fairly close to what we are interested in here. Particularly we might be interested in how w...\"],[\"--\\ntitle: \\\"Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model\\\"\\nthumb...\"],[\"The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is t...\"],[\"IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the origina...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fatlas.nomic.ai\\u002fmap\\u002ff2fba2aa-3647-4f49-a0f3-9347daeee499\\u002fee4a...\"],[\"As part of the release process, we internally evaluated the model for potential biases by adversaria...\"],[\"```python\\nimport torch\\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\\n\\ndevice = \\\"...\"],[\"generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, m...\"],[\"--\\ntitle: \\\"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"### NLP\\n\\n[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fgpt2-medium-wikitext-103) (Generative Pre-trained ...\"],[\"[BART](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fbart-base-ipu) is a transformer encoder-encoder (seq2seq) mo...\"],[\"### Speech\\n\\n[HuBERT](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fhubert-base-ipu) (Hidden-Unit BERT) is a self-...\"],[\"Developers can now use Graphcore systems to train 10 different types of state-of-the-art transformer...\"],[\"## Get started with Hugging Faceâ€™s Optimum Graphcore models\\n\\nIf youâ€™re interested in combining the b...\"],[\"--\\ntitle: \\\"Showcase Your Projects in Spaces using Gradio\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f28_gradio-spaces\\u002ft...\"],[\"```\\n\\nYou can play with the Story Generation model [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fmerve\\u002fGPT-2-s...\"],[\"```\\n\\n![big-gan](assets\\u002f28_gradio-spaces\\u002fbig-gan.png)\\n\\n\\n## Serving Custom Model Checkpoints with Grad...\"],[\"```\\n\\nYou can check out the French Story Generator [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fmerve\\u002ffrench-...\"],[\"Some Notes on Pros of Open Science and Open Source\\n- **Pooling Resources**: Building off of one anot...\"],[\"# Cons of Closed Source\\n- **Centralization** of power.\\n- **Opacity** of subtle bias\\u002fharm issues.\\n- H...\"],[\"--\\ntitle: \\\"We are hiring interns!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002finterns-2023\\u002fthumbnail.png\\nauthors:\\n- use...\"],[\"The following other internship positions are available:\\n\\n* [Social Impact Evaluation Internship](htt...\"],[\"--\\ntitle: \\\"Announcing the Open Source AI Game Jam ğŸ®\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_gamejam\\u002fthumbnail.p...\"],[\"\\u003ch2\\u003eWho Can Participate?\\u003c\\u002fh2\\u003e\\n\\n**Everyone is welcome to join the Open Source AI Game Jam**, regardle...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #3: Ethical Openness at Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fass...\"],[\"We are crafting tools and safeguards in addition to improving our documentation practices to ensure ...\"],[\"We engage directly with contributors and have addressed pressing issues. To bring this to the next l...\"],[\"**How to use the flagging function:**\\nClick on the flag icon on any Model, Dataset, Space, or Discus...\"],[\"Should a specific model be flagged as high risk by our community, we consider:\\n- Downgrading the ML ...\"],[\"Open science requires safeguards, and one of our goals is to create an environment informed by trade...\"],[\"```\\n@misc{hf_ethics_soc_blog_3,\\n  author    = {Irene Solaiman and\\n               Giada Pistilli and\\n...\"],[\"--\\ntitle: \\\"Deep Learning over the Internet: Training Language Models Collaboratively\\\"\\nthumbnail: \\u002fbl...\"],[\"Unfortunately, we use these pretrained models not only because it's convenient. The hardware resourc...\"],[\"### Training with volunteers\\n\\nIn its most frequently used version, distributed training with multipl...\"],[\"\\u003cdiv class=\\\"aspect-w-16 aspect-h-9\\\"\\u003e\\n\\u003ciframe src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembed\\u002fzdVsg5zsGdc\\\" title=\\\"...\"],[\"![Adaptative strategy](assets\\u002f24_sahajBERT\\u002fadaptive.png)\\n\\u003cdiv style=\\\"line-height:105%;font-size:80%\\\"...\"],[\"### Architecture\\n\\nFor our experiment, we chose ALBERT _(A Lite BERT)_ â€” a model for language represe...\"],[\"1. **Normalization:** includes all preprocessing operations on raw text data. This was the step at w...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"```\\n\\n### Dataset\\n\\nThe last thing we need to cover is the training dataset. As you probably know, the...\"],[\"```\\n\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp...\"],[\"\\u003ciframe width=\\\"100%\\\" height=\\\"670\\\" frameborder=\\\"0\\\"\\n  src=\\\"https:\\u002f\\u002fobservablehq.com\\u002fembed\\u002f@huggingface...\"],[\"### Evaluation\\n\\nTo evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in...\"],[\"| Model       | NER F1 (mean Â± std) | NCC Accuracy (mean Â± std)           |\\n|:-------------:|:------...\"],[\"```\\n\\n#### sahajBERT-NCC\\nModel card: [https:\\u002f\\u002fhf.co\\u002fneuropark\\u002fsahajBERT-NER](https:\\u002f\\u002fhf.co\\u002fneuropark\\u002f...\"],[\"```\\n\\n## Conclusion\\n\\nIn this blog post, we have discussed the method that can enable collaborative pr...\"],[\"Below, you can see all participants of the collaborative experiment:\\n\\n\\u003ciframe width=\\\"100%\\\" height=\\\"3...\"],[\"--\\ntitle: \\\"Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fbl...\"],[\"## Benchmarks\\n\\nWithout any further delay let's show some numbers.\\n\\nFor the sake of consistency, unle...\"],[\"```\\nGenerate args {'max_length': 100, 'do_sample': False}...\"],[\"```\\nThe input prompt is comprised of just a few tokens. The previous token caching is on as well, as...\"],[\"Here is the throughput in msecs on 8x80GB GPUs:\\n\\n| project      \\\\ bs |      1 |     8 |    16 |    3...\"],[\"Let's revisit again how these numbers were calculated. To generate 100 new tokens for a batch size o...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers-bloom-inference\\ncd transformers-bloom-infe...\"],[\"```\\n\\n\\n### Run\\n\\nThe simple execution is:\\n\\n```\\npython bloom-inference-scripts\\u002fbloom-accelerate-inferen...\"],[\"```\\ndeepspeed --num_gpus 8 bloom-inference-scripts\\u002fbloom-ds-inference.py --name microsoft\\u002fbloom-deep...\"],[\"```\\ndeepspeed --num_gpus 4 bloom-inference-scripts\\u002fbloom-ds-inference.py --name microsoft\\u002fbloom-deep...\"],[\"```\\npip install deepspeed\\n```\\n\\n\\n### Run\\n\\nNote that the script currently runs the same inputs on all ...\"],[\"```\\n\\nmake sure to adjust `\\u002fpath\\u002fto\\u002fnvme_offload` to somewhere you have ~400GB of free memory on a fa...\"],[\"--\\ntitle: \\\"Summer at Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f27_summer_at_huggingface\\u002fsummer_intro.gif...\"],[\"Spaces lets you [set up secrets](\\u002fdocs\\u002fhub\\u002fspaces-overview#managing-secrets), permits [custom requir...\"],[\"![Image of a TensorBoard Instance](assets\\u002f27_summer_at_huggingface\\u002ftensorboard.png)\\n\\n### Metrics\\n\\nIn...\"],[\"You can try our early demo of [structured data classification](https:\\u002f\\u002fhuggingface.co\\u002fjulien-c\\u002fwine-...\"],[\"![Course topics](assets\\u002f27_summer_at_huggingface\\u002fcourse.png)\\n\\n### JAX\\u002fFLAX Sprint\\n\\nIn July we hosted...\"],[\"![Generated 3D object with NeRF](assets\\u002f27_summer_at_huggingface\\u002fdiet_nerf.png)\\n\\n3. [CLIP RSIC](http...\"],[\"## Bonus\\n\\nOn top of everything we just shared, our team has been doing lots of other things. Here ar...\"],[\"You can now easily publish your model to the Hub, including automatically authored model cards, eval...\"],[\"```\\n\\nThe last 4 releases introduced many new cool models!\\n\\n- [DETR](https:\\u002f\\u002fhuggingface.co\\u002ftransform...\"],[\"![DETR image](assets\\u002f27_summer_at_huggingface\\u002fdetr.png)\\n\\n- [ByT5](https:\\u002f\\u002fhuggingface.co\\u002ftransformer...\"],[\"![LayoutLM object detection](assets\\u002f27_summer_at_huggingface\\u002flayout.png)\\n\\n- [BEiT](https:\\u002f\\u002fhuggingfa...\"],[\"Users can also directly host and share their datasets to the community simply by uploading their dat...\"],[\"![spaCy NER example](assets\\u002f27_summer_at_huggingface\\u002fspacy_ner.jpeg)\\n\\nAnother exciting integration i...\"],[\"### **NEW: Hardware Acceleration**\\n\\nHugging Face is [partnering with leading AI hardware accelerator...\"],[\"![AutoNLP on the web.gif](assets\\u002f27_summer_at_huggingface\\u002fautonlp.gif)\\n\\n### Inference API\\n\\n**Webinar...\"],[\"**Hugging Face + Google Sheets Demo**\\n\\nWith the [Inference API](https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002finfer...\"],[\"## Research\\n\\nAt BigScience we held our first live event (since the kick off) in July BigScience Epis...\"],[\"\\u003cdiv class=\\\"aspect-w-16 aspect-h-9\\\"\\u003e\\n\\u003ciframe \\nsrc=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembed\\u002fv8ShbLasRF8\\\" \\nframe...\"],[\"![Prompt](assets\\u002f27_summer_at_huggingface\\u002fprompt.png)\\n\\n\\nWe're looking forward to EMNLP this year whe...\"],[\"--\\ntitle: \\\"Model Cards\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f121_model-cards\\u002fthumbnail.png\\nauthors:\\n- user: Ezi\\n...\"],[\"4) A [User Study](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fmodel-cards-user-studies) on model card usage at H...\"],[\"### Standardising Model Card Structure\\nThrough our background research and user studies, which are d...\"],[\"As ML continues to be more intertwined with different domains, collaborative and open-source ML proc...\"],[\"* The Hugging Face ecosystem will continue to advance methods that streamline Model Card creation [t...\"],[\"--\\ntitle: \\\"Introducing RWKV - An RNN with the advantages of a transformer\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"### Transformer Architecture vs RNNs\\n\\nThe RNN architecture is one of the first widely used Neural Ne...\"],[\"In the transformer architecture, the input tokens are processed simultaneously in the self-attention...\"],[\"During inference, RNNs have some advantages in speed and memory efficiency. These advantages include...\"],[\"1. Traditional RNN models are unable to utilize very long contexts (LSTM can only manage ~100 tokens...\"],[\"### Existing checkpoints\\n\\n#### Pure language models: RWKV-4 models\\n\\nMost adopted RWKV models range f...\"],[\"All the HF converted models are available on Hugging Face Hub, in the [`RWKV` organization](https:\\u002f\\u002f...\"],[\"```\\n\\nOr you can run and start from the snippet below:\\n\\n```python\\nimport torch\\nfrom transformers impo...\"],[\"```\\n\\n### Use the raven models (chat models)\\n\\nYou can prompt the chat model in the alpaca style, here...\"],[\"```\\n\\nAccording to Bo, better instruction techniques are detailed in [this discord message (make sure...\"],[\"```\\n\\n## Future work\\n\\n### Multi-lingual RWKV\\n\\nBo is currently working on a multilingual corpus to tra...\"],[\"## Acknowledgements\\n\\nThe Hugging Face team would like to thank Bo and RWKV community for their time ...\"],[\"--\\ntitle: Stable Diffusion with ğŸ§¨ Diffusers\\nthumbnail: \\u002fblog\\u002fassets\\u002f98_stable_diffusion\\u002fthumbnail.pn...\"],[\"Now, let's get started by generating some images ğŸ¨.\\n\\n## Running Stable Diffusion\\n\\n### License\\n\\nBefor...\"],[\"```\\n\\nIn this post we'll use model version [`v1-4`](https:\\u002f\\u002fhuggingface.co\\u002fCompVis\\u002fstable-diffusion-v...\"],[\"```\\n\\nThe result would look as follows\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_12_1.png)\\n...\"],[\"```\\n\\nThe result would look as follows\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_14_1.png)\\n...\"],[\"```\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_16_1.png)\\n\\nNote how the structure is the sam...\"],[\"```\\n\\nWe can generate multiple images for the same prompt by simply using a list with the same prompt...\"],[\"```\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_26_1.png)\\n    \\n\\n## How does Stable Diffusion...\"],[\"There are three main components in latent diffusion.\\n\\n1. An autoencoder (VAE).\\n2. A [U-Net](https:\\u002f\\u002f...\"],[\"To prevent the U-Net from losing important information while downsampling, short-cut connections are...\"],[\"The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed i...\"],[\"After this brief introduction to Latent and Stable Diffusion, let's see how to make advanced use of ...\"],[\"# 2. Load the tokenizer and text encoder to tokenize and encode the text. \\ntokenizer = CLIPTokenizer...\"],[\"```\\n\\nNow instead of loading the pre-defined scheduler, we load the [K-LMS scheduler](https:\\u002f\\u002fgithub....\"],[\"```\\n\\nFirst, we get the `text_embeddings` for the passed prompt. \\nThese embeddings will be used to co...\"],[\"```\\n\\nIf we examine the `latents` at this stage we'll see their shape is `torch.Size([1, 4, 64, 64])`...\"],[\"```\\n\\nWe now use the `vae` to decode the generated `latents` back into the image.\\n\\n\\n```python\\n# scale...\"],[\"```\\n@article{patil2022stable,\\n  author = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von...\"],[\"--\\ntitle: 'Deploy Hugging Face models easily with Amazon SageMaker'\\nthumbnail: \\u002fblog\\u002fassets\\u002f17_the_p...\"],[\"```\\n\\n\\nThat's it! ğŸš€\\n\\nTo learn more about accessing and using the new Hugging Face DLCs with the Amazo...\"],[\"---\\n\\n\\n# **SageMaker Hugging Face Inference Toolkit âš™ï¸**\\n\\nIn addition to the Hugging Face Transformer...\"],[\"```python\\n# text-classification request body\\n{\\n\\t\\\"inputs\\\": \\\"Camera - You are awarded a SiPix Digital ...\"],[\"```\\n\\n# **Getting started ğŸ§­**\\n\\nIn this guide we will use the new Hugging Face Inference DLCs and Amaz...\"],[\"```\\n\\n---\\n\\n## **Deploy a trained Hugging Face Transformer model to SageMaker for inference**\\n\\nThere a...\"],[\"```\\n\\n\\nAfter we run our request we can delete the endpoint again with.\\n\\n\\n```python\\n# delete endpoint\\n...\"],[\"```\\n\\nAfter we run our request, we can delete the endpoint again with:\\n\\n\\n```python\\n# delete endpoint\\n...\"],[\"```\\n\\nAfter we run our request we can delete the endpoint again with.\\n\\n\\n```python\\n# delete endpoint\\np...\"],[\"```\\n\\n---\\n\\n# **FAQ ğŸ¯**\\n\\nYou can find the complete [Frequently Asked Questions](https:\\u002f\\u002fhuggingface.co...\"],[\"_Q: Do I have to use the SageMaker Python SDK to use the Hugging Face Deep Learning Containers (DLCs...\"],[\"_Q: How is my data and code secured by Amazon SageMaker?_\\n\\nA: Amazon SageMaker provides numerous sec...\"],[\"---\\n\\nIf you need premium support from the Hugging Face team to accelerate your NLP roadmap, our[ Exp...\"],[\"--\\ntitle: \\\"Introducing Prodigy-HF: a direct integration with Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"\\u003cfigure\\u003e\\n    \\u003cdiv style=\\\"background-color: #eee; padding-top: 8px; padding-bottom: 8px;\\\"\\u003e\\n        \\u003ci...\"],[\"```\\npython -m prodigy hf.train.ner fashion-train,eval:fashion-eval path\\u002fto\\u002fmodel-out --model \\\"distil...\"],[\"```\\npython -m prodigy hf.upload \\u003cdataset_name\\u003e \\u003cusername\\u003e\\u002f\\u003crepo_name\\u003e\\n```\\n\\nWe're particularly fond o...\"],[\"--\\ntitle: \\\"How to Install and Use the Hugging Face Unity API\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-gam...\"],[\"6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https:\\u002f\\u002fh...\"],[\"```\\nusing HuggingFace.API;\\n\\n\\u002f* other code *\\u002f\\n\\n\\u002f\\u002f Make a call to the API\\nvoid Query() {\\n    string in...\"],[\"```\\n\\n## Supported Tasks and Custom Models\\n\\nThe Hugging Face Unity API also currently supports the fo...\"],[\"--\\ntitle: \\\"Proximal Policy Optimization (PPO)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f93_deep_rl_ppo\\u002fthumbnail.png\\n...\"],[\"- *An Actor*Â that controlsÂ **how our agent behaves**Â (policy-based method).\\n- *A Critic*Â that measur...\"],[\"- [The intuition behind PPO](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-rl-ppo#the-intuition-behind-ppo)\\n- [In...\"],[\"- [Let's code our PPO Agent](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-rl-ppo#lets-code-our-ppo-agent)\\n  \\n## ...\"],[\"The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability o...\"],[\"However, the problem comes from the step size:\\n- Too small,Â **the training process was too slow**\\n- ...\"],[\"This ratio **can replace the log probability we use in the policy objective function**. This gives u...\"],[\"This clipped part is a version where rt(theta) is clipped between  \\\\\\\\( [1 - \\\\epsilon, 1 + \\\\epsilon] ...\"],[\"In situation 1, we have a positive advantage: theÂ **action is better than the average**Â of all the a...\"],[\"### Case 5 and 6: the ratio is above the range\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n ...\"],[\"**You might wonder why, when the minimum is the clipped ratio, the gradient is 0.** When the ratio i...\"],[\"So, to be able to code it, we're going to use two resources:\\n- A tutorial made by [Costa Huang](http...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fgiphy.com\\u002fembed\\u002fpynZagVcYxVUk\\\" width=\\\"480\\\" height=\\\"480\\\" frameBorder=\\\"0\\\" class=\\\"...\"],[\"--\\ntitle: \\\"Very Large Language Models and How to Evaluate Them\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f106_zero_sh...\"],[\"Weâ€™ve upgraded the AutoTrain infrastructure for this project so that large models can be evaluated f...\"],[\"## Case study: Zero-shot evaluation on the WinoBias task\\n\\nThe [WinoBias](https:\\u002f\\u002fgithub.com\\u002fuclanlp\\u002f...\"],[\"![Winobias](.\\u002fassets\\u002f106_zero_shot_eval_on_the_hub\\u002fwinobias.png)\\n\\n## Enabling better research tools ...\"],[\"## Send us feedback!\\n\\nAt Hugging Face, weâ€™re excited to continue democratizing access to state-of-th...\"],[\"--\\ntitle: Training Stable Diffusion with Dreambooth using Diffusers\\nthumbnail: \\u002fblog\\u002fassets\\u002fsd_dream...\"],[\"## TL;DR: Recommended Settings\\n\\n* Dreambooth tends to overfit quickly. To get good-quality images, w...\"],[\"## Experiments Settings\\n\\nAll our experiments were conducted using the [`train_dreambooth.py`](https:...\"],[\"Low Learning Rate (`2e-6`)\\n![Pighead, Low Learning Rate](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface...\"],[\"## Using Prior Preservation when training Faces\\n\\nPrior preservation is a technique that uses additio...\"],[\"`LMSDiscrete`, Kramer face. Results are terrible!\\n![LMSDiscrete Cosmo](https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"Fine-tuned text encoder\\n![Fine-tuned text encoder](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"--\\ntitle: 'Faster Text Generation with TensorFlow and XLA'\\nthumbnail: \\u002fblog\\u002fassets\\u002f91_tf_xla_generat...\"],[\"Let's start with the basics. Text generation can be deterministic or stochastic, depending on the\\n`d...\"],[\"```\\n\\nDepending on the target application, longer outputs might be desirable. You can control the len...\"],[\"```\\n\\nContrarily to Sampling, Greedy Decoding will always pick the most likely token at each iteratio...\"],[\"```\\n\\nThe basics of text generation, as you can see, are straightforward to control. However, there a...\"],[\"For those of you familiar with TensorFlow 1 ğŸ§“, the concept of a TensorFlow graph comes naturally, as...\"],[\"```python\\n# Note: execution times are deeply dependent on hardware -- a 3090 was used here.\\nimport t...\"],[\"```\\n\\nIn one line, you can create an XLA-accelerated function from the function above.\\n\\n```python\\nxla...\"],[\"```\\n\\n## Text Generation using TensorFlow with XLA\\n\\nAs with any optimization procedure, there is no f...\"],[\"# Slow: Different tensor shape\\nmax_plus_constant(tf.constant([0, 0, 0, 0]), 1)\\n# \\u003e Execution time --...\"],[\"```\\n\\nIn practice, for text generation, it simply means the input should be padded to a multiple of a...\"],[\"print(\\\"Calling XLA generation with tokenized_input_1...\\\")\\nprint(\\\"(will be slow as it is the first ca...\"],[\"```\\n\\nOh no, that's terribly slow! A solution to keep the different combinations of shapes in check i...\"],[\"```\\n\\nThat's much better, successive generation calls performed this way will be orders of magnitude ...\"],[\"```\\n\\nFrom a developer perspective, relying on XLA implies being aware of a few additional nuances. X...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"--\\ntitle: \\\"Perceiver IO: a scalable, fully-attentional model that works on any modality\\\"\\nthumbnail: ...\"],[\"Not long after that, AI researchers started to apply the idea of BERT to other domains. To name a fe...\"],[\"## The Perceiver\\n\\nThe [Perceiver](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2103.03206) aims to solve this limitation by...\"],[\"Note that each of these are optional. A `preprocessor` is only required in case one hasn't already e...\"],[\"Let's start off by showing how the Perceiver is implemented to work on text.\\n\\n## Perceiver for text\\n...\"],[\"```\\n\\nIn this case, one provides `PerceiverTextPreprocessor` as preprocessor to the model, which will...\"],[\"```\\n\\nIn the Perceiver IO paper, one uses 256 latents, and sets the dimensionality of the latents to ...\"],[\"Ok, so now one has final hidden states of shape (batch_size, 256, 1280). Great, but one actually wan...\"],[\"Great, isn't it? The Perceiver authors also show that it is straightforward to pre-train the Perceiv...\"],[\"``` python\\nfrom torch import nn\\nfrom transformers import PerceiverModel\\nfrom transformers.models.per...\"],[\"```\\n\\nOne can see that `PerceiverImagePreprocessor` is initialized with `prep_type = \\\"conv1x1\\\"` and t...\"],[\"```\\n\\n`PerceiverImagePreprocessor` (with the settings defined above) will first apply a convolutional...\"],[\"The Perceiver authors show that the model is capable of achieving strong results compared to models ...\"],[\"class PerceiverForOpticalFlow(nn.Module):\\n    def __init__(self, config):\\n        super().__init__(c...\"],[\"```\\nAs one can see, `PerceiverImagePreprocessor` is used as preprocessor (i.e. to prepare the 2 imag...\"],[\"To decode the final hidden states of the latents to an actual predicted flow, `PerceiverOpticalFlowD...\"],[\"The video below shows the predicted flow on 2 examples. \\n\\n\\u003cp float=\\\"left\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002flh3.g...\"],[\"## Perceiver for multimodal autoencoding\\n\\nThe authors also use the Perceiver for multimodal autoenco...\"],[\"Next, `PerceiverMultimodalPreprocessor` will pad the preprocessed modalities with modality-specific ...\"],[\"- For the image modality, the total size of the decoder query is 16x3x224x224 = 802,816. However, wh...\"],[\"Finally, there is `PerceiverMultimodalPostprocessor`. This class postprocesses the output of the dec...\"],[\"\\u003cimg src=\\\"assets\\u002f41_perceiver\\u002fpredicted_labels.png\\\" width=\\\"500\\\"\\u003e\\n\\n\\u003csmall\\u003eTop 5 predicted labels for ...\"],[\"The authors also used the Perceiver to replace the original Transformer in [AlphaStar](https:\\u002f\\u002fdeepm...\"],[\"## Conclusion\\n\\nIn this blog post, we went over the architecture of Perceiver IO, an extension of the...\"],[\"--\\ntitle: \\\"Train a Sentence Embedding Model with 1B Training Pairs\\\"\\nauthors:\\n- user: asi\\n  guest: tr...\"],[\"![snippet](assets\\u002f32_1b_sentence_embeddings\\u002fmodel.png)\\n\\n### Multiple Negative Ranking Loss\\n\\nThe para...\"],[\"![snippet](assets\\u002f32_1b_sentence_embeddings\\u002fcontrastive_2.png)\\n\\nIn the loss equation, `sim` indicate...\"],[\"### Improving Quality with Better Batches\\n\\nIn our method, we build batches of sample pairs \\\\\\\\( (a_i ...\"],[\"## Training infrastructure and data\\n\\nAs mentioned earlier, the quantity of data and the batch size d...\"],[\"General sentence embeddings might be used for many applications. We built a [Spaces demo](https:\\u002f\\u002fhu...\"],[\"--\\ntitle: \\\"Large-scale Near-deduplication Behind BigCode\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fdedup\\u002fthumbnail.pn...\"],[\"## From BigScience to BigCode\\n\\nAllow me to share a story first on how I jumped on this near-deduplic...\"],[\"| Dataset                              | Input Size                       | Output Size or Deduction...\"],[\"| BNE5[[6]](#6)                        | 2TB                              | 570 GB                  ...\"],[\"| The BigScience ROOTS Corpus[[9]](#9) |                                  | 0.07% ~ 2.7% **â†“** (docu...\"],[\"This is the one for code datasets we created for BigCode as well. Model names are used when the data...\"],[\"## MinHash Walkthrough\\n\\nIn this section, we will cover each step of MinHash, the one used in BigCode...\"],[\"This operation has a time complexity of \\\\\\\\( \\\\mathcal{O}(NM) \\\\\\\\) where \\\\\\\\( N \\\\\\\\) is the number of doc...\"],[\"Taking the minimum value of each column within each document â€” the \\\"Min\\\" part of the \\\"MinHash\\\", we a...\"],[\"```python\\ndef embed_func(\\n    content: str,\\n    idx: int,\\n    *,\\n    num_perm: int,\\n    ngram_size: ...\"],[\"```\\n\\nIf you are familiar with [Datasketch](https:\\u002f\\u002fgithub.com\\u002fekzhu\\u002fdatasketch), you might ask, why ...\"],[\"```\\n\\nAfter the fingerprint calculation, one particular document is mapped to one array of integer va...\"],[\"If two documents share the same hashes in a band at a particular location (band index), they will be...\"],[\"### Beyond Duplicate Pairs\\n\\nThis is where many deduplication descriptions in papers or tutorials sto...\"],[\"**Option 2: Use popular python frameworks such as `dask` to allow more efficient `groupby` operation...\"],[\"```\\n\\n**Option 4: For large datasets, use Spark.**\\n\\nWe already know that steps up to the LSH part can...\"],[\"```\\n\\nAdditionally, thanks to cloud providers, we can set up Spark clusters like a breeze with servic...\"],[\"These graphs can help us understand why it was necessary to double-check the false positives for Cod...\"],[\"We still encourage you to perform similar analysis on your datasets before training. For example, it...\"],[\"## Future Directions\\n\\n1. Substring deduplication. Even though it showed some benefits for English[[3...\"],[\"## Supporting Resources\\n\\n- [Datasketch](https:\\u002f\\u002fgithub.com\\u002fekzhu\\u002fdatasketch)Â (MIT)\\n- [simhash-py](ht...\"],[\"- \\u003ca id=\\\"1\\\"\\u003e[1]\\u003c\\u002fa\\u003e : Nikhil Kandpal, Eric Wallace, Colin Raffel, [Deduplicating Training Data Mitig...\"],[\"- \\u003ca id=\\\"8\\\"\\u003e[8]\\u003c\\u002fa\\u003e : Xi Victoria Lin, Todor Mihaylov, et al., [Few-shot Learning with Multilingual ...\"],[\"- \\u003ca id=\\\"15\\\"\\u003e[15]\\u003c\\u002fa\\u003e : Lewis Tunstall, Leandro von Werra, Thomas Wolf, [Natural Language Processing...\"],[\"--\\ntitle: 'Getting Started With Embeddings'\\nthumbnail: \\u002fblog\\u002fassets\\u002f80_getting_started_with_embeddin...\"],[\"## What are embeddings for?\\n\\n\\u003e \\\"[...] once you understand this ML multitool (embedding), you'll be a...\"],[\"But first, we need to embed our dataset (other texts use the terms encode and embed interchangeably)...\"],[\"```\\nTo generate the embeddings you can use the `https:\\u002f\\u002fapi-inference.huggingface.co\\u002fpipeline\\u002ffeatur...\"],[\"```\\n\\nThe current API does not enforce strict rate limitations. Instead, Hugging Face balances the lo...\"],[\"```\\nIt looks similar to this matrix:\\n\\n```py\\n[[-0.02388945  0.05525852 -0.01165488 ...  0.00577787  0...\"],[\"```\\n\\n## 2. Host embeddings for free on the Hugging Face Hub\\n\\nğŸ¤— Datasets is a library for quickly acc...\"],[\"```\\n\\nFollow the next steps to host `embeddings.csv` in the Hub.\\n\\n* Click on your user in the top rig...\"],[\"Install the ğŸ¤— Datasets library with `pip install datasets`. Then, load the embedded dataset from the...\"],[\"```\\nWe use the query function we defined before to embed the customer's question and convert it to a...\"],[\"```\\n\\n`util.semantic_search` identifies how close each of the 13 FAQs is to the customer query and re...\"],[\"```\\n\\nThis list represents the 5 FAQs closest to the customer's query. Nice! We used here PyTorch and...\"],[\"--\\ntitle: Getting Started with Transformers on Habana Gaudi\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_getting_start...\"],[\"Starting from the [EC2 console](https:\\u002f\\u002fconsole.aws.amazon.com\\u002fec2sp\\u002fv2\\u002f) in the us-east-1 region, I...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f61_getting_started_habana\\u002fhabana08.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nNext, I assign an Amazon I...\"],[\"```\\nssh -i ~\\u002f.ssh\\u002fjulsimon-keypair.pem ubuntu@ec2-18-207-189-109.compute-1.amazonaws.com\\n```\\n\\nOn thi...\"],[\"```\\n\\nThen, I move to the subdirectory containing the text classification example and install the req...\"],[\"```\\n\\nAfter 2 minutes and 12 seconds, the job is complete and has achieved an excellent F1 score of 0...\"],[\"--\\ntitle: \\\"Personal Copilot: Train Your Own Coding Assistant\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f170_personal_...\"],[\"## Data Collection Workflow\\n\\nOur desired dataset is conceptually simple, we structured it like so:\\n\\n...\"],[\"We also excluded all file paths that were not directly related to code. These include: `.git`, `__py...\"],[\"[This is the code we used to generate this dataset](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002ftr...\"],[\"Why PEFT? Full fine-tuning is expensive. Letâ€™s have some numbers to put things in perspective:\\n\\nMini...\"],[\"In the above calculations, we didn't consider memory required for intermediate activation checkpoint...\"],[\"**Resources**\\n1. Codebase: [link](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002ftree\\u002fmain\\u002fpersonal_c...\"],[\"```\\naccelerate launch --config_file \\\"configs\\u002ffsdp_config.yaml\\\"  train.py \\\\\\n    --model_path \\\"bigcode...\"],[\"```\\n\\nThe total training time was **9 Hours**. Taking the cost of $12.00 \\u002f hr based on [lambdalabs](h...\"],[\"**Resources**\\n1. Codebase: [link](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002ftree\\u002fmain\\u002fpersonal_c...\"],[\"## Comparison\\n\\nThe plot below shows the eval loss, train loss and learning rate scheduler for QLoRA ...\"],[\"![qualitative_comparison_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"In the second example above, **GitHub Copilot didn't give any completion**. This can be due to the f...\"],[\"### Setting an Inference Endpoint\\nBelow are the screenshots with the steps we followed to create our...\"],[\"**Resources**\\n\\n1. Codebase: [link](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002ftree\\u002fmain\\u002fcode_assi...\"],[\"Our notebook [Dance_of_LoRAs.ipynb](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002fblob\\u002fmain\\u002fpersonal...\"],[\"##### Let us now consider the `code-completion` task.\\n\\nOn disabling adapters, we observe that the co...\"],[\"We can observe that `code_buddy` is performing much better than the `assistant` or `copilot` adapter...\"],[\"**Performance on the Chatting\\u002fQA task**\\n\\nAs Octocoder is trained to answer questions and carry out c...\"],[\"The training loss, evaluation loss as well as learning rate schedules are plotted below:\\n\\n![loss_plo...\"],[\"```\\ngit clone --recursive https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fmlc-llm.git && cd mlc-llm\\u002f\\n```\\n\\n2. Install th...\"],[\"```\\n\\n5. Run the local server:\\n```\\n python -m mlc_chat.rest --model dist\\u002fstarcoder1B-v2-personal-copi...\"],[\"```\\n\\n6. Change the endpoint of HF Code Completion extension in VS Code to point to the local server:...\"],[\"--\\ntitle: \\\"Hugging Face on PyTorch \\u002f XLA TPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f13_pytorch_xla\\u002fpytorch_xla_th...\"],[\"### XLA:TPU Device Type\\n\\nPyTorch \\u002f XLA adds a new `xla` device type to PyTorch. This device type wor...\"],[\"```\\n\\nThis code should look familiar. PyTorch \\u002f XLA uses the same interface as regular PyTorch with a...\"],[\"```\\n\\n### PyTorch \\u002f XLA Input Pipeline\\n\\nThere are two main parts to running a PyTorch \\u002f XLA model: (1...\"],[\"```\\n\\n### Checkpoint Writing and Loading\\n\\nWhen a tensor is checkpointed from a XLA device and then lo...\"],[\"```\\n\\n\\n## PyTorch \\u002f XLA Library\\n\\nPyTorch \\u002f XLA is a Python package that uses the XLA linear algebra c...\"],[\"While a user runs their forward and backward passes, an intermediate representation (IR) graph is tr...\"],[\"```\\n\\nThis live graph is accumulated while the forward and backward passes are run on the user's prog...\"],[\"```python\\n\\u003e\\u003e\\u003e import torch_xla.debug.metrics as met\\n\\u003e\\u003e\\u003e print(met.metrics_report())\\nMetric: CompileT...\"],[\"Metric: ExecuteTime\\n  TotalSamples: 1220\\n  Accumulator: 04m22s555ms668.071us\\n  ValueRate: 923ms872.8...\"],[\"```\\n\\n### Train Your Transformer on Cloud TPUs\\n\\nTo configure your VM and Cloud TPUs, please follow [â€œ...\"],[\"```bash\\nconda activate torch-xla-1.7\\nexport TPU_IP_ADDRESS=\\\"ENTER_YOUR_TPU_IP_ADDRESS\\\"  # ex. 10.0.0...\"],[\"```\\n\\nThe above should complete training in roughly less than 200 minutes with an eval perplexity of ...\"],[\"## Get Started with PyTorch \\u002f XLA on TPUs\\nSee the [â€œRunning on TPUsâ€](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"--\\ntitle: Deploying TensorFlow Vision Models in Hugging Face with TF Serving\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"To get the complete working code shown throughout this post, refer to\\nthe Colab Notebook shown at th...\"],[\"```\\n\\nBy default, `save_pretrained()` will first create a version directory\\ninside the path we provid...\"],[\"```\\n\\nThis should print:\\n\\n```bash\\nViTImageProcessor {\\n  \\\"do_normalize\\\": true,\\n  \\\"do_resize\\\": true,\\n  ...\"],[\"```\\n\\nYou also need to resize the image and transpose it so that it has leading\\nchannel dimensions si...\"],[\"```\\n\\n**Note on making the model accept string inputs**:\\n\\nWhen dealing with images via REST or gRPC r...\"],[\"```\\n\\nYou can first derive the [concrete function](https:\\u002f\\u002fwww.tensorflow.org\\u002fguide\\u002ffunction)\\nfrom th...\"],[\"```\\n\\nYou can notice that the model's signature has now changed. Specifically,\\nthe input type is now ...\"],[\"```\\n\\nFrom the above command, the important parameters are:\\n\\n- `rest_api_port` denotes the port numbe...\"],[\"```\\n\\nTF Serving's request payload format specification for the REST endpoint\\nis available [here](htt...\"],[\"```\\n\\nThe REST API is -\\n`http:\\u002f\\u002flocalhost:8501\\u002fv1\\u002fmodels\\u002fvit:predict` following the specification fro...\"],[\"```\\n\\nNow, you can get some predictions:\\n\\n```py\\ngrpc_predictions = stub.Predict(request, 10.0)  # 10 ...\"],[\"--\\ntitle: \\\"Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU\\\" \\nthumbnail: assets\\u002f133_trl_peft\\u002fth...\"],[\"2- Collect a human annotated dataset and train a reward model\\n\\n3- Further fine-tune the LLM from ste...\"],[\"### What is TRL?\\n\\nThe `trl` library aims at making the RL step much easier and more flexible so that...\"],[\"In `trl` you can also use shared layers between reference and active models to avoid entire copies. ...\"],[\"Many techniques have been adopted to tackle these challenges at scale. The most familiar paradigms a...\"],[\"Therefore, we asked ourselves the following question: how far can we go with just data parallelism? ...\"],[\"In a nutshell, you can reduce the size of a full-precision model by 4 (thus, by 2 for half-precision...\"],[\"The library is still under extensive and active development, with many upcoming features to be annou...\"],[\"So in the first place, letâ€™s just load the active model in 8-bit. Letâ€™s see what we need to do for t...\"],[\"Overall there were three key steps and training scripts:\\n\\n1. **[Script](https:\\u002f\\u002fgithub.com\\u002flvwerra\\u002ft...\"],[\"| ![loss-20b](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftrl-internal-testing\\u002fexample-images\\u002fresolve\\u002fmain\\u002fblog\\u002f...\"],[\"We have identified some interesting directions for the next steps to push the limits of this integra...\"],[\"--\\ntitle: \\\"Sentiment Analysis on Encrypted Data with Homomorphic Encryption\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"```\\npip install -U pip setuptools\\n```\\n\\nNow we can install all the required libraries for the this bl...\"],[\"```\\n\\nThe output, then, looks like this:\\n\\n```\\nProportion of positive examples: 16.14%\\nProportion of n...\"],[\"```\\n\\nThis should download the model, which is now ready to be used.\\n\\nUsing the hidden representation...\"],[\"# Send the model to the device\\n   transformer_model = transformer_model.to(device)\\n   output_hidden_...\"],[\"```\\n\\n```python\\n# Let's vectorize the text using the transformer\\nlist_text_X_train = text_X_train.tol...\"],[\"```\\n\\nThe output is as follows:\\n\\n```\\nBest score: 0.8378111718275654\\nBest parameters: {'max_depth': 1,...\"],[\"```\\n\\nWith the following output:\\n```\\nAccuracy: 0.8504\\n```\\n\\n## Predicting over encrypted data\\n\\nNow let...\"],[\"```\\n\\nThe output becomes:\\n\\n```\\nCompilation time: 9.3354 seconds\\nFHE inference time: 4.4085 seconds\\n``...\"],[\"```\\n\\nThese few lines are enough to export all the files needed for both the client and the server. Y...\"],[\"We have presented a way to leverage the power of transformers where the representation is then used ...\"],[\"--\\ntitle: \\\"How to host a Unity game in a Space\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002funity-in-sp...\"],[\"```\\n\\n## Step 3: Open your Unity Project\\n\\nOpen the Unity project you want to host in your Space.\\n\\n\\u003cfi...\"],[\"## Step 7: Change the Compression Format to Disabled\\n\\nIn the Player Settings panel, navigate to the ...\"],[\"```\\ngit lfs install\\ngit lfs track Build\\u002f* \\n```\\n\\n## Step 11: Push your Changes\\n\\nFinally, use the foll...\"],[\"--\\ntitle: \\\"Llama 2 on Amazon SageMaker a Benchmark\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fllama_sagemaker_benchma...\"],[\"We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Be...\"],[\"### What is Llama 2?\\n\\nLlama 2 is a family of LLMs from Meta, trained on 2 trillion tokens. Llama 2 c...\"],[\"As metrics, we used Throughput and Latency defined as: \\n\\n- Throughput (tokens\\u002fsec): Number of tokens...\"],[\"| Model       | Quantization | Instance       | concurrent requests | Latency (ms\\u002ftoken) median | Th...\"],[\"### Best Throughput Deployment\\n\\nThe Best Throughput configuration maximizes the number of tokens tha...\"],[\"| Model       | Quantization | Instance        | concurrent requests | Latency (ms\\u002ftoken) median | T...\"],[\"### Best Latency Deployment\\n\\nThe Best Latency configuration minimizes the time it takes to generate ...\"],[\"| Model       | Quantization | Instance        | concurrent requests | Latency (ms\\u002ftoken) median | T...\"],[\"## Conclusions\\n\\nIn this benchmark, we tested 60 configurations of Llama 2 on Amazon SageMaker. For c...\"],[\"--\\ntitle: \\\"Announcing Evaluation on the Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f82_eval_on_the_hub\\u002fthumbnail.pn...\"],[\"Progress in AI has been nothing short of amazing, to the point where some people are now seriously d...\"],[\"## On the Hub\\n\\nEvaluation on the Hub opens the door to so many interesting use cases. From the data ...\"],[\"Evaluation on the Hub is meant to make your life easier. But of course, thereâ€™s a lot happening in t...\"],[\"### Configuring an evaluation job\\n\\nTo get started, head over to the [`model-evaluator` Space](https:...\"],[\"![Selecting Metrics](\\u002fblog\\u002fassets\\u002f82_eval_on_the_hub\\u002fselect-metrics.png)\\n\\nAnd thatâ€™s all it takes to...\"],[\"Looks like the Swin Transformer came out on top!\\n\\n### Try it yourself!\\n\\nIf youâ€™d like to evaluate yo...\"],[\"## Next Steps\\n\\nA few weeks ago, we launched the Hugging Face [Evaluate library](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"--\\ntitle: \\\"Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models\\\"\\nthumbnail: ...\"],[\"Similar to BERT and GPT2, massive pre-trained encoder-decoder models\\nhave shown to significantly boo...\"],[\"This notebook is divided into 4 parts:\\n\\n-   **Introduction** - *Short summary of pre-trained languag...\"],[\"## **Introduction**\\n\\nRecently, pre-trained language models \\\\\\\\({}^1\\\\\\\\) have revolutionized the\\nfield ...\"],[\"The capability of pre-trained language models to effectively transfer\\n*task-agnostic* knowledge to *...\"],[\"Let\\\\'s see how BERT and GPT2 would be fit to model sequence-to-sequence\\ntasks.\\n\\n### **BERT**\\n\\nBERT i...\"],[\"Let\\\\'s visualize BERT.\\n\\n![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientif...\"],[\"### **GPT2**\\n\\nGPT2 is a *decoder-only* model, which makes use of *uni-directional*\\n(*i.e.* \\\\\\\"causal\\\\...\"],[\"\\\\\\\\(p_{\\\\theta_{\\\\text{GPT2}}}(\\\\mathbf{y}_i | \\\\mathbf{Y}_{0:i-1})\\\\\\\\) hereby\\npresents the probability di...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fgpt2.pn...\"],[\"GPT2 is mainly used for *open-domain* text generation. First, an input\\nprompt \\\\\\\\(\\\\mathbf{Y}_{0:i-1}\\\\...\"],[\"### **Encoder-Decoder**\\n\\nBecause *encoder-only* models require to know the output length *a\\npriori*,...\"],[\"In 2020, Sascha Rothe, Shashi Narayan, and Aliaksei Severyn investigated\\nexactly this question in th...\"],[\"\\\\\\\\({}^4\\\\\\\\) Without loss of generalitiy, we exclude the normalization layers\\nto not clutter the equat...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"$$\\n p_{\\\\theta_{\\\\text{enc, dec}}}(\\\\mathbf{Y}_{1:m} | \\\\mathbf{X}_{1:n}) = p_{\\\\theta_{\\\\text{dec}}}(\\\\mat...\"],[\"### **Warm-staring Encoder-Decoder with BERT**\\n\\nLet\\\\'s now illustrate how a pre-trained BERT model c...\"],[\"Before fine-tuning, the encoder therefore behaves exactly like a\\npre-trained BERT model. Assuming th...\"],[\"The architecture of the decoder is different from BERT\\\\'s architecture\\nin three ways.\\n\\n1.  First, th...\"],[\"2.  Second, BERT\\\\'s *bi-directional* self-attention layers have to be\\n    changed to *uni-directiona...\"],[\"3.  Third, the decoder outputs a sequence of logit vectors\\n    \\\\\\\\(\\\\mathbf{L}_{1:m}\\\\\\\\) in order to de...\"],[\"We can see that decoder is more similar to GPT2 than it is to BERT. The\\nweight parameters of decoder...\"],[\"### **Encoder-Decoder Weight Sharing**\\n\\nIn [Raffel et al. (2020)](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1910.10683.p...\"],[\"$$ \\\\mathbf{W}^{\\\\text{self-attn}, 3}_{k} = \\\\mathbf{W}^{\\\\text{self-attn}, 3}_{\\\\text{enc}, k} \\\\equiv \\\\m...\"],[\"In the same way, we can warm-start an encoder-decoder model by sharing\\nthe encoder weights with the ...\"],[\"To be more precise, the publicly available pre-trained checkpoints of\\n**BERT**, **RoBERTa**, and **G...\"],[\"|Model          |random   |leveraged   |total\\n  |-------------- |:------- |----------  |-------\\n  |R...\"],[\"### **Experiments**\\n\\nThe above models were trained and evaluated on four sequence-to-sequence\\ntasks ...\"],[\"|Seq2Seq Task               |Datasets                                                               ...\"],[\"|WMT14 DE =\\\\\\u003e EN            |[Bojar et al. (2014)](http:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002fW\\u002fW14\\u002fW14-3302)  ...\"],[\"Depending on the task, a slightly different training regime was used.\\n*E.g.* according to the size o...\"],[\"can be simplified into\\n\\n*Street Rod is the first in a series of two games **.** **It** was released\\n...\"],[\"Let\\\\'s see how the models perform on sentence fusion and -splitting.\\n\\n  |Model                  | 10...\"],[\"The first two columns show the performance of the encoder-decoder models\\non the DiscoFuse evaluation...\"],[\"In addition to the 12-layer model variants, the authors also trained and\\nevaluated a 24-layer *Rober...\"],[\"|Model                       |En \\\\\\\\(\\\\to\\\\\\\\) De (BLEU-4)   |De \\\\\\\\(\\\\to\\\\\\\\) En (BLEU-4)\\n  |--------------...\"],[\"Again, we observe a significant performance boost by warm-starting the\\nencoder-part, with *BERT2Rnd*...\"],[\"### Summarization (CNN\\u002fDailymail, BBC XSum, Gigaword)\\n\\nFinally, the encoder-decoder models were eval...\"],[\"Alright, let\\\\'s take a look at the results.\\n\\n  |Model                  |CNN\\u002fDailymail (Rouge-2)   |B...\"],[\"Furthermore, the shared encoder-decoder models are the best performing\\nmodels for summarization. *Ro...\"],[\"-   Next, we noticed that it is often beneficial to share encoder and\\n    decoder weights, especiall...\"],[\"For each of the above tasks, the most performant models were ported to\\nğŸ¤—Transformers and can be acce...\"],[\"------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) To retrieve BLE...\"],[\"In addition, the following list provides a condensed version of this and\\nother notebooks on warm-sta...\"],[\"```\\nLet's start by downloading the *CNN\\u002fDailymail* dataset.\\n\\n```python\\nimport datasets\\ntrain_data = ...\"],[\"```python\\nOUTPUT:\\n-------\\nArticle:...\"],[\"\\\"\\\"\\\"It's official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military...\"],[\"no doubt that the Syrian government was behind it, while Syrian officials have denied responsibility...\"],[\"On Friday night, the president made a last-minute decision to consult lawmakers. What will happen if...\"],[\"lawmaker in Russia -- which has stood by Syria and criticized the United States -- had his own theor...\"],[\"with the rebels used them in an effort to turn global sentiments against it. British intelligence ha...\"],[\"Summary:\\n\\\"\\\"\\\"Syrian official: Obama climbed to the top of the tree, \\\"doesn't know how to get down\\\"\\\\nO...\"],[\"```\\n\\nThe input data seems to consist of short news articles. Interestingly,\\nthe labels appear to be ...\"],[\"```\\n\\nNext, we make use of `.map()` to compute the length of the article and\\nits summary. Since we kn...\"],[\"```\\n\\nHaving computed the length for the first 10000 samples, we should now\\naverage them together. Fo...\"],[\"```\\n\\nWe can see that on average an article contains 848 tokens with *ca.* 3\\u002f4\\nof the articles being ...\"],[\"```python\\nencoder_max_length=512\\ndecoder_max_length=128\\n\\ndef process_data_to_model_inputs(batch):\\n  ...\"],[\"```\\n\\nIn this notebook, we train and evaluate the model just on a few training\\nexamples for demonstra...\"],[\"```\\n\\nSo far, the data was manipulated using Python\\\\'s `List` format. Let\\\\'s\\nconvert the data to PyTo...\"],[\"```\\n\\nIn contrast to other model classes in ğŸ¤—Transformers, the\\n`EncoderDecoderModel` class has two me...\"],[\"```python\\nOUTPUT:\\n-------\\n\\\"\\\"\\\"Some weights of the model checkpoint at bert-base-uncased were not used...\"],[\"Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased ...\"],[\"'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention....\"],[\"'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output...\"],[\"'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.outp...\"],[\"'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.d...\"],[\"'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.v...\"],[\"'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.val...\"],[\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and ...\"],[\"```\\n\\nFor once, we should take a good look at the warning here. We can see\\nthat two weights correspon...\"],[\"```python\\nOUTPUT:\\n-------\\n    EncoderDecoderModel(\\n      (encoder): BertModel(\\n        (embeddings):...\"],[\"(dropout): Dropout(p=0.1, inplace=False)\\n              )\\n            ),\\n\\t\\t\\t\\t\\t\\t...\\n\\t\\t\\t\\t\\t\\t,\\n          ...\"],[\"(position_embeddings): Embedding(512, 768)\\n            (token_type_embeddings): Embedding(2, 768)\\n  ...\"],[\"(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n                    (dropout): Dr...\"],[\"(dropout): Dropout(p=0.1, inplace=False)\\n                  )\\n                  (output): BertSelfOut...\"],[\"```\\n\\nWe see that `bert2bert.encoder` is an instance of `BertModel` and that\\n`bert2bert.decoder` one ...\"],[\"```python\\nOUTPUT:\\n-------\\n    EncoderDecoderConfig {\\n      \\\"_name_or_path\\\": \\\"bert2bert\\\",\\n      \\\"arch...\"],[\"\\\"output_attentions\\\": false,\\n        \\\"output_hidden_states\\\": false,\\n        \\\"pad_token_id\\\": 0,\\n      ...\"],[\"\\\"is_encoder_decoder\\\": false,\\n        \\\"label2id\\\": {\\n          \\\"LABEL_0\\\": 0,\\n          \\\"LABEL_1\\\": 1\\n  ...\"],[\"```\\n\\nThe config is similarly composed of an encoder config and a decoder\\nconfig both of which are in...\"],[\"```\\n\\n```python\\nOUTPUT:\\n-------\\nNum Params. Shared: 137298244, Non-Shared: 247363386\\n```\\n\\nIn this not...\"],[\"```\\n\\nNext, let\\\\'s define all parameters related to beam search decoding.\\nSince `bart-large-cnn` yiel...\"],[\"```\\n\\nThe `Seq2SeqTrainer` extends ğŸ¤—Transformer\\\\'s Trainer for encoder-decoder\\nmodels. In short, it a...\"],[\"```\\n\\nAlso, we need to define a function to correctly compute the ROUGE score\\nduring validation. Sinc...\"],[\"```\\n\\nGreat, now we can pass all arguments to the `Seq2SeqTrainer` and start\\nfinetuning. Executing th...\"],[\"```\\n\\n### **Evaluation**\\n\\nIn a final step, we might want to evaluate the *BERT2BERT* model on the\\ntes...\"],[\"```\\n\\nLet\\\\'s run the map function to obtain the *results* dictionary that has\\nthe model\\\\'s predicted ...\"],[\"--\\ntitle: \\\"Introducing Hugging Face for Education ğŸ¤—\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_education\\u002fthumbnail....\"],[\"ğŸ—£ï¸ Our goal is to make the potential and limitations of machine learning understandable to everyone....\"],[\"## ğŸ¤—Â **Education for Beginners**\\n\\nğŸ—£ï¸ We want to lower the barrier to becoming a machine learning eng...\"],[\"## ğŸ¤—Â **Education for Instructors**\\n\\nğŸ—£ï¸ We want to empower educators with tools and offer collaborati...\"],[\"1ï¸âƒ£Â [A Tour through the Hugging Face Hub](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002feducation-toolkit\\u002fblob\\u002fmain...\"],[\"## ğŸ¤—Â **Education Events & News**\\n\\n- **09\\u002f08**[EVENT]: ML Demo.cratization tour in Argentina at 2pm (...\"],[\"--\\ntitle: \\\"How Hugging Face Accelerated Development of Witty Works Writing Assistant\\\"\\nthumbnail: \\u002fbl...\"],[\"### First experiments \\nWitty Works first chose a basic machine learning approach to build their assi...\"],[\"```\\n\\n### Solutions provided by the [Hugging Face Experts](https:\\u002f\\u002fhuggingface.co\\u002fsupport?utm_source=...\"],[\"```\\n\\nTo fine-tune a vanilla transformers-based classifier, such as a simple BERT model, Witty Works ...\"],[\"```\\n\\nReducing the number of sentences was essential to ensure that model training remained fast and ...\"],[\"```\\n\\nWith the guidance of the Hugging Face experts, Witty Works saved time and money by implementing...\"],[\"--\\ntitle: Inference for PROs\\nthumbnail: \\u002fblog\\u002fassets\\u002finference_pro\\u002fthumbnail.png\\nauthors:\\n  - user: ...\"],[\"## Supported Models\\n\\nIn addition to thousands of public models available in the Hub, PRO users get f...\"],[\"| Model               | Size                                                                        ...\"],[\"| Code Llama Base     | [7B](https:\\u002f\\u002fhuggingface.co\\u002fcodellama\\u002fCodeLlama-7b-hf) and [13B](https:\\u002f\\u002fhug...\"],[\"Inference for PROs makes it easy to experiment and prototype with new models without having to deplo...\"],[\"```\\n\\nWhich would print something like this:\\n\\n```json\\n[\\n  {\\n    \\\"generated_text\\\": \\\"In a surprising tu...\"],[\"```\\n\\nIf you don't want to pass the token explicitly every time you instantiate the client, you can u...\"],[\"```\\n\\nThis example shows the structure of the first message in a multi-turn conversation. Note how th...\"],[\"```\\n\\nThis same format can be used with Code Llama Instruct to engage in technical conversations with...\"],[\"```\\n\\nAs you can see, the format used for infilling follows this pattern:\\n\\n```\\nprompt = f\\\"\\u003cPRE\\u003e {prom...\"],[\"```\\n\\n![SDXL example generation](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"In addition to the sampling parameters above, you can also control general aspects of the generation...\"],[\"output = client.text_generation(\\\"In a surprising turn of events, \\\", do_sample=True)\\nprint(output)...\"],[\"```\\n\\n### Streaming\\n\\nToken streaming is the mode in which the server returns the tokens one by one as...\"],[\"```\\n\\n## Subscribe to PRO\\n\\nYou can sign up today for a PRO subscription [here](https:\\u002f\\u002fhuggingface.co...\"],[\"--\\ntitle: \\\"Towards Encrypted Large Language Models with FHE\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fencrypted-llm\\u002f...\"],[\"## Fully Homomorphic Encryption (FHE) Can Solve LLM Privacy Challenges\\n\\nZamaâ€™s solution to the chall...\"],[\"## Implementation of a LLM layer with FHE\\n\\nNext, youâ€™ll see how to encrypt a single attention head o...\"],[\"This graph shows that 4-bit quantization maintains 96% of the original accuracy. The experiment is d...\"],[\"```\\n\\nThe forward pass is then overwritten so that the first head of the multi-head attention mechani...\"],[\"# Extract the queries, keys and vales\\n        q_qkv = q_qkv.expand_dims(axis=1, key=f\\\"unsqueeze_{sel...\"],[\"```\\n\\nOther computations in the model remain in floating point, non-encrypted and are expected to be ...\"],[\"```\\n\\nRunning this, you will see the following print out: â€œCircuit compiled with 8 bit-widthâ€. This c...\"],[\"Zama libraries [Concrete](https:\\u002f\\u002fgithub.com\\u002fzama-ai\\u002fconcrete) and [Concrete-ML](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"--\\ntitle: \\\"Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny\\\"\\nthumbnail:...\"],[\"## Knowledge Distillation\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingf...\"],[\"In this particular type of knowledge distillation, the student model is trained to do the normal dif...\"],[\"Image taken from the [paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2305.15798)  â€œOn Architectural Compression of Tex...\"],[\"```\\n\\n## Speed in terms of inference latency\\n\\nWe have observed that distilled models are up to 100% f...\"],[\"## LoRA Training\\n\\nOne of the advantages of LoRA training on a distilled model is faster training. Be...\"],[\"--\\ntitle: \\\"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\\\"\\nthumbnail: \\u002fblog\\u002fass...\"],[\"You can read the whole post or jump to the most relevant sections! \\n\\n- If you don't know what the Hu...\"],[\"Spaces make hosting and making your application accessible for others to use much more straightforwa...\"],[\"We can find NER models on the Hub by filtering models by task. In this case, we choose `token-classi...\"],[\"We can make datasets available via the Hugging Face hub in various ways. I'll walk through an exampl...\"],[\"Once you have done this you can choose a name for your new dataset repository. You can also create t...\"],[\"![Example metadata](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fbl...\"],[\"## Why might Galleries, Libraries, Archives and Museums want to use the Hugging Face hub?\\n\\nThere are...\"],[\"### [Nasjonalbiblioteket AI Lab](https:\\u002f\\u002fhuggingface.co\\u002fNbAiLab) \\nThe AI lab at the National Library...\"],[\"## Hub features for Galleries, Libraries, Archives and Museums\\n\\nThe Hub supports many features which...\"],[\"If you require any assistance while using the Hugging Face Hub, there are several avenues you can ex...\"],[\"--\\ntitle: \\\"Putting ethical principles at the core of the research lifecycle\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"## Limitations of this ethical charter\\n\\nThis document is a work in progress and reflects a state of ...\"],[\"## Values for the project\\n\\n- **Be transparent:** We are transparent and open about the intent, sourc...\"],[\"--\\ntitle: \\\"StarCoder: A State-of-the-Art LLM for Code\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f141_starcoder\\u002fstarco...\"],[\"## Evaluation\\n\\nWe thoroughly evaluated StarCoder and several similar models and a variety of benchma...\"],[\"An interesting aspect of StarCoder is that it's multilingual and thus we evaluated it on MultiPL-E w...\"],[\"## About BigCode\\n\\nBigCode is an open scientific collaboration led jointly by Hugging Face and Servic...\"],[\"### Data & Governance\\n- [StarCoderData](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fbigcode\\u002fstarcoderdata): Pret...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 M...\"],[\"In keeping with our core value of *democratization*, we have also spent a lot of time speaking publi...\"],[\"- Comments from [Sasha](https:\\u002f\\u002fhuggingface.co\\u002fsasha) on **AIâ€™s energy use and carbon emissions** ([...\"],[\"[Futurism](https:\\u002f\\u002ffuturism.com\\u002fthe-byte\\u002fai-expert-were-all-going-to-die), [Sky News](https:\\u002f\\u002fwww.yo...\"],[\"- Comments from [Nathan](https:\\u002f\\u002fhuggingface.co\\u002fnatolambert) on the state of the art on **language m...\"],[\"- Comments from [Meg](https:\\u002f\\u002fhuggingface.co\\u002fmeg) on **AI and misinformation** ([CNN](https:\\u002f\\u002fwww.cn...\"],[\"- Comments from [Irene](https:\\u002f\\u002fhuggingface.co\\u002firenesolaiman) on understanding the **regulatory land...\"],[\"- Comments from [Giada](https:\\u002f\\u002fhuggingface.co\\u002fgiadap) on the concepts of **AI â€œsingularityâ€** ([Pop...\"],[\"Some of our talks released this summer include [Giada](https:\\u002f\\u002fhuggingface.co\\u002fgiadap)â€™s [TED present...\"],[\"Of course, we have also made progress on our regular work (our â€œwork workâ€). The fundamental value o...\"],[\"Text-to-Image AI with a participatory, cross-disciplinary approach](https:\\u002f\\u002favidml.org\\u002fevents\\u002ftti202...\"],[\"We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](h...\"],[\"Finally, we have been surprised and delighted by public recognition for many of the society & ethics...\"],[\"--\\ntitle: 'Convert Transformers to ONNX with Hugging Face Optimum'\\nthumbnail: \\u002fblog\\u002fassets\\u002f81_conver...\"],[\"Let's get started! ğŸš€\\n\\n---\\n\\nIf you are interested in optimizing your models to run with maximum effic...\"],[\"â¡ï¸[Learn more about ONNX.](https:\\u002f\\u002fonnx.ai\\u002fabout.html)\\n\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\u003chtml itemsc...\"],[\"[â¡ï¸Â Learn more about Optimum](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fhardware-partners-program)\\n\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdi...\"],[\"- ALBERT\\n- BART\\n- BERT\\n- DistilBERT\\n- ELECTRA\\n- GPT Neo\\n- GPT-J\\n- GPT-2\\n- RoBERTa\\n- T5\\n- ViT\\n- XLM\\n-...\"],[\"```\\n\\nexporting our checkpoint with `export` \\n\\n```python\\nimport torch\\nfrom transformers import AutoMo...\"],[\"```\\n\\nExporting our checkpoint with the `transformers.onnx`.\\n\\n```python\\nfrom pathlib import Path\\nimpo...\"],[\"```\\n\\nExporting our checkpoint with `ORTModelForSequenceClassification`\\n\\n```python\\nfrom optimum.onnxr...\"],[\"```\\n\\nThe best part about the conversion with Optimum is that you can immediately use the `model` to ...\"],[\"--\\ntitle: \\\"Comments on U.S. National AI Research Resource Interim Report\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f92...\"],[\"- Monitor for Open-Source and Open-Science for High Misuse and Malicious Use Potential\\n    - Harm mu...\"],[\"--\\ntitle: \\\"3D Asset Generation: AI for Game Development #3\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games...\"],[\"### The Current State of Text-to-3D\\n\\nAs discussed in [Part 1](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fml-for-gam...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fdeveloper-blogs.nvidia.com\\u002fwp-content\\u002fuploads...\"],[\"The practical use of assets generated using the text-to-NeRF-to-mesh pipeline is limited in a simila...\"],[\"1. Improvements in NeRF-to-mesh and mesh generation. As we've seen, current generation models are si...\"],[\"--\\ntitle: \\\"Opinion Classification with Kili and HuggingFace AutoTrain\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f59_op...\"],[\"## AutoTrain with HuggingFace\\n\\nAutomated Machine Learning is a term for automating a Machine Learnin...\"],[\"Kili is a commercial tool but you can also create a free developer account to try Kiliâ€™s tools. You ...\"],[\"**From the web interface:**\\n\\nFrom the project list page, we create a multi-class text classification...\"],[\"```\\n\\nIn order to access the platform, we need to authenticate our client\\n\\n```python\\nAPI_KEY = os.get...\"],[\"```\\n\\nNow we can start to prepare our interface, the interface is just a dictionary in Python. We wil...\"],[\"for label, color in entity_dict.items():\\n    label_upper = label.strip().upper().replace(' ', '_')\\n ...\"],[\"```\\n\\nWe are ready to upload our data to the project. The `append_many_to_dataset` method can be used...\"],[\"```\\n\\nIt simply imports the given `dataset` DataFrame to a project specified by project_id.\\n\\nWe can s...\"],[\"```\\n\\nIt wasnâ€™t difficult to use the Python API, the helper methods we used covered many difficulties...\"],[\"# The changes should be given as an array that \\n# contains the change for every single sample. \\n# Th...\"],[\"```\\n\\n## Labeling\\n\\nNow that we have the source data uploaded, the platform has a built-in labeling in...\"],[\"def extract_labels(labels_dict):\\n    response = labels_dict[-1]  # pick the latest version of the sa...\"],[\"# we can drop the `labels` column now\\ndf_ns = df_ns.drop(columns=['labels'])\\n\\n# we'll remove the mul...\"],[\"```\\n\\nNice! We now have the labeled data as a csv file. Let's create a dataset repository in HuggingF...\"],[\"![](assets\\u002f59_opinion-classification-with-kili\\u002f9.png)\\n\\nAutoTrain will try different models and selec...\"],[\"Ray tune is a popular library for hyper-parameter optimization which comes with many SOTA algorithms...\"],[\"# progress bar\\nfrom tqdm import tqdm\\n\\n# data manipulation \\u002f reading\\nimport numpy as np\\nimport pandas...\"],[\"```\\n\\nWe will set a seed for the libraries we use for reproducibility\\n\\n```python\\ndef seed_all(seed):\\n...\"],[\"```\\n\\nWe can download the model easily by specifying HuggingFace hub repository. It is also needed to...\"],[\"```\\n\\nI also defined two dictionaries for mapping labels to indices and indices to labels.\\n\\n```python...\"],[\"```\\n\\nAnother utility function that returns stratified and tokenized Torch dataset splits:\\n\\n```python...\"],[\"```\\n\\nNow we can perform the search! Letâ€™s start by processing the data:\\n\\n```python\\ntokenized_train_s...\"],[\"```\\n\\nWe performed the search with 20 and 40 trials respectively, the results are shown below. The we...\"],[\"We use Google Colab for the inference ([here](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1kGYl_YcMmA2gj...\"],[\"We won't do a detailed analysis of the reviews, a basic understanding of potential problems would su...\"],[\"--\\ntitle: \\\"AI for Game Development: Creating a Farming Game in 5 Days. Part 2\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"### The Short Version\\n\\nThe short version is straightforward: ask [ChatGPT](https:\\u002f\\u002fchat.openai.com\\u002fc...\"],[\"**Transformers**, [introduced in 2017](https:\\u002f\\u002fproceedings.neurips.cc\\u002fpaper\\u002f2017\\u002ffile\\u002f3f5ee243547dee...\"],[\"#### Limitations\\n\\nChatGPT often sounds very convincing, while being wrong. Here is an [archive of Ch...\"],[\"--\\ntitle: How to train a new language model from scratch using Transformers and Tokenizers\\nthumbnail...\"],[\"\\u003e N.B. You wonâ€™t need to understand Esperanto to understand this post, but if you do want to learn i...\"],[\"We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same spec...\"],[\"```\\n\\nAnd hereâ€™s a slightly accelerated capture of the output:\\n\\n![tokenizers](assets\\u002f01_how-to-train\\u002f...\"],[\"```\\n\\nWhat is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer...\"],[\"```\\n\\n## 3. Train a language model from scratch\\n\\n**Update:** The associated Colab notebook uses our n...\"],[\"Hereâ€™s a simple version of our EsperantoDataset.\\n\\n```python\\nfrom torch.utils.data import Dataset\\n\\ncl...\"],[\"```\\n\\nIf your dataset is very large, you can opt to load and tokenize examples on the fly, rather tha...\"],[\"```\\n\\nAs usual, pick the largest batch size you can fit on your GPU(s). \\n\\n**ğŸ”¥ğŸ”¥ğŸ”¥ Letâ€™s start training!...\"],[\"# The sun \\u003cmask\\u003e.\\n# =\\u003e\\n\\nresult = fill_mask(\\\"La suno \\u003cmask\\u003e.\\\")\\n\\n# {'score': 0.2526160776615143, 'sequ...\"],[\"```\\n\\nOk, simple syntax\\u002fgrammar works. Letâ€™s try a slightly more interesting prompt:\\n\\n```python\\nfill_...\"],[\"```\\n\\n\\u003e â€œ**Jen la komenco de bela tago**â€, indeed!\\n\\nWith more complex prompts, you can probe whether ...\"],[\"nlp(\\\"Mi estas viro kej estas tago varma.\\\")\\n\\n# {'entity': 'PRON', 'score': 0.9979867339134216, 'word'...\"],[\"```\\n\\n**Looks like it worked! ğŸ”¥**\\n\\n\\u003csmall\\u003eFor a more challenging dataset for NER, \\u003ca href=\\\"https:\\u002f\\u002fgi...\"],[\"--\\ntitle: \\\"Using & Mixing Hugging Face Models with Gradio 2.0\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f22_gradio\\u002fgra...\"],[\"Do you want to customize the demo? You can override any of the default parameters of the [Interface ...\"],[\"--\\ntitle: \\\"Hugging Face and Graphcore partner for IPU-optimized Transformers\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Making Poplar compatible with these widely used, third-party systems allows developers to easily por...\"],[\"The dramatic benchmark results for BERT running on a Graphcore system, compared with a comparable GP...\"],[\"--\\ntitle: \\\"AudioLDM 2, but faster âš¡ï¸\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f161_audioldm2\\u002fthumbnail.png\\nauthors:\\n...\"],[\"Read to the end to find out how to generate a 10 second audio sample in just 1 second!\\n\\n## Model ove...\"],[\"3. A [GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002fgpt2) language model (LM) is ...\"],[\"where the initial latent variable \\\\\\\\(\\\\boldsymbol{z}_{0}\\\\\\\\) is drawn from a normal distribution \\\\\\\\(\\\\m...\"],[\"For full details on how the AudioLDM 2 model is trained, the reader is referred to the [AudioLDM 2 p...\"],[\"Now that we've covered a high-level overview of how the AudioLDM 2 generation process works, let's p...\"],[\"```\\n**Output:**\\n```\\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ...\"],[\"```\\n\\nCool! That run took about 13 seconds to generate. Let's have a listen to the output audio:\\n\\n```...\"],[\"```\\n\\n\\u003caudio controls\\u003e \\n  \\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n**Output:**\\n```\\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200\\u002f200 [00:12\\u003c00:00, 16.60it...\"],[\"```\\n\\n**Output:**\\n\\n```\\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200\\u002f200 [00:09\\u003c00:00, 20.94i...\"],[\"```\\n\\n**Output:**\\n```\\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200\\u002f200 [00:04\\u003c00:00, 48.98it...\"],[\"```\\n\\n**Output:**\\n```\\n[diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler,\\n diffusers....\"],[\"```\\n\\nAlright! We've got a long list of schedulers to choose from ğŸ“. By default, AudioLDM 2 uses the ...\"],[\"```\\n\\n**Output:**\\n```\\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20\\u002f20 [00:00\\u003c00:00, 49.14it\\u002fs...\"],[\"```\\n\\n\\u003caudio controls\\u003e \\n  \\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We've already mentioned that loading the model in float16 half precision gives strong memory savings...\"],[\"```\\n\\n**Output:**\\n```\\n---------------------------------------------------------------------------\\nOut...\"],[\"```\\n\\nUnless you have a GPU with high RAM, the code above probably returned an OOM error. While the A...\"],[\"```\\n\\n**Output:**\\n```\\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20\\u002f20 [00:36\\u003c00:00,  1.82s\\u002fit...\"],[\"--\\ntitle: \\\"Accelerating Stable Diffusion Inference on Intel CPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f136_stable...\"],[\"## The Diffusers library\\n\\nThe [Diffusers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002findex) library makes...\"],[\"```\\nvirtualenv sd_inference\\nsource sd_inference\\u002fbin\\u002factivate\\npip install pip --upgrade\\npip install t...\"],[\"```\\n\\nThe average latency is **32.3 seconds**. As demonstrated by this [Intel Space](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the avera...\"],[\"```\\n\\nWith a static shape, average latency is slashed to **4.7 seconds**, an additional 3.5x speedup....\"],[\"```\\n\\nNext, we install the `libiomp` library to optimize parallel processing. It's part of [Intel Ope...\"],[\"```\\npip install intel_extension_for_pytorch==1.13.100\\n```\\n\\nWe then update our code to optimize each ...\"],[\"```\\n\\nWe also enable the `bloat16` data format to leverage the AMX tile matrix multiply unit (TMMU) a...\"],[\"```\\n\\nWith this final version, inference latency is now down to **5.05 seconds**. Compared to our ini...\"],[\"--\\ntitle: \\\"A Complete Guide to Audio Datasets\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f116_audio_datasets\\u002fthumbnail...\"],[\"## The Hub\\n\\nThe Hugging Face Hub is a platform for hosting models, datasets and demos, all open sour...\"],[\"The Dataset Preview is a brilliant way of experiencing audio datasets before committing to using the...\"],[\"```\\n\\n**Print Output:**\\n```python\\nDatasetDict({\\n    train: Dataset({\\n        features: ['segment_id',...\"],[\"```\\n\\n**Print Output:**\\n```python\\nDataset({\\n    features: ['segment_id', 'speaker', 'text', 'audio', ...\"],[\"```\\n\\n**Print Output:**\\n```python\\n{'segment_id': 'YOU0000000315_S0000660',\\n 'speaker': 'N\\u002fA', \\n 'text...\"],[\"```\\n\\nWe can see that there are a number of features returned by the training split, including `segme...\"],[\"```\\n\\nGreat! We can see that we've got the two required columns `text` and `audio`. The `text` is a s...\"],[\"```\\n\\nRe-loading the first audio sample in the GigaSpeech dataset will resample it to the desired sam...\"],[\"```\\n\\n**Print Output:**\\n\\n```python\\n{'text': \\\"AS THEY'RE LEAVING \\u003cCOMMA\\u003e CAN KASH PULL ZAHRA ASIDE REA...\"],[\"```\\n\\nGreat! Now we can write a function that takes a single training sample and passes it through th...\"],[\"```\\n\\nWe can apply this filtering function to all of our training examples using ğŸ¤— Datasets' [`filter...\"],[\"```\\n\\nAnd with that, we have the GigaSpeech dataset fully prepared for our model! In total, this proc...\"],[\"This is analogous to _downloading_ a TV show versus _streaming_ it. When we download a TV show, we d...\"],[\"There is one caveat to streaming mode. When downloading a dataset, both the raw data and processed d...\"],[\"```\\n\\nAll the steps covered so far in this tutorial can be applied to the streaming dataset without a...\"],[\"| Dataset                                                                                 | Domain  ...\"],[\"| [GigaSpeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fspeechcolab\\u002fgigaspeech)                    | Audioboo...\"],[\"Refer to the [Google Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fsanchit-gandhi\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"```\\n\\n#### [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmozilla-foundation\\u002fcommon_voice_11_0)\\nCommo...\"],[\"```\\n\\n#### [TED-LIUM](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fLIUM\\u002ftedlium)\\nTED-LIUM is a dataset based on En...\"],[\"```\\n\\n#### [Earnings-22](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002frevdotcom\\u002fearnings22)\\nEarnings-22 is a 119-h...\"],[\"```\\n\\n### Multilingual Speech Recognition\\n\\nMultilingual speech recognition refers to speech recogniti...\"],[\"#### [FLEURS](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgoogle\\u002ffleurs)\\nFLEURS (Few-shot Learning Evaluation of...\"],[\"### Audio Classification\\n\\nAudio classification is the task of mapping a raw audio input to a class l...\"],[\"## Closing Remarks\\n\\nIn this blog post, we explored the Hugging Face Hub and experienced the Dataset ...\"],[\"--\\ntitle: The Annotated Diffusion Model\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_annotated-diffusion\\u002fthumbnail.png...\"],[\"We'll go over the original DDPM paper by ([Ho et al., 2020](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2006.11239)), impl...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002fddpm_paper.png\\\" width=\\\"500\\\" \\u002f\\u003e\\n\\u003c...\"],[\"```\\n\\n## What is a diffusion model?\\n\\nA (denoising) diffusion model isn't that complex if you compare ...\"],[\"## In more mathematical form\\n\\nLet's write this down more formally, as ultimately we need a tractable...\"],[\"Recall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a ...\"],[\"Now, if we knew the conditional distribution \\\\\\\\(p(\\\\mathbf{x}_{t-1} | \\\\mathbf{x}_t)\\\\\\\\), then we could...\"],[\"Hence, our neural network needs to learn\\u002frepresent the mean and variance. However, the DDPM authors ...\"],[\"So we continue, assuming that our neural network only needs to learn\\u002frepresent the mean of this cond...\"],[\"A direct consequence of the constructed forward process \\\\\\\\(q\\\\\\\\), as shown by Sohl-Dickstein et al., ...\"],[\"Another beauty of this property, as shown in Ho et al. is that one can (after some math, for which w...\"],[\"Here, \\\\\\\\(\\\\mathbf{x}_0\\\\\\\\) is the initial (real, uncorrupted) image, and we see the direct noise level...\"],[\"What is typically used here is very similar to that of an [Autoencoder](https:\\u002f\\u002fen.wikipedia.org\\u002fwik...\"],[\"```python\\ndef exists(x):\\n    return x is not None\\n\\ndef default(val, d):\\n    if exists(val):\\n        ...\"],[\"```\\n\\n### Position embeddings\\n\\nAs the parameters of the neural network are shared across time (noise ...\"],[\"```\\n\\n### ResNet block\\n\\nNext, we define the core building block of the U-Net model. The DDPM authors ...\"],[\"if exists(scale_shift):\\n            scale, shift = scale_shift\\n            x = x * (scale + 1) + shi...\"],[\"```\\n\\n### Attention module\\n\\nNext, we define the attention module, which the DDPM authors added in bet...\"],[\"def forward(self, x):\\n        b, c, h, w = x.shape\\n        qkv = self.to_qkv(x).chunk(3, dim=1)\\n    ...\"],[\"q = q.softmax(dim=-2)\\n        k = k.softmax(dim=-1)\\n\\n        q = q * self.scale\\n        context = to...\"],[\"```\\n\\n### Group normalization\\n\\nThe DDPM authors interleave the convolutional\\u002fattention layers of the ...\"],[\"```\\n\\n### Conditional U-Net\\n\\nNow that we've defined all building blocks (position embeddings, ResNet ...\"],[\"# determine dimensions\\n        self.channels = channels\\n        self.self_condition = self_condition...\"],[\"mid_dim = dims[-1]\\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\\n  ...\"],[\"t = self.time_mlp(time)\\n\\n        h = []\\n\\n        for block1, block2, attn, downsample in self.downs:...\"],[\"```\\n\\n## Defining the forward diffusion process\\n\\nThe forward diffusion process gradually adds noise t...\"],[\"def sigmoid_beta_schedule(timesteps):\\n    beta_start = 0.0001\\n    beta_end = 0.02\\n    betas = torch....\"],[\"```\\n\\nTo start with, let's use the linear schedule for \\\\\\\\(T=300\\\\\\\\) time steps and define the various ...\"],[\"```\\n\\nWe'll illustrate with a cats image how noise is added at each time step of the diffusion proces...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"```\\n\\nLet's test it on a particular time step:\\n\\n```python\\ndef get_noisy_image(x_start, t):\\n  # add no...\"],[\"```\\n\\n```python\\n# take time step\\nt = torch.tensor([40])\\n\\nget_noisy_image(x_start, t)\\n```\\n\\n\\u003cimg src=\\\"a...\"],[\"```\\n\\n```python\\nplot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])\\n`...\"],[\"```\\n\\nThe `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the tr...\"],[\"```\\n\\nNext, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"Ideally, we end up with an image that looks like it came from the real data distribution.\\n\\nThe code ...\"],[\"```\\n\\nNote that the code above is a simplified version of the original implementation. We found our s...\"],[\"```\\n\\nLet's start training!\\n\\n```python\\nfrom torchvision.utils import save_image\\n\\nepochs = 6\\n\\nfor epoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n\\n## Sampling (inference)\\n\\nTo sample from the model, we can just use our sample function defi...\"],[\"```\\n\\n\\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002foutput.png\\\" width=\\\"300\\\" \\u002f\\u003e\\n\\nSeems like the model is cap...\"],[\"```\\n\\n\\u003cimg src=\\\"\\nassets\\u002f78_annotated-diffusion\\u002fdiffusion-sweater.gif\\\" width=\\\"300\\\" \\u002f\\u003e\\n\\n# Follow-up rea...\"],[\"- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f210...\"],[\"Note that this list only includes important works until the time of writing, which is June 7th, 2022...\"],[\"--\\ntitle: \\\"How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML ro...\"],[\"## Transcription:\\n\\n### Introduction\\n\\nMy name is Swaraj. I'm the CTO and co-founder at Sempre Health....\"],[\"### What surprised you about the Expert Acceleration Program?\\n\\nWe knew what we wanted to get out of ...\"],[\"--\\ntitle: \\\"Databricks â¤ï¸ Hugging Face: up to 40% faster training and tuning of Large Language Models...\"],[\"```swift\\nfrom datasets import load_dataset\\n\\ntrain_df = train.write.parquet(train_dbfs_path, mode=\\\"ov...\"],[\"```\\nNot only was this cumbersome, but it also meant that data had to be written to disk and then rea...\"],[\"```\\nThis allows users to use Spark to efficiently load and transform data for training or fine-tunin...\"],[\"In order to become the best platform for users to jump into the world of AI, weâ€™re working hard to p...\"],[\"--\\ntitle: 'Introducing Snowball Fight â˜ƒï¸, our first ML-Agents environment'\\nthumbnail: \\u002fblog\\u002fassets\\u002f3...\"],[\"## Be part of the conversation: join our discord server!\\n\\nIf you're using ML-Agents or interested in...\"],[\"--\\ntitle: \\\"Deep Learning with Proteins\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f119_deep_learning_with_proteins\\u002ffol...\"],[\"Letâ€™s say that you want to train a DL model to take a sentence in English as input and decide if itâ€™...\"],[\"Now itâ€™s easy - the task is just predicting whether a movie review is positive (1) or negative (0). ...\"],[\"![transfer learning](assets\\u002f119_deep_learning_with_proteins\\u002ftransfer_learning.png)\\n\\n*This figure fro...\"],[\"## Introduction for machine learning people: What the hell is a protein?\\n\\nTo condense an entire degr...\"],[\"In fact, because there are so few amino acids, biologists can assign a unique letter of the alphabet...\"],[\"Also, information is lost if you just consider parts of a protein in isolation, in the same way that...\"],[\"Like a lot of other fields, though, the arrival of deep learning changed everything. AlphaFold and e...\"],[\"The key takeaway, though, is that even though proteins are very different to language, they can be h...\"],[\"If youâ€™re a biologist, on the other hand, you probably have several ideas for what you want to try, ...\"],[\"## Conclusion\\n\\nThe intersection of deep learning and biology is going to be an incredibly active and...\"],[\"--\\ntitle: \\\"Sentence Transformers in the Hugging Face Hub\\\"\\nauthors:\\n- user: osanseviero\\n- user: nreim...\"],[\"```\\n\\nBut not only this. People will probably want to either demo their models or play with other mod...\"],[\"\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fsentence-transformers\\u002fdistilbert-base-nli-ma...\"],[\"data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingface.co&quot;,&quot;model&quot;:{...\"],[\"quot;cardSource&quot;:true,&quot;config&quot;:{&quot;architectures&quot;:[&quot;DistilBertModel&quot...\"],[\"kens&quot;,&quot;private&quot;:false,&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattribut...\"],[\":&quot;special_tokens_map.json&quot;},{&quot;rfilename&quot;:&quot;tokenizer.json&quot;},{&quot;rfil...\"],[\"t;,&quot;pipeline_tag:feature-extraction&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;feature-...\"],[\"Extraction&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;pytorch&quot;,&quo...\"],[\"Transformers&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;transformers&quot;,&q...\"],[\"xmlns=\\\"http:\\u002f\\u002fwww.w3.org\\u002f2000\\u002fsvg\\\" xmlns:xlink=\\\"http:\\u002f\\u002fwww.w3.org\\u002f1999\\u002fxlink\\\" aria-hidden=\\\"true\\\" foc...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"\\u002fdocs\\\"\\u003e\\u003csvg class=\\\"ml-1.5 text-sm text-gray-400 ...\"],[\"meet\\\" viewBox=\\\"0 0 32 32\\\"\\u003e\\u003cpath d=\\\"M27 3H5a2 2 0 0 0-2 2v22a2 2 0 0 0 2 2h22a2 2 0 0 0 2-2V5a2 2 0 0...\"],[\"preserveAspectRatio=\\\"xMidYMid meet\\\" viewBox=\\\"0 0 32 32\\\" style=\\\"transform: rotate(360deg);\\\"\\u003e\\u003cpath d=\\\"...\"],[\"JSON Output\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"flex items-center ml-auto\\\"\\u003e\\u003csvg class=\\\"mr-1\\\" xmlns=\\\"http:\\u002f\\u002fwww.w...\"],[\"But seeing a bunch of numbers might not be very useful to you (unless you're able to understand the ...\"],[\"\\u003c!-- Hackiest hack ever for the draft --\\u003e\\n\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fse...\"],[\"\\u003cdiv class=\\\"p-5 shadow-sm rounded-xl bg-white max-w-md\\\"\\u003e\\u003cdiv class=\\\"SVELTE_HYDRATER \\\"...\"],[\"rounded-xl bg-white max-w-md\\\"\\u003e\\u003cdiv class=\\\"SVELTE_HYDRATER \\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;ht...\"],[\"Similarity&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;pytorch&quot;,&quo...\"],[\"\\\"\\u003e \\u003cdiv class=\\\"font-semibold flex items-center mb-2\\\"\\u003e\\u003cdiv class=\\\"text-lg flex items-center\\\"\\u003e\\u003csvg xml...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"\\u002fdocs\\\"\\u003e\\u003csvg class=\\\"ml-1.5 text-sm text-gray-400 ...\"],[\"role=\\\"img\\\" width=\\\"1em\\\" height=\\\"1em\\\" preserveAspectRatio=\\\"xMidYMid meet\\\" viewBox=\\\"0 0 32 32\\\"\\u003e\\u003cpath d=...\"],[\"Sentence\\u003c\\u002fspan\\u003e \\u003cinput class=\\\"mt-1.5 form-input-alt block w-full \\\" placeholder=\\\"Your sentence here.....\"],[\"height=\\\"1em\\\" preserveAspectRatio=\\\"xMidYMid meet\\\" viewBox=\\\"0 0 32 32\\\" style=\\\"transform: rotate(360deg...\"],[\"JSON Output\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"flex items-center ml-auto\\\"\\u003e\\u003csvg class=\\\"mr-1\\\" xmlns=\\\"http:\\u002f\\u002fwww.w...\"],[\"Of course, on top of the widgets, we also provide API endpoints in our Inference API that you can us...\"],[\"```\\n\\n## Unleashing the Power of Sharing\\n\\nSo why is this powerful? In a matter of minutes, you can sh...\"],[\"```\\n\\nIf you don't have any model in the Hub and want to learn more about Sentence Transformers, head...\"],[\"--\\ntitle: \\\"Running IF with ğŸ§¨ diffusers on a Free Tier Google Colab\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fif\\u002fthumb...\"],[\"IF has two distinct advantages compared to existing text-to-image models\\nlike Stable Diffusion:\\n\\n- T...\"],[\"ğŸ’¡ **Note**: Some of the larger images have been compressed to load faster \\nin the blog format. When ...\"],[\"```\\n\\nrun the login function in a Python shell\\n\\n```py\\nfrom huggingface_hub import login\\n\\nlogin()...\"],[\"```\\n\\nand enter your [Hugging Face Hub access token](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fsecurity-tokens#...\"],[\"Let\\\\'s map out the size of IF\\\\'s model components in full float32\\nprecision:\\n\\n- [T5-XXL Text Encoder...\"],[\"Let\\\\'s give it a try ğŸš€\\n\\n![t2i_64](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fr...\"],[\"```\\n\\n```bash\\nMemTotal:       13297192 kB\\n```\\n\\nAnd an NVIDIA T4 with 15 GB VRAM:\\n\\n``` python\\n!nvidia-...\"],[\"```bash\\nSun Apr 23 23:14:19 2023       \\n+-----------------------------------------------------------...\"],[\"+-----------------------------------------------------------------------------+\\n| Processes:        ...\"],[\"```\\n\\n## Install dependencies\\n\\nSome optimizations can require up-to-date versions of dependencies. If...\"],[\"```\\n\\n## 1. Text-to-image generation\\n\\nWe will walk step by step through text-to-image generation with...\"],[\"```\\n\\n### 1.2 Create text embeddings\\n\\nThe Diffusers API for accessing diffusion models is the\\n`Diffus...\"],[\"```\\n\\nDeleting the python object is not enough to free the GPU memory.\\nGarbage collection is when the...\"],[\"```\\n\\nLet\\\\'s manually convert the raw tensors to PIL and have a sneak peek at\\nthe final result. The o...\"],[\"```\\n\\n![t2i_upscaled](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fd...\"],[\"```\\n\\nView output image\\n\\n``` python\\npil_image[0]\\n```\\n\\n![t2i_upscaled_2](https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nand load it into a PIL Image\\n\\n``` python\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\noriginal...\"],[\"```\\n\\nFor image variation, we load the checkpoint with\\n[`IFImg2ImgPipeline`](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"```\\n\\nThe image variation pipeline requires both the original image and the\\nprompt embeddings.\\n\\nWe ca...\"],[\"```\\nğŸ’¡ **Note**: The image variation super resolution pipeline requires the\\ngenerated image as well a...\"],[\"```\\n\\n![inpainting_sample](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fm...\"],[\"```\\n\\n![masking_by_hand](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmai...\"],[\"```\\n\\nNow, we need to pass the input image, the mask image, and the prompt\\nembeddings.\\n\\n``` python\\nim...\"],[\"```\\n\\n![inpainted_final_output](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"--\\ntitle: \\\"Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI ...\"],[\"All of this will only happen with standardization and automation. Organizations can't afford to buil...\"],[\"IBM decided that open source should be at the core of watsonx.ai. We couldn't agree more! Built on [...\"],[\"Our joint team is hard at work at the moment. We can't wait to show you what we've been up to! The m...\"],[\"--\\ntitle: The State of Computer Vision at Hugging Face ğŸ¤—\\nthumbnail: \\u002fblog\\u002fassets\\u002fcv_state\\u002fthumbnail....\"],[\"Each of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furtherm...\"],[\"## Support for Pipelines\\n\\nWe developed [Pipelines](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002f...\"],[\"```\\n\\n\\u003cdiv align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-im...\"],[\"```\\n\\n## Training your own models\\n\\nWhile being able to use a model for off-the-shelf inference is a g...\"],[\"[Hugging Face example scripts](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples) inclu...\"],[\"## Integrations with Datasets\\n\\n[Datasets](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets) provides easy access...\"],[\"```\\n\\nBesides these datasets, we provide integration support with augmentation libraries like [albume...\"],[\"```\\n\\n\\u003cdiv align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"## Spaces for computer vision demos\\n\\nWith [Spaces](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fspaces-overview),...\"],[\"## ğŸ¤— AutoTrain\\n\\n[AutoTrain](https:\\u002f\\u002fhuggingface.co\\u002fautotrain) provides a â€œno-codeâ€ solution to train...\"],[\"Unlike tokenizers, we have preprocessors (such as [this](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmo...\"],[\"```\\n\\nEven for a difficult task like object detection, the user experience doesnâ€™t change very much:\\n...\"],[\"```\\n\\n## Zero-shot models for vision\\n\\nThereâ€™s been a surge of models that reformulate core vision tas...\"],[\"Thereâ€™s been a surge of models that reformulate core vision tasks like segmentation and detection in...\"],[\"The community can expect to see more zero-shot models for computer vision being supported from ğŸ¤—Tran...\"],[\"As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! ğŸ¤—\\n\\n*A...\"],[\"--\\ntitle: \\\"Fine-tuning Stable Diffusion models on Intel CPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fstable-diffusi...\"],[\"Let's get started.\\n\\n## Setting up the cluster\\n\\nOur friends at [Intel](https:\\u002f\\u002fhuggingface.co\\u002fintel) ...\"],[\"```\\nArchitecture:            x86_64\\n  CPU op-mode(s):        32-bit, 64-bit\\n  Address sizes:        ...\"],[\"Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush...\"],[\"avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme av...\"],[\"```\\n\\nLet's first list the IP addresses of our servers in `nodefile.` The first line refers to the pr...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers.git\\ncd diffusers\\npip install .\\n```\\n\\nNext, we ...\"],[\"```\\nmkdir \\u002fhome\\u002fdevcloud\\u002fdicoo\\ncd \\u002fhome\\u002fdevcloud\\u002fdicoo\\nwget https:\\u002f\\u002fhuggingface.co\\u002fsd-concepts-libra...\"],[\"```\\n\\nHere are the images:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fsd-concepts-library\\u002fdicoo\\u002fresolve\\u002fmain\\u002fc...\"],[\"```\\nexport I_MPI_HYDRA_IFACE=ens786f1\\noneccl_bindings_for_pytorch_path=$(python -c \\\"from oneccl_bind...\"],[\"```\\nmpirun -f nodefile -n 16 -ppn 4                                                         \\\\\\naccele...\"],[\"```\\npython diffusers\\u002fexamples\\u002ftextual_inversion\\u002ftextual_inversion.py                        \\\\\\n--pret...\"],[\"```\\npip install optimum[openvino]\\n```\\n\\nHere, we load the model, optimize it for a static shape, and ...\"],[\"```\\n\\nHere's a generated image. It is impressive that the model only needed five images to learn that...\"],[\"If you have questions or feedback, we'd love to read them on the [Hugging Face forum](https:\\u002f\\u002fdiscus...\"],[\"--\\ntitle: \\\"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"Start by creating a new HTML file, if you don't have one already. Importing the JavaScript and CSS c...\"],[\"```\\n\\nNote that you should generally use the latest version of `@gradio\\u002flite` that is available. You ...\"],[\"```\\n\\n### 3. Write your Gradio app inside of the tags\\n\\nNow, write your Gradio app as you would normal...\"],[\"```\\n\\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio ...\"],[\"```\\n\\n### Additional Requirements\\n\\nIf your Gradio app has additional requirements, it is usually poss...\"],[\"```\\n\\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https:\\u002f\\u002fhu...\"],[\"## Try it out!\\n\\nYou can immediately try out `@gradio\\u002flite` by copying and pasting this code in a loc...\"],[\"```\\n\\n\\nWe've also created a playground on the Gradio website that allows you to interactively edit co...\"],[\"--\\ntitle: \\\"BERT 101 - State Of The Art NLP Model Explained\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f52_bert_101\\u002fthum...\"],[\"In this guide, you'll learn what BERT is, why itâ€™s different, and how to get started using BERT:\\n\\n1....\"],[\"**There are many more language\\u002fNLP tasks + more detail behind each of these.**\\n\\n***Fun Fact:*** You ...\"],[\"**Note:** Demand for smaller BERT models is increasing in order to use BERT within smaller computati...\"],[\"\\u003cdiv class=\\\"bg-white pb-1\\\"\\u003e...\"],[\"\\u003cdiv class=\\\"SVELTE_HYDRATER contents\\\"...\"],[\"data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingface.co&quot;,&quot;apiToken&quot...\"],[\"sts&quot;:true,&quot;config&quot;:{&quot;architectures&quot;:[&quot;BertForMaskedLM&quot;],&quot;mod...\"],[\"error, can&#39;t generate link to Papers With...\"],[\"Code.&quot;},&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattributes&quot;},{&quot;rfilena...\"],[\"&quot;tokenizer_config.json&quot;},{&quot;rfilename&quot;:&quot;vocab.txt&quot;}],&quot;tags&quot;:[...\"],[\"objs&quot;:[{&quot;id&quot;:&quot;fill-mask&quot;,&quot;label&quot;:&quot;Fill-Mask&quot;,&quot;subT...\"],[\"t;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;rust&quot;,&quot;label&quot;:&quot;Rust&quot...\"],[\"ipedia&quot;,&quot;type&quot;:&quot;dataset&quot;},{&quot;id&quot;:&quot;en&quot;,&quot;label&quot;:...\"],[\"uot;bert&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;exbert&quot;,&quot;label&qu...\"],[\"Compatible&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;infinity_compatible&quot;...\"],[\"\\u003cdiv class=\\\"flex flex-col w-full max-w-full\\\"\\u003e\\n            \\u003cdiv class=\\\"font-semibold flex items-cente...\"],[\"\\u003cpath d=\\\"M16 8a1.5 1.5 0 1 0 1.5 1.5A1.5 1.5 0 0 0 16 8z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\n              ...\"],[\"\\u003cpath d=\\\"M5.8375 8.41246H4.75V6.23746C4.75029 5.94913 4.86496 5.67269 5.06884 5.4688C5.27272 5.26492...\"],[\"\\u003cpath d=\\\"M15.625 5.14998H13.45V2.97498C13.4497 2.68665 13.335 2.4102 13.1312 2.20632C12.9273 2.00244...\"],[\"\\u003c\\u002fsvg\\u003e\\n                         \\u003cspan\\u003eFill-Mask\\u003c\\u002fspan\\u003e\\n                     \\u003c\\u002fdiv\\u003e\\n                \\u003c...\"],[\"\\u003c\\u002flabel\\u003e\\n            \\u003cbutton class=\\\"btn-widget w-24 h-10 px-5 mt-2\\\" type=\\\"submit\\\"\\u003eCompute\\u003c\\u002fbutton\\u003e\\n ...\"],[\"\\u003c\\u002fsvg\\u003e\\n                JSON Output\\n            \\u003c\\u002fbutton\\u003e\\n            \\u003cbutton class=\\\"flex items-cente...\"],[\"**Fun Fact:** Masking has been around a long time - [1953 Paper on Cloze procedure (or â€˜Maskingâ€™)](h...\"],[\"Timeline of popular Transformer model releases:\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e...\"],[\"Fun Fact: Google has been using your reCAPTCHA selections to label training data since 2011. The ent...\"],[\"ML Architecture Glossary:\\n\\n| ML Architecture Parts | Definition                                     ...\"],[\"Hereâ€™s how many of the above ML architecture parts BERTbase and BERTlarge has:\\n\\n\\n|           | Trans...\"],[\"#### 4.2 SWAG\\n[SWAG](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fswag) (Situations With Adversarial Generations)...\"],[\"While some of these tasks may seem irrelevant and banal, itâ€™s important to note that these evaluatio...\"],[\"BERT models pre-trained for specific tasks:\\n\\n-   [Twitter sentiment analysis](https:\\u002f\\u002fhuggingface.co...\"],[\"```\\n\\n### 7.2 Try out BERT\\n\\nFeel free to swap out the sentence below for one of your own. However, le...\"],[\"```\\n\\nWhen you run the above code you should see an output that looks something like:\\n\\n```python\\n[{'s...\"],[\"```\\n\\nYou should see an output that looks something like:\\n```python\\n[{'score': 0.21981535851955414,\\n ...\"],[\"```\\n\\nBERT predicted the woman's job to be a Nurse, Waitress, Maid, Prostitute, or Cook displaying a ...\"],[\"\\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fQuestion\\\"\\u003e\\n    \\u003ch3 itemprop=\\\"name\\\"...\"],[\"\\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fQuestion\\\"\\u003e\\n...\"],[\"\\u003c\\u002fol\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\u003c\\u002fhtml\\u003e...\"],[\"## 9. Conclusion\\n\\nBERT is a highly complex and advanced language model that helps people automate la...\"],[\"--\\ntitle: \\\"We Raised $100 Million for Open & Collaborative Machine Learning ğŸš€\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"Over 10,000 companies are now using Hugging Face to build technology with machine learning. Their Ma...\"],[\"It's been a hell of a ride to grow from 30 to 120+ team members in the past 12 months. We were super...\"],[\"--\\ntitle: \\\"Efficient Table Pre-training without Real Data: An Introduction to TAPEX\\\"\\nthumbnail: \\u002fblo...\"],[\"![snippet](assets\\u002f74_tapex\\u002ftapex-overview.png)\\n\\u003e Note: [Table] is a placeholder for the user provide...\"],[\"data = {\\n    \\\"year\\\": [1896, 1900, 1904, 2004, 2008, 2012],\\n    \\\"city\\\": [\\\"athens\\\", \\\"paris\\\", \\\"st. loui...\"],[\"```\\n\\n### Fine-tuning\\n\\nDuring fine-tuning, we feed the concatenation of the natural language question...\"],[\"\\u003cdiv class=\\\"bg-white pb-1\\\"\\u003e\\u003cdiv class=\\\"SVELTE_HYDRATER contents\\\"...\"],[\"contents\\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingface.co&quot;,&quot;mod...\"],[\"error, can't generate link to Papers With Code.&quot;},&quot;tags&quot;:[&quot;pytorch&quot;,&quot;b...\"],[\"Answering&quot;,&quot;subType&quot;:&quot;nlp&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quo...\"],[\"107.07653&quot;,&quot;label&quot;:&quot;arxiv:2107.07653&quot;,&quot;type&quot;:&quot;arxiv&quot;},{...\"],[\"quot;:&quot;tapex&quot;,&quot;label&quot;:&quot;tapex&quot;,&quot;type&quot;:&quot;other&quot;},{&qu...\"],[\"Compatible&quot;,&quot;type&quot;:&quot;other&quot;}],&quot;transformersInfo&quot;:{&quot;auto_model...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"https:\\u002f\\u002fapi-inference.huggingface.co\\u002f\\\"\\u003e\\u003csvg clas...\"],[\"focusable=\\\"false\\\" role=\\\"img\\\" width=\\\"1em\\\" height=\\\"1em\\\" preserveAspectRatio=\\\"xMidYMid meet\\\" viewBox=\\\"0...\"],[\"13.3062V16.025H6.0375V13.3062H10.3875ZM4.95 12.2187H2.775V9.49998H4.95V12.2187ZM10.3875 5.69373V8.41...\"],[\"false...\"],[\"false\\\"\\u003e\\u003cdiv class=\\\"inline-flex justify-between w-32 lg:w-44 rounded-md border border-gray-100 px-4 p...\"],[\"h-6\\\"\\u003eStars \\u003c\\u002fth\\u003e\\u003cth contenteditable=\\\"true\\\" class=\\\"border-2 border-gray-100 h-6\\\"\\u003eContributors \\u003c\\u002fth\\u003e\\u003ct...\"],[\"contenteditable=\\\"\\\"\\u003e34\\u003c\\u002ftd\\u003e\\u003ctd class=\\\"border-gray-100 border-2 h-6\\\" contenteditable=\\\"\\\"\\u003eRust, Python a...\"],[\"Add row\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"btn-widget flex-1 lg:flex-none mt-2 lg:mr-1.5\\\" type=\\\"button\\\"\\u003e\\u003csvg cl...\"],[\"Add col\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"btn-widget flex-1 mt-2 lg:flex-none lg:ml-auto\\\" type=\\\"button\\\"\\u003eReset ...\"],[\"### Experiments\\n\\nWe evaluate TAPEX on four benchmark datasets, including [WikiSQL (Weak)](https:\\u002f\\u002fhu...\"],[\"Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a ...\"],[\"![comparsion](assets\\u002f74_tapex\\u002fcomparsion-tapex.png)\\n\\nWe believe the SQL execution task is closer to ...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #4: Bias in Text-to-Image Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"For example, if the training data are mainly in English they probably convey rather Western values. ...\"],[\"## Sources of Bias\\n\\nRecent years have seen much important research on bias detection in AI systems w...\"],[\"**Biases in training data:** Popular multimodal datasets such as [LAION-5B](https:\\u002f\\u002flaion.ai\\u002fblog\\u002fla...\"],[\"**Biases in pre-training data filtering:** There is often some form of filtering carried out on data...\"],[\"**Biases in post-hoc filtering:** Many image generation models come with built-in safety filters tha...\"],[\"Other tools, like the [Face Clustering tool](https:\\u002f\\u002fhf.co\\u002fspaces\\u002fsociety-ethics\\u002fDiffusionFaceCluste...\"],[\"**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https:\\u002f...\"],[\"Also, as we have mentioned in a [previous newsletter](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fethics-soc-2#addre...\"],[\"## Other updates\\n\\nWe are also continuing work on other fronts of ethics and society, including:\\n\\n- *...\"],[\"--\\ntitle: \\\"Active Learning with AutoNLP and Prodigy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f43_autonlp_prodigy\\u002fthum...\"],[\"## Prodigy\\n\\n[Prodigy](https:\\u002f\\u002fprodi.gy\\u002f) is an annotation tool developed by Explosion (the makers of...\"],[\"\\u003cimg src=\\\"assets\\u002f43_autonlp_prodigy\\u002fautonlp_create_project.png\\\"\\u003e\\n\\nStep 3: Upload the training datase...\"],[\"Once you have Prodigy installed, you can simply run:\\n\\n    $ prodigy ner.manual bbc blank:en BBC_News...\"],[\"dataset = []\\n\\nfor doc, eg in nlp.pipe(examples, as_tuples=True):\\n    try:\\n        doc.ents = [doc.ch...\"],[\"```\\n\\nThis will provide us with a `JSONL` file which can be used for training a model using AutoNLP. ...\"],[\"Let's take a look at how this model performs on the same unseen sample.\\n\\n\\u003cimg src=\\\"assets\\u002f43_autonlp...\"],[\"We have open-sourced the best model created using this process. You can try it [here](https:\\u002f\\u002fhuggin...\"],[\"--\\ntitle: \\\"AI Policy @ğŸ¤—: Open ML Considerations in the EU AI Act\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002feu_ai_act_...\"],[\"Hugging Face is where it is today thanks to its community of developers, so weâ€™ve seen firsthand wha...\"],[\"--\\ntitle: \\\"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\\\"\\nthu...\"],[\"In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published...\"],[\"With Optimum Intel, you can apply state-of-the-art optimization techniques to your Transformers with...\"],[\"```\\npip -q uninstall torch -y \\npip -q install torch==1.11.0+cpu --extra-index-url https:\\u002f\\u002fdownload.p...\"],[\"```\\n\\nWe then set up the quantization job using a [configuration]. You can find details on this confi...\"],[\"```\\n\\nThe log tells us that Optimum Intel has quantized 38 ```Linear``` and 2 ```Embedding``` operato...\"],[\"```\\n# Original model\\n\\nTransformerBlock(\\n  (attention): MultiHeadSelfAttention(\\n    (dropout): Dropou...\"],[\"```\\n\\n```\\n# Quantized model\\n\\nTransformerBlock(\\n  (attention): MultiHeadSelfAttention(\\n    (dropout): ...\"],[\"```\\n\\nVery well, but how does this impact accuracy and prediction time?\\n\\nBefore and after each quanti...\"],[\"```\\nfrom optimum.intel.neural_compressor import INCModelForSequenceClassification\\n\\ninc_model = INCMo...\"],[\"--\\ntitle: \\\"CO2 Emissions and the ğŸ¤— Hub: Leading the Charge\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f60_carbon_emissi...\"],[\"```\\n\\nThere were quite a few! This also helps to find smaller models, given they typically did not re...\"],[\"```\\n\\nThat's a lot of CO2!\\n\\nAs you can see, in just a few lines of code we can quickly vet models we ...\"],[\"```\\n\\n...you'll be left with a file within the `codecarbon-text-classification` directory called `emi...\"],[\"--\\ntitle: \\\"What's going on with the Open LLM Leaderboard?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fevaluating-mmlu-l...\"],[\"Ready? Then buckle up, weâ€™re taking off ğŸš€.\\n\\n## What's the Open LLM Leaderboard?\\n\\nFirst, note that th...\"],[\"When diving further, we found yet another interesting implementation for evaluating on the very same...\"],[\"You can find the full evaluation numbers at the end of the post.\\n\\nThese different implementations of...\"],[\"```\\nQuestion: Glucose is transported into the muscle cell:\\n\\n\\nChoices:\\nA. via protein transporters ca...\"],[\"```\\n\\nNote: you can very easily explore more of this dataset [in the dataset viewer](https:\\u002f\\u002fhuggingf...\"],[\"\\u003cdiv\\u003e\\n\\u003ctable\\u003e\\u003cp\\u003e\\n  \\u003ctbody\\u003e\\n \\u003ctr style=\\\"text-align: left;\\\"\\u003e\\n  \\u003ctd\\u003eOriginal implementation \\u003ca href=\\\"ht...\"],[\"Answer:\\n\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003eQuestion: How did the 2008 financial crisis affect America's international rep...\"],[\"The differences between them can seem small, did you spot them all? Here they are:\\n- First sentence,...\"],[\"Here, the model has one example of the expected behavior and is thus less likely to predict answers ...\"],[\"![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fevaluating...\"],[\"Here is a table summary of the answers provided and generated by the model to summarize what weâ€™ve s...\"],[\"Weâ€™ve covered them all!\\n\\nNow letâ€™s compare the model scores on these three possible ways to evaluate...\"],[\"We can see that for the same dataset, both absolute scores and model rankings (see the first figure)...\"],[\"This is why open, standardized, and reproducible benchmarks such as the [EleutherAI Eval Harness](ht...\"],[\"## Reproducibility hashes:\\nHere are the commit hashes of the various code implementations used in th...\"],[\"--\\ntitle: \\\"The Technology Behind BLOOM Training\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f86_bloom_megatron_deepspeed...\"],[\"This article focuses specifically on the engineering side of the training of the model. The most imp...\"],[\"## Overview\\n\\nBLOOM's architecture is very similar to [GPT3](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fGPT-3) wit...\"],[\"The training of the 176B BLOOM model occurred over Mar-Jul 2022 and took about 3.5 months to complet...\"],[\"Please note that both Megatron-LM and DeepSpeed have Pipeline Parallelism and BF16 Optimizer impleme...\"],[\"## Data Parallelism\\n\\nMost users with just a few GPUs are likely to be familiar with `DistributedData...\"],[\"The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear ...\"],[\"Here `f` is an identity operator in the forward pass and all reduce in the backward pass while `g` i...\"],[\"```\\nwe just sliced it in 2 vertically, placing layers 0-3 onto GPU0 and 4-7 to GPU1.\\n\\nNow while data...\"],[\"![mp-pp](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fparallelism-g...\"],[\"To calculate the global batch size of the DP + PP setup we then do: `mbs*chunks*dp_degree` (`8*32*4=...\"],[\"While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-Deep...\"],[\"## DP+PP+TP\\n\\nTo get an even more efficient training PP is combined with TP and DP which is called 3D...\"],[\"ZeRO stage 3 can also be used to train models at this scale, however, it requires more communication...\"],[\"So back in January as we knew we would be training on A100s which support the BF16 format Olatunji R...\"],[\"All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no...\"],[\"If we were to fuse these two operations, i.e. put them into a single \\\"fused kernel\\\", and just launch...\"],[\"## Datasets\\n\\nAnother important feature from Megatron-LM is the efficient data loader. During start u...\"],[\"## Training Difficulties\\n\\nWith the architecture, hardware and software in place we were able to star...\"],[\"One other issue was that SLURM wasn't designed to be used by a team of people. A SLURM job is owned ...\"],[\"Training large language models is still a challenging task, but we hope by building and sharing this...\"],[\"Joint Megatron-LM and Deepspeeed:\\n\\n- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B...\"],[\"--\\ntitle: \\\"Happy 1st anniversary ğŸ¤— Diffusers!\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusers-turns-1\\u002fdiffusers-...\"],[\"**Table of Contents**\\n\\n* [Striving for photorealism](#striving-for-photorealism)\\n* [Video pipelines]...\"],[\"Head over to the DeepFloyd IF [docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fv0.18.2\\u002fen\\u002fapi\\u002fpipelines\\u002f...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"## Image editing pipelines\\n\\nImage editing is one of the most practical use cases in fashion, materia...\"],[\"To learn more about how we optimize inference with ğŸ¤—Â Diffusers, check out the [docs](https:\\u002f\\u002fhugging...\"],[\"## Support for LoRA\\n\\nFine-tuning diffusion models is expensive and out of reach for most consumer GP...\"],[\"\\u003cdiv class=\\\"mx-auto max-w-screen-xl py-8\\\"\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-inside-avoid\\\"\\u003e\\n    \\u003cblockquot...\"],[\"\\u003c\\u002fblockquote\\u003e\\n    \\u003cdiv class=\\\"flex items-center gap-4\\\"\\u003e\\n      \\u003cimg src=\\\"https:\\u002f\\u002favatars.githubuserco...\"],[\"\\u003cblockquote class=\\\"rounded-xl !mb-0 bg-gray-50 p-6 shadow dark:bg-gray-800\\\"\\u003e\\n      \\u003cp class=\\\"leading...\"],[\"\\u003cblockquote class=\\\"rounded-xl !mb-0 bg-gray-50 p-6 shadow dark:bg-gray-800\\\"\\u003e\\n      \\u003cp class=\\\"leading...\"],[\"\\u003cp class=\\\"font-medium\\\"\\u003eTianhe Ren\\u003c\\u002fp\\u003e\\n      \\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-i...\"],[\"\\u003cdiv class=\\\"text-sm\\\"\\u003e\\n        \\u003cp class=\\\"font-medium\\\"\\u003emmagic\\u003c\\u002fp\\u003e\\n      \\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003c...\"],[\"We also collaborated with Google Cloud (who generously provided the compute) to provide technical gu...\"],[\"Finally, we were delighted to receive contributions to our codebase from over 300 contributors, whic...\"],[\"Besides these, a heartfelt shoutout to the following contributors who helped us ship some of the mos...\"],[\"## Building products with ğŸ¤— Diffusers\\n\\nOver the last year, we also saw many companies choosing to bu...\"],[\"## Looking forward\\n\\nAs we celebrate our first anniversary, we're grateful to our community and open-...\"],[\"--\\ntitle: \\\"Getting Started with Hugging Face Transformers for IPUs with Optimum\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"### Getting started with IPUs and Optimum\\n\\nLetâ€™s use BERT as an example to help you get started with...\"],[\"```\\n$ cd \\u002fopt\\u002fgc\\u002fpoplar_sdk-ubuntu_18_04-2.3.0+774-b47c577c2a\\u002f\\n$ source poplar-ubuntu_18_04-2.3.0+77...\"],[\"```\\n(poptorch_env) user@host:~\\u002fworkspace\\u002fpoptorch_env$ pip3 install optimum[graphcore] optuna\\n```\\n\\n#...\"],[\"```\\n$ python3 run_qa.py \\\\\\n\\t--ipu_config_name=.\\u002f \\\\\\n\\t--model_name_or_path bert-base-uncased \\\\\\n\\t--datas...\"],[\"```\\n\\n### A closer look at Optimum-Graphcore\\n \\n#### Getting the data\\n \\nA very simple way to get datas...\"],[\"```\\n\\nThe argument ```--model_name_or_path==bert-base-uncased`` loads the [bert-base-uncased](https:\\u002f...\"],[\"```\\n  \\nYou can see the rest of the IPU BERT implementation in the [Optimum-Graphcore: SQuAD Examples...\"],[\"--\\ntitle: Deprecation of Git Authentication using password\\nthumbnail: \\u002fblog\\u002fassets\\u002fpassword-git-depr...\"],[\"```\\nwhere `\\u003crepo_path\\u003e` is in the form of:\\n- `\\u003cuser_name\\u003e\\u002f\\u003crepo_name\\u003e` for models\\n- `datasets\\u002f\\u003cuser_...\"],[\"--\\ntitle: \\\"Finetune Stable Diffusion Models with DDPO via TRL\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f166_trl_ddpo...\"],[\"## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to ...\"],[\"If youâ€™re interested in learning more details about DDPO, we encourage you to check out the [origina...\"],[\"To get on with our venture to get a diffusion model to output images more in line with the human per...\"],[\"```\\n\\nThis should get the main library installed. The following dependencies are for tracking and ima...\"],[\"```\\n\\nThe following table contains key hyperparameters that are directly correlated with positive res...\"],[\"## Lessons learned\\n\\n1. The results seem to generalize over a wide variety of prompts despite the min...\"],[\"The following are pre-finetuned (left) and post-finetuned (right) outputs for the prompts `bear`, `h...\"],[\"## Conclusion\\n\\nDiffusion models like Stable Diffusion, when fine-tuned using DDPO, can offer signifi...\"],[\"--\\ntitle: \\\"Fine-Tune Whisper For Multilingual ASR with ğŸ¤— Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f111_...\"],[\"## Introduction\\n\\nWhisper is a pre-trained model for automatic speech recognition (ASR) \\npublished in...\"],[\"When scaled to 680,000 hours of labelled pre-training data, Whisper models \\ndemonstrate a strong abi...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"assets\\u002f111_fine_tune_whisper\\u002fwhisper_architecture.svg\\\" alt=\\\"Trulli\\\" style=\\\"width:...\"],[\"The Whisper checkpoints come in five configurations of varying model sizes.\\nThe smallest four are tr...\"],[\"| Size   | Layers | Width | Heads | Parameters | English-only                                       ...\"],[\"For demonstration purposes, we'll fine-tune the multilingual version of the \\n[`small`](https:\\u002f\\u002fhuggi...\"],[\"```\\n\\nWe strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https:\\u002f\\u002fhugg...\"],[\"```\\n\\n### Load Dataset\\n\\nCommon Voice is a series of crowd-sourced datasets where speakers \\nrecord tex...\"],[\"Using ğŸ¤— Datasets, downloading and preparing data is extremely simple. \\nWe can download and prepare t...\"],[\"```\\n\\n**Print Output:**\\n```\\nDatasetDict({\\n    train: Dataset({\\n        features: ['client_id', 'path'...\"],[\"```\\n\\nCommon Voice is but one multilingual ASR dataset that we can download from the Hub - \\nthere are...\"],[\"It is crucial that we match the sampling rate of our audio inputs to the sampling\\nrate expected by o...\"],[\"The Mel channels (frequency bins) are standard in speech processing and chosen to approximate\\nthe hu...\"],[\"```\\n\\n### Load WhisperTokenizer\\n\\nNow let's look at how to load a Whisper tokenizer. The Whisper model...\"],[\"```\\n\\n\\u003e **Tip:** the blog post can be adapted for *speech translation* by setting the task to `\\\"trans...\"],[\"```\\n**Print Output:**\\n```bash\\nInput:                 à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ ...\"],[\"```\\n\\n### Prepare Data\\nLet's print the first example of the Common Voice dataset to see \\nwhat form th...\"],[\"```\\nWe can see that we've got a 1-dimensional input audio array and the \\ncorresponding target transc...\"],[\"```\\n\\nRe-loading the first audio sample in the Common Voice dataset will resample \\nit to the desired ...\"],[\"```\\nGreat! We can see that the sampling rate has been downsampled to 16kHz. The \\narray values are al...\"],[\"```\\n\\nAlright! With that we have our data fully prepared for training! \\nLet's continue and take a loo...\"],[\"The `input_features` are already padded to 30s and converted to a log-Mel spectrogram \\nof fixed dime...\"],[\"# replace padding with -100 to ignore loss correctly\\n        labels = labels_batch[\\\"input_ids\\\"].mask...\"],[\"```\\n\\nLet's initialise the data collator we've just defined:\\n\\n```python\\ndata_collator = DataCollatorS...\"],[\"```\\n\\n### Load a Pre-Trained Checkpoint\\n\\nNow let's load the pre-trained Whisper `small` checkpoint. A...\"],[\"```\\n\\n### Define the Training Arguments\\nIn the final step, we define all the parameters related to tr...\"],[\"```python\\nfrom transformers import Seq2SeqTrainingArguments\\n\\ntraining_args = Seq2SeqTrainingArgument...\"],[\"```\\n\\n**Note**: if one does not want to upload the model checkpoints to the Hub, \\nset `push_to_hub=Fa...\"],[\"```\\n\\nTraining will take approximately 5-10 hours depending on your GPU or the one \\nallocated to the ...\"],[\"Our fine-tuned model significantly improves upon the zero-shot performance of the Whisper \\n`small` c...\"],[\"```\\n\\nThe training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` comman...\"],[\"```\\n\\nWhile the fine-tuned model yields satisfactory results on the Common \\nVoice Hindi test data, it...\"],[\"```\\n\\n## Closing Remarks\\n\\nIn this blog, we covered a step-by-step guide on fine-tuning Whisper for mu...\"],[\"--\\ntitle: \\\"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\\\" \\nthumbnail...\"],[\"**\\u003cspan style=\\\"text-decoration:underline;\\\"\\u003eTable of contents:\\u003c\\u002fspan\\u003e**\\n* **\\u003cspan style=\\\"text-decorat...\"],[\"These same systems are also likely to reproduce discriminatory and abusive behaviors represented in ...\"],[\"**These issues are deeply personal** for many of us ML researchers and developers at Hugging Face an...\"],[\"While our own experiences do not come close to covering the myriad ways in which ML-mediated discrim...\"],[\"This may not come as much of a surprise given the ML research communityâ€™s [focus on the value of â€œge...\"],[\"1. \\u003cspan style=\\\"text-decoration:underline;\\\"\\u003eThe model is integrated into a website creation service\\u003c...\"],[\"* In this case, the machine biases directly cause discrimination by systematically directing police ...\"],[\"So, whoâ€™s on the hook for machine biases in ML? These three cases illustrate one of the reasons why ...\"],[\"In the next section, we review these various stages along with some of the tools that can help us ad...\"],[\"For example, letâ€™s go back to one of the first highly-publicized cases of a Machine Learning system ...\"],[\"So what does this have to do with bias? Doesnâ€™t showing people content that theyâ€™re likely to enjoy ...\"],[\"This example serves to illustrate that the impact of machine biases in an ML-supported product depen...\"],[\"#### Task definition: recommendations\\n\\nThere are as many ways for the ML task definition and deploym...\"],[\"You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it come...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"### I am \\u003cspan style=\\\"text-decoration:underline;\\\"\\u003etraining\\u002fselecting a model\\u003c\\u002fspan\\u003e for my ML system...\"],[\"Documentation is a great first step for sharing general insights about a modelâ€™s behavior, but it is...\"],[\"Visualization of model outputs isnâ€™t just for generative models though! For classification models, w...\"],[\"Finally, a few benchmarks exist that can measure bias-related phenomena in models. For language mode...\"],[\"Even with access to a benchmark for the models you are considering, you might find that running eval...\"],[\"#### Model selection\\u002fdevelopment: recommendations\\n\\nFor models just as for datasets, different tools ...\"],[\"Summary of linked tools:\\n* Tasks:\\n    * Explore our directory of [ML Tasks](https:\\u002f\\u002fhuggingface.co\\u002ft...\"],[\"* Use a [Text-to-image bias explorer](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fsasha\\u002fStableDiffusionBiasExplore...\"],[\"Thanks for reading! ğŸ¤—\\n\\n~ Yacine, on behalf of the Ethics and Society regulars\\n    \\nIf you want to ci...\"],[\"```\\n@inproceedings{hf_ethics_soc_blog_2,\\n  author    = {Yacine Jernite and\\n               Alexandra ...\"],[\"--\\ntitle: 'Distributed Training: Train BART\\u002fT5 for Summarization using ğŸ¤— Transformers and Amazon Sag...\"],[\"listed again here:\\n\\n- [ğŸ¤— Transformers Documentation: Amazon SageMaker](https:\\u002f\\u002fhuggingface.co\\u002ftransf...\"],[\"As [distributed training strategy](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fsagemaker.html#distributed-tr...\"],[\"```\\n\\nIn this tutorial, we will use an Amazon SageMaker Notebook Instance for running our training jo...\"],[\"```\\n\\n---\\n\\n## Set up a development environment and install sagemaker\\n\\nAfter our SageMaker Notebook In...\"],[\"```\\n\\n---\\n\\n# Choose ğŸ¤— Transformers `examples\\u002f` script\\n\\nThe [ğŸ¤— Transformers repository](https:\\u002f\\u002fgithub...\"],[\"```\\n\\n---\\n\\n## Configure distributed training and hyperparameters\\n\\nNext, we will define our `hyperpara...\"],[\"```\\n\\nSince, we are using [SageMaker Data Parallelism](https:\\u002f\\u002faws.amazon.com\\u002fblogs\\u002faws\\u002fmanaged-data-...\"],[\"```\\n```bash\\n2021-04-01 13:00:35 Starting - Starting the training job...\\n2021-04-01 13:01:03 Starting...\"],[\"```\\n\\nThe training seconds are 2882 because they are multiplied by the number of instances. If we cal...\"],[\"```\\n\\nBefore we are going to upload our model to [huggingface.co](http:\\u002f\\u002fhuggingface.co) we need to c...\"],[\"```\\n\\nAfter we extract all the metrics we want to include we are going to create our `README.md`. Add...\"],[\"## `{model_name}`\\n\\nThis model was trained using Amazon SageMaker and the new Hugging Face Deep Learn...\"],[\"## Results\\n\\n| key | value |\\n| --- | ----- |\\n{eval_table}\\n{test_table}\\n\\n\\n\\n\\\"\\\"\\\"\\n\\n# Generate model card ...\"],[\"```\\n\\nAfter we have our unzipped model and model card located in `my_bart_model` we can use the eithe...\"],[\"```\\n\\nAnd use the \\\"Hosted Inference API\\\" widget to test it. \\n\\n[https:\\u002f\\u002fhuggingface.co\\u002fphilschmid\\u002fbart...\"],[\"--\\ntitle: \\\"SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Compared to LLM based methods, SetFitABSA has two unique advantages:\\n\\n\\u003cp\\u003eğŸ—£ \\u003cstrong\\u003eNo prompts needed...\"],[\"**2. Aspect\\u002fNon-aspect classification**\\n\\nNow that we have aspect candidates, we need to train a mode...\"],[\"Now that we have all the aspect candidates labeled, how do we use it to train the candidate aspect c...\"],[\"```\\naspect_candidate:training_sentence...\"],[\"```\\n\\nApplying the template to the example above will generate 3 training instances â€“ two with `True`...\"],[\"| Text                                                                          | Label |\\n|:--------...\"],[\"```\\n\\\"their dinner specials are fantastic.\\\"\\n```\\n\\n**Model Output:**\\n\\n```\\n[{'span': 'dinner specials', ...\"],[\"```\\n\\n## Benchmarking\\n\\nSetFitABSA was benchmarked against the recent state-of-the-art work by [AWS AI...\"],[\"**SetFitABSA vs GPT2**\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface...\"],[\"```\\nAdditionally, we must install the `en_core_web_lg` spaCy model:\\n```shell\\npython -m spacy downloa...\"],[\"```\\n\\nWe continue by preparing the training set. The format of the training set is a `Dataset` with t...\"],[\"```python\\nfrom datasets import load_dataset\\nfrom setfit import AbsaTrainer, AbsaModel\\n\\n# Create a tr...\"],[\"```\\n\\nThat's it! We have trained a domain-specific ABSA model. We can save our trained model to disk ...\"],[\"```\\n\\nFor more details on training options, saving and loading models, and inference see the SetFit [...\"],[\"--\\ntitle: \\\"Boosting Wav2Vec2 with n-grams in ğŸ¤— Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f44_boost_wav2ve...\"],[\"Using Connectionist Temporal Classification (CTC), pre-trained\\nWav2Vec2-like checkpoints are extreme...\"],[\"Until recently, the ğŸ¤— Transformers library did not offer a simple user\\ninterface to decode audio fil...\"],[\"```\\n\\nLet's load a small excerpt of the [Librispeech\\ndataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrisp...\"],[\"```\\n\\nNext, we process the data\\n\\n```python\\ninputs = processor(audio_sample[\\\"audio\\\"][\\\"array\\\"], samplin...\"],[\"```\\n\\nFor demonstration purposes, we have prepared a new model repository\\n[patrickvonplaten\\u002fwav2vec2-...\"],[\"```\\n\\nIntuitively, one can understand the decoding process of\\n`Wav2Vec2ProcessorWithLM` as applying b...\"],[\"```\\n\\nCool! Recalling the words `facebook\\u002fwav2vec2-base-100h` without a\\nlanguage model transcribed in...\"],[\"------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1 \\\\\\\\) Some research ...\"],[\"The language model should be good at modeling language that corresponds\\nto the target transcriptions...\"],[\"```\\n\\nLet's download the data.\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"...\"],[\"```\\n\\n**Output:**\\n```bash\\n    Login successful\\n    Your token has been saved to \\u002froot\\u002f.huggingface\\u002fto...\"],[\"```\\n\\nThat was easy! The dataset viewer is automatically enabled when\\nuploading a new dataset, which ...\"],[\"The reason why an *n-gram* is preferred over a *Transformer*-based LM is that *n-grams* come at a si...\"],[\"```\\n\\nbefore downloading and unpacking the KenLM repo.\\n\\n```bash\\nwget -O - https:\\u002f\\u002fkheafield.com\\u002fcode\\u002f...\"],[\"**Output:**\\n```bash\\n    === 1\\u002f5 Counting and sorting n-grams ===\\n    Reading \\u002fcontent\\u002fswedish_text.t...\"],[\"Chain sizes: 1:4322508 2:1062772928 3:1992699264 4:3188318720 5:4649631744\\n    tcmalloc: large alloc...\"],[\"4 30374983 D1=0.909146 D2=1.20496 D3+=1.37235\\n    5 37231651 D1=0.944104 D2=1.25164 D3+=1.344\\n    Me...\"],[\"Chain sizes: 1:4322496 2:87627856 3:363553620 4:728999592 5:1042486228\\n    ----5---10---15---20---25...\"],[\"```\\n\\nGreat, we have built a *5-gram* LM! Let's inspect the first couple of\\nlines.\\n\\n```bash\\nhead -20 ...\"],[\"```\\n\\nThere is a small problem that ğŸ¤— Transformers will not be happy about\\nlater on. The *5-gram* cor...\"],[\"```\\n\\nLet's now inspect the corrected *5-gram*.\\n\\n```bash\\nhead -20 5gram_correct.arpa\\n```\\n\\n**Output:**...\"],[\"```\\n\\nGreat, this looks better! We're done at this point and all that is left\\nto do is to correctly i...\"],[\"```\\n\\nWe can safely ignore the warning and all that is left to do now is to\\nwrap the just created `de...\"],[\"```\\n\\nLet's inspect the local repository. The `tree` command conveniently can\\nalso show the size of t...\"],[\"**Output:**\\n```bash\\n    xls-r-300m-sv\\u002f\\n    â”œâ”€â”€ [  23]  added_tokens.json\\n    â”œâ”€â”€ [ 401]  all_results...\"],[\"â”œâ”€â”€ [  43]  debug.log -\\u003e run-20220109_220240-1g372i3v\\u002flogs\\u002fdebug.log\\n        â”œâ”€â”€ [  28]  latest-run ...\"],[\"9 directories, 34 files...\"],[\"```\\n\\nAs can be seen the *5-gram* LM is quite large - it amounts to more than\\n4 GB. To reduce the siz...\"],[\"**Output:**\\n```bash\\n    xls-r-300m-sv\\u002f\\n    â”œâ”€â”€ [  23]  added_tokens.json\\n    â”œâ”€â”€ [ 401]  all_results...\"],[\"```\\n\\nNice, we reduced the *n-gram* by more than half to less than 2GB now. In\\nthe final step, let's ...\"],[\"--\\ntitle: \\\"Hugging Face Machine Learning Demos on arXiv\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002farxiv\\u002fthumbnail.pn...\"],[\"![An interactive demo of a protein structure model, available on Hugging Face Spaces](\\u002fblog\\u002fassets\\u002fa...\"],[\"--\\ntitle: \\\"Illustrating Reinforcement Learning from Human Feedback (RLHF)\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"RLHF's most recent success was its use in [ChatGPT](https:\\u002f\\u002fopenai.com\\u002fblog\\u002fchatgpt\\u002f). Given ChatGPT...\"],[\"This initial model *can* also be fine-tuned on additional text or conditions, but does not necessari...\"],[\"These LMs for reward modeling can be both another fine-tuned LM or a LM trained from scratch on the ...\"],[\"An interesting artifact of this process is that the successful RLHF systems to date have used reward...\"],[\"### Fine-tuning with RL\\n\\nTraining a language model with reinforcement learning was, for a long time,...\"],[\"The reward function is where the system combines all of the models we have discussed into one RLHF p...\"],[\"Finally, the **update rule** is the parameter update from PPO that maximizes the reward metrics in t...\"],[\"# Open-source tools for RLHF\\n\\nThe first [code](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002flm-human-preferences) relea...\"],[\"[RL4LMs](https:\\u002f\\u002fgithub.com\\u002fallenai\\u002fRL4LMs) offers building blocks for fine-tuning and evaluating LL...\"],[\"Generating well-written human text answering specific prompts is very costly, as it often requires h...\"],[\"With these limitations, huge swaths of unexplored design options could still enable RLHF to take sub...\"],[\"### Further reading\\n\\nHere is a list of the most prevalent papers on RLHF to date. The field was rece...\"],[\"And here is a snapshot of the growing set of \\\"key\\\" papers that show RLHF's performance for LMs:\\n- [F...\"],[\"- [ChatGPT: Optimizing Language Models for Dialogue](https:\\u002f\\u002fopenai.com\\u002fblog\\u002fchatgpt\\u002f) (OpenAI 2022)...\"],[\"The field is the convergence of multiple fields, so you can also find resources in other areas:\\n* Co...\"],[\"```\\nLambert, et al., \\\"Illustrating Reinforcement Learning from Human Feedback (RLHF)\\\", Hugging Face ...\"],[\"--\\ntitle: ğŸ§¨ Stable Diffusion  in JAX \\u002f Flax !\\nthumbnail: \\u002fblog\\u002fassets\\u002f108_stable_diffusion_jax\\u002fthumb...\"],[\"## Setup\\n\\n\\n``` python\\nimport jax\\nnum_devices = jax.device_count()\\ndevice_type = jax.devices()[0].dev...\"],[\"```\\n\\n*Output*:\\n```bash \\n    Found 8 JAX devices of type TPU v2.\\n```\\n\\n\\n\\nMake sure `diffusers` is inst...\"],[\"```\\n\\n\\n## Model Loading\\n\\nBefore using the model, you need to accept the model [license](https:\\u002f\\u002fhuggi...\"],[\"```\\n\\n\\nTPU devices support `bfloat16`, an efficient half-float type. We'll use it for our tests, but ...\"],[\"```\\n\\n\\n``` python\\nprompt_ids = shard(prompt_ids)\\nprompt_ids.shape\\n```\\n\\n*Output*:\\n```bash \\n    (8, 1, ...\"],[\"```\\n\\n\\nJAX code can be compiled to an efficient representation that runs very fast. However, we need ...\"],[\"```\\n\\n\\n``` python\\nimage_grid(images, 2, 4)\\n```\\n\\n![png](assets\\u002f108_stable_diffusion_jax\\u002fjax_stable_dif...\"],[\"```\\n\\n![png](assets\\u002f108_stable_diffusion_jax\\u002fjax_stable_diffusion_2.png)\\n\\n\\n--------------------------...\"],[\"```\\n\\n\\nAfter we use `pmap`, the prepared function `p_generate` will conceptually do the following:\\n\\n-...\"],[\"--\\ntitle: 'Pre-Train BERT with Hugging Face Transformers and Habana Gaudi'\\nthumbnail: \\u002fblog\\u002fassets\\u002f9...\"],[\"_Note: Steps 1 to 3 can\\u002fshould be run on a different instance size since those are CPU intensive tas...\"],[\"Read more about BERT in our [BERT 101 ğŸ¤— State Of The Art NLP Model Explained](https:\\u002f\\u002fhuggingface.co...\"],[\"```\\nRead more about Masked Language Modeling [here](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fbert-101).\\n\\n---\\n\\nLet...\"],[\"```\\n\\n\\nThe [original BERT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1810.04805) was pretrained on [Wikipedia](https:\\u002f\\u002fhu...\"],[\"```\\n_We are not going to do some advanced dataset preparation, like de-duplication, filtering or any...\"],[\"```\\n\\n## 3. Preprocess the dataset\\n\\nBefore we can get started with training our model, the last step ...\"],[\"```\\n\\nAs data processing function we will concatenate all texts from our dataset and generate chunks ...\"],[\"```\\n\\n## 4. Pre-train BERT on Habana Gaudi\\n\\nIn this example, we are going to use Habana Gaudi on AWS ...\"],[\"```\\n\\nWhen using GPUs you would use the [Trainer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.19.4\\u002fen...\"],[\"```\\n\\nThe `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-pa...\"],[\"```python\\nfrom huggingface_hub import HfFolder\\n\\n# hyperparameters\\nhyperparameters = {\\n    \\\"model_con...\"],[\"```\\n\\nWe can start our training by creating a `EC2RemoteRunner` and then `launch` it. This will then ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f99_pretraining_bert\\u002ftens...\"],[\"To compare the cost we can use the [p3dn.24xlarge](https:\\u002f\\u002faws.amazon.com\\u002fde\\u002fec2\\u002finstance-types\\u002fp3\\u002f)...\"],[\"Those results are incredible since it will allow companies to adapt their pre-trained models to thei...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Sasha Luccioni\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f69_sasha_luccioni_inte...\"],[\"*Note: Transcription has been slightly modified\\u002freformatted to deliver the highest-quality reading e...\"],[\"### What are you most excited about that you're working on now?\\n\\n**Sasha:** I think the [Big Science...\"],[\"### Love the idea of evaluating the carbon footprint of an email!?\\n\\n**Sasha:** Yeah, people did it, ...\"],[\"So first we just [created this online calculator](https:\\u002f\\u002fmlco2.github.io\\u002fimpact\\u002f) where someone cou...\"],[\"For example; France is mostly nuclear, mostly energy, and Canada has a lot of hydroelectric energy. ...\"],[\"### What are some of the ways that machine learning teams and engineers could be a bit more proactiv...\"],[\"**Sasha:** Yeah, we wrote a paper a couple of years ago that was a cool experience. It's almost a hu...\"],[\"Then instead of powering up a diesel generator which is cool because you can just power them up, and...\"],[\"### For people listening that are interested in this effort, but perhaps work at an organization whe...\"],[\"**Sasha:** Actually, machine learning people or AI people, in general, have this stereotype from oth...\"],[\"And I've participated in organizing workshops where people submit ideas that are super great on pape...\"],[\"### So sad. That's such a great story though and how there are opportunities like that.\\n\\n**Sasha:** ...\"],[\"**Sasha:** Yeah, there's this concept that my mom read about in some magazine ages ago when I was a ...\"],[\"For example, what I feel like we're doing at Hugging Face is really that machine learning needs more...\"],[\"### What other examples or applications do you find and see potential meaning in AI machine learning...\"],[\"### And you've talked before about the power of data and how it's not talked about enough.\\n\\n**Sasha:...\"],[\"### Wow. That is so interesting!\\n\\n**Sasha:** It's actually really, camera trap data is a really huge...\"],[\"**Sasha:** Yeah, I guess another anecdote, I have a lot of these anecdotes, but at some point we wan...\"],[\"So that was really cool because we really saw a year and some before they had no trace of anything, ...\"],[\"### Exactly, that's so interesting. That's so amazing that you were able to jump in there and provid...\"],[\"### All right, so we're going to dive into rapid-fire questions. If you could go back and do one thi...\"],[\"### That's so funny, and itâ€™s interesting to hear that because I often hear people say you need to k...\"],[\"### So besides maybe a mathematical foundation, what advice would you give to someone looking to get...\"],[\"### I love that, find something that you're interested in.\\n\\n**Sasha:** Exactly. And one of my favori...\"],[\"### So should people be afraid of AI taking over the world?\\n\\n**Sasha:** I think that we're really fa...\"],[\"### What are you interested in right now? It could be anything, a movie, a recipe, a podcast, etc.?\\n...\"],[\"### What are some of your favorite machine learning papers?\\n\\n**Sasha:** My favorite currently, paper...\"],[\"### Wow, we'll definitely be linking to that paper as well, so people can check that out. Yeah, very...\"],[\"### Where can people find you online?\\n\\n**Sasha:** I'm on [Twitter @SashaMTL](https:\\u002f\\u002ftwitter.com\\u002fSas...\"],[\"--\\ntitle: Simple considerations for simple people building fancy neural networks\\nthumbnail: \\u002fblog\\u002fas...\"],[\"At the same time, deep learning frameworks, tools, and specialized libraries democratize machine lea...\"],[\"*   Are the labels balanced?\\n*   Are there gold-labels that you do not agree with?\\n*   How were the ...\"],[\"As developers, it easy to feel good when building something fancy but it is sometimes hard to ration...\"],[\"Some common errors include:\\n\\n*   Wrong indexingâ€¦ (these are really the worst ğŸ˜…). Make sure you are g...\"],[\"\\u003e Some people report successes using fancy hyperparameter tuning methods such as Bayesian optimizati...\"],[\"A few related pointers to complete your reading:\\n\\n*   [Reproducibility (in ML) as a vehicle for engi...\"],[\"--\\ntitle: \\\"Fine-tuning Llama 2 70B using PyTorch FSDP\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f160_fsdp_llama\\u002fthumb...\"],[\"## Challenges with fine-tuning LLaMa 70B\\n\\nWe encountered three main challenges when trying to fine-t...\"],[\"### Pre-requisites\\n\\nFirst follow these steps to install Flash Attention V2:  Dao-AILab\\u002fflash-attenti...\"],[\"```bash\\naccelerator.process_index=0 GPU Memory before entering the loading : 0\\naccelerator.process_i...\"],[\"```\\n\\n### Addressing Challenge 2\\nIt is addressed via choosing `SHARDED_STATE_DICT` state dict type wh...\"],[\"```\\n\\n### Addressing Challenge 3\\nFlash Attention and enabling gradient checkpointing are required for...\"],[\"(Source: [link](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2205.14135.pdf))\\n\\nThis is precisely the problem that Flash Att...\"],[\"## Bringing it all-together\\n\\nTo run the training using `Accelerate` launcher with SLURM, refer this ...\"],[\"```\\naccelerate launch \\\\\\n    --config_file configs\\u002ffsdp_config.yaml \\\\\\n    --main_process_ip $MASTER_A...\"],[\"```\\n\\nFine-tuning completed in ~13.5 hours and below is the training loss plot.\\n\\n![train_loss](https:...\"],[\"- Human: Now explain it like a chef.\\n\\n+ Assistant: Certainly! Here's an explanation of deep learning...\"],[\"```\\n\\nThe whole conversation is formatted as below: \\n```\\n\\u003c|system|\\u003e system message \\u003c|endoftext|\\u003e \\u003c|pr...\"],[\"--\\ntitle: Optimizing Stable Diffusion for Intel CPUs with NNCF and ğŸ¤— Optimum\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"## Stable Diffusion optimization\\n\\nIn the [Stable Diffusion pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdif...\"],[\"However, it turns out that the traditional model optimization methods, such as post-training 8-bit q...\"],[\"## Optimization workflow\\n\\nWe usually start the optimization of a model after it's trained. Here, we ...\"],[\"## Going beyond Quantization-Aware Training\\n\\nQuantization alone can bring significant enhancements b...\"],[\"\\u003cdiv class=\\\"flex flex-row\\\"\\u003e\\n\\u003cdiv class=\\\"grid grid-cols-2 gap-4\\\"\\u003e\\n\\u003cfigure\\u003e\\n\\u003cimg class=\\\"max-w-full rou...\"],[\"\\u003c\\u002ffigure\\u003e\\n\\u003cfigure\\u003e\\n\\u003cimg class=\\\"max-w-full rounded-xl border-2 border-solid border-gray-600\\\" src=\\\"htt...\"],[\"Results of image generation [demo](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhelenai\\u002fstable_diffusion) using dif...\"],[\"Below we show how to perform inference with the final pipeline optimized to run on Intel CPUs:\\n\\n```p...\"],[\"```\\n\\nYou can find the training and quantization [code](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel\\u002f...\"],[\"--\\ntitle: \\\"A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using tran...\"],[\"After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways ...\"],[\"In the float16 (FP16) data type, 5 bits are reserved for the exponent and 10 bits are reserved for t...\"],[\"While, ideally the training and inference should be done in FP32, it is two times slower than FP16\\u002fB...\"],[\"To remediate that, we introduce 8-bit quantization. This method uses a quarter precision, thus needi...\"],[\"![quantization](assets\\u002f96_hf_bitsandbytes_integration\\u002fquantization.png)\\n\\n(Image taken from: [this bl...\"],[\"These tricks can be combined in several ways, for example, row-wise or vector-wise quantization, whe...\"],[\"These steps can be summarized in the following animation:\\n\\n![Mixed-int8.gif](assets\\u002f96_hf_bitsandbyt...\"],[\"![Matmul.png](assets\\u002f96_hf_bitsandbytes_integration\\u002fMatmul.png)\\n\\n### What does 0 degradation mean?\\n\\n...\"],[\"For BLOOM-176:\\n\\n| benchmarks | - | -   | -  |   -        |     difference - value  |\\n| ---------- | ...\"],[\"### Is it faster than native models?\\n\\n\\nThe main purpose of the LLM.int8() method is to make large mo...\"],[\"| Precision      | Number of parameters | Hardware     | Time per token in milliseconds for Batch Si...\"],[\"Next let's discuss the specifics of the Hugging Face `transformers` integration. Let's look at the u...\"],[\"```\\n\\n2. Then you can define your own model. Note that you can convert a checkpoint or model of any p...\"],[\"```\\n\\nNote that the quantization step is done in the second line once the model is set on the GPU. If...\"],[\"```\\n\\nWhereas if you print it after the second line's call you get:\\n\\n```\\nint8_model[0].weight\\nParamet...\"],[\"```\\n\\nAnd you will get:\\n\\n```\\ntensor([[ 0.0028, -0.0459,  0.0522,  ..., -0.0049, -0.0428,  0.0462],\\n  ...\"],[\"```\\n\\nCheck out [the example script](\\u002fassets\\u002f96_hf_bitsandbytes_integration\\u002fexample.py) for the full ...\"],[\"```\\n\\nto\\n\\n```py\\nparam_cls = type(module._parameters[name])\\nkwargs = module._parameters[name].__dict__...\"],[\"```\\n\\nThis function recursively replaces all `nn.Linear` layers of a given model initialized on the `...\"],[\"### Be very careful on how to set devices with `accelerate`\\n\\nHere we played a very delicate balancin...\"],[\"### Installation\\n\\nJust install the latest version of the libraries using the commands below (make su...\"],[\"```\\n\\n### Example demos - running T5 11b on a Google Colab\\n\\nCheck out the Google Colab demos for runn...\"],[\"### Support for Kepler GPUs (GTX 1080 etc)\\n\\nWhile we support all GPUs from the past four years, some...\"],[\"--\\ntitle: \\\"Speech Synthesis, Recognition, and More With SpeechT5\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fspeecht5\\u002ft...\"],[\"At the heart of SpeechT5 is a regular **Transformer encoder-decoder** model. Just like any other Tra...\"],[\"## Text-to-speech\\n\\nSpeechT5 is the **first text-to-speech model** weâ€™ve added to ğŸ¤— Transformers, and...\"],[\"```\\n\\nFirst, we load the [fine-tuned model](https:\\u002f\\u002fhuggingface.co\\u002fmicrosoft\\u002fspeecht5_tts) from the H...\"],[\"```\\n\\nThe speaker embedding is a tensor of shape (1, 512). This particular speaker embedding describe...\"],[\"```\\n\\nTo make audio from the spectrogram, do the following:\\n\\n```python\\nwith torch.no_grad():\\n    spee...\"],[\"```\\n\\nThe output sounds like this ([download audio](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"As an example of a speech-to-speech task, the authors of SpeechT5 provide a [fine-tuned checkpoint](...\"],[\"```\\n\\nWe will need some speech audio to use as input. For the purpose of this example, weâ€™ll load the...\"],[\"```\\n\\nChanging to a different voice is as easy as loading a new speaker embedding. You could even mak...\"],[\"- **Speech encoder pre-net.** This is the same pre-net used by the speech-to-speech model and consis...\"],[\"```\\n\\nAs speech audio, weâ€™ll use the same input as in the previous section, but any audio file will w...\"],[\"```\\n\\nPlay with an interactive demo for the [speech-to-text task](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fMatth...\"],[\"--\\ntitle: \\\"Code Llama: Llama 2 learns to code\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f160_codellama\\u002fthumbnail.jpg\\n...\"],[\"## Whatâ€™s Code Llama?\\n\\nThe Code Llama release introduces a family of models of 7, 13, and 34 billion...\"],[\"## How to use Code Llama?\\n\\nCode Llama is available in the Hugging Face ecosystem, starting with `tra...\"],[\"```\\n#### A Note on dtypes\\n\\nWhen using models like Code Llama, it's important to take a look at the d...\"],[\"sequences = pipeline(\\n    'def fibonacci(',\\n    do_sample=True,\\n    temperature=0.2,\\n    top_p=0.9,\\n...\"],[\"```\\n\\nThis may produce output like the following:\\n\\n```python\\nResult: def fibonacci(n):\\n    if n == 0:...\"],[\"```\\n\\nCode Llama is specialized in code understanding, but it's a language model in its own right. Yo...\"],[\"```\\n\\n```Python\\ndef remove_non_ascii(s: str) -\\u003e str:\\n    \\\"\\\"\\\" Remove non-ASCII characters from a strin...\"],[\"```\\n\\nNote that the system prompt is optional - the model will work without it, but you can use it to...\"],[\"```\\n\\n3. **On-going conversation with previous answers**\\n\\nThe process is the same as in [Llama 2](htt...\"],[\"```\\n\\n#### 4-bit Loading\\n\\nIntegration of Code Llama in Transformers means that you get immediate supp...\"],[\"```\\n\\n### Using text-generation-inference and Inference Endpoints\\n\\n[Text Generation Inference](https:...\"],[\"### Using VS Code extension\\n\\n[HF Code Autocomplete](https:\\u002f\\u002fmarketplace.visualstudio.com\\u002fitems?itemN...\"],[\"| Model                  | License            | Dataset known | Commercial use? | Pretraining length...\"],[\"| CodeLlaMa-7B-Instruct  | Llama 2 license    | âŒ             | âœ…               | 2,620B            ...\"],[\"**Note:** The scores presented in the table above are sourced from our code leaderboard, where we ev...\"],[\"--\\ntitle: \\\"Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU plat...\"],[\"On the CPU side, the two companies will work on optimizing inference for both the client Ryzen and s...\"],[\"## Conclusion\\n\\nWe're excited to work with a world-class hardware company like AMD. Open-source means...\"],[\"--\\ntitle: \\\"Accelerating over 130,000 Hugging Face models with ONNX Runtime\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"## Learn More\\nTo learn more about accelerating Hugging Face models with ONNX Runtime, check out our ...\"],[\"--\\ntitle: \\\"AI Speech Recognition in Unity\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002funity-asr-thumbn...\"],[\"```\\n[SerializeField] private Button startButton;\\n[SerializeField] private Button stopButton;\\n[Serial...\"],[\"```\\n\\nThen, in `StopRecording()`, truncate the recording and encode it in WAV format:\\n```\\nprivate voi...\"],[\"```\\nusing System.IO;\\nusing TMPro;\\nusing UnityEngine;\\nusing UnityEngine.UI;\\n\\npublic class SpeechRecog...\"],[\"foreach (var sample in samples) {\\n                    writer.Write((short)(sample * short.MaxValue))...\"],[\"```\\n\\nTo test whether this code is working correctly, you can add the following line to the end of th...\"],[\"```\\nusing System.IO;\\nusing HuggingFace.API;\\nusing TMPro;\\nusing UnityEngine;\\nusing UnityEngine.UI;\\n\\np...\"],[\"private byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\\n        using (var memory...\"],[\"```\\n\\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity!\\n\\nIf you have any...\"],[\"--\\ntitle: 'Building a Playlist Generator with Sentence Transformers'\\nthumbnail: \\u002fblog\\u002fassets\\u002f87_play...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"Weâ€™ll be looking at a slightly advanced use of Gradio, so if youâ€™re a beginner to the library I reco...\"],[\"[The ST documentation highlights many of the choices](https:\\u002f\\u002fwww.sbert.net\\u002fdocs\\u002fpretrained_models.h...\"],[\"```python\\nfrom sentence_transformers import SentenceTransformer\\nimport pickle\\n\\nembedder = SentenceTr...\"],[\"```\\n\\nTo be able to share you embeddings with others, you can even upload the Pickle file to a Huggin...\"],[\"```\\n\\nSince weâ€™re searching for any verse that matches the text prompt, thereâ€™s a good chance that th...\"],[\"# I'm loading the lyrics from my private dataset, with my own API token\\nauth_token = os.environ.get(...\"],[\"```\\n\\nThe Gradio Blocks API lets you build *multi-step* interfaces, which means that youâ€™re free to c...\"],[\"```\\n\\nIn that function, we use the text prompt to conduct the semantic search. As seen above, to push...\"],[\"While the song *lyrics* arenâ€™t being released, Iâ€™ve **[published the verse embeddings along with the...\"],[\"--\\ntitle: \\\"Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Pre-trained with only 4M images (see the detail [below](#benchmark)), BridgeTower achieves state-of-...\"],[\"[Habana Gaudi2](https:\\u002f\\u002fhabana.ai\\u002fproducts\\u002fgaudi2\\u002f) is the second-generation AI hardware accelerator...\"],[\"Hyperparameters are the same for all accelerators. We used a batch size of 48 samples for each devic...\"],[\"Let's run the three following experiments:\\n- a mixed-precision (*bfloat16*\\u002f*float32*) run distribute...\"],[\"We first see that **Gaudi2 is x1.28 faster than H100** with `dataloader_num_workers=2`, x1.29 faster...\"],[\"\\u003c!-- ### Optimum Habana's fast DDP\\n\\nBefore delving into how to perform hardware-accelerated data loa...\"],[\"So adding just two training arguments (`dataloader_num_workers=1` and `distribution_strategy=\\\"fast_d...\"],[\"\\u003c!-- To achieve this on Gaudi2, Habana's media pipeline enables us to:\\n- Initialize a media pipeline...\"],[\"We are now going to re-run the previous experiments adding the `mediapipe_dataloader` argument since...\"],[\"Then, you need to install the latest version of Optimum Habana and run `run_bridgetower.py` which yo...\"],[\"```\\n\\nThe base command line to run the script is:\\n```bash\\npython ..\\u002fgaudi_spawn.py --use_mpi --world_...\"],[\"```\\n\\nFor A100 and H100, you can use the same `run_bridgetower.py` script with a few small changes:\\n-...\"],[\"To go further, we are looking forward to using HPU graphs for training models even faster and to pre...\"],[\"--\\ntitle: \\\"Introducing ğŸ¤— Accelerate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f20_accelerate_library\\u002faccelerate_diff.p...\"],[\"```\\n\\nBy just adding five lines of code to any standard PyTorch training script, you can now run said...\"],[\"```\\n\\nIn contrast, here are the changes needed to have this code run with distributed training are th...\"],[\"```\\n\\nOn top of giving the main object that you will use, this line will analyze from the environment...\"],[\"```\\n\\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torc...\"],[\"```\\n\\nThis last line adds the necessary steps for the backward pass (mostly for mixed precision but o...\"],[\"```\\n\\nThis will trigger a little questionnaire about your setup, which will create a config file you ...\"],[\"--\\ntitle: \\\"Interactively explore your Huggingface dataset with one line ofÂ code\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fRenumics\\u002fspotlight\\\"\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface...\"],[\"```\\n\\nData inspection is a very important task in almost all ML development stages, but it can also b...\"],[\"```\\n\\n## **Leveraging model results for data inspection**\\n\\nExploring raw unstructured datasets often ...\"],[\"```\\n\\nNow we can compute the enrichment:\\n\\n\\n```python\\nimport torch\\nimport transformers\\n\\ndevice = torch...\"],[\"```\\n\\nWe can now use the results to interactively explore relevant data samples and clusters in Spotl...\"],[\"```\\n\\n\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"## Whatâ€™s next?\\n\\nWith Spotlight you can create **interactive visualizations** and leverage data enri...\"],[\"--\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthu...\"],[\"One of the main problems developers and organizations face is how difficult it is to deploy and scal...\"],[\"Within minutes, you can test your endpoint and add its inference API to your application. Itâ€™s never...\"],[\"The Hugging Face Blog Repository ğŸ¤—\\nThis is the official repository of the [Hugging Face Blog](https:...\"],[\"```\\n---\\ntitle: \\\"PUT YOUR TITLE HERE\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f101_decision-transformers-train\\u002fthumbn...\"],[\"--\\ntitle: Training CodeParrot ğŸ¦œ from Scratch\\nthumbnail: \\u002fblog\\u002fassets\\u002f40_codeparrot\\u002fthumbnail.png\\naut...\"],[\"The cleaned dataset is still 50GB big and available on the Hugging Face Hub: [codeparrot-clean](http...\"],[\"```\\n\\nLearn more about tokenizers and how to build them in the [Hugging Face course](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nNow that we have an efficient tokenizer and a freshly initialized model we can start with the a...\"],[\"```\\n\\nWe can directly load the tokenizer and model from the local repository. Since we are dealing wi...\"],[\"```\\n\\nNext up is the dataset. We make training simpler with a dataset that yields examples with a fix...\"],[\"def __iter__(self):\\n        iterator = iter(self.dataset)\\n        more_examples = True\\n        while...\"],[\"```\\n\\nTexts in the buffer are tokenized in parallel and then concatenated. Chunked samples are then y...\"],[\"```\\n\\nBefore we start training we need to set up the optimizer and learning rate schedule. We donâ€™t w...\"],[\"```\\nUnder the hood it'll use DistributedDataParallel, which means a batch is sent to each GPU worker...\"],[\"```\\n\\nWe are now ready to write the main training loop. It will look pretty much like a normal PyTorc...\"],[\"```\\n\\nWhen we call `wait_for_everyone()` and `unwrap_model()` we make sure that all workers are ready...\"],[\"```\\nDone! That's all the code to train a full GPT-2 model from scratch with as little as 150 lines. ...\"],[\"```Python\\nfrom datasets import load_metric\\n\\ncode_eval = datasets.load_metric(\\\"code_eval\\\")\\ntest_cases...\"],[\"```\\n\\n\\n\\nYou can also load OpenAI's HumanEval dataset with `datasets`:\\n\\n```Python\\nfrom datasets import...\"],[\"```\\n\\n**Completion:**\\n```Python\\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\\nmodel ...\"],[\"--\\ntitle: \\\"Optimizing Bark using ğŸ¤— Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fbark_optimization\\u002fthumbnai...\"],[\"This blog post is organized as follows:\\n\\n## Table of Contents\\n\\n1.   A [reminder](#bark-architecture)...\"],[\"At the time of writing, two Bark checkpoints are available, a [smaller](https:\\u002f\\u002fhuggingface.co\\u002fsuno\\u002f...\"],[\"```\\n\\nPlace the model to an accelerator device to get the most of the optimization techniques:\\n\\n```py...\"],[\"```\\n\\nMeasuring the latency and GPU memory footprint requires the use of specific CUDA methods. We de...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 9.3841625 seconds\\nMax memory footprint 1.914612224  GB\\n```\\n\\nNo...\"],[\"```\\n\\n\\nThe output sounds like this ([download audio](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"Turns out that Flash Attention is supported by ğŸ¤— Better Transformer out of the box! It requires one ...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 5.43284375 seconds\\nMax memory footprint 1.9151841280000002  GB...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 7.00045390625 seconds\\nMax memory footprint 2.7436124160000004 ...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 8.97633828125 seconds\\nMax memory footprint 1.3231160320000002 ...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 7.4496484375000005 seconds\\nMax memory footprint 0.468710912000...\"],[\"```\\n\\n\\nThe output sounds like this (download [first](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"## How to read the results?\\n\\n### Latency\\n\\nIt measures the duration of a single call to the generatio...\"],[\"### Comment\\n\\nAs expected, CPU offload greatly reduces memory footprint while slightly increasing lat...\"],[\"### Comment\\n\\nThis is where we can see the potential of combining all three optimization features!\\n\\nT...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 4]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_ml_director_in...\"],[\"**Background:** Seasoned entrepreneur and leader, Javier was co-founder and CTO of Machinalis, a hig...\"],[\"E-commerce is scaling its share of the market year after year, and Machine Learning is always a prob...\"],[\"**Background:** Dr. Shaun Gittens is the Director of the Machine Learning Capability of MasterPeace ...\"],[\"#### **1. How has ML made a positive impact on Engineering?**\\n\\nEngineering is vast in its applicatio...\"],[\"#### **3. Whatâ€™s a common mistake you see people make when trying to integrate ML into Engineering?*...\"],[\"**Background:** Samuel is a senior Data Science and ML Engineering leader at Pluralsight with a Ph.D...\"],[\"#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into existing products?...\"],[\"**Fun Fact:** Met Paul McCarthy. ğŸ¤\\n\\n**MasterPeace Solutions:** MasterPeace Solutions has emerged as ...\"],[\"#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into SaaS?**\\nTo get it ...\"],[\"--\\ntitle: \\\"Panel on Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fpanel-on-hugging-face\\u002fthumbnail.png\\nautho...\"],[\"Here are some notable features of Panel that our users find valuable. \\n\\n- Panel provides extensive s...\"],[\"--\\ntitle: \\\"Diffusion Models Live Event\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusion-models-event\\u002fthumbnail.png...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003cdiv class=\\\"text-center flex flex-col items-center\\\"\\u003e\\n        \\u003cimg src=\\\"https:\\u002f\\u002fhuggingfac...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eApolinÃ¡rio Passos: \\u003cem\\u003eDALL-E 2 is cool but... what will come after the generative media ...\"],[\"--\\ntitle: \\\"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\\\"\\nthumbnail:...\"],[\"With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of...\"],[\"--\\ntitle: \\\"Fine-tune Llama 2 with DPO\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f157_dpo_trl\\u002fdpo_thumbnail.png\\nauthor...\"],[\"## DPO vs PPO\\n\\nIn the traditional model of optimising human derived preferences via RL, the goto met...\"],[\"- `prompt` this consists of the context prompt which is given to a model at inference time for text ...\"],[\"```\\n\\nOnce we have the dataset sorted the DPO loss is essentially a supervised loss which obtains an ...\"],[\"```\\n\\n## Experiment with Llama v2\\n\\nThe benefit of implementing the DPO trainer in TRL is that one can...\"],[\"# add LoRA layers on top of the quantized base model\\npeft_config = LoraConfig(\\n    r=script_args.lor...\"],[\"```\\n\\n### DPO Training\\n\\nOnce the SFT has finished, we can save the resulting model and move onto the ...\"],[\"```\\n\\nSo as can be seen we load the model in the 4-bit configuration and then train it via the QLora ...\"],[\"We hope with the code release it lowers the barrier to entry for you the readers to try out this met...\"],[\"--\\ntitle: \\\"Welcome fastText to the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f147_fasttext\\u002fthumbnail....\"],[\"### How to use\\n\\nHere is how to load and use a pre-trained vectors:\\n\\n```python\\n\\u003e\\u003e\\u003e import fasttext\\n\\u003e\\u003e...\"],[\"```\\n\\nHere is how to use this model to query nearest neighbors of an English word vector:\\n\\n```python\\n...\"],[\"```\\n\\n## Would you like to integrate your library to the Hub?\\n\\nThis integration is possible thanks to...\"],[\"--\\ntitle: \\\"Introducing Decision Transformers on Hugging Face ğŸ¤—\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f58_decision-...\"],[\"*A comparison between Reinforcement Learning in an Online and Offline setting, figure taken from [th...\"],[\"The main idea is that instead of training a policy using RL methods, such as fitting a value functio...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo \\n        alt=\\\"WalkerEd-expert\\\"\\n      ...\"],[\"```\\n\\n### Loading the model\\n\\nUsing the Decision Transformer is relatively easy, but as it is an autor...\"],[\"```\\n### Autoregressive prediction function\\n\\nThe model performs an [autoregressive prediction](https:...\"],[\"`````python\\n# Function that gets an action from the model using autoregressive prediction \\n# with a ...\"],[\"timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\\n\\t\\n\\t# perform ...\"],[\"```\\n### Evaluating the model\\n\\nIn order to evaluate the model, we need some additional information; t...\"],[\"state_mean = torch.from_numpy(state_mean)\\nstate_std = torch.from_numpy(state_std)\\n\\nstate = env.reset...\"],[\"```\\nYou will find a more detailed example, with the creation of videos of the agent in our [Colab no...\"],[\"--\\ntitle: \\\"Course Launch Community Event\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f34_course_launch\\u002fspeakers_day1_thu...\"],[\"To register, please fill out [this form](https:\\u002f\\u002fdocs.google.com\\u002fforms\\u002fd\\u002fe\\u002f1FAIpQLSd17_u-wMCdO4fcOPO...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eMargaret Mitchell: \\u003cem\\u003eOn Values in ML Development\\u003c\\u002fem\\u003e\\u003c\\u002fstrong\\u003e\\u003c\\u002fp\\u003e\\n        \\u003cp\\u003eMargaret ...\"],[\"\\u003cp\\u003eJakob Uszkoreit is the co-founder of Inceptive. Inceptive designs RNA molecules for vaccines and ...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_course_launch\\u002fchen_qian.png\\\" width=50% style=\\\"border-radius: 50%;\\\"\\u003e\\n      ...\"],[\"## Day 2 (November 16th): The tools you will use\\n\\nDay 2 will be focused on talks by the Hugging Face...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003cdiv class=\\\"text-center flex flex-col items-center\\\"\\u003e\\n        \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_co...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_course_launch\\u002fabubakar_abid.png\\\" width=50% style=\\\"border-radius: 50%;\\\"\\u003e\\n  ...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003ePhilipp Schmid: \\u003cem\\u003eManaged Training with Amazon SageMaker and ğŸ¤— Transformers\\u003c\\u002fem\\u003e\\u003c\\u002fstron...\"],[\"--\\ntitle: 'Accelerated Inference with Optimum and Transformers Pipelines'\\nthumbnail: \\u002fblog\\u002fassets\\u002f66...\"],[\"Let's get started! ğŸš€\\n\\n## 1. What is Optimum? An ELI5\\n\\n[Hugging Face Optimum](https:\\u002f\\u002fgithub.com\\u002fhugg...\"],[\"```diff\\nfrom transformers import AutoTokenizer, pipeline\\n-from transformers import AutoModelForQuest...\"],[\"```\\n\\nIn the first release, we added [support for ONNX Runtime](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fm...\"],[\"### 3.1 Install `Optimum` for Onnxruntime\\n\\nOur first step is to install  `Optimum` with the `onnxrun...\"],[\"```\\n\\nThis will install all required packages for us including `transformers`, `torch`, and `onnxrunt...\"],[\"# save onnx checkpoint and tokenizer\\nmodel.save_pretrained(onnx_path)\\ntokenizer.save_pretrained(onnx...\"],[\"```\\n\\nWe successfully converted our vanilla transformers to `onnx` and used the model with the `trans...\"],[\"```\\n\\nTo test performance we can use the `ORTModelForQuestionAnswering` class again and provide an ad...\"],[\"```\\n\\nWe will evaluate the performance changes in step [3.6 Evaluate the performance and speed](#36-e...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f66_optimum_inference\\u002fmod...\"],[\"```\\n\\nNice! The model predicted the same answer.\\n\\n### 3.5 Run accelerated inference using Transformer...\"],[\"```\\n\\nIn addition to this we added a `pipelines` API to Optimum to guarantee more safety for your acc...\"],[\"```\\n\\n### 3.6 Evaluate the performance and speed\\n\\nDuring this [End-to-End tutorial on accelerating Ro...\"],[\"```\\n\\nWe can now leverage the [map](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.1.0\\u002fen\\u002fprocess#map) funct...\"],[\"```\\n\\nNow lets compare the results\\n\\n```python\\ndefault_acc = metric.compute(predictions=result[\\\"defaul...\"],[\"```\\n\\nOur optimized & quantized model achieved an exact match of `78.75%` and an f1 score of `81.83%`...\"],[\"```\\n\\nTo keep it simple, we are going to use a python loop and calculate the avg\\u002fmean latency for our...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f66_optimum_inference\\u002fres...\"],[\"## 5. Optimum Inference FAQ\\n\\n**Which tasks are supported?**\\n\\nYou can find a list of all supported ta...\"],[\"**How can I use a quantized and optimized model with pipelines?**\\n\\nYou can load the optimized or qua...\"],[\"If you have any questions, feel free to contact me, through [Github](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"--\\ntitle: \\\"Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\\\"\\nthu...\"],[\"When it comes to respecting peopleâ€™s privacy rights, the recent developments in ML and AI pose new q...\"],[\"--\\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusers_cor...\"],[\"The rest of this post guides you on how to use the converted weights in your own code or convert add...\"],[\"## Notes on Performance\\n\\nThere are several variants per model:\\n\\n- \\\"Original\\\" attention vs \\\"split_ein...\"],[\"```\\ncoreml-stable-diffusion-v1-4\\nâ”œâ”€â”€ README.md\\nâ”œâ”€â”€ original\\nâ”‚   â”œâ”€â”€ compiled\\nâ”‚   â””â”€â”€ packages\\nâ””â”€â”€ sp...\"],[\"```\\n\\n`\\u003coutput-mlpackages-directory\\u003e` should point to the checkpoint you downloaded in the step above...\"],[\"```\\n\\n## Core ML inference in Swift\\n\\nRunning inference in Swift is slightly faster than in Python, be...\"],[\"```\\n\\nYou have to specify in `--resource-path` one of the checkpoints downloaded in the previous step...\"],[\"--\\ntitle: \\\"Exploring simple optimizations for SDXL\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fsimple_sdxl_optimization...\"],[\"```\\n\\nThis isnâ€™t very practical and can slow you down because youâ€™re often generating more than 4 ima...\"],[\"With ğŸ¤—Â Diffusers, you can use fp16 for inference by specifying the `torch.dtype` parameter to conver...\"],[\"```\\n\\nCompared to a completely unoptimized SDXL pipeline, using fp16 takes 21.7GB of memory and only ...\"],[\"```\\n\\nCompared to a completely unoptimized SDXL pipeline, using fp16 and SDPA takes the same amount o...\"],[\"```\\n\\nCompared to the previous baseline (fp16 + SDPA), wrapping the UNet with `torch.compile` improve...\"],[\"```\\n\\nCompared to the baseline, it now takes 20.2GB of memory which saves you 1.5GB of memory.\\n\\n### S...\"],[\"```\\n\\nWith sliced computations, we reduce the memory to 15.4GB. If we add sequential CPU offloading, ...\"],[\"```\\n\\nNext, flush the GPU memory to remove the text encoders:\\n\\n```jsx\\ndel text_encoder, text_encoder_...\"],[\"```\\n\\nCombined with SDPA and fp16, we can reduce the memory to 21.9GB. Other techniques discussed abo...\"],[\"```\\n\\nWith this setup, we reduce the memory requirement to 15.6GB while reducing the inference latenc...\"],[\"We hope these optimizations make it a breeze to run your favorite pipelines. Try these techniques ou...\"],[\"--\\ntitle: \\\"Fine-Tune ViT for Image Classification with ğŸ¤— Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f51_fi...\"],[\"It turns out that once you've done the above, you can pre-train and fine-tune transformers just as y...\"],[\"```\\n\\n## Load a dataset\\n\\nLet's start by loading a small image classification dataset and taking a loo...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n\\n\\n\\n    'bean_rust'\\n\\n\\n\\nTurns out the leaf shown above is infected with Bean Rust, a serious dise...\"],[\"```\\n\\n\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)...\"],[\"```\\n\\nYou can see the image processor configuration by printing it.\\n\\n\\n    ViTImageProcessor {\\n      \\\"...\"],[\"```\\n\\n\\n```python\\nprocess_example(ds['train'][0])\\n```\\n\\n\\n    {\\n      'pixel_values': tensor([[[[-0.6157...\"],[\"```\\n\\nNow, whenever you get an example from the dataset, the transform will be \\napplied in real time ...\"],[\"```\\n\\n### Define an evaluation metric\\n\\nThe [accuracy](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002faccuracy) metric...\"],[\"```\\n\\n\\nAlmost ready to train! The last thing needed before that is to set up the training configurati...\"],[\"```\\n\\n### Train ğŸš€\\n\\n\\n```python\\ntrain_results = trainer.train()\\ntrainer.save_model()\\ntrainer.log_metric...\"],[\"--\\ntitle: \\\"~Don't~ Repeat Yourself\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f59_transformers_philosophy\\u002ftransformers....\"],[\"In short the reasons are:\\n- **1. Transformers is built by and for the open-source community.**\\n- **2...\"],[\"As an example, two years ago, one might have defined BERT's self attention layer as the standard att...\"],[\"A second realization is that models do **not** depend on each other in a bidirectional way. More rec...\"],[\"Now, we explain why we put the asterisk \\\\\\\\( {}^{\\\\textbf{*}} \\\\\\\\) after *\\\"Repeat Yourself\\\"*. We don't ...\"],[\"### Drawbacks\\nClearly, there are also drawbacks to the single file policy two of which we quickly wa...\"],[\"--\\ntitle: \\\"Introducing WÃ¼rstchen: Fast Diffusion for Image Generation\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fwuer...\"],[\"## Why another text-to-image model?\\n\\nWell, this one is pretty fast and efficient. WÃ¼rstchenâ€™s bigges...\"],[\"## How to use WÃ¼rstchen?\\nYou can either try it using the Demo here:\\n\\n\\u003cscript type=\\\"module\\\" src=\\\"http...\"],[\"```\\n\\n![Anthropomorphic cat dressed as a fire-fighter](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"### Diffusers integration\\nBecause WÃ¼rstchen is fully integrated in `diffusers`, it automatically com...\"],[\"## Optimisation Technique 1: Flash Attention\\n\\nStarting from version 2.0, PyTorch has integrated a hi...\"],[\"```\\n\\nFor an in-depth look at how `diffusers` leverages SDPA, check out the [documentation](https:\\u002f\\u002fh...\"],[\"```\\n\\nAnd the good news is that this compilation is a one-time execution. Post that, you're set to ex...\"],[\"--\\ntitle: Faster TensorFlow models in Hugging Face Transformers\\nthumbnail: \\u002fblog\\u002fassets\\u002f10_tf-servin...\"],[\"| Batch size | Google implementation | v4.2.0 implementation | Relative difference Google\\u002fv4.2.0 imp...\"],[\"### What is a SavedModel?\\n\\nA SavedModel contains a standalone TensorFlow model, including its weight...\"],[\"```\\nsavedmodel\\n    \\u002fassets\\n        -\\u003e here the needed assets by the model (if any)\\n    \\u002fvariables\\n  ...\"],[\"```\\nThe given SavedModel SignatureDef contains the following input(s):\\n  inputs['attention_mask'] te...\"],[\"```\\n\\nTo directly pass `inputs_embeds` (the token embeddings) instead of `input_ids` (the token IDs) ...\"],[\"```\\n\\nThe serving method has to be overridden by the new `input_signature` argument of the `tf.functi...\"],[\"```\\n\\n## How to deploy and use a SavedModel?\\n\\nLetâ€™s see step by step how to deploy and use a BERT mod...\"],[\"```\\ndocker run -d -p 8501:8501 -p 8500:8500 --name bert my_bert_model\\n```\\n\\n### Step 3\\n\\nQuery the mod...\"],[\"```\\n\\nThis should return POSITIVE. It is also possible to pass by the gRPC (google Remote Procedure C...\"],[\"# Create a gRPC request made for prediction\\nrequest = predict_pb2.PredictRequest()\\n\\n# Set the name o...\"],[\"```\\n\\n## Conclusion\\nThanks to the last updates applied on the TensorFlow models in transformers, one ...\"],[\"--\\ntitle: \\\"Object Detection Leaderboard\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fobject-detection-leaderboard\\u002fthumbn...\"],[\"## Table of Contents\\n\\n- [Introduction](#object-detection-leaderboard-decoding-metrics-and-their-pote...\"],[\"The following image, for instance, shows five detections: one \\\"ball\\\" with a confidence of 98% and fo...\"],[\"Evaluating an object detection model encompasses several components, like a dataset with ground-trut...\"],[\"\\u003cdiv display=\\\"block\\\" margin-left=\\\"auto\\\" margin-right=\\\"auto\\\" width=\\\"50%\\\"\\u003e\\n\\u003ccenter\\u003e\\n    \\u003cimg src=\\\"http...\"],[\"Conversely, negatives are evaluated based on a ground-truth bounding box and can be defined as False...\"],[\"Note that TP, FP, and FN depend on a predefined IoU threshold, as do Precision and Recall.\\n\\nAverage ...\"],[\"\\u003cdiv display=\\\"block\\\" margin-left=\\\"auto\\\" margin-right=\\\"auto\\\" width=\\\"50%\\\"\\u003e\\n\\u003ccenter\\u003e\\n    \\u003cimg src=\\\"http...\"],[\"Note that by rule 2, in image 1, \\\"E\\\" is TP while \\\"D\\\" is FP because IoU between \\\"E\\\" and the ground-tr...\"],[\"\\u003cp style=\\\"text-align: center;\\\"\\u003e\\n\\\\\\\\( \\\\text{Precision} = \\\\frac{\\\\text{acc TP}}{(\\\\text{acc TP} + \\\\text{a...\"],[\"By examining the curve, one may infer the potential trade-offs between Precision and Recall and find...\"],[\"The red points are placed according to the following:\\n\\n\\u003cp style=\\\"text-align: center;\\\"\\u003e\\n\\\\\\\\( \\\\rho_{\\\\te...\"],[\"### What are the variants of Average Precision and Average Recall?\\n\\nBased on predefined IoU threshol...\"],[\"* **AP-S**: It applies AP@[.5:.05:.95] considering (small) ground-truth objects with \\\\\\\\( \\\\text{area}...\"],[\"We recently released the [Object Detection Leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhf-vision\\u002fobje...\"],[\"### How to pick the best model based on the metrics?\\n\\nSelecting an appropriate metric to evaluate an...\"],[\"#### Thresholding detections before evaluation\\n\\nOur sample model uses the [`DetrImageProcessor` clas...\"],[\"```\\n\\nThe parameter `threshold` in function `post_process_object_detection` is used to filter the det...\"],[\"Figure 10 shows the process with batch size = 2, where the same two images are processed with `DetrI...\"],[\"It's important to recognize that models can produce boxes in various formats, and that also may be t...\"],[\"For such models, different prompts (e.g. \\\"Find the dog\\\" and \\\"Where's the bulldog?\\\") may result in th...\"],[\"| Use Case                                     | Real-world Scenarios                  | Recommended...\"],[\"The results shown in our ğŸ¤— [Object Detection Leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhf-vision\\u002fob...\"],[\"--\\ntitle: \\\"The Falcon has landed in the Hugging Face ecosystem\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f147_falcon\\u002f...\"],[\"## The Falcon models\\n\\nThe Falcon family is composed of two base models: [Falcon-40B](https:\\u002f\\u002fhugging...\"],[\"Falcon-7B and Falcon-40B have been trained on 1.5 trillion and 1 trillion tokens respectively, in li...\"],[\"| Model | License | Commercial use? | Pretraining length [tokens] | Pretraining compute [PF-days] | ...\"],[\"Under the hood, this playground uses Hugging Face's [Text Generation Inference](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"# Inference\\n\\nYou can use the familiar transformers APIs to run the models on your own hardware, but ...\"],[\"```\\n\\nAnd then, you'd run text generation using code like the following:\\n\\n```python\\nsequences = pipel...\"],[\"```\\n\\nNote, however, that mixed 8-bit inference will use `torch.float16` instead of `torch.bfloat16`,...\"],[\"Text Generation Inference is now integrated inside Hugging Face's [Inference Endpoints](https:\\u002f\\u002fhugg...\"],[\"The results show that the 40B base and instruct models are very strong, and currently rank 1st and 2...\"],[\"| Model | Type | Average leaderboard score |\\n| :---: | :---: | :---: |\\n| [tiiuae\\u002ffalcon-40b-instruct...\"],[\"Although the open LLM leaderboard doesn't measure chat capabilities (where human evaluation is the g...\"],[\"More specifically, after selecting the target modules to adapt (in practice the query \\u002f key layers o...\"],[\"```python\\nfrom datasets import load_dataset\\nfrom trl import SFTTrainer\\nfrom transformers import Auto...\"],[\"```\\n\\nCheck out the [original qlora repository](https:\\u002f\\u002fgithub.com\\u002fartidoro\\u002fqlora\\u002f) for additional de...\"],[\"--\\ntitle: \\\"Gradio 3.0 is Out!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f68_gradio_blocks\\u002fblock-party.png\\nauthors:\\n- u...\"],[\"\\u003cimg class=\\\"max-w-full mx-auto my-6\\\" style=\\\"width: 54rem\\\" src=\\\"\\u002fblog\\u002fassets\\u002f68_gradio_blocks\\u002fdalle.j...\"],[\"```python\\nimport numpy as np\\nimport gradio as gr\\n\\ndef flip_text(x):\\n    return x[::-1]\\n\\ndef flip_ima...\"],[\"```\\n\\nOnce you run `launch()`, the following demo will appear:\\n\\n\\u003cimg class=\\\"max-w-full mx-auto my-6\\\" ...\"],[\"--\\ntitle: \\\"Releasing Swift Transformers: Run On-Device LLMs in Apple Devices\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Let's go!\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cvideo controls title=\\\"Llama 2 (7B) chat model running on an M1 MacB...\"],[\"## Tasks Overview\\n\\nWhen I published tweets showing [Falcon](https:\\u002f\\u002ftwitter.com\\u002fpcuenq\\u002fstatus\\u002f166460...\"],[\"- [Conversion to Core ML](#conversion-to-core-ml). We'll use Llama 2 as a real-life example.\\n- [Opti...\"],[\"## Conversion to Core ML\\n\\nCore ML is Apple's native framework for Machine Learning, and also the nam...\"],[\"2. Use [`exporters`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fexporters), a Python conversion package built on...\"],[\"- If you have to use `coremltools`, use the latest version: `7.0b1`. Despite technically being a bet...\"],[\"There are a few key optimization areas we've identified. They are a very important topic for us and ...\"],[\"### Tokenizers\\n\\nTokenization solves two complementary tasks: adapt text input to the tensor format u...\"],[\"```\\n  \\\"normalizer\\\": {\\n    \\\"type\\\": \\\"Sequence\\\",\\n    \\\"normalizers\\\": [\\n      {\\n        \\\"type\\\": \\\"Prepend\\\"...\"],[\"```\\n\\nIt reads like this: normalization is a sequence of operations applied in order. First, we `Prep...\"],[\"```\\n\\nHowever, you don't usually need to tokenize the input text yourself - the [`Generation` code](h...\"],[\"- Greedy decoding. This is the obvious algorithm: select the token with the highest probability, app...\"],[\"The app is intentionally simple to make it readable and concise. It also lacks a few features, prima...\"],[\"In my experience, there are two frequent reasons why PyTorch models fail to convert to Core ML using...\"],[\"![Llama 2 conversion error](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"What the error screenshot is telling us is that there's a type mismatch trying to fill the mask tens...\"],[\"Fortunately, `coremltools` coverage for new operations is good and the team reacts very fast.\\n\\n## Re...\"],[\"---\\ntitle: \\\"Making ML-powered web games with Transformers.js\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fml-web-games\\u002f...\"],[\"## Overview\\n\\nBefore we start, let's talk about what we'll be creating. The game is inspired by Googl...\"],[\"### Model architecture\\n\\nWe'll be finetuning [`apple\\u002fmobilevit-small`](https:\\u002f\\u002fhuggingface.co\\u002fapple\\u002fm...\"],[\"3. Defining our [collate function](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_classes\\u002fdata_collat...\"],[\"### Converting our model to ONNX\\n\\nFortunately, the [ğŸ¤— Optimum](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum) ...\"],[\"```\\n\\n2. Run the conversion script (it uses `Optimum` under the hood):\\n\\n    ```bash\\n    python -m scr...\"],[\"```\\n\\nWe can now use this worker in our `App.jsx` file by adding the following code to the `App` comp...\"],[\"```\\n\\n*Woohoo!* ğŸ¥³ Although the above code is just a small part of the [final product](https:\\u002f\\u002fgithub....\"],[\"### Quality of life improvements\\n\\nThe original dataset contains 345 different classes, and since our...\"],[\"--\\ntitle: \\\"Deploy MusicGen in no time with Inference Endpoints\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002frun-musicge...\"],[\"### Let's go!\\n\\nFirst, we will duplicate the [facebook\\u002fmusicgen-large](https:\\u002f\\u002fhuggingface.co\\u002ffaceboo...\"],[\"```\\n\\nLet's hear what it sounds like:\\n\\n\\u003caudio controls\\u003e\\n\\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002f...\"],[\"```\\n\\nLet's give it a listen:\\n\\n\\u003caudio controls\\u003e\\n\\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingf...\"],[\"# preprocess\\n        inputs = self.processor(\\n            text=[inputs],\\n            padding=True,\\n ...\"],[\"```\\n\\nTo keep things simple, in this example we are only generating audio from text, and not conditio...\"],[\"```\\n\\nHere's how it sounds like:\\n\\n\\u003caudio controls\\u003e\\n\\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggi...\"],[\"--\\n\\ntitle: \\\"Results of the Open Source AI Game Jam\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fgame-jam-first-edition-r...\"],[\"The games were evaluated by their peers and contributors based on three key criteria: **fun, creativ...\"],[\"In addition to be the winner of the Game Jam, Snip it has been selected as the top participant selec...\"],[\"In this game, you are the village's last hope. Arm yourself before embarking on your adventure, and ...\"],[\"In this game, you are dropped into an arena battle. Defeat your opponents, then upgrade your deck an...\"],[\"In this game, you dive into an exhilarating bullet-hell journey to become the Star Prince's bride an...\"],[\"For those interested in AI for games, we have compiled a list of valuable resources, including AI to...\"],[\"Contrastive Search\\n\\nThis is a companion notebook to the [Hugging Face guest blog post entry about co...\"],[\"```\\n\\n## 3. Contrastive Search:\\n\\n### 3.1. Generating Text with Contrastive Search:\\n\\n\\n```python\\nimport...\"],[\"```\\n\\n#### 4.1.1. Generating Text with Greedy Search:\\n\\n\\n```python\\noutput = model.generate(input_ids, ...\"],[\"```\\n\\n#### 4.2.1. Generating Text with Greedy Search:\\n\\n\\n```python\\noutput = model.generate(input_ids, ...\"],[\"```\\n\\n# TensorFlow\\n\\nâš ï¸ The TensorFlow version of Contrastive Search is not yet released -- it will be...\"],[\"```\\n\\n### 2.2. Stochastic Methods:\\n\\n\\n```python\\nimport tensorflow as tf\\nfrom transformers import AutoT...\"],[\"```\\n\\n## 4. More Generated Examples:\\n\\n### 4.1. Example One - GPT-2:\\n\\n\\n```python\\n# Load the language m...\"],[\"```\\n\\n### 4.2. Example Two - OPT:\\n\\n\\n```python\\n# Load the language model and prepare the prefix text:\\n...\"],[\"--\\ntitle: ğŸ§¨ Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e\\nthumbnail: \\u002fblog\\u002fas...\"],[\"[Google Cloud TPUs](https:\\u002f\\u002fcloud.google.com\\u002ftpu) are custom-designed AI accelerators, which are opt...\"],[\"In this blog post,\\n1. [We describe why JAX + TPU + Diffusers is a powerful framework to run SDXL](#w...\"],[\"#### High-performance throughput for high batch sizes\\n\\nWorkloads can be scaled across multiple devic...\"],[\"```\\n\\nWe'll now load the base SDXL model and the rest of the components required for inference. The d...\"],[\"```\\n\\nTo take advantage of parallelization, we'll replicate the inputs across devices. A Cloud TPU v5...\"],[\"```\\n\\n`jit=True` indicates that we want the pipeline call to be compiled. This will happen the first ...\"],[\"```\\n\\nIt now took about 2s to generate the 4 images!\\n\\n## Benchmark\\n\\nThe following measures were obtai...\"],[\"## How does the demo work?\\n\\nThe [demo we showed before](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgoogle\\u002fsdxl) w...\"],[\"--\\ntitle: \\\"Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia\\\"\\nthumbnail: \\u002f...\"],[\"The real value of AWS Inferentia instances compared to GPU comes through the multiple Neuron Cores a...\"],[\"## 1. Convert your Hugging Face Transformer to AWS Neuron\\n\\nWe are going to use the [AWS Neuron SDK f...\"],[\"```\\n\\nAfter we have installed the Neuron SDK we can load and convert our model. Neuron models are con...\"],[\"```\\n\\nAt the time of writing, the [AWS Neuron SDK does not support dynamic shapes](https:\\u002f\\u002fawsdocs-ne...\"],[\"```\\n\\n## 2. Create a custom `inference.py` script for `text-classification`\\n\\nThe [Hugging Face Infere...\"],[\"```\\n\\nWe are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core ...\"],[\"```\\n\\n## 3. Create and upload the neuron model and inference script to Amazon S3\\n\\nBefore we can deplo...\"],[\"```\\n\\nNow we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`.\\n\\n```python\\nfrom...\"],[\"```\\n\\n## 5. Run and evaluate Inference performance of BERT on Inferentia\\n\\nTheÂ `.deploy()`Â returns anÂ ...\"],[\"```\\n\\nThe average latency for our BERT model is `5-6ms` for a sequence length of 128.\\n\\u003cbr\\u003e\\n\\u003cfigure cl...\"],[\"```\\n\\n## Conclusion\\n\\nWe successfully managed to compile a vanilla Hugging Face Transformers model to ...\"],[\"--\\ntitle: \\\"How to train your model dynamically using adversarial data\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f88_mn...\"],[\"![](https:\\u002f\\u002fi.imgur.com\\u002f1OiMHhE.png)\\n\\n\\u003e Image source: [mnist | Tensorflow Datasets](https:\\u002f\\u002fwww.tens...\"],[\"def forward(self, x):\\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\\n        x = F.relu(F.max_po...\"],[\"```\\n\\nNow that you have defined the structure of your model, you need to train it on the standard MNI...\"],[\"### Flagging your model\\n  \\nWere you able to fool the model above?ğŸ˜€ If yes, then it's time to _flag_ ...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fchrisjay-mnist-adversarial.hf.space\\\" frameBorder=\\\"0\\\" width=\\\"100%\\\" height=\\\"1400p...\"],[\"--\\ntitle: \\\"Rocket Money x Hugging Face: Scaling Volatile ML Models in Productionâ€‹\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"We decided to start from a clean slate, assembling both a new team and a new mandate. Our first task...\"],[\"In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for...\"],[\"Once the contract was signed, we began the migration of moving off our regex based system to direct ...\"],[\"Speaking of scale, as we started to witness a significant increase in traffic to our model, it becam...\"],[\"_If you want to learn how Hugging Face can manage your ML inference workloads, contact the Hugging F...\"],[\"--\\ntitle: \\\"Leveraging Hugging Face for complex generative AI use cases\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_m...\"],[\"--\\ntitle: \\\"ControlNet in ğŸ§¨ Diffusers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fcontrolnet\\u002fthumbnail.png \\nauthors:\\n- ...\"],[\"Or even  use it as your interior designer.\\n\\n\\u003ctable\\u003e\\n\\u003ctr style=\\\"text-align: center;\\\"\\u003e\\n    \\u003cth\\u003eBefore\\u003c...\"],[\"Also, make some of the famous logos coming to life.\\n\\n\\u003ctable\\u003e\\n\\u003ctr style=\\\"text-align: center;\\\"\\u003e\\n    \\u003ct...\"],[\"Training ControlNet is comprised of the following steps:\\n\\n1. Cloning the pre-trained parameters of a...\"],[\"A sample from the training set for ControlNet-like training looks like this (additional conditioning...\"],[\"Every new type of conditioning requires training a new copy of ControlNet weights. \\nThe paper propos...\"],[\"We will explore different use cases with the `StableDiffusionControlNetPipeline` in this blog post. ...\"],[\"```\\n\\nTo process different conditionings depending on the chosen ControlNet, we also need to install ...\"],[\"```\\n\\nAs we can see, it is essentially edge detection:\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nInstead of using Stable Diffusion's default [PNDMScheduler](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffuse...\"],[\"```\\n\\nInstead of loading our pipeline directly to GPU, we instead enable smart CPU offloading which \\n...\"],[\"```\\n\\nNow we are ready to run the ControlNet pipeline!\\n\\nWe still provide a prompt to guide the image ...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve\\u002fmai...\"],[\"```\\n\\nIt is noticeable that Mr Potato Head is not the best candidate but he tried his best and did a ...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We welcome you to combine these different elements and share your results with [@diffuserslib](https...\"],[\"If you cannot wait to try out ControlNet directly, we got you covered as well! Simply click on one o...\"],[\"--\\ntitle: Universal Image Segmentation with Mask2Former and OneFormer\\nthumbnail: \\u002fblog\\u002fassets\\u002f127_ma...\"],[\"Image segmentation can largely be split into 3 subtasks - instance, semantic and panoptic segmentati...\"],[\"Over the last years, researchers have come up with several architectures that were typically very ta...\"],[\"[Mask2Former](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fmodel_doc\\u002fmask2former) extends this to i...\"],[\"Note that Mask2Former still needs to be trained on each task separately to obtain state-of-the-art r...\"],[\"```\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = Au...\"],[\"```\\nprediction = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1...\"],[\"```\\nfrom collections import defaultdict\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f12...\"],[\"We hope you enjoyed this post and learned something. Feel free to let us know whether you are satisf...\"],[\"--\\ntitle: \\\"A Dive into Text-to-Video Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f140_text-to-video\\u002fthumbnail.png...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"The text-to-video task faces unique challenges on multiple fronts. Some of these main challenges inc...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"Text2Video-Zero is a text-guided video generation and manipulation framework that works in a fashion...\"],[\"These large datasets experience similar issues to those found in text-to-image datasets. The most co...\"],[\"### Hugging Face Demos\\nAt Hugging Face, our goal is to make it easier to use and build upon state-of...\"],[\"\\u003cgradio-app theme_mode=\\\"light\\\" space=\\\"Tune-A-Video-library\\u002fTune-A-Video-Training-UI\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\n...\"],[\"```\\ngit clone https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdamo-vilab\\u002fmodelscope-text-to-video-synthesis\\ncd modelsc...\"],[\"```\\n\\n### Community Contributions and Open Source Text-to-Video Projects\\nFinally, there are various o...\"],[\"That was it! We are continuing to integrate the most impactful computer vision and multi-modal model...\"],[\"--\\ntitle: \\\"Stable Diffusion XL on Mac with Advanced Core ML Quantization\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fst...\"],[\"For Stable Diffusion XL weâ€™ve done a few things:\\n* Ported the [base model to Core ML](https:\\u002f\\u002fhuggin...\"],[\"## Using SD XL Models from the Hugging Face Hub\\n\\nAs part of this release, we published two different...\"],[\"For reference, these are the performance figures we achieved on different devices:\\n\\n|        Device ...\"],[\"We explored a different alternative instead: **mixed-bit palettization**. Instead of using 6 bits pe...\"],[\"```\\n\\nWhat this tells us is that the original model quality, as measured by PSNR in float16, is about...\"],[\"Some initial conclusions:\\n- In our opinion, all the images have good quality in terms of how realist...\"],[\"Mixed-bit palettization runs in two phases: _analysis_ and _application_.\\n\\nThe goal of the analysis ...\"],[\"For an introduction to the process, check the [instructions in the repo](https:\\u002f\\u002fgithub.com\\u002fapple\\u002fml...\"],[\"### Published Resources\\n\\n* [`apple\\u002fml-stable-diffusion`](https:\\u002f\\u002fgithub.com\\u002fapple\\u002fml-stable-diffusio...\"],[\"--\\ntitle: \\\"An Introduction to Deep Reinforcement Learning\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f63_deep_rl_intro\\u002f...\"],[\"Deep RL is a type of Machine Learning where an agent learnsÂ **how to behave**Â in an environmentÂ **by...\"],[\"In this free course, you will:\\n\\n- ğŸ“– Study Deep Reinforcement Learning in **theory and practice**.\\n- ...\"],[\"So letâ€™s get started! ğŸš€\\n\\n- [What is Reinforcement Learning?](#what-is-reinforcement-learning)\\n  - [T...\"],[\"Your brother will interact with the environment (the video game) by pressing the right button (actio...\"],[\"To understand the RL process, letâ€™s imagine an agent learning to play a platform game:\\n\\n\\u003cfigure clas...\"],[\"### **Markov Property**\\n\\nIn papers, youâ€™ll see that the RL process is called theÂ **Markov Decision P...\"],[\"In Super Mario Bros, we only see a part of the level close to the player, so we receive an observati...\"],[\"Taking this information into consideration is crucial because it willÂ **have importance when choosin...\"],[\"As we can see in theÂ diagram,Â **itâ€™s more probable to eat the cheese near us than the cheese close t...\"],[\"For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ...\"],[\"In this game, our mouse can have anÂ **infinite amount of small cheese**Â (+1 each). But at the top of...\"],[\"## **The two main approaches for solving RL problems**\\n\\nâ‡’ Now that we learned the RL framework, how ...\"],[\"In Policy-Based Methods,Â **we learn a policy function directly.**\\n\\nThis function will map from each ...\"],[\"If we recap:\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f63_deep_rl_intr...\"],[\"Thanks to our value function, at each step our policy will select the state with the biggest value d...\"],[\"That was a lot of information, if we summarize:\\n\\n- Reinforcement Learning is a computational approac...\"],[\"---\\nNow that you've studied the bases of Reinforcement Learning, youâ€™re ready to train your first la...\"],[\"Naturally, during the course,Â **weâ€™re going to use and explain these terms again**, but itâ€™s better ...\"],[\"--\\ntitle: \\\"Scaling up BERT-like model Inference on modern CPU  - Part 2\\\"\\nauthors:\\n- user: echarlaix\\n...\"],[\"Back in April, Intel launched its [latest generation of Intel Xeon processors](https:\\u002f\\u002fwww.intel.com...\"],[\"In this area, Intel plays an essential role by providing software components under the oneAPI umbrel...\"],[\"## Deep Dive: Leveraging advanced Intel features to improve AI performances\\n\\n### Performance tuning ...\"],[\"While itâ€™s very complex to apply generic optimizations to different object structures and layouts, t...\"],[\"Still, implementing parallel algorithms might not be as simple as throwing more cores to do the work...\"],[\"In every programmer toolkit, there are multiple levels which can bring mathematical operations suppo...\"],[\"## More Efficient AI Processing on latest Intel Ice Lake CPUs\\n\\nIn order to report the performances o...\"],[\"You can find the same processors on the various cloud providers: \\n- [AWS m6i \\u002f c6i instances](https:...\"],[\"#### Baseline: Eager frameworks latencies \\n\\nFrameworks operating in eager mode usually discover the ...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003c\\u002ffigure\\u003e\\n\\u003cbr\\u003e\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"r...\"],[\"The global trend highlights the positive impact of the number of cores on the observed latencies. \\nI...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003c\\u002ffigure\\u003e\\n\\u003cbr\\u003e\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"r...\"],[\"This is often referred to as â€œtracingâ€ the graph and, as you can see here, the results are not that ...\"],[\"Among these libraries, we can cite a few of them such as [tcmalloc](), [jemalloc]() and [mimalloc]()...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003c\\u002ffigure\\u003e\\n\\u003cbr\\u003e\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"r...\"],[\"As per the graph above, you can notice that the standard library allocator (glibc) is often behind p...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003c\\u002ffigure\\u003e\\n\\u003cbr\\u003e\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"r...\"],[\"This time, by knowing the underlying structure of the operator flows and matrix shapes involved then...\"],[\"OpenMP exposes [many environment variables](https:\\u002f\\u002fwww.openmp.org\\u002fspec-html\\u002f5.0\\u002fopenmpch6.html) to ...\"],[\"As stated above, tuning OpenMP is something you can start to tweak when you tried all the other, sys...\"],[\"Fortunately, Intel's [SigOpt](https:\\u002f\\u002fsigopt.com\\u002f), through Bayesian optimization, allows us to make...\"],[\"\\u003ctable class=\\\"block mx-auto\\\"\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e\\n        \\u003cfigure class=\\\"image table text-center m-0 w-f...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e\\n        \\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n            \\u003cmed...\"],[\"As expected, the number of cores is, by far, the most important parameter, but the others play a par...\"],[\"## Conclusion - Accelerating Transformers for Production\\n\\nIn this post, we showed how the new Intel ...\"],[\"--\\ntitle: \\\"Supercharged Searching on the ğŸ¤— Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f48_hubsearch\\u002fthumbnail.png\\na...\"],[\"```\\n\\n## Situating the Problem:\\n\\nFirst, let's imagine the scenario you are in. You'd like to find all...\"],[\"```\\n\\n    Available Attributes or Keys:\\n     * author\\n     * dataset\\n     * language\\n     * library\\n ...\"],[\"```\\n```\\n    ModelInfo: {\\n        modelId: Jiva\\u002fxlm-roberta-large-it-mnli\\n        sha: c6e64469ec4aa1...\"],[\"```\\n\\n## Taking it up a Notch\\n\\nWe saw how we could use the `ModelSearchArguments` and `DatasetSearchA...\"],[\"```\\n```\\n    [ModelInfo: {\\n     \\tmodelId: Jiva\\u002fxlm-roberta-large-it-mnli\\n     \\tsha: c6e64469ec4aa17fe...\"],[\"```\\n\\n\\nVery quickly we see that it's a much more coordinated approach for searching through the API, ...\"],[\"```\\n\\n\\n```python\\n\\u003e\\u003e\\u003e # As an attribute\\n\\u003e\\u003e\\u003e ad.3_c\\n```\\n     File \\\"\\u003cipython-input-6-c0fe109cf75d\\u003e\\\", lin...\"],[\"--\\ntitle: \\\"Optimization story: Bloom inference\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fbloom-inference-pytorch-scri...\"],[\"There are several things to note that will come back later:\\n\\nWe needed to have smaller models [bigsc...\"],[\"To be perfectly fair `bfloat16` -\\u003e `float16` conversion seemed to be OK in inference\\nmode (`bfloat16...\"],[\"```\\nNote: Pipeline Parallelism (PP) means in this context that each GPU will own\\nsome layers so each...\"],[\"```\\n\\nNow we have a workable `transformers` clean version of the start\\nworking on running this.\\n\\nBloo...\"],[\"We made a testing script in [locust](https:\\u002f\\u002flocust.io\\u002f) which is exactly this:\\n\\n```python\\nfrom locu...\"],[\"```\\n**Note: This is not the best nor the only load testing we used, but it was\\nalways the first to b...\"],[\"Let's do the math and we are getting `17 TFlop` for a single forward pass.\\nLooking at the [specs](ht...\"],[\"```\\nNote: Tensor Parallelism (TP) means in this context that each GPU will own\\npart of the weights, ...\"],[\"```\\n\\nNow that we have a good understanding of where we stand it's time to get to work.\\n\\nWe tried man...\"],[\"Results:\\n\\n  - Porting was not an easy task as some conditions and kernels were hard to\\n    reproduce...\"],[\"## Using ONNX\\u002fTRT or other compiled approaches\\n  - They are supposed to handle most of the optimizat...\"],[\"## DeepSpeed\\n  - This is the technology that powered training, it seemed only fair to use\\n    it for...\"],[\"Results:\\n\\n  - We recoded everything in `Rust` with the excellent bindings [tch-rs](https:\\u002f\\u002fgithub.co...\"],[\"- Alibi is used in Bloom to add position embeddings and it was calculated in too\\nmany places, we cou...\"],[\"## Supporting TP\\n\\nOk, we have removed most of the low-hanging fruits now we went roughly from 350ms\\u002f...\"],[\"```\\n\\nto \\n\\n```python\\n@torch.jit.script\\ndef bloom_gelu_forward(x):\\n    return x * 0.5 * (1.0 + torch.t...\"],[\"```\\n\\nThis transforms the operations from multiple small element-wise kernels (and hence tensor copie...\"],[\"Another place where we had to be extra careful, was the initial forward pass (without\\npast) and the ...\"],[\"```\\n\\nThe first masked fill is creating a new tensor, which is here only to \\nsay to the softmax opera...\"],[\"Then we had to drop the use [generate](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.22.2\\u002fen\\u002fmain_clas...\"],[\"## Have you tried ...?\\n\\nStuff we know exists and haven't used because of various reasons. It \\ncould ...\"],[\"## [OpenAI Triton](https:\\u002f\\u002fopenai.com\\u002fblog\\u002ftriton\\u002f)\\n\\n[Triton](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002ftriton) is a...\"],[\"# Acknowledgments\\n\\nAll this work results of the collaboration of many HF team members. In no particu...\"],[\"--\\ntitle: \\\"Welcome spaCy to the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f23_spacy\\u002fthumbnail.png\\n\\nau...\"],[\"\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fspacy\\u002fen_core_web_sm\\\"\\u003e\\u003ccode\\u003espacy\\u002fen_core_we...\"],[\"\\u003cdiv class=\\\"SVELTE_HYDRATER \\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingfac...\"],[\"Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.8379609817}]}},{&quot;tasks&quo...\"],[\"Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.893607046}]}},{&quot;tasks&quot...\"],[\"ot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot...\"],[\"},{&quot;rfilename&quot;:&quot;LICENSE&quot;},{&quot;rfilename&quot;:&quot;LICENSES_SOURCES&quot;},{...\"],[\"uot;},{&quot;rfilename&quot;:&quot;lemmatizer\\u002flookups\\u002flookups.bin&quot;},{&quot;rfilename&quot;:&quo...\"],[\"cense:mit&quot;,&quot;model-index&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;token-classific...\"],[\"Classification&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;spacy&quot;,&q...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"\\u002fdocs\\\"\\u003e\\u003csvg class=\\\"ml-1.5 text-sm text-gray-400 ...\"],[\"meet\\\" viewBox=\\\"0 0 18 18\\\"\\u003e\\u003cpath d=\\\"M11.075 10.1875H12.1625V11.275H11.075V10.1875Z\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003cpath d=\\\"M...\"],[\"13.1616 9.87255 12.8852 9.66867 12.6813C9.46478 12.4774 9.18834 12.3628 8.90001 12.3625V12.3625ZM2.3...\"],[\"5.83781 9.42273 5.95247 9.21885 6.15636C9.01496 6.36024 8.9003 6.63668 8.90001 6.92502V8.01252C8.900...\"],[\"2.28668 1.40246 2.01024 1.60635 1.80636C1.81023 1.60247 2.08667 1.48781 2.37501 1.48752H4.55001C4.83...\"],[\"items-center text-xs text-gray-500\\\"\\u003e\\u003cbutton class=\\\"flex items-center cursor-not-allowed text-gray-30...\"],[\"### Using existing models\\n\\nAll models from the Hub can be directly installed using `pip install`. \\n\\n...\"],[\"```\\n\\n```python\\n# Using spacy.load().\\nimport spacy\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\n# Importing as...\"],[\"```\\n\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browse...\"],[\"--\\ntitle: \\\"Porting fairseq wmt19 translation system to transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f07_port...\"],[\"## Let's cheat\\n\\nThe first step was to cheat, of course. Why make a big effort when one can make a li...\"],[\"```\\nmkdir ~\\u002fporting\\ncd ~\\u002fporting\\n```\\n\\nWe need to install a few things for this work:\\n\\n```\\n# install ...\"],[\"```\\n\\n## Files\\n\\nAs a quick overview, the following files needed to be created and written:...\"],[\"* [`src\\u002ftransformers\\u002fconfiguration_fsmt.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002f129fda...\"],[\"* [`tests\\u002ftest_tokenization_fsmt.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002f129fdae04033f...\"],[\"There are other files that needed to be modified as well, we will talk about those towards the end.\\n...\"],[\"```\\nimport torch\\ntorch.hub.load('pytorch\\u002ffairseq', 'transformer.wmt19.en-ru', checkpoint_file='model...\"],[\"```\\nYou may have more than one entry there if you have been using the `hub` for other models.\\n\\nLet's...\"],[\"```\\nwe have:\\n1. `model*.pt` - 4 checkpoints (pytorch `state_dict` with all the pre-trained weights, ...\"],[\"```\\n\\nIf we combine the first two and the last two steps we get 3 stages:\\n\\n1. **Encode input**: break...\"],[\"Let's see how this approach helps to reduce memory and computation requirements. If we have an input...\"],[\"### fairseq's tokenizer workings\\n\\nLet's understand how `fairseq`'s tokenizer works.\\n\\n`fairseq` (*) u...\"],[\"```\\nimport torch\\nsentence = \\\"Machine Learning is great\\\"\\ncheckpoint_file='model4.pt'\\nmodel = torch.hu...\"],[\"```\\n\\nYou can see that `model.encode` does `tokenize+apply_bpe+binarize` - as we get the same output....\"],[\"```\\ne n\\u003c\\u002fw\\u003e 1423551864\\ne r\\u003c\\u002fw\\u003e 1142368899\\nth e\\u003c\\u002fw\\u003e 432025210\\n```\\nIf the second column doesn't includ...\"],[\"```\\n$ grep -i ^mach  ~\\u002fporting\\u002fpytorch_fairseq_model\\u002fbpecodes\\nmach ine\\u003c\\u002fw\\u003e 463985\\nMach t 376252\\nMach...\"],[\"```\\n('apply_bpe: ', 'Mach@@ ine Lear@@ ning is great')\\n('binarize: ', 7, tensor([10217,  1419,     3...\"],[\"```\\nfrom fairseq.data.dictionary import Dictionary\\ndef rewrite_dict_keys(d):\\n    # (1) remove word b...\"],[\"```\\n\\nAfter running the conversion script, let's check the converted dictionary:\\n\\n```\\n$ grep '\\\"Mach\\\"'...\"],[\"```\\nto:\\n```\\n['Mach', 'ine\\u003c\\u002fw\\u003e', 'Lear', 'ning\\u003c\\u002fw\\u003e', 'is\\u003c\\u002fw\\u003e', 'great\\u003c\\u002fw\\u003e']\\n```\\nInstead of marking ch...\"],[\"```\\nand with very few changes I had a working encoder part of the tokenizer. There was a lot of code...\"],[\"```\\nJust make sure you're checking out the repository [around the time fsmt was released](https:\\u002f\\u002fgi...\"],[\"```\\n\\nThis was my starting point that I needed to tweak to work with the model weights provided by `f...\"],[\"```\\nFirst I looked at the model:\\n```\\nprint(ru2en[\\\"models\\\"][0])\\n```\\n```\\nTransformerModel(\\n  (encoder)...\"],[\"```\\nwhich looked very similar to BART's architecture, with some slight differences in a few layers -...\"],[\"```\\nargs = dict(vars(ru2en[\\\"args\\\"]))\\npprint(args)\\n```\\n```\\n 'activation_dropout': 0.0,\\n 'activation_f...\"],[\"```\\n    model_conf = {\\n        \\\"architectures\\\": [\\\"FSMTForConditionalGeneration\\\"],\\n        \\\"model_typ...\"],[\"```\\n\\nWe have the configuration and the model's `state_dict` ported - yay!\\n\\nYou will find the final c...\"],[\"Now that I have used this debugger to port FSMT, I know that it would have taken me many times over ...\"],[\"I first did this process for the simpler no-beam search, and once the outputs were 100% matching I r...\"],[\"```\\n     def convert_tokens_to_string(self, tokens):\\n         \\\"\\\"\\\" Converts a sequence of tokens (str...\"],[\"```\\nSo if, for example, I only needed to update all the `config.json` files, the script above gave m...\"],[\"```\\ntokenizer = FSMTTokenizer.from_pretrained(\\\"\\u002fcode\\u002fhuggingface\\u002ftransformers-fair-wmt\\u002fdata\\u002fwmt19-en...\"],[\"```\\n\\nThen the are the pipelines, which completely hide all the NLP complexities from the end user an...\"],[\"```\\nIn the `grep` search I excluded the files that also include those classes.\\n\\n\\n## Manual testing\\n\\n...\"],[\"## Porting other models\\n\\nI next proceeded to port the `en-de` and `de-en` models. \\n\\nI was surprised ...\"],[\"Just like with the code, I started by copying `tests\\u002ftest_modeling_bart.py` and converting it to use...\"],[\"## SinusoidalPositionalEmbedding\\n\\n`fairseq` used a slightly different implementation of `SinusoidalP...\"],[\"```\\nexport PAIR=ru-en\\nexport MODEL=facebook\\u002fwmt19-$PAIR\\nexport DATA_DIR=data\\u002f$PAIR\\nexport SAVE_DIR=d...\"],[\"```\\nYou can see that the BLEU score was `39.0498` and that it evaluated using 2000 test inputs, prov...\"],[\"Currently, this ported model is slightly behind the original on the BLEU scores, because model ensem...\"],[\"```\\n# search space\\nexport PAIR=ru-en\\nexport DATA_DIR=data\\u002f$PAIR\\nexport SAVE_DIR=data\\u002f$PAIR\\nexport BS...\"],[\"```\\nYou can see that in the case of `transformers` `early_stopping=False` performs better (`fairseq`...\"],[\"```\\n---\\nlanguage: \\n- en\\n- ru\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- facebook\\nlicense: apache-2.0\\nd...\"],[\"```\\nmake docs\\n```\\nto test that the newly added document was building correctly. The file I needed to...\"],[\"```\\n\\nThen I went to github and submitted this [PR](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fpull\\u002f...\"],[\"- The PR merging process took a good couple of weeks before it was accepted. During this stage, besi...\"],[\"```\\nc = get_config()\\n# Run all nodes interactively\\nc.InteractiveShell.ast_node_interactivity = \\\"all\\\"...\"],[\"--\\ntitle: \\\"Understanding BigBird's Block Sparse Attention\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f18_big_bird\\u002fattn....\"],[\"**BigBird RoBERTa-like** model is now available in ğŸ¤—Transformers. The goal of this post is to give t...\"],[\"---\\n\\nIn this blog post, we will try to answer those questions.\\n\\n### What tokens should be attended t...\"],[\"```\\n\\nNearby tokens should be important because, in a sentence (sequence of words), the current word ...\"],[\"```\\n\\n* **Random tokens:** Select some tokens randomly which will transfer information by transferrin...\"],[\"```\\n\\nThis way, the query token attends only to a subset of all possible tokens while yielding a good...\"],[\"![](assets\\u002f18_big_bird\\u002fgraph.gif)\\n\\u003cimg src=\\\"assets\\u002f18_big_bird\\u002ffull.png\\\" width=230 height=230\\u003e\\n\\n**Bi...\"],[\"In case, we have many global tokens, then we may not need random connections since there will be mul...\"],[\"## BigBird block sparse attention\\n\\nBigBird block sparse attention is just an efficient implementatio...\"],[\"```python\\n# pseudo code\\n\\nQ -\\u003e Query martix (seq_length, head_dim)\\nK -\\u003e Key matrix (seq_length, head_...\"],[\"```\\n\\n### Sliding Attention\\n\\nThe sequence of key tokens is copied 2 times with each element shifted t...\"],[\"```\\n\\n### Random Attention\\n\\nRandom attention is ensuring that each query token will attend a few rand...\"],[\"```\\n\\n**Note:** The current implementation further divides sequence into blocks & each notation is de...\"],[\"![BigBird block sparse attention](assets\\u002f18_big_bird\\u002fq1.png)\\n\\\\\\\\(q_1\\\\\\\\) represents 1st block, \\\\\\\\(g_i\\\\...\"],[\"![BigBird block sparse attention](assets\\u002f18_big_bird\\u002fqlast_sec.png)\\n\\n---\\n\\nAttention score for \\\\\\\\(\\\\ma...\"],[\"*Comparison of time & space complexity of BERT attention and BigBird block sparse attention.*\\n\\n\\u003cdeta...\"],[\"```\\n\\n\\u003c\\u002fdetails\\u003e\\n\\n## ITC vs ETC\\n\\nThe BigBird model can be trained using 2 different strategies: **ITC...\"],[\"|                                              | ITC                                   | ETC        ...\"],[\"## Using BigBird with ğŸ¤—Transformers\\n\\nYou can use `BigBirdModel` just like any other ğŸ¤— model. Let's s...\"],[\"```\\n\\nThere are total **3 checkpoints** available in **ğŸ¤—Hub** (at the point of writing this article):...\"],[\"# forward pass\\n        output = model(**batch)\\n\\n        # back-propogation\\n        output[\\\"loss\\\"].ba...\"],[\"```\\n\\nIt's important to keep the following points in mind while working with big bird:\\n\\n* Sequence le...\"],[\"You will soon find **BigBird Pegasus-like** model in the library for **long document summarization**...\"],[\"--\\ntitle: \\\"Train your first Decision Transformer\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f101_train-decision-transfo...\"],[\"## What are Decision Transformers?\\n\\nThe Decision Transformer model was introduced byÂ **[â€œDecision Tr...\"],[\"There are different types of Decision Transformers, but today, weâ€™re going to train an offline Decis...\"],[\"First we need to import the `load_dataset` function from the ğŸ¤— datasets package and download the dat...\"],[\"```\\n\\nWhile most datasets on the hub are ready to use out of the box, sometimes we wish to perform so...\"],[\"def __init__(self, dataset) -\\u003e None:\\n        self.act_dim = len(dataset[0][\\\"actions\\\"][0])\\n        se...\"],[\"# get sequences from dataset\\n            s.append(np.array(feature[\\\"observations\\\"][si : si + self.ma...\"],[\"# padding and state + reward normalization\\n            tlen = s[-1].shape[1]\\n            s[-1] = np....\"],[\"s = torch.from_numpy(np.concatenate(s, axis=0)).float()\\n        a = torch.from_numpy(np.concatenate(...\"],[\"```\\n\\nThat was a lot of code, the TLDR is that we defined a class that takes our dataset, performs th...\"],[\"```\\n\\nThe transformers Trainer class required a number of arguments, defined in the TrainingArguments...\"],[\"```\\n\\nNow that we explained the theory behind Decision Transformer, the Trainer, and how to train it....\"],[\"--\\ntitle: Goodbye cold boot - how we made LoRA Inference 300% faster\\nthumbnail: \\u002fblog\\u002fassets\\u002f171_loa...\"],[\"Instead of fine-tuning the model by performing tiny changes to all its weights, we freeze most of th...\"],[\"There are far less blue base models than there are yellow ones on the Hub. If we can go quickly from...\"],[\"Now: request time has decreased from 35s to 13s since adapters will use only a few distinct \\\"blue\\\" b...\"],[\"### Loading\\u002fOffloading LoRA for Diffusers ğŸ§¨\\n\\n\\u003cdiv class=\\\"alert\\\"\\u003e\\n\\u003cp\\u003e\\nNote that there is a more seeml...\"],[\"inputs = \\\"elephant\\\"\\nkwargs = {}\\n\\nif torch.cuda.is_available():\\n    kwargs[\\\"torch_dtype\\\"] = torch.flo...\"],[\"```\\n\\n## Loading figures\\n\\nAll numbers below are in seconds:\\n\\n\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003cth\\u003eGPU\\u003c\\u002fth\\u003e\\n    \\u003ctd\\u003e...\"],[\"When a LoRA is requested, we'll look at the one that is loaded and change it only if required, then ...\"],[\"```\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fapi-inference-community.git\\n\\n$ cd api-inference-commu...\"],[\"```\\n\\n### What about batching ?\\n\\nRecently a really interesting [paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2311.032...\"],[\"--\\ntitle: \\\"Open-Source Text Generation & LLM Ecosystem at Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fos_l...\"],[\"![Causal LM Output](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fbl...\"],[\"On the Hugging Face Hub, you can find both causal language models and causal language models fine-tu...\"],[\"The second type of text generation model is commonly referred to as the text-to-text generation mode...\"],[\"### Models created with love by Hugging Face with BigScience and BigCode ğŸ’—\\n\\nHugging Face has co-led ...\"],[\"- [Falcon 40B](https:\\u002f\\u002fhuggingface.co\\u002ftiiuae\\u002ffalcon-40b)\\n- [XGen](https:\\u002f\\u002fhuggingface.co\\u002ftiiuae\\u002ffalc...\"],[\"There are two code generation models, [StarCoder by BigCode](https:\\u002f\\u002fhuggingface.co\\u002fmodels?sort=tren...\"],[\"- [StarChat Beta](https:\\u002f\\u002fhuggingface.co\\u002fHuggingFaceH4\\u002fstarchat-beta) is the instruction fine-tuned ...\"],[\"If you're looking to fine-tune a model on an existing instruction dataset, you need to know how a da...\"],[\"| Model                                                                                    | Dataset...\"],[\"| [MPT-30B](https:\\u002f\\u002fhuggingface.co\\u002fmosaicml\\u002fmpt-30b)                                       | Mix of ...\"],[\"| [FLAN-T5-XXL](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fflan-t5-xxl)                                 | [gsm8k]...\"],[\"| [StarChat-Î²](https:\\u002f\\u002fhuggingface.co\\u002fHuggingFaceH4\\u002fstarchat-beta)                     | [OpenAssist...\"],[\"### Text Generation Inference\\n\\nResponse time and latency for concurrent users are a big challenge fo...\"],[\"![HuggingChat Search](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002f...\"],[\"## Parameter Efficient Fine Tuning (PEFT)\\n\\nIf youâ€™d like to fine-tune one of the existing large mode...\"],[\"--\\ntitle: \\\"Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action\\\" \\nthumbnail: \\u002fblog\\u002fass...\"],[\"1. **Define a model**: Before you can use BentoML, you need a machine learning model (or multiple mo...\"],[\"## Table of contents\\n\\n- [A brief introduction to DeepFloyd IF](#a-brief-introduction-to-deepfloyd-if...\"],[\"## Preparing the environment\\n\\n[This GitHub repository](https:\\u002f\\u002fgithub.com\\u002fbentoml\\u002fIF-multi-GPUs-demo...\"],[\"```\\n\\nBefore building the application, letâ€™s briefly explore the key files within this directory:\\n\\n- ...\"],[\"```\\n\\nOnce the downloads are complete, view the models in the Model store.\\n\\n```bash\\n$ bentoml models ...\"],[\"```\\n\\n## Testing the server\\n\\nOnce the server starts, you can visit the web UI at http:\\u002f\\u002flocalhost:786...\"],[\"```\\n\\nView the Bento in the local Bento Store.\\n\\n```bash\\n$ bentoml list\\n\\nTag                          ...\"],[\"```\\n\\nYou can then deploy the model on Kubernetes.\\n\\n## Whatâ€™s next?\\n\\n[BentoML](https:\\u002f\\u002fgithub.com\\u002fben...\"],[\"--\\ntitle: \\\"Deep Q-Learning with Space Invaders\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_deep_rl_dqn\\u002fthumbnail.gif...\"],[\"We got excellent results with this simple algorithm. But these environments were relatively simple b...\"],[\"## From Q-Learning to Deep Q-Learning\\n\\nWe learned thatÂ **Q-Learning is an algorithm we use to train ...\"],[\"\\u003cimg src=\\\"assets\\u002f78_deep_rl_dqn\\u002fatari.jpg\\\" alt=\\\"Atari State Space\\\"\\u002f\\u003e\\n\\nTherefore, the state space is ...\"],[\"We can also **crop a part of the screen in some games** if it does not contain important information...\"],[\"The difference is that, during the training phase, instead of updating the Q-value of a state-action...\"],[\"### Experience Replay to make more efficient use of experiences\\n\\nWhy do we create a replay memory?\\n\\n...\"],[\"### Fixed Q-Target to stabilize the training\\n\\nWhen we want to calculate the TD error (aka the loss),...\"],[\"Instead, what we see in the pseudo-code is that we:\\n- Use a **separate network with a fixed paramete...\"],[\"Therefore, Double DQN helps us reduce the overestimation of q values and, as a consequence, helps us...\"],[\"Thatâ€™sÂ **normal if you still feel confused**Â with all these elements.Â **This was the same for me and...\"],[\"--\\ntitle: \\\"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"\\u003cp\\u003eIn 2017 a group of Google AI researchers published a paper introducing the transformer model arch...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002ftransformers_chrono.png?width=1024&amp;name=transf...\"],[\"\\u003cp\\u003eA timeline showing releases of prominent transformer language models (credit: Hugging Face)\\u003c\\u002fp\\u003e\\n\\u003c...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fvit%20diag.png?width=1024&amp;name=vit%20diag.png\\\"...\"],[\"\\u003cdiv class=\\\"blog-caption\\\" style=\\\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: ...\"],[\"\\u003ch2\\u003eViT models â€“ a perfect fit for IPU\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eGraphcore IPUs are particularly well-suited to ViT mo...\"],[\"\\u003cp\\u003eFor this blog post, we will use a ViT model pre-trained on ImageNet-21k, based on the paper \\u003ca hr...\"],[\"\\u003cp\\u003eAs an X-ray image can have multiple diseases, we will work with a multi-label classification mode...\"],[\"\\u003ch2\\u003eTraining ViT on the ChestXRay-14 dataset\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eFirst, we need to download the National Institu...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fchest%20x-ray%20examples.png?width=700&amp;name=ch...\"],[\"\\u003cdiv class=\\\"blog-caption\\\" style=\\\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: ...\"],[\"\\u003cp style=\\\"font-weight: bold;\\\"\\u003eWeâ€™ve even made it easier and created the HF Optimum Gradient so you c...\"],[\"300w, https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fgradient-badge-gradient-05-d-05.png?width=400&amp;name=gr...\"],[\"\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003ch2\\u003eGetting the dataset\\u003c\\u002fh2\\u003e\\n\\u003ca id=\\\"getting-the-dataset\\\" data-hs-anchor=...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003e\\u003cspan style=\\\"color: #6b7a8c;\\\"\\u003e\\u003ccode\\u003e\\u003c\\u002fcode\\u003e\\u003c\\u002fspan\\u003e\\u003ccode\\u003e\\u003cspan style=\\\"color: #6b7a8c;\\\"\\u003e\\u003c\\u002fsp...\"],[\"\\u003cp\\u003eFirst we identify the unique labels in the dataset.\\u003c\\u002fp\\u003e\\n\\u003cdiv style=\\\"font-size: 14px; line-height:...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eWhen loading data using the \\u003ccode\\u003edatasets.load_dataset\\u003c\\u002fcode\\u003e function, labels can be pro...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003ch2\\u003eCreating the dataset\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eWe are now ready to create the PyTorch dataset and split it ...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eTo fine-tune a pre-trained model, the new dataset must have the same properties as the ori...\"],[\"\\u003cp\\u003eTo examine the dataset, we display the first 10 rows of metadata.\\u003c\\u002fp\\u003e\\n\\u003cdiv style=\\\"font-size: 14px...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fx-ray%20images%20transformed.jpg?width=1024&amp;na...\"],[\"3072w\\\" sizes=\\\"(max-width: 1024px) 100vw, 1024px\\\"\\u003e\\u003c\\u002fp\\u003e...\"],[\"\\u003cdiv class=\\\"blog-caption\\\" style=\\\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: ...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eTo use this model on the IPU we need to load the IPU configuration, \\u003ccode\\u003eIPUConfig\\u003c\\u002fcode\\u003e...\"],[\"\\u003cp\\u003eFor this dataset, the AUC_ROC represents the ability of the model to separate the different disea...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmax...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eWe plot the training loss and the learning rate.\\u003c\\u002fp\\u003e\\n\\u003cdiv style=\\\"font-size: 14px; line-hei...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fvit%20output.png?width=1024&amp;name=vit%20...\"],[\"\\u003ch2\\u003eRunning the evaluation\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eNow that we have trained the model, we can evaluate its ability t...\"],[\"\\u003cp\\u003e\\u003ca href=\\\"https:\\u002f\\u002fconsole.paperspace.com\\u002fgithub\\u002fgradient-ai\\u002fGraphcore-HuggingFace?machine=Free-IPU...\"],[\"500w, https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fgradient-badge-gradient-05-d-05.png?width=600&amp;name=gr...\"],[\"\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003cp\\u003eIf youâ€™re interested in trying Hugging Face Optimum with IPUs on Pape...\"],[\"--\\ntitle: \\\"NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m met...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"As shown in the second line, \\\\\\\\(\\\\hat{P}\\\\\\\\) can be expressed as a product of three matrices. The reas...\"],[\"$$\\\\tilde{F} = softmax(\\\\frac{Q\\\\tilde{K}^T}{\\\\sqrt{d}}) \\\\hspace{40pt} \\\\tilde{A} = softmax(\\\\frac{\\\\tilde{...\"],[\"## How do we select landmarks?\\n\\nInstead of sampling \\\\\\\\(m\\\\\\\\) rows from \\\\\\\\(Q\\\\\\\\) and \\\\\\\\(K\\\\\\\\), the autho...\"],[\"## How is NystrÃ¶mformer implemented?\\n\\nThe original implementation of NystrÃ¶mformer can be found [her...\"],[\"attention_scores = torch.matmul(q_landmarks, key_layer.transpose(-1, -2)) # \\\\tilde{B} before softmax...\"],[\"```\\n\\n\\n## Using NystrÃ¶mformer with HuggingFace\\n\\nNystrÃ¶mformer for Masked Language Modeling (MLM) is a...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"--\\ntitle: \\\"Llama 2 is here - get it on Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fllama2\\u002fthumbnail.jpg\\na...\"],[\"## Why Llama 2?\\n\\nThe Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging ...\"],[\"| Model | License | Commercial use? | Pretraining length [tokens] | Leaderboard score |\\n| --- | --- ...\"],[\"*weâ€™re currently running evaluation of the Llama 2 70B (non chatty version). This table will be upda...\"],[\"```\\npip install transformers\\nhuggingface-cli login\\n```\\n\\nIn the following code snippet, we show how t...\"],[\"```\\n\\nAnd although the model has *only* 4k tokens of context, you can use techniques supported in `tr...\"],[\"_Note: You might need to request a quota upgrade via email to **[api-enterprise@huggingface.co](mail...\"],[\"```\\n\\nThen you can run the script:\\n```bash\\npython trl\\u002fexamples\\u002fscripts\\u002fsft_trainer.py \\\\\\n    --model_n...\"],[\"```\\n\\u003cs\\u003e[INST] \\u003c\\u003cSYS\\u003e\\u003e\\n{{ system_prompt }}\\n\\u003c\\u003c\\u002fSYS\\u003e\\u003e\\n\\n{{ user_message }} [\\u002fINST]\\n```\\n\\nThis template fo...\"],[\"```\\n\\nAs you can see, the instructions between the special `\\u003c\\u003cSYS\\u003e\\u003e` tokens provide context for the m...\"],[\"```\\n\\nThe model is stateless and does not \\\"remember\\\" previous fragments of the conversation, we must ...\"],[\"## Additional Resources\\n\\n- [Paper Page](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2307.09288)\\n- [Models on the H...\"],[\"--\\ntitle: \\\"Introducing The World's Largest Open Multilingual Language Model: BLOOM\\\"\\nthumbnail: \\u002fblog...\"],[\"Researchers can [now download, run and study BLOOM](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fbloom) to inve...\"],[\"--\\ntitle: \\\"Optimum-NVIDIA Unlocking blazingly fast LLM inference in just 1 line of code\\\" \\nthumbnail:...\"],[\"```diff\\n- from transformers.pipelines import pipeline\\n+ from optimum.nvidia.pipelines import pipelin...\"],[\"```\\nYou can also enable FP8 quantization with a single flag, which allows you to run a bigger model ...\"],[\"```\\n\\nFor more details, check out our [documentation](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-nvidia)\\n...\"],[\"### Next steps\\n\\nOptimum-NVIDIA currently provides peak performance for the LLaMAForCausalLM architec...\"],[\"--\\ntitle: \\\"Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\\\" \\nthumbn...\"],[\"\\u003e We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a ...\"],[\"## Resources\\n\\nThis blogpost and release come with several resources to get started with 4bit models ...\"],[\"For more information we recommend reading the fundamentals of floating point representation through ...\"],[\"Although the precision is substantially reduced by reducing the number of bits from 32 to 8, both ve...\"],[\"## QLoRA paper, a new way of democratizing quantized large transformer models\\n\\nIn few words, QLoRA r...\"],[\"For a more detailed reading, we recommend you read the [QLoRA paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2305.1431...\"],[\"```\\n\\n### Quickstart\\n\\nThe basic way to load a model in 4bit is to pass the argument `load_in_4bit=Tru...\"],[\"```\\nThat's all you need!\\n\\nAs a general rule, we recommend users to not manually set a device once th...\"],[\"model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)...\"],[\"```\\n\\n#### Changing the compute dtype\\n\\nAs mentioned above, you can also change the compute dtype of t...\"],[\"```\\n\\nAnd of course, as mentioned in the beginning of the section, all of these components are compos...\"],[\"For text models, at this time of writing, this would include most used architectures such as Llama, ...\"],[\"```\\nNote that if your favorite model is not there, you can open a Pull Request or raise an issue in ...\"],[\"| Model name                          | Half precision model size (in GB) | Hardware type \\u002f total VR...\"],[\"| decapoda-research\\u002fllama-7b-hf       | 14GB                              | 1xNVIDIA-T4 \\u002f 16GB      ...\"],[\"| decapoda-research\\u002fllama-13b-hf      | 27GB                              | 1xNVIDIA-T4 \\u002f 16GB      ...\"],[\"We have used the recent `SFTTrainer` from TRL library, and the benchmarking script can be found [her...\"],[\"--\\ntitle: \\\"Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models\\\"\\nthumbnail: \\u002f...\"],[\"Additional details on configuration settings to turn on Optimum for training acceleration can be fou...\"],[\"```\\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFa...\"],[\"```\\n\\n## Optimum Library\\n\\nHugging Face is a fast-growing open community and platform aiming to democr...\"],[\"## ONNX Runtime Training\\n\\n[ONNX Runtime](https:\\u002f\\u002fonnxruntime.ai\\u002f) accelerates [large model training]...\"],[\"## ONNX Runtime Training in Optimum\\n\\nOptimum provides an `ORTTrainer` API that extends the `Trainer`...\"],[\"```diff\\n-from transformers import Trainer, TrainingArguments\\n+from optimum.onnxruntime import ORTTra...\"],[\"```\\n\\n## Looking Forward\\n\\nThe Hugging Face team is working on open sourcing more large models and low...\"],[\"## Getting Started\\n\\nWe invite you to check out the links below to learn more about, and get started ...\"],[\"--\\ntitle: \\\"SafeCoder vs. Closed-source Code Assistants\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fsafecoder-vs-closed-...\"],[\"StarCoder is a 15.5 billion parameter model trained for code generation in over 80 programming langu...\"],[\"Unfortunately, closed-source services stick to vague information, such as \\\"[the model was trained on...\"],[\"Unfortunately, closed-source services are only available as managed services.\\n\\n## Security and priva...\"],[\"--\\ntitle: \\\"Optimizing your LLM in production\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f163_optimize_llm\\u002foptimize_llm....\"],[\"In this blog post, we will go over the most effective techniques at the time of writing this blog po...\"],[\"At the time of writing this post, LLMs consist of at least a couple billion parameters. Each paramet...\"],[\"To give some examples of how much VRAM it roughly takes to load a model in bfloat16:\\n\\n-   **GPT3** r...\"],[\"Naive pipeline parallelism is supported out of the box. For this, simply load the model with `device...\"],[\"```\\n```python\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretr...\"],[\"```\\n\\n**Output**:\\n```\\nHere is a Python function that transforms bytes to Giga bytes:\\\\n\\\\n```python\\\\nde...\"],[\"```\\n\\n**Output**:\\n```bash\\n29.0260648727417\\n```\\n\\nClose enough to our back-of-the-envelope computation!...\"],[\"```\\n\\nLet's call it now for the next experiment.\\n\\n```python\\nflush()\\n```\\nIn the recent version of the ...\"],[\"```\\n\\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be q...\"],[\"for every matrix multiplication. Dequantization and re-quantization is performed sequentially for al...\"],[\"```\\n\\nWe can then load models in 8-bit quantization by simply adding a `load_in_8bit=True` flag to `f...\"],[\"```\\n\\n```python\\nflush()\\n```\\n\\nLet's see what peak GPU memory consumption 4-bit quantization gives. Qua...\"],[\"```\\n```python\\nflush()...\"],[\"```\\n\\nOverall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32...\"],[\"# 2. Flash Attention: A Leap Forward\\n\\nToday's top-performing LLMs share more or less the same fundam...\"],[\"LLMs usually have multiple attention heads, thus doing multiple self-attention computations in paral...\"],[\"with \\\\\\\\( s^a_{ij} \\\\\\\\) and \\\\\\\\( s^b_{ij} \\\\\\\\) being some softmax normalization statistics that need to ...\"],[\"```python\\nsystem_prompt = \\\"\\\"\\\"Below are a series of dialogues between various people and an AI techni...\"],[\"Question: Modify the function so that it returns all input elements when the lists have uneven lengt...\"],[\"```\\nFor demonstration purposes, we duplicate the system by ten so that the input length is long enou...\"],[\"```\\n\\n**Output**:\\n```bash\\n37.668193340301514\\n```\\n\\nAs we can see the peak GPU memory requirement is no...\"],[\"```\\n\\nWe're getting the exact same result as before, but can observe a very significant speed-up than...\"],[\"```\\n\\n## 3. The Science Behind LLM Architectures: Strategic Selection for Long Text Inputs and Chat\\n\\n...\"],[\"A LLM based on self-attention, but without position embeddings would have great difficulties in unde...\"],[\"Instead of using fixed position embeddings, others (such as [Devlin et al.](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f18...\"],[\"Without going into too many details, *RoPE* notes that positional information can be encoded into qu...\"],[\"![](\\u002fblog\\u002fassets\\u002f163_optimize_llm\\u002falibi.png)\\n\\nAs shown in the [ALiBi](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2108.124...\"],[\"\\u003e Both RoPE and ALiBi are relative positional embeddings that are *not* learned during training, but...\"],[\"```python\\ninput_ids = tokenizer(prompt, return_tensors=\\\"pt\\\")[\\\"input_ids\\\"].to(\\\"cuda\\\")\\n\\nfor _ in range...\"],[\"```\\n\\n**Output**:\\n```\\nshape of input_ids torch.Size([1, 21])\\nshape of input_ids torch.Size([1, 22])\\ns...\"],[\"```\\n\\nAs we can see every time we increase the text input tokens by the just sampled token.\\n\\nWith ver...\"],[\"for _ in range(5):\\n  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_va...\"],[\"```\\n\\n**Output**:\\n```\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 20\\nshape of inp...\"],[\"```\\nUser: How many people live in France?\\nAssistant: Roughly 75 million people live in France\\nUser: ...\"],[\"```\\n\\nIn this chat, the LLM runs auto-regressive decoding twice:\\n- 1. The first time, the key-value c...\"],[\"There is however one catch. While the required peak memory for the \\\\\\\\( \\\\mathbf{QK}^T \\\\\\\\) matrix is s...\"],[\"```\\n\\n**Output**:\\n```\\n7864320000...\"],[\"```\\n\\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires ...\"],[\"The important part to understand here is that reducing the number of key-value attention heads to 1 ...\"],[\"GQA was only recently proposed which is why there is less adoption at the time of writing this noteb...\"],[\"--\\ntitle: \\\"Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker\\\"\\nthum...\"],[\"In this blog post, you will learn how to easily deploy `GPT-J` using [Amazon SageMaker](https:\\u002f\\u002faws....\"],[\"```\\n\\nThe caveat of this example is that it takes a very long time until the model is loaded into mem...\"],[\"*â€œSaving a model in this way will save the entire module using Pythonâ€™sÂ [pickle](https:\\u002f\\u002fdocs.python...\"],[\"```python\\nfrom transformers import AutoTokenizer,GPTJForCausalLM\\nimport torch\\n\\n# load fp 16 model\\nmo...\"],[\"```\\n\\nNow we are able to load our `GPT-J` model with `torch.load()` to run predictions. \\n\\n```python\\nf...\"],[\"```\\n\\n---\\n\\n### Create `model.tar.gz` for the Amazon SageMaker real-time endpoint\\n\\nSince we can load o...\"],[\"```bash\\n# clone directory\\ngit clone https:\\u002f\\u002fgithub.com\\u002fphilschmid\\u002famazon-sagemaker-gpt-j-sample.git\\n...\"],[\"```\\n\\nThe `convert_gpt.py` should print out an S3 URI similar to this. `s3:\\u002f\\u002fhf-sagemaker-inference\\u002fg...\"],[\"```\\n\\nIf you want to use your own `model.tar.gz` just replace the `model_uri` with your S3 Uri.\\n\\nThe ...\"],[\"```\\n\\n### Parameterized request\\n\\nThis is an example of a request using a custom parameter, e.g. `min_...\"],[\"```\\n\\n---\\n\\nTo delete your endpoint you can run. \\n\\n```python\\npredictor.delete_endpoint()\\n```\\n\\n## Concl...\"],[\"--\\ntitle: \\\"Visualize proteins on Hugging Face Spaces\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f98_spaces_3dmoljs\\u002fthum...\"],[\"## Taking a Look at the Code\\n\\nLet's take a look at how to create the minimal working demo of our int...\"],[\"```\\n\\n`update`: This is the function that does the processing of our proteins and returns an `iframe`...\"],[\"```\\nThis is a bit clunky to setup but is necessary because of the security rules in modern browsers....\"],[\"```\\nThe styles for `.mol-container` can be used to modify the size of the molecule viewer. \\n\\nThe `bo...\"],[\"```\\nWe use a template literal (denoted by backticks) to store our pdb file in the html document dire...\"],[\"--\\ntitle: \\\"Announcing our new Content Guidelines and Policy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fcontent-guideli...\"],[\"## Consent as a Core Value\\n\\nAs we prioritize respecting people's rights throughout the development a...\"],[\"--\\ntitle: \\\"Fine-Tune MMS Adapter Models for low-resource ASR\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f151_mms\\u002fmms_ma...\"],[\"**Wav2Vec2** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [Septe...\"],[\"In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates af...\"],[\"You can find the pretrained-only checkpoints on the ğŸ¤— Hub for model sizes of 300 million parameters ...\"],[\"You can see that the base models are saved (as usual) as a [`model.safetensors` file](https:\\u002f\\u002fhuggin...\"],[\"The work done in **MMS** leverages this idea of adapters for speech recognition across different lan...\"],[\"```bash\\n%%capture\\n!pip install --upgrade pip \\n!pip install datasets[audio]\\n!pip install evaluate\\n!pi...\"],[\"```\\n\\nWe strongly suggest to upload your training checkpoints directly to the [ğŸ¤— Hub](https:\\u002f\\u002fhugging...\"],[\"```\\n\\n\\n## Prepare Data, Tokenizer, Feature Extractor\\n\\nASR models transcribe speech to text, which mea...\"],[\"For this notebook, we will use [Common Voice's 6.1 dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmozilla-...\"],[\"```\\nMany ASR datasets only provide the target text (`'sentence'`) for each audio array (`'audio'`) a...\"],[\"```\\n\\n```python\\nshow_random_elements(common_voice_train.remove_columns([\\\"path\\\", \\\"audio\\\"]), num_exampl...\"],[\"```\\n\\nAlright! The transcriptions look fairly clean. Having translated the transcribed sentences, it ...\"],[\"```\\nLet's look at the processed text labels again.\\n\\n```python\\nshow_random_elements(common_voice_trai...\"],[\"```\\n\\nGood! This looks better. We have removed most special characters from transcriptions and normal...\"],[\"```\\n\\n```python\\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\\ncommon_voice_t...\"],[\"```\\n\\n```python\\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\\nvocab_dict\\n```\\n\\n```bash...\"],[\"```\\n\\nCool, we see that all letters of the alphabet occur in the dataset (which is not really surpris...\"],[\"```\\n\\n```bash\\n    37\\n```\\n\\nCool, now our vocabulary is complete and consists of 37 tokens, which means...\"],[\"```\\n\\nLet's define an empty dictionary to which we can append the just created vocabulary\\n\\n```python\\n...\"],[\"```\\n\\nIf one wants to re-use the just created tokenizer with the fine-tuned model of this notebook, i...\"],[\"```\\n\\nGreat, you can see the just created repository under `https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-username\\u003e\\u002fwa...\"],[\"```python\\nfrom transformers import Wav2Vec2FeatureExtractor\\n\\nfeature_extractor = Wav2Vec2FeatureExtr...\"],[\"```\\n\\nGreat, MMS's feature extraction pipeline is thereby fully defined!\\n\\nFor improved user-friendlin...\"],[\"```\\n\\nIn the example above we can see that the audio data is loaded with a sampling rate of 48kHz whe...\"],[\"```\\n\\nLet's take a look at `\\\"audio\\\"` again.\\n\\n```python\\ncommon_voice_train[0][\\\"audio\\\"]\\n```\\n\\n\\n    {'pat...\"],[\"```\\n\\nGood! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corre...\"],[\"```python\\ndef prepare_dataset(batch):\\n    audio = batch[\\\"audio\\\"]\\n\\n    # batched output is \\\"un-batche...\"],[\"```\\n\\nLet's apply the data preparation function to all examples.\\n\\n```python\\ncommon_voice_train = comm...\"],[\"```\\n\\n**Note**: `datasets` automatically takes care of audio loading and resampling. If you wish to i...\"],[\"Without going into too many details, in contrast to the common data collators, this data collator tr...\"],[\"processor: Wav2Vec2Processor\\n    padding: Union[bool, str] = True\\n\\n    def __call__(self, features: ...\"],[\"```\\n\\n```python\\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\\n```\\n\\nNe...\"],[\"```\\n\\nThe model will return a sequence of logit vectors:\\n \\\\\\\\( \\\\mathbf{y}_1, \\\\ldots, \\\\mathbf{y}_m \\\\\\\\) ...\"],[\"```\\n\\nNow, we can load the pretrained checkpoint of [`mms-1b-all`](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fmm...\"],[\"```\\n\\n**Note**: It is expected that some weights are newly initialized. Those weights correspond to t...\"],[\"```\\n\\nIn a final step, we define all parameters related to training.\\nTo give more explanation on some...\"],[\"```\\n\\nNow, all instances can be passed to Trainer and we are ready to start training!\\n\\n```python\\nfrom...\"],[\"```\\n\\n| Training Loss | Training Steps | Validation Loss | Wer    |\\n|:-------------:|:----:|:--------...\"],[\"The adapter weights will be uploaded as part of the model checkpoint, but we also want to make sure ...\"],[\"```\\n\\nFinally, you can upload the result of the training to the ğŸ¤— Hub.\\n\\n```python\\ntrainer.push_to_hub...\"],[\"```\\n\\nOne of the main advantages of adapter weights training is that the \\\"base\\\" model which makes up ...\"],[\"```\\n\\nLet's check that the model can correctly transcribe Turkish\\n\\n```python\\nfrom datasets import Aud...\"],[\"```\\n\\nWe again load the Swedish test set from common voice\\n\\n```python\\ncommon_voice_test_swe = load_da...\"],[\"```\\n\\n**Output**:\\n\\n```bash\\n    Prediction:\\n    jag lÃ¤mnade grovjobbet Ã¥t honom\\n\\n    Reference:\\n    ja...\"],[\"--\\ntitle: \\\"Policy Gradient with PyTorch\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f85_policy_gradient\\u002fthumbnail.gif\\nau...\"],[\"Indeed, since the beginning of the course, we only studied value-based methods,Â **where we estimate ...\"],[\"### An Overview of Policy Gradients\\nWhy do we optimize the policy directly by estimating the weights...\"],[\"This has two consequences:\\n\\na. We **don't need to implement an exploration\\u002fexploitation trade-off by...\"],[\"Indeed, the problem with Deep Q-learning is that their **predictions assign a score (maximum expecte...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f85_policy_gradient\\u002fpolicy.jpg...\"],[\"- Update the weights of the policy: \\\\\\\\(\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\hat{g}\\\\\\\\)\\n\\nThe interpretat...\"],[\"Take time to really grasp the material before continuing.\\n\\nDon't hesitate to train your agent in oth...\"],[\"--\\ntitle: \\\"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\\\"\\nthumbnail: \\u002fblog...\"],[\"## Getting Started\\n\\nBefore you can connect your AWS Account with your Hugging Face account, you need...\"],[\"After clicking the button, you will be redirected to the Hugging Face Platform, where you can select...\"],[\"---\\n\\nThanks for reading! If you have any questions, feel free to contact us at [api-enterprise@huggi...\"],[\"--\\ntitle: \\\"Practical 3D Asset Generation: A Step-by-Step Guide\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-g...\"],[\"Enter \\\"Dilapidated Shack\\\" as your prompt and click 'Generate'. When you're happy with the model, dow...\"],[\"Once installed and enabled, open the addon preferences. Search for and download the [texture-diffusi...\"],[\"When you're happy with your model, it's time to export it. Navigate to File -\\u003e Export -\\u003e FBX, and vo...\"],[\"--\\ntitle: \\\"Making LLMs lighter with AutoGPTQ and transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f159_autogptq...\"],[\"This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs.\\n\\n## Table of contents...\"],[\"## Resources\\n\\nThis blogpost and release come with several resources to get started with GPTQ quantiz...\"],[\"The GPTQ paper tackles the layer-wise compression problem: \\n\\nGiven a layer \\\\\\\\(l\\\\\\\\) with weight matri...\"],[\"The GPTQ paper improves this framework by introducing a set of optimizations that reduces the comple...\"],[\"```python\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretraine...\"],[\"```\\n\\nCheck out the Transformers [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmai...\"],[\"| gptq  | act_order | bits | group_size | kernel            | Load time (s) | Per-token latency (ms)...\"],[\"Quantizing ğŸ¤—Â Transformers models with the GPTQ method can be done in a few lines:\\n\\n```python\\nfrom tr...\"],[\"```\\n\\nQuantizing a model may take a long time. Note that for a 175B model, at least 4 GPU-hours are r...\"],[\"Note that the kernel integrated in TGI does not scale very well with larger batch sizes. Although th...\"],[\"On the quantization side, letâ€™s emphasize again that this method only quantizes the weights. There h...\"],[\"This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step ...\"],[\"## Acknowledgements\\n\\nWe would like to thank [William](https:\\u002f\\u002fgithub.com\\u002fPanQiWei) for his support a...\"],[\"--\\ntitle: \\\"Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets\\\"\\nthum...\"],[\"## Why have we created this tool?\\nThoughtful curation and analysis of Machine Learning datasets is o...\"],[\"A new wave of research in AI has called for a fundamental paradigm shift in how the field approaches...\"],[\"Despite this, there are few tools openly available to the public to enable people from different dis...\"],[\"- The dataset vocabulary size and word distribution, for both [open- and closed-class words](https:\\u002f...\"],[\"In general, an alpha greater than 2 or a minimum rank greater than 10 (take with a grain of salt) me...\"],[\"## What is the status of ğŸ¤— Data Measurements Tool development?\\nWe currently present the alpha versio...\"],[\"--\\ntitle: \\\"New ViT and ALIGN Models From Kakao Brain\\\" \\nthumbnail: \\u002fblog\\u002f\\u002fassets\\u002f132_vit_align\\u002fthumbn...\"],[\"This blog will introduce the new [COYO](https:\\u002f\\u002fgithub.com\\u002fkakaobrain\\u002fcoyo-dataset) dataset, Kakao B...\"],[\"## COYO DATASET\\n\\n\\u003cp\\u003e\\n\\u003ccenter\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-im...\"],[\"| COYO | LAION 2B| ALIGN 1.8B |\\n| :----: | :----: | :----: |\\n| Image-text similarity score calculate...\"],[\"## How ViT and ALIGN work\\n\\nSo what do these models do? Let's breifly discuss how the ViT and ALIGN m...\"],[\"[Google then introduced ALIGN](https:\\u002f\\u002fai.googleblog.com\\u002f2021\\u002f05\\u002falign-scaling-up-visual-and-vision....\"],[\"## How to use the COYO dataset\\nWe can conveniently download the `COYO` dataset with a single line of...\"],[\"```\\n\\nWhile it is significantly smaller than the `LAION` dataset, the `COYO` dataset is still massive...\"],[\"```shell\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e dataset = load_dataset('kakaobrain\\u002fcoyo-700m', s...\"],[\"```\\n\\n## How to use ViT and ALIGN from the Hub\\nLetâ€™s go ahead and experiment with the new ViT and ALI...\"],[\"```\\n\\nThe rest is simple, we will forward preprocess the image and use it as input to the model to re...\"],[\"```\\n\\nAnd we are done! To make things even easier and shorter, we can also use the convenient image c...\"],[\"```\\n\\nIf you want to experiment more with the Kakao Brain ViT model, head over to its [Space](https:\\u002f...\"],[\"```\\n\\nWe will start with zero-shot image classification first. To do this, we will suppy candidate la...\"],[\"```\\n\\nAlternatively, we can use the stand-along vision and text encoders of ALIGN to retrieve multi-m...\"],[\"```\\n\\nLet's do the same with `AlignVisionModel` and retrieve the multi-modal embedding of an image.\\n\\n...\"],[\"```\\n\\nSimilar to ViT, we can use the zero-shot image classification [pipeline](https:\\u002f\\u002fhuggingface.co...\"],[\"```\\n\\n## Conclusion\\n\\nThere have been incredible advances in multi-modal models in recent years, with ...\"],[\"--\\ntitle: \\\"Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon\\\"\\nthumbnail: \\u002fbl...\"],[\"In a nutshell, quantization rescales model parameters to smaller value ranges. When successful, it s...\"],[\"The best quantization techniques to date quantize activations token-wise, causing either truncated o...\"],[\"Now, letâ€™s see how SmoothQuant works when applied to popular LLMs.\\n\\n## Quantizing LLMs with SmoothQu...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog...\"],[\"The example shows the additional benefits you can get from 8bit quantization coupled with 4th Gen Xe...\"],[\"## Appendix: detailed results\\n\\nA negative value indicates that the benchmark has improved.\\n\\n\\u003ckbd\\u003e\\n  ...\"],[\"--\\ntitle: \\\"An overview of inference solutions on Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f116_inference...\"],[\"Here's a sentence similarity example with the `sentence-transformers\\u002fall-MiniLM-L6-v2` [model](https...\"],[\"```\\ncurl https:\\u002f\\u002fapi-inference.huggingface.co\\u002fmodels\\u002fxlm-roberta-base \\\\\\n\\t-X POST \\\\\\n\\t-d '{\\\"inputs\\\": \\\"...\"],[\"```\\n\\nThe Inference API is the simplest way to build a prediction service that you can immediately ca...\"],[\"## Spaces\\n\\nFinally, Spaces is another production-ready option to deploy your model for inference on ...\"],[\"--\\ntitle: \\\"Hugging Face's TensorFlow Philosophy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f96_tensorflow_philosophy\\u002fth...\"],[\"```\\n\\nThis one line will instantiate the model architecture and load the weights, giving you an exact...\"],[\"```\\n\\nNow our `model` has an output head and, optionally, a loss function appropriate for its new tas...\"],[\"# Let's load some data and tokenize it\\ntest_strings = [\\\"This is a sentence!\\\", \\\"This is another one!\\\"...\"],[\"```\\n\\nThis is just a taste of the library, of course - if you want more, you can check out our [noteb...\"],[\"```\\n\\nAnd if you want to train that model instead, it's just:\\n\\n```py\\nmodel.fit(my_data, my_labels)\\n``...\"],[\"```\\n\\n#### Philosophy #2: Loss functions are provided by default, but can be easily changed.\\n\\nIn Kera...\"],[\"```\\n\\nIn the past, we instead asked users to pass labels in the input dict when using the default los...\"],[\"# Load and compile our model\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\\\"bert-base...\"],[\"```\\n\\nThis approach is great when it works, but for larger datasets you might find it starting to bec...\"],[\"```\\nWhy is [prepare_tf_dataset()](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmain_classes\\u002fmode...\"],[\"```\\n\\nWeâ€™ve made a number of major improvements recently in this area. Most significantly, weâ€™ve upda...\"],[\"One major obstacle in deploying NLP models, however, is that inputs will still need to be tokenized,...\"],[\"```\\n\\n#### Conclusion: Weâ€™re an open-source project, and that means community is everything\\n\\nMade a c...\"],[\"```\\n\\nI think the fact that thereâ€™s no distinction between big famous foundation models and models fi...\"],[\"\\u003csmall\\u003e(And if you can make a meme to troll the PyTorch team with after your cool new feature is mer...\"],[\"--\\ntitle: The Age of Machine Learning As Code HasÂ Arrived\\nthumbnail: \\u002fblog\\u002fassets\\u002f31_age_of_ml_as_co...\"],[\"Well, here's what I think.\\n\\n\\n### Machine Learning For TheÂ Masses!\\n\\nMachine Learning is everywhere, o...\"],[\"There's no need to reinvent the wheel either. The DevOps movement solved these problems over 10 year...\"],[\"Now, let's talk about Transformers.\\n\\n---\\n\\n### Transformers! Transformers! Transformers! ([BallmerÂ st...\"],[\"It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can k...\"],[\"We believe in built-in best practices. \\n\\nWe believe in making infrastructure as transparent as possi...\"],[\"--\\ntitle: \\\"Introducing Storage Regions on the HF Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f172_regions\\u002fthumbnail....\"],[\"## Regulatory and legal compliance\\n\\nIn many regulated industries, you may have a requirement to stor...\"],[\"--\\ntitle: \\\"Announcing the ğŸ¤— AI Research Residency Program\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f57_ai_residency\\u002fr...\"],[\"We are actively working to build a culture that values diversity, equity, and inclusivity. We are in...\"],[\"--\\ntitle: \\\"Hugging Face Reads, Feb. 2021 - Long-range Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f14_long_...\"],[\"This topic has been a key part of our research discussions from the start, and our own Patrick Von P...\"],[\"### [Longformer - The Long-Document Transformer](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2004.05150)\\n\\nIz Beltagy, Matt...\"],[\"Longformer uses different attention patterns for autoregressive language modeling, encoder pre-train...\"],[\"#### Main findings\\n\\n* The authors proposed the dilated windowed self-attention (Figure c) and showed...\"],[\"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap\\n\\n[Transformer-XL (2019)](ht...\"],[\"A compression factor \\\\\\\\(c\\\\\\\\) (equal to 3 in the illustration) is chosen to decide the rate at which ...\"],[\"#### Follow-up questions\\n\\n* Compressive Transformer requires a special optimization schedule in whic...\"],[\"Linformer projects the sequence length into a smaller dimension by learning a low-rank decomposition...\"],[\"### [Rethinking Attention with Performers](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2009.14794)\\n\\nKrzysztof Choromanski,...\"],[\"#### Main findings\\n\\n* The FAVOR+ procedure can be used to approximate self-attention matrices with h...\"],[\"These different inductive biases have implications in terms of computational speed and generalizatio...\"],[\"From a practical point of view, the question of positional embeddings is also a crucial methodologic...\"],[\"For further reading, we recommend checking Patrick Platenâ€™s blog on [Reformer](https:\\u002f\\u002farxiv.org\\u002fabs...\"],[\"--\\ntitle: \\\"VQ-Diffusion\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f117_vq_diffusion\\u002fthumbnail.png\\nauthors:\\n- user: wi...\"],[\"```\\n\\n![png](assets\\u002f117_vq_diffusion\\u002fvq_diffusion_teddy_bear_pool.png)\\n\\n### Architecture\\n\\n![svg](asse...\"],[\"#### Approximating the reverse process\\n\\nAn encoder-decoder transformer approximates the classes of t...\"],[\"There is a smaller amount of literature covering discrete diffusion models than continuous diffusion...\"],[\"AR image generative models have evolved architecturally with much work towards making transformers c...\"],[\"[Image Transformer](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1802.05751) uses transformers by restricting self attentio...\"],[\"Despite having made tremendous strides, AR models still suffer from linear decreases in inference sp...\"],[\"[Improved Vector Quantized Diffusion Models](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.16007) improves upon VQ-Diff...\"],[\"--\\ntitle: Zero-shot image segmentation with CLIPSeg\\nthumbnail: \\u002fblog\\u002fassets\\u002f123_clipseg-zero-shot\\u002fth...\"],[\"One limitation of most image segmentation models is that they only work with a fixed list of categor...\"],[\"## CLIP: the magic model behind CLIPSeg\\n\\n[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmod...\"],[\"Whatâ€™s more, CLIP is not only useful for classification, but it can also be used for [image search](...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"The CLIPSeg paper contains some tips on improving the effectiveness of visual prompting. They find t...\"],[\"```\\n\\nTo download the model, simply instantiate it.\\n\\n```python\\nfrom transformers import CLIPSegProces...\"],[\"```\\n\\nNow that we have our inputs, we can process them and input them to the\\nmodel.\\n\\n```python\\nimport...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-6\\u002f12\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n```python\\n_, ax = plt.subplots(1, 2, figsize=(6, 4))\\n[a.axis('off') for a in ax.flatten()]\\nax[0...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n```python\\nfrom segments import SegmentsClient\\nfrom getpass import getpass\\n\\napi_key = getpass('E...\"],[\"```\\n\\nNow we can use CLIPSeg on the image as before. This time, we\\\\'ll also\\nscale up the outputs so t...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\nLet\\\\'s quickly visualize the result.\\n\\n```python\\nplt.imshow(inds)\\n```\\n\\n\\u003cfigure class=\\\"image tabl...\"],[\"```\\n\\nIf you take a look at the [uploaded prediction on\\nSegments.ai](https:\\u002f\\u002fsegments.ai\\u002fadmin-tobias...\"],[\"If youâ€™re interested in learning how to fine-tune a state-of-the-art segmentation model, check out o...\"],[\"--\\ntitle: \\\"Getting Started with Sentiment Analysis on Twitter\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f85_sentiment_...\"],[\"Buckle up and enjoy the ride! ğŸ¤—\\n\\n## What is Sentiment Analysis?\\n\\nSentiment analysis uses [machine le...\"],[\"Why do sentiment analysis on Twitter? Companies use this for a wide variety of use cases, but the tw...\"],[\"In this section, we'll show you how to do it with a cool little project: we'll do sentiment analysis...\"],[\"```\\n\\n2. Setting up Twitter credentials\\n\\nThen, you need to set up the [Twitter API credentials](https...\"],[\"```\\n\\n4. Analyzing tweets with sentiment analysis\\n\\nNow that you have data, you are ready to analyze t...\"],[\"```\\n\\nNext, you will create the API call using the `model id` and `hf_token`:\\n\\n```python\\nAPI_URL = \\\"h...\"],[\"```\\n\\nResults:\\n\\n```\\n@thenotionbar @hypefury @NotionHQ Thatâ€™s genuinely smart. So basically youâ€™ve set...\"],[\"```\\n\\nIt's cool to see that 50% of all tweets are positive and only 8.2% are negative:\\n\\n\\u003cfigure class...\"],[\"As a last step, let's create some wordclouds to see which words are the most used for each sentiment...\"],[\"```\\n\\nCuriously, some of the words that stand out from the positive tweets include \\\"notes\\\", \\\"cron\\\", a...\"],[\"To get started with sentiment analysis, you don't need to be a developer or know how to code. ğŸ¤¯ \\n\\nTh...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"Once you have your model ID and your Hugging Face token ID, go back to your Zap and follow these ins...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"Then, follow these instructions to configure this last step:\\n1. Select Google Sheets as an app, and ...\"],[\"To turn it on, just click on \\\"Publish\\\" button at the bottom of your screen:\\n\\n\\u003cfigure class=\\\"image ta...\"],[\"If you have questions, you can ask them in the [Hugging Face forum](https:\\u002f\\u002fdiscuss.huggingface.co\\u002f)...\"],[\"--\\ntitle: \\\"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\\"...\"],[\"* Section of the YAML metadata for the IMDB dataset*\\n\\nIt is perhaps unsurprising that English is by ...\"],[\"#### Why is language metadata important?\\n\\nLanguage metadata can be a vital tool for finding relevant...\"],[\"```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole datase...\"],[\"We pass 20 examples to the model representing rows from a dataset. This results in 20 individual lan...\"],[\"For some ISO 639-3 codes, there is no ISO 639-1 equivalent. For these cases we manually specify a ma...\"],[\"As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic ...\"],[\"--\\ntitle: \\\"Jupyter X Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f135_notebooks-hub\\u002fbefore_after_notebook_...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f135_notebooks-hub\\u002fbefore_after_notebook_rendering.png\\\" alt=\\\"A side...\"],[\"--\\ntitle: \\\"Gradio is joining Hugging Face!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f42_gradio_joins_hf\\u002fthumbnail.png...\"],[\"I recruited my talented housemates Ali Abdalla, Ali Abid, and Dawood Khan to release the first versi...\"],[\"--\\ntitle: \\\"Generating Stories: AI for Game Development #5\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002f...\"],[\"### Process\\n\\n**Requirements:** I'm using [ChatGPT](https:\\u002f\\u002fopenai.com\\u002fblog\\u002fchatgpt\\u002f) throughout this...\"],[\"This is already much better. I continue to refine the result, such as asking to remove elements of m...\"],[\"\\u003e âš ï¸ **Limitation:** Using outputs from language models directly may have unintended legal, ethical,...\"],[\"For my simple farming game, this may be an effective approach to producing all the story content for...\"],[\"There are many other models which are not yet publicly accessible. Check out [this](https:\\u002f\\u002fhuggingf...\"],[\"#### In-Game Development\\n\\n**NPCs:** Aside from the clear uses of language models and dialog agents i...\"],[\"--\\ntitle: \\\"Making automatic speech recognition work on large files with Wav2Vec2 in ğŸ¤— Transformers\\\"\\n...\"],[\"```\\n\\n\\n**Wav2Vec2** is a popular pre-trained model for speech recognition.\\nReleased in [September 202...\"],[\"```\\n\\n```python\\nfrom transformers import pipeline\\n\\n# This will work on any of the thousands of models...\"],[\"Chunking with stride\\n--------------------\\n\\nWav2Vec2 uses the [CTC algorithm](https:\\u002f\\u002fdistill.pub\\u002f201...\"],[\"```\\n\\n\\nChunking with stride on LM augmented models\\n-------------------------------------------\\n\\nIn [t...\"],[\"--\\ntitle: \\\"Red-Teaming Large Language Models\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fred-teaming\\u002fthumbnail.png\\naut...\"],[\"**Red-teaming** *is a form of evaluation that elicits model vulnerabilities that might lead to undes...\"],[\"Since red-teaming requires creative thinking of possible model failures, it is a problem with a larg...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"The caveat in evaluating LLMs for such malicious behaviors is that we donâ€™t know what they are capab...\"],[\"1. Few-shot-prompted LMs with helpful, honest, and harmless behavior are *not* harder to red-team th...\"],[\"--\\ntitle: \\\"2023, year of open LLMs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fcv_state\\u002fthumbnail.png\\nauthors:\\n- user: ...\"],[\"A **tokenizer** defines how the text from the training dataset is converted to numbers (as a model i...\"],[\"## ğŸ—ï¸ 2022, from a race for size to a race for data\\nWhat open models were available to the community...\"],[\"3. [GLM-130B](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2210.02414) (General Language Model)\\n[GLM-130B](https:\\u002f\\u002f...\"],[\"However, in March 2022, a [new paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2203.15556) by DeepMind came out...\"],[\"All these releases a) included model weights (under varyingly open licenses) and b) had good perform...\"],[\"The [Pythia](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2304.01373) models were released by the open-source non-p...\"],[\"Where previous models were public about their data, from then on, following releases gave close to n...\"],[\"A couple of months later, the first [model](https:\\u002f\\u002fhuggingface.co\\u002fmistralai\\u002fMistral-7B-v0.1) from t...\"],[\"In parallel, a notable event of the end of the year 2023 was the rise of performances and a number o...\"],[\"**Chat-based fine-tuning** is a variant of supervised fine-tuning, where the annotated data is chat ...\"],[\"Both these methods are relatively easy to implement: you just need to find or generate related datas...\"],[\"So, to come back to our wave of small open weights models from (mostly) private companies, a lot of ...\"],[\"At the beginning of 2023, a few datasets for instruction\\u002fchat finetuning were already released. For ...\"],[\"â„ï¸ Winter 2022\\u002f2023: In January this year, the [Human ChatGPT Instruction corpus](https:\\u002f\\u002fhuggingfac...\"],[\"ğŸŒ± Spring: In April, BAIR (Berkeley AI Research lab) released [Koala](https:\\u002f\\u002fbair.berkeley.edu\\u002fblog\\u002f...\"],[\"[Airoboros](https:\\u002f\\u002fgithub.com\\u002fjondurbin\\u002fairoboros) framework to fine-tune models using model-genera...\"],[\"ğŸŒ»Summer: In August, [UltraLM](https:\\u002f\\u002fgithub.com\\u002fthunlp\\u002fUltraChat) (a high-performing chat fine-tune...\"],[\"ğŸ‚ Autumn: In October, Hugging Face released [Zephyr](https:\\u002f\\u002fhuggingface.co\\u002fHuggingFaceH4\\u002fzephyr-7b-...\"],[\"*Some more specialized datasets (such as [MetaMath](https:\\u002f\\u002fmeta-math.github.io\\u002f) or [MathInstruct](...\"],[\"But what does it mean to merge a model?\\n\\n**Model merging** is a way to fuse the weights of different...\"],[\"You might want to use what is called **parameter efficient fine-tuning** (PEFT).\\nThis technique firs...\"],[\"To go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G ...\"],[\"It's still a bit too early to say if these new approaches will take over the Transformer, but state ...\"],[\"--\\ntitle: \\\"The N Implementation Details of RLHF with PPO\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f167_the_n_implemen...\"],[\"- In [Matching Learning Curves](#matching-learning-curves), we show our main contribution: creating ...\"],[\"**Here are the important links:**\\n\\n- ğŸ’¾Â Our reproduction codebase [*https:\\u002f\\u002fgithub.com\\u002fvwxyzjn\\u002flm-hum...\"],[\"- OAIâ€™s dataset was partially corrupted\\u002flost (so we replaced them with similar HF datasets, which ma...\"],[\"1. **The reward model and policyâ€™s value head take input as the concatenation of `query` and `respon...\"],[\"2. So, for example, if `query = \\\"he was quiet for a minute, his eyes unreadable\\\"`., and the `respons...\"],[\"2. **Pad with a special padding token and truncate inputs.** \\n    1. OAI sets a fixed input length f...\"],[\"1. **Note on HFâ€™s transformers â€” padding token.** According to  ([transformers#2630#issuecomment-578...\"],[\"return_attention_mask=True,\\n    )\\n    print(\\\"inputs\\\", inputs)\\n    \\n    \\\"\\\"\\\"prints are\\n    tokens [[23...\"],[\"```\\n    \\n3. **Adjust position indices correspondingly for padding tokens**\\n    1. When calculating t...\"],[\"-35.36577 ]\\n          [ -35.28693   -34.2875    -38.16074  ...  -41.595802  -41.082108\\n            -...\"],[\"```\\n        \\n    3. **Note on HFâ€™s transformers â€” `position_ids` and `padding_side`.** We can replic...\"],[\"-27.4360],\\n                 [ -27.1677,  -26.7330,  -30.2386,  ...,  -33.6813,  -33.6931,\\n          ...\"],[\"```\\n        \\n    4. **Note on HFâ€™s transformers â€” `position_ids` during `generate`:** during generat...\"],[\"2. **Note on HFâ€™s transformers â€” sampling could stop at `eos_token`:** in `transformers`, the genera...\"],[\"pretrained_model.generation_config.pad_token_id = None  # generate tokens without truncation \\u002f paddi...\"],[\"```\\n        \\n    3. Note that in a more recent codebase https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fsummarize-from-fee...\"],[\"6. **Use different seeds for different processes**\\n    1. When spawning 8 GPU processes to do data p...\"],[\"# Reward Model Implementation Details\\n\\nIn this section, we discuss reward-model-specific implementat...\"],[\"1. **The reward model only outputs the value at the last token.**\\n    1. Notice that the rewards obt...\"],[\"2. Note that in a more recent codebase [*openai\\u002fsummarize-from-feedback*](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002f...\"],[\"2. **Reward head layer initialization**\\n    1. The weight of the reward head is initialized accordin...\"],[\"2. The bias of the reward head is set to 0 ([lm_human_preferences\\u002flanguage\\u002fmodel.py#L254](https:\\u002f\\u002fgi...\"],[\"2. When performing the normalization process, the code first sets `reward_gain=1, reward_bias=0` ([l...\"],[\"$$\\\\begin{aligned}g*\\\\mathcal{N}(\\\\mu_{\\\\mathcal{D}}, \\\\sigma_{\\\\mathcal{D}}) + b &= \\\\mathcal{N}(g*\\\\mu_{\\\\m...\"],[\"5. Note that responses  \\\\\\\\( y \\\\sim \\\\rho(Â·|x) \\\\\\\\) we generated for the normalization purpose are from...\"],[\"# Policy Training Implementation Details\\n\\nIn this section, we will delve into details, such as layer...\"],[\"1. **Scale the logits by sampling temperature.** \\n    1. When calculating the log probability of res...\"],[\"2. The bias of the reward head is set to 0 ([lm_human_preferences\\u002flanguage\\u002fmodel.py#L254](https:\\u002f\\u002fgi...\"],[\"2. When running `openai\\u002flm-human-preferences`, OAIâ€™s datasets were partially corrupted\\u002flost ([openai...\"],[\"2. Specifically, this is achieved with the following steps:\\n        1. **Token truncation**: We want...\"],[\"1. Code comment: â€œcentral example: ensure that the sample contains `truncate_token`\\\"\\n            2. ...\"],[\"7. **Terminology of the training loop: batches and minibatches in PPO**\\n    1. OAI uses the followin...\"],[\"micro_batch_end = micro_batch_start + micro_batch_size \\n                    micro_batch_inds = mini_...\"],[\"# ____â© a forward pass on [3. 2.]\\n        # âª a backward pass on [0. 7. 3. 2.]\\n        # epoch: 3 ba...\"],[\"```\\n        \\n8. **Per-token KL penalty**\\n    - The code adds a per-token KL penalty ([lm_human_prefe...\"],[\"- Then the `non_score_reward = beta * kl` , where `beta` is the KL penalty coefficient  \\\\\\\\(\\\\beta\\\\\\\\),...\"],[\"```\\n    \\n    1. In each minibatch, OAI then whitens the reward `whiten(rewards, shift_mean=False)` w...\"],[\"if not shift_mean:\\n                whitened += mean\\n            return whitened\\n        \\n        def...\"],[\"```\\n        \\n        ```jsx\\n        mean[1.5999999]\\n        var[0.0666666627]\\n        [[0.05080712 0...\"],[\"```\\n        \\n10. **Clipped value function**\\n    1. As done in the original PPO ([baselines\\u002fppo2\\u002fmode...\"],[\"self.value = init_kl_coef\\n                self.hparams = hparams\\n        \\n            def update(sel...\"],[\"```\\n        \\n    - For the `sentiment` and `descriptiveness` tasks examined in this work, we have `i...\"],[\"```python\\n### pytorch adam implementation:\\nbias_correction1 = 1 - beta1 ** step\\nbias_correction2 = 1...\"],[\"```\\n\\n- Letâ€™s compare the update equations of pytorch-style and tensorflow-style adam. Following the ...\"],[\"$$\\\\begin{aligned}\\\\text{tensorflow adam:}\\\\quad \\\\theta_t & =\\\\theta_{t-1}-\\\\alpha_t m_t \\u002f\\\\left(\\\\sqrt{v_t...\"],[\"- The equations above highlight that the distinction between pytorch and tensorflow implementation i...\"],[\"- The above figure shows that, if we set the same `eps` in pytorch adam and tensorflow adam, then py...\"],[\"| ratio_var | 0.0007716546 | 0.005374275613576174 | 0.0007942612282931805 |\\n    | ratio_max | 1.2272...\"],[\"- **PyTorchâ€™s `Adam` presents a more extreme ratio max and min.** Here `ratio = torch.exp(logprobs_d...\"],[\"- Furthermore, because of the larger KL, many other training metrics are affected as well. For examp...\"],[\"![adam_gpt2.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftrl-internal-testing\\u002fexample-images\\u002fresolve\\u002fmain\\u002frl...\"],[\"# Conclusion\\n\\nIn this work, we took a deep dive into OAIâ€™s original RLHF codebase and compiled a lis...\"],[\"--\\ntitle: \\\"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\\\"\\nthumbnail: \\u002fbl...\"],[\"Distributed training is the key to enable training such large ML models. There have been major recen...\"],[\"In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch fe...\"],[\"Sample FSDP config after running the command `accelerate config`:\\n```bash\\ncompute_environment: LOCAL...\"],[\"```\\n\\n## Multi-GPU FSDP\\n\\nHere, we experiment on the Single-Node Multi-GPU setting. We compare the per...\"],[\"```\\nSample FSDP Run:\\n![Sample FSDP Run](.\\u002fassets\\u002f62_pytorch_fsdp\\u002fsample_fsdp_run.png)\\n\\n\\n| Method | B...\"],[\"### CPU Offloading to enable training humongous models that wonâ€™t fit the GPUÂ memory\\n\\nCommand for tr...\"],[\"```\\n\\n| Method | Batch Size Max ($BS) | Num GPUs | Approx Train Time (Hours) | Notes |\\n| --- | --- | ...\"],[\"Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\\n\\nFrom Table 2, we can observe that DDP (w and w\\u002f...\"],[\"After creating an instance of this class, users can pass it when creating the Accelerator object.\\n\\nF...\"],[\"We can observe that the DDP takes twice as much memory as FSDP with auto wrap.  FSDP without auto wr...\"],[\"```\\n\\n- In case of a single model, if you have created optimizer with multiple parameter groups and c...\"],[\"```\\n- In case of multiple models, it is necessary to prepare the models before creating optimizers e...\"],[\"FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters acros...\"],[\"[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f191...\"],[\"--\\ntitle: \\\"Hosting your Models and Datasets on Hugging Face Spaces using Streamlit\\\"\\nthumbnail: \\u002fblog...\"],[\"``` python\\nimport streamlit as st\\n\\n# adding the text that will show in the text box as default\\ndefau...\"],[\"```\\n\\nThe inference code returns the generated output, you can print the output using simple ```st.wr...\"],[\"```\\n\\n If you'd like to use libraries like matplotlib, seaborn or bokeh, all you have to do is to put...\"],[\"--\\ntitle: \\\"Mixture of Experts Explained\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fmoe\\u002fthumbnail.png\\nauthors:\\n- user: ...\"],[\"Letâ€™s dive in!\\n\\n## Table of Contents\\n\\n- [What is a Mixture of Experts?](#what-is-a-mixture-of-expert...\"],[\"Letâ€™s dive in!\\n\\n## What is a Mixture of Experts (MoE)?\\n\\nThe scale of a model is one of the most impo...\"],[\"Although MoEs provide benefits like efficient pretraining and faster inference compared to dense mod...\"],[\"Between 2010-2015, two different research areas contributed to later MoE advancement:\\n\\n- **Experts a...\"],[\"## What is Sparsity?\\n\\nSparsity uses the idea of conditional computation. While in dense models all t...\"],[\"Shazeerâ€™s work also explored other gating mechanisms, such as Noisy Top-k Gating. This gating approa...\"],[\"## MoEs and Transformers\\n\\nTransformers are a very clear case that scaling up the number of parameter...\"],[\"The GShard paper has contributions by expressing parallel computation patterns that work well for Mo...\"],[\"- The router computation is reduced\\n- The batch size of each expert can be at least halved\\n- Communi...\"],[\"This [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1aGGVHZmtKmcNBbAwa9hbu58DDpIuB5O4?usp=sharin...\"],[\"## What does an expert learn?\\n\\nThe ST-MoE authors observed that encoder experts specialize in a grou...\"],[\"Switch Transformers observed that at a fixed pretrain perplexity, the sparse model does worse than t...\"],[\"One last part to consider when fine-tuning sparse MoEs is that they have different fine-tuning hyper...\"],[\"## When to use sparse MoEs vs dense models?\\n\\nExperts are useful for high throughput scenarios with m...\"],[\"### Capacity Factor and communication costs\\n\\nIncreasing the capacity factor (CF) increases the quali...\"],[\"Megablocks (Nov 2022) explores efficient sparse pretraining by providing new GPU kernels that can ha...\"],[\"In the realm of released open access MoEs, you can check:\\n\\n- [Switch Transformers (Google)](https:\\u002f\\u002f...\"],[\"- [Adaptive Mixture of Local Experts (1991)](https:\\u002f\\u002fwww.cs.toronto.edu\\u002f~hinton\\u002fabsps\\u002fjjnh91.pdf)\\n- ...\"],[\"## Citation\\n\\n```bibtex\\n@misc {sanseviero2023moe,\\n    author       = { Omar Sanseviero and\\n          ...\"],[\"```\\n\\n```\\nSanseviero, et al., \\\"Mixture of Experts Explained\\\", Hugging Face Blog, 2023.\\n```...\"],[\"--\\ntitle: \\\"An Introduction to Q-Learning Part 1\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f70_deep_rl_q_part1\\u002fthumbnai...\"],[\"So today, we're going toÂ **dive deeper into one of the Reinforcement Learning methods: value-based m...\"],[\"So let's get started!\\n\\n- [What is RL? A short recap](#what-is-rl-a-short-recap)\\n- [The two types of ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fpolicy.jpg...\"],[\"\\u003e But what does it mean to act according to our policy? After all, we don't have a policy in value-b...\"],[\"Consequently, whatever method you use to solve your problem,Â **you will have a policy**, but in the ...\"],[\"### **The Action-Value function**\\n\\nIn the Action-value function, for each state and action pair, the...\"],[\"This can be a tedious process, and that'sÂ **where the Bellman equation comes to help us.**\\n\\n## **The...\"],[\"So you see, that's a pretty tedious process if you need to do it for each state value or state-actio...\"],[\"For simplification, here we don't discount, so gamma = 1.\\n\\n- The value of  \\\\\\\\(V(S_{t+1}) \\\\\\\\)  = Imme...\"],[\"If we take an example:\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_de...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fMC-4.jpg\\\" ...\"],[\"### **Temporal Difference Learning: learning at each step**\\n\\n- **Temporal difference, on the other h...\"],[\"- We just started to train our Value function, so it returns 0 value for each state.\\n- Our learning ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fSummary.jp...\"],[\"**Thatâ€™s normal if you still feel confused with all these elements**. This was the same for me and f...\"],[\"--\\ntitle:  Introducing the Hugging Face LLM Inference Container for Amazon SageMaker\\nthumbnail: \\u002fblo...\"],[\"## What is Hugging Face LLM Inference DLC?\\n\\nHugging Face LLM DLC is a new purpose-built Inference Co...\"],[\"Officially supported model architectures are currently:\\n\\n- [BLOOM](https:\\u002f\\u002fhuggingface.co\\u002fbigscience...\"],[\"## 1. Setup development environment\\n\\nWe are going to use the `sagemaker` python SDK to deploy BLOOM ...\"],[\"```\\n\\nIf you are going to use Sagemaker in a local environment, you need access to an IAM Role with t...\"],[\"```\\n\\n## 2. Retrieve the new Hugging Face LLM DLC\\n\\nCompared to deploying regular Hugging Face models,...\"],[\"```\\n\\n## 3. Deploy Open Assistant 12B to Amazon SageMaker\\n\\n_Note: Quotas for Amazon SageMaker can var...\"],[\"```\\n\\nAfter we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `de...\"],[\"```\\n\\nSageMaker will now create our endpoint and deploy the model to it. This can take 5-10 minutes.\\n...\"],[\"You can find the open api specification of TGI in the [swagger documentation](https:\\u002f\\u002fhuggingface.gi...\"],[\"```\\n\\u003c|prompter|\\u003e[Instruction]\\u003c|endoftext|\\u003e\\n\\u003c|assistant|\\u003e\\n```\\n\\nlets give it a first try and ask about...\"],[\"```\\n\\n```python\\nimport gradio as gr\\n\\n# hyperparameters for llm\\nparameters = {\\n    \\\"do_sample\\\": True,\\n...\"],[\"```\\n\\n![Gradio Chat application](assets\\u002f145_sagemaker-huggingface-llm\\u002fgradio.png \\\"Gradio Chat applica...\"],[\"--\\ntitle: \\\"Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face\\\"\\nthumbnail:...\"],[\"Throughout the project, Fetch had weekly calls with the AWS team and received support from a subject...\"],[\"Fetch heavily relied on the ML training features of Amazon SageMaker, particularly its [training job...\"],[\"In addition to its custom ML models, Fetch uses [AWS Deep Learning Containers ](https:\\u002f\\u002faws.amazon.c...\"],[\"Users enjoy the updates too; Fetch has grown from 10 million to 18 million monthly active users sinc...\"],[\"--\\ntitle: \\\"Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\\\"\\nthumbnail: ...\"],[\"Such large models raise new challenges in terms of memory and speed for both [training](https:\\u002f\\u002fhugg...\"],[\"Moreover, support for [HPU graphs](https:\\u002f\\u002fdocs.habana.ai\\u002fen\\u002flatest\\u002fPyTorch\\u002fInference_on_PyTorch\\u002fInf...\"],[\"## Benchmarks\\n\\nIn this section, we are going to provide an early benchmark of BLOOMZ on Gaudi2, firs...\"],[\"Runs were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-val...\"],[\"*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, ...\"],[\"### Running inference on a complete dataset\\n\\nThe script we wrote enables using your model to complet...\"],[\"```\\nBatch nÂ°1\\nInput: ['Facebook has released a report that shows what content was most widely viewed...\"],[\"--------------------------------------------------------------------------------------------------\\nB...\"],[\"```\\n\\nIn the next section, we explain how to use the script we wrote to perform this benchmark or to ...\"],[\"```\\n\\nFor multi-node inference, you can follow [this guide](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fhaban...\"],[\"We also presented the results achieved with first-generation Gaudi. For smaller models, it can perfo...\"],[\"[^1]: â€œZero-shotâ€ refers to the ability of a model to complete a task on new or unseen input data, i...\"],[\"--\\ntitle: Deploying ğŸ¤— ViT on Kubernetes with TF Serving\\nthumbnail: \\u002fblog\\u002fassets\\u002f94_tf_serving_kubern...\"],[\"- **Containerizing the application logic**: The application logic\\n  involves a served model that can...\"],[\"**Note**: The code snippets shown in this post can be executed on a Unix terminal\\nas long as you hav...\"],[\"```bash\\n$ MODEL_TAR=model.tar.gz\\n$ MODEL_NAME=hf-vit\\n$ MODEL_VERSION=1\\n$ MODEL_PATH=models\\u002f$MODEL_NA...\"],[\"```\\n\\nBelow, we show how the `models` directory is structured in our case:\\n\\n```bash\\n$ find \\u002fmodels\\n\\u002fm...\"],[\"```\\n\\nWe used the official Docker image of TensorFlow Serving as the base, but\\nyou can use ones that ...\"],[\"```\\n\\n## Running the Docker image locally\\n\\nLastly, you can run the newly built Docker image locally t...\"],[\"```\\n\\nSince weâ€™re using GCR, you need to prefix the\\nDocker image tag ([\\u003cu\\u003enote\\u003c\\u002fu\\u003e](https:\\u002f\\u002fcloud.goo...\"],[\"```\\n\\nGCP offers a variety of machine types to configure the deployment in a\\nway you want. We encoura...\"],[\"```\\n\\nThe `gcloud container clusters get-credentials` command takes care of\\nboth connecting to the cl...\"],[\"Next, we go through the important parts of each of these manifests.\\n\\n**`deployment.yaml`**:\\n\\n```yaml...\"],[\"```\\n\\nYou can configure the names like `tfs-server`, `tfs-k8s` any way you\\nwant. Under `containers`, ...\"],[\"```\\n\\nWe made the service type â€˜LoadBalancerâ€™ so the endpoints are\\nexposed externally to the Kubernet...\"],[\"```\\n\\nHPA stands for **H**orizontal **P**od **A**utoscaler. It sets criteria\\nto decide when to scale ...\"],[\"```bash\\n$ kubectl apply -f deployment.yaml\\n$ kubectl apply -f service.yaml\\n$ kubectl apply -f hpa.ya...\"],[\"```\\n\\nWhile using `kubectl` is fine for applying each of the manifests to\\nperform the deployment, it ...\"],[\"```\\n\\nNote down the external IP when it becomes available.\\n\\nAnd that sums up all the steps you need t...\"],[\"```\\n\\nIf youâ€™re interested to know how this deployment would perform if it\\nmeets more traffic then we...\"],[\"# Conclusion\\n\\nIn this post and the associated [repository](https:\\u002f\\u002fgithub.com\\u002fsayakpaul\\u002fdeploy-hf-tf...\"],[\"--\\ntitle: 'Welcome Stable-baselines3 to the Hugging Face Hub ğŸ¤—'\\nthumbnail: \\u002fblog\\u002fassets\\u002f47_sb3\\u002fthumb...\"],[\"```\\n\\n### Finding Models\\n\\nWeâ€™re currently uploading saved models of agents playing Space Invaders, Br...\"],[\"```\\n\\n### Sharing a model to the Hub\\nIn just a minute, you can get your saved model in the Hub.\\n\\nFirs...\"],[\"```\\nTry it out and share your models with the community!\\n\\n### What's next?\\n\\nIn the coming weeks and ...\"],[\"### Would you like to integrate your library to the Hub?\\n\\nThis integration is possible thanks to the...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 2: SaaS Edition]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f67_...\"],[\"### [Omar Rahman](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fomar-rahman-4739713a\\u002f) - Director of Machine Learning ...\"],[\"#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into SaaS?**\\nNot focuss...\"],[\"**Fun Fact:** Cao is a cat-lover and is a mom to two cats: one Singapura girl and one British shorth...\"],[\"**Background:**  Raphael has a Ph.D. in the field of understanding health records and genetics, has ...\"],[\"This helps make people much better at their jobs.\\n\\n\\n#### **2. What are the biggest ML challenges wit...\"],[\"Think outside the box and donâ€™t just take something someone built and think I have an idea of how to...\"],[\"### [Martin Ostrovsky](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fmartinostrovsky\\u002f) Founder\\u002fCEO & Machine Learning ...\"],[\"#### **2. What is your biggest ML challenge?**\\nThe biggest ML challenge is audio to text transcripti...\"],[\"**Better staff and patient experiences:**\\n\\nPredictive healthcare networks are expected to reduce wai...\"],[\"--\\ntitle: \\\"From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community\\\" \\nthumbnail: ...\"],[\"## Background\\n\\nThe efforts to bring Machine Learning to Elixir started almost 2 years ago with [the ...\"],[\"Next, we plan to focus on training and transfer learning of Neural Networks in Elixir, allowing deve...\"],[\"--\\ntitle: \\\"ğŸ¶Safetensors audited as really safe and becoming the default\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f142...\"],[\"```\\n\\nIt also has a number of [cool features](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fsafetensors#yet-another-...\"],[\"```\\n\\nis likely to be the only thing needed to run `safetensors` files safely.\\n\\nGoing forward and tha...\"],[\"Since the Hugging Face Hub is a platform where anyone can upload and share models, it is important t...\"],[\"One import thing to note is that the library is written in Rust. This adds\\nan extra layer of [securi...\"],[\"--\\ntitle: \\\"Federated Learning using Hugging Face and Flower\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002ffl-with-flower...\"],[\"```\\n\\n## Standard Hugging Face workflow\\n\\n### Handling the data\\n\\nTo fetch the IMDB dataset, we will us...\"],[\"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\\n    trainloader = DataLoader(\\n        t...\"],[\"```\\n\\n### Training and testing the model\\n\\nOnce we have a way of creating our trainloader and testload...\"],[\"```\\n\\n## Federating the example\\n\\nThe idea behind Federated Learning is to train a model between multi...\"],[\"```\\n\\nThe `get_parameters` function lets the server get the client's parameters. Inversely, the `set_...\"],[\"```\\n\\nThe `weighted_average` function is there to provide a way to aggregate the metrics distributed ...\"],[\"--\\ntitle: \\\"Creating open machine learning datasets? Share them on the Hugging Face Hub!\\\"\\nthumbnail: ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e \\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"Alongside this many of the datasets on the Hub can also be loaded directly into [`Pandas`](https:\\u002f\\u002fp...\"],[\"You can learn more about how you can use this tool in this [blog post](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fs...\"],[\"## API and client library interaction with the Hub\\n\\nInteracting with the Hugging Face Hub via an [AP...\"],[\"### Other important features for researchers\\n\\nSome other features of the Hub may be of particular in...\"],[\"The following pages will be useful if you want to share large datasets:\\n- [Repository limitations an...\"],[\"--\\ntitle: \\\"Assisted Generation: a new direction toward low-latency text generation\\\"\\nthumbnail: \\u002fblog...\"],[\"\\u003c!-- [GIF 1 -- FWD PASS] --\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        ...\"],[\"From the description above, the latency bottleneck in text generation is clear: running a model forw...\"],[\"```python\\n# Example showcasing the impact of batched generation. Measurement device: RTX3090\\nfrom tr...\"],[\"```\\n\\nFinally, if you have multiple devices available to you, you can distribute the workload using [...\"],[\"```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\ntok = AutoTokenizer.from_pre...\"],[\"```\\n\\n\\nThis means that you can use a model forward pass for a different purpose: in addition to feedi...\"],[\"Obviously, there are no latency-free assistant models. Nevertheless, it is relatively easy to find a...\"],[\"Wrapping all up, hereâ€™s our original implementation of the assisted generation loop ([code](https:\\u002f\\u002f...\"],[\"Weâ€™ve designed the API in ğŸ¤— Transformers such that this process is hassle-free for you. All you need...\"],[\"```\\n\\n\\nIs the additional internal complexity worth it? Letâ€™s have a look at the latency numbers for t...\"],[\"Drawing samples from a probability distribution for the next token will cause our greedy assistant t...\"],[\"Finally, assisted generation resurfaces a crucial question in text generation. The field has been ev...\"],[\"```\\n\\n\\n## Acknowledgements\\n\\nI'd like to thank Sylvain Gugger, Nicolas Patry, and Lewis Tunstall for s...\"],[\"--\\ntitle: \\\"Introducing the Private Hub: A New Way to Build With Machine Learning\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"With this in mind, we launched the [Private Hub](https:\\u002f\\u002fhuggingface.co\\u002fplatform) (PH), a new way to...\"],[\"Each model, dataset or space uploaded to the Hub is a [Git-based repository](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"These models span 180 languages and support up to 25 ML libraries (including Transformers, Keras, sp...\"],[\"### Datasets\\n\\nData is a key part of building machine learning models; without the right data, you wo...\"],[\"### Spaces\\n\\nA few months ago, we introduced a new feature on the ğŸ¤— Hub called [Spaces](https:\\u002f\\u002fhuggi...\"],[\"With the Private Hub, data scientists can seamlessly work with [Transformers](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"- **On-cloud Private Hub**: runs in a cloud account on AWS, Azure or GCP owned by the customer. This...\"],[\"We built the Private Hub to change this. Like Git and GitHub forever changed how companies build sof...\"],[\"For our demo example, one of the requirements for building this ML app for financial analysts is doi...\"],[\"Then, we use [AutoTrain](https:\\u002f\\u002fhuggingface.co\\u002fautotrain) to quickly fine-tune the FinBert model wi...\"],[\"Besides the performance metrics, we can easily test the [fine-tuned models](https:\\u002f\\u002fhuggingface.co\\u002fF...\"],[\"If you take a look at the [app.py file](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fFinanceInc\\u002fFinancial_Analyst_A...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"```\\n\\nWith just 12 lines of code, we are up and running in running inferences with an infrastructure ...\"],[\"--\\ntitle: \\\"Speculative Decoding for 2x Faster Whisper Inference\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fwhisper-sp...\"],[\"In this blog post, we demonstrate how Speculative Decoding can be employed to reduce the \\ninference ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        style=\\\"max-width: 90%; margin...\"],[\"The inference process then repeats, the assistant model generating a new set of \\\\\\\\( N \\\\\\\\) candidate ...\"],[\"The only constraint for selecting an assistant model is that it must share the same vocabulary as th...\"],[\"## English Speech Transcription\\n\\n### Baseline Implementation\\n\\nWe start by benchmarking Whisper [larg...\"],[\"model_id = \\\"openai\\u002fwhisper-large-v2\\\"\\n\\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\\n    model_i...\"],[\"```\\n\\nLet's load the English speech transcription dataset that we will use for benchmarking. We'll lo...\"],[\"```\\n\\n**Output:**\\n```\\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73\\u002f73 [01:37\\u003c00:00,  1.33s\\u002fit]\\n72.99542546272278\\n```\\n\\nAlright!...\"],[\"```\\n**Output:**\\n0.03507271171941831\\n```\\n\\nOur final baseline number is 73 seconds for a WER of 3.5%.\\n...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) We intend ...\"],[\"```\\n\\nLet's run the benchmark with speculative decoding, using Distil-Whisper as the assistant to Whi...\"],[\"```\\n**Outputs:**\\n```\\n0.03507271171941831\\n```\\n\\nPerfect! 3.5% WER again, as we have identical outputs ...\"],[\"```\\n\\nAn end-to-end code snippet for running speculative decoding with Whisper and Distil-Whisper can...\"],[\"```\\n\\nFor our benchmarking dataset, we'll load 73 samples from the Dutch (\\\"nl\\\") split of the [VoxPopu...\"],[\"```\\n\\nRight! We have our baseline time of 117 seconds and a WER of 12.8%. Let's re-run the generation...\"],[\"```\\n\\nAgain, we achieve 12.8% WER, but this time in just 62 seconds of inference time, representing a...\"],[\"#### Batch Size\\n\\nIt is worth noting that the largest speed gains with speculative decoding come with...\"],[\"--\\ntitle: \\\"Snorkel AI x Hugging Face: unlock foundation models for enterprises\\\"\\nthumbnail: \\u002fblog\\u002fass...\"],[\"## Foundation models in Snorkel Flow\\n\\nThe Snorkel Flow development platform enables users to [adapt ...\"],[\"Hugging Faceâ€™s service allows users to create a model API in a few clicks and begin using it immedia...\"],[\"Clement Delangue, co-founder and CEO, Hugging Face\\n\\n## Conclusion\\n\\nTogether, Snorkel and Hugging Fac...\"],[\"--\\ntitle: \\\"An Introduction to Q-Learning Part 2\\u002f2\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f73_deep_rl_q_part2\\u002fthumbn...\"],[\"So, in the second part, weâ€™ll **study Q-Learning**, **and implement our first RL agent from scratch*...\"],[\"**Q-Learning is the algorithm we use to train our Q-Function**, anÂ **action-value function**Â that de...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002fQ-function...\"],[\"So now that we understand what Q-Learning, Q-Function, and Q-Table are,Â **let's dive deeper into the...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002fQ-learning...\"],[\"How do we form the TD target? \\n1. We obtain the reward after taking the action \\\\\\\\(R_{t+1}\\\\\\\\).\\n2. To ...\"],[\"For instance, with Sarsa, another value-based algorithm,Â **the Epsilon-Greedy Policy selects the nex...\"],[\"- **+0:**Â Going to a state with no cheese in it.\\n- **+1:**Â Going to a state with a small cheese in i...\"],[\"By going right, I've got a small cheese, so \\\\\\\\(R_{t+1} = 1\\\\\\\\), and I'm in a new state.\\n\\n\\n\\u003cfigure cla...\"],[\"Because I go to the poison state,Â **I get \\\\\\\\(R_{t+1} = -10\\\\\\\\), and I die.**\\n\\n\\u003cfigure class=\\\"image ta...\"],[\"Start the tutorial here ğŸ‘‰ https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fhuggingface\\u002fdeep-rl-class\\u002fblob\\u002fma...\"],[\"--\\ntitle: \\\"Overview of natively supported quantization schemes in ğŸ¤— Transformers\\\" \\nthumbnail: \\u002fblog\\u002f...\"],[\"## Resources\\n\\n- [GPTQ blogpost](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fgptq-integration) â€“ gives an overview on...\"],[\"### What are the benefits of bitsandbytes?\\n**easy**: bitsandbytes still remains the easiest way to q...\"],[\"**n-bit support**: The GPTQ algorithm makes it possible to quantize models up to 2 bits! However, th...\"],[\"**works only for language models (for now)**: As of today, the API for quantizing a model with auto-...\"],[\"with batch size = 1: \\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token laten...\"],[\"with batch size = 16:\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token laten...\"],[\"The benchmark was run on an A100 with a prompt length of 30 and we generated exactly 30 tokens. The ...\"],[\"with a Titan RTX: \\n\\n![Benchmark TITAN RTX](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation...\"],[\"![Benchmark A100 finetuning](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolv...\"],[\"with 13b model: \\n\\n| model_id                           | Average | ARC   | Hellaswag | MMLU  | Truth...\"],[\"We hope that this overview will make it easier for everyone to use LLMs in their applications and us...\"],[\"--\\ntitle: 'Train and Fine-Tune Sentence Transformers Models'\\nthumbnail: \\u002fblog\\u002fassets\\u002f95_training_st_...\"],[\"This is how the Sentence Transformers models work:\\n\\n1. **Layer 1** â€“ The input text is passed throug...\"],[\"```\\n\\nFrom the code above, you can see that Sentence Transformers models are made up of modules, that...\"],[\"```\\n\\nNow for the most critical part: the dataset format.\\n\\n## How to prepare your dataset for trainin...\"],[\"Most dataset configurations will take one of four forms (below you will see examples of each case):\\n...\"],[\"Note that Sentence Transformers models can be trained with human labeling (cases 1 and 3) or with la...\"],[\"- Case 4: The [Quora Triplets dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fembedding-data\\u002fQQP_triplets) ...\"],[\"```\\n\\nThis guide uses an unlabeled triplets dataset, the fourth case above.\\n\\nWith the `datasets` libr...\"],[\"```\\nYou can see that `query` (the anchor) has a single sentence, `pos` (positive) is a list of sente...\"],[\"```\\n\\nThe next step is to choose a suitable loss function that can be used with the data format.\\n\\n## ...\"],[\"Case 3: When your samples are triplets of the form `[anchor, positive, negative]` and you have an in...\"],[\"```\\nOnce the dataset is in the desired format and a suitable loss function is in place, fitting and ...\"],[\"```\\n\\nThen, you can share your models by calling the `save_to_hub` method from the trained model. By ...\"],[\"```\\n\\nIn the [Notebook Companion](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain...\"],[\"--\\ntitle: \\\"SDXL in 4 steps with Latent Consistency LoRAs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002flcm_sdxl\\u002flcm_thumb...\"],[\"## Contents\\n\\n- [Method Overview](#method-overview)\\n- [Why does this matter](#why-does-this-matter)\\n-...\"],[\"1. Select an available teacher model from the Hub. For example, you can use [SDXL (base)](https:\\u002f\\u002fhu...\"],[\"## Fast Inference with SDXL LCM LoRAs\\n\\nThe version of `diffusers` released today makes it very easy ...\"],[\"```\\n\\nNote how the code:\\n- Instantiates a standard diffusion pipeline with the SDXL 1.0 base model.\\n-...\"],[\"```\\n\\nThese are the 8 images displayed in a grid:\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingfa...\"],[\"```\\n\\nThen we can run inference as usual for SDXL. Weâ€™ll gather results using varying number of steps...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"prompt = \\\"collage style kid sits looking at the night sky, full of stars\\\"\\n\\ngenerator = torch.Generat...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"## Benchmarks\\n\\nThis section is not meant to be exhaustive, but illustrative of the generation speed ...\"],[\"For cards with a lot of capacity, such as A100, performance increases significantly when generating ...\"],[\"- [`latent-consistency\\u002flcm-sdxl`](https:\\u002f\\u002fhuggingface.co\\u002flatent-consistency\\u002flcm-sdxl). Full fine-tun...\"],[\"prompt = \\\"a toy_face man\\\"\\nnegative_prompt = \\\"blurry, low quality, render, 3D, oversaturated\\\"\\nimages ...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We hope these scripts inspire the community to try their own fine-tunes. Please, do let us know if y...\"],[\"- [LoRA the Explorer (experimental LCM version)](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002flatent-consistency\\u002flc...\"],[\"--\\ntitle: \\\"Introducing Skops\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f94_skops\\u002fintroducing_skops.png\\nauthors:\\n- user...\"],[\"```\\n\\nYou can use any model filename and serialization method, like `pickle` or `joblib`. At the mome...\"],[\"```\\n\\nThe repository now contains the serialized model and the configuration file. \\nThe configuration...\"],[\"```python\\nfrom skops import card\\n\\n# create the card \\nmodel_card = card.Card(model, metadata=card.met...\"],[\"```\\n\\nWe will now evaluate the model and add a description of the evaluation method with `add`. The m...\"],[\"```\\n\\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_...\"],[\"```\\n\\nYou can see the example repository pushed with above code [here](https:\\u002f\\u002fhuggingface.co\\u002fscikit-...\"],[\"--\\ntitle: \\\"Run a Chatgpt-like Chatbot on a Single GPU with ROCm\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fchatbot-am...\"],[\"It was released on [Github](https:\\u002f\\u002fgithub.com\\u002flm-sys\\u002fFastChat) on Apr\\n11, just a few weeks ago. It ...\"],[\"By leveraging this technique, several 4-bit quantized Vicuna models are\\navailable from Hugging Face ...\"],[\"**Quick Start**\\n\\n**1 ROCm installation and Docker container setup (Host machine)**\\n\\n**1.1 ROCm** **i...\"],[\"```\\nsudo apt update && sudo apt upgrade -y\\nwget https:\\u002f\\u002frepo.radeon.com\\u002famdgpu-install\\u002f5.4.3\\u002fubuntu\\u002f...\"],[\"```\\n**2 Model** **quantization and Model inference (Inside the docker)**\\n\\nYou can either download qu...\"],[\"```\\nNow that you have everything set up, it's time to run the Vicuna 13B\\nmodel on your AMD GPU. Use ...\"],[\"```\\nNow the 4-bit quantized Vicuna-13B model can be fitted in RX6900XT GPU\\nDDR memory, which has 16G...\"],[\"**Vicuna fp16 and 4bit quantized model comparison**\\n\\nTest environment:\\n\\n\\\\- GPU: Instinct MI210, RX69...\"],[\"- Vicuna 13b â€“ quant (4bit\\u002ffp16): 4bits datatype parameter, fp16 Matmul\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg s...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002flm-sys\\u002fFastChat\\ncd FastChat\\n```\\nConvert the LLaMA parameters by usi...\"],[\"```\\nNow the model is ready and saved as\\n**Vicuna-13b-4bit-act-order.safetensors**.\\n\\n**GPTQ Dequantiz...\"],[\"--\\ntitle: \\\"Zero-shot image-to-text generation with BLIP-2\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fblip-2\\u002fthumbnail...\"],[\"## Introduction\\n\\nRecent years have seen rapid advancements in computer vision and natural language p...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"As a visual encoder, BLIP-2 uses ViT, and for an LLM, the paper authors used OPT and Flan T5 models....\"],[\"```\\n\\nNext, we'll need an input image. Every week The New Yorker runs a [cartoon captioning contest](...\"],[\"```\\n\\nNotice that BLIP-2 is a rare case where you cannot load the model with Auto API (e.g. AutoModel...\"],[\"```\\n\\n```\\n\\\"two monsters sitting around a campfire\\\"\\n```\\n\\n```\\nprompt = \\\"they look like they are\\\"\\n\\ninput...\"],[\"```\\ncontext = [\\n   (\\\"What is a dinosaur holding?\\\", \\\"a torch\\\"),\\n   (\\\"Where are they?\\\", \\\"In the woods....\"],[\"```\\n\\n```\\nTo light a fire.\\n```\\n\\n## Conclusion\\n\\nBLIP-2 is a zero-shot visual-language model that can b...\"],[\"--\\ntitle: \\\"Scaling-up BERT Inference on CPU (Part 1)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f21_bert_cpu_scaling_pa...\"],[\"We decided to focus on the most famous Transformer model architecture, \\n[BERT (Delvin & al. 2018) (4...\"],[\"On the 2021 version, we kept the ability to run inference workloads through PyTorch and Tensorflow a...\"],[\"- AVX512 instructions set (_which might not be leveraged out-of-the-box by the various frameworks_)\\n...\"],[\"One possible way to explain such difference between the two frameworks might be the underlying techn...\"],[\"To illustrate this, imagine two tasks **A** and **B**, executing in parallel, each on its own softwa...\"],[\"Back to our model inference workload... If you think about it, in a perfect world with a fully optim...\"],[\"```\\n\\nIn our case we have a machine with **2 sockets**, each socket providing **24 physical cores** w...\"],[\"_Note: Setting both cores and memory affinities is important here. Having computations done on socke...\"],[\"```\\n\\nThen we specify the core and memory affinity through `numactl` using all the **physical** cores...\"],[\"```\\n\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg class=\\\"centered\\\" alt=\\\"htop CPU usage without and with numact...\"],[\"As we are targeting just 1 thread per physical core, as explained earlier, we pick only thread 0 on ...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg alt=\\\"\\\" src=\\\"assets\\u002f21_bert_cpu_scaling_part_1\\u002fimgs\\u002fcore_count_sca...\"],[\"```\\n\\nStarting from here, each instance does not share any resource with the other, and everything is...\"],[\"```\\n\\nThe outcomes remain the same, our 4 instances are effectively running in a truly parallel manne...\"],[\"```\\n\\n\\n## 8. Batch size scaling - Improving throughput and latency with multiple parallel & independe...\"],[\"Also, it is important to notice the results might look totally different on another system _(i.e. Op...\"],[\"\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg alt=\\\"Batch scaling experiment for PyTorch and Tensorflow\\\" src=\\\"assets\\u002f...\"],[\"Last but not least, many of the knobs discussed along this blog post can be automatically tuned thro...\"],[\"1. [Benchmarking Transformers: PyTorch and TensorFlow](https:\\u002f\\u002fmedium.com\\u002fhuggingface\\u002fbenchmarking-t...\"],[\"13. [Introduction to Hyper-Threading Technology](https:\\u002f\\u002fsoftware.intel.com\\u002fcontent\\u002fwww\\u002fus\\u002fen\\u002fdevelo...\"],[\"--\\ntitle: \\\"Welcome PaddlePaddle to the Hugging Face Hub\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f126_paddlepaddle\\u002ft...\"],[\"## Find PaddlePaddle Models\\n\\nYou can find all PaddlePaddle models on the Model Hub by filtering with...\"],[\"Models that support a [task](https:\\u002f\\u002fhuggingface.co\\u002ftasks) are equipped with an interactive widget t...\"],[\"```\\n\\n## Conclusion\\n\\nPaddlePaddle is an open source Deep Learning platform that originated from indus...\"],[\"--\\ntitle: Image Similarity with Hugging Face Datasets and Transformers\\nthumbnail: \\u002fblog\\u002fassets\\u002fimage...\"],[\"Also, the approach presented in the post can potentially be extended to other modalities as well.\\n\\nT...\"],[\"```\\n\\nIn this case, the checkpoint was obtained by fine-tuning a [Vision Transformer based model](htt...\"],[\"```\\n\\nThis is how a single sample from the training split looks like:\\n\\n\\u003cdiv align=\\\"center\\\"\\u003e\\n    \\u003cimg ...\"],[\"```\\n\\n## The process of finding similar images\\n\\nBelow, you can find a pictorial overview of the proce...\"],[\"```\\n\\nAnd we can map `extract_embeddings()` like so:\\n\\n```py\\ndevice = \\\"cuda\\\" if torch.cuda.is_availabl...\"],[\"```\\n\\nWe'll use [cosine similarity](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fCosine_similarity) to compute the s...\"],[\"```\\n\\n## Perform a query\\n\\nGiven all the utilities, we're equipped to do a similarity search. Let's ha...\"],[\"```\\n\\nSeems like our system got the right set of similar images. When visualized, we'd get:\\n\\n\\u003cdiv ali...\"],[\"```\\n\\nOnce the index is built, `dataset_with_embeddings` can be used to retrieve the nearest examples...\"],[\"```\\n\\nThe method returns scores and corresponding candidate examples. To know more, you can check out...\"],[\"--\\ntitle: \\\"Japanese Stable Diffusion\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f106_japanese_stable_diffusion\\u002fjsd_thu...\"],[\"[rinna Co., Ltd](https:\\u002f\\u002frinna.co.jp\\u002f). has developed a Japanese-specific text-to-image model named ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fma...\"],[\"- Generate Japanese-style images\\n- Understand Japanese words adapted from English\\n- Understand Japan...\"],[\"#### 1st stage: Train a Japanese-specific text encoder \\nIn the 1st stage, the latent diffusion model...\"],[\"```\\n\\nOn the other hand, by using our Japanese tokenizer, the prompt is split into interpretable toke...\"],[\"```\\n\\nThis stage enables the model to understand Japanese prompts but does not still output Japanese-...\"],[\"*\\\"ã‚µãƒ©ãƒªãƒ¼ãƒãƒ³ æ²¹çµµ\\\", which means exactly \\\"salary man, oil painting\\\", from the 2nd-stage Japanese Stable Dif...\"],[\"--\\ntitle: \\\"Fine-Tune Wav2Vec2 for English ASR in Hugging Face with ğŸ¤— Transformers\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"![wav2vec2\\\\_structure](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fw...\"],[\"I highly recommend reading the blog post [Sequence Modeling with CTC\\n(2017)](https:\\u002f\\u002fdistill.pub\\u002f201...\"],[\"```\\n\\nNext we strongly suggest to upload your training checkpoints directly to the [Hugging Face Hub]...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) Timit is u...\"],[\"Let\\\\'s start by creating the tokenizer responsible for decoding the\\nmodel\\\\'s predictions.\\n\\n### Creat...\"],[\"```\\n\\n**Print Output:**\\n```bash\\n    DatasetDict({\\n        train: Dataset({\\n            features: ['fi...\"],[\"```\\n\\nLet\\\\'s write a short function to display some random samples of the\\ndataset and run it a couple...\"],[\"```\\n\\n**Print Output:**\\n\\n| Idx |  Transcription     |\\n|----------|:-------------:|\\n|\\t1  | Who took th...\"],[\"```\\n\\nLet's take a look at the preprocessed transcriptions.\\n\\n```python\\nshow_random_elements(timit[\\\"tr...\"],[\"```\\n\\nNow, we create the union of all distinct letters in the training dataset\\nand test dataset and c...\"],[\"```\\n\\nCool, we see that all letters of the alphabet occur in the dataset\\n(which is not really surpris...\"],[\"```\\n\\nIn a final step, we use the json file to instantiate an object of the\\n`Wav2Vec2CTCTokenizer` cl...\"],[\"```\\n\\nGreat, you can see the just created repository under `https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-username\\u003e\\u002fwa...\"],[\"A Wav2Vec2 feature extractor object requires the following parameters to\\nbe instantiated:\\n\\n-   `feat...\"],[\"```\\n\\nGreat, Wav2Vec2\\\\'s feature extraction pipeline is thereby fully defined!\\n\\nTo make the usage of ...\"],[\"```\\n\\n**Print Output:**\\n```bash\\n{'array': array([-2.1362305e-04,  6.1035156e-05,  3.0517578e-05, ...,...\"],[\"```\\n\\nIt can be heard, that the speakers change along with their speaking rate, accent, etc. Overall,...\"],[\"```\\n\\nGood! Everything looks fine - the data is a 1-dimensional array, the\\nsampling rate always corre...\"],[\"```\\n\\nLet's apply the data preparation function to all examples.\\n\\n```python\\ntimit = timit.map(prepare...\"],[\"```\\n\\n**Note**: Currently `datasets` make use of [`torchaudio`](https:\\u002f\\u002fpytorch.org\\u002faudio\\u002fstable\\u002finde...\"],[\"-   Define the training configuration.\\n\\nAfter having fine-tuned the model, we will correctly evaluat...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass, field\\nfrom typing import Any, Dict, List,...\"],[\"processor: Wav2Vec2Processor\\n    padding: Union[bool, str] = True\\n    max_length: Optional[int] = No...\"],[\"```\\n\\nLet's initialize the data collator.\\n\\n```python\\ndata_collator = DataCollatorCTCWithPadding(proce...\"],[\"```\\n\\nThe model will return a sequence of logit vectors:\\n\\n$$ \\\\mathbf{y}_1, \\\\ldots, \\\\mathbf{y}_m $$, \\n...\"],[\"```\\n\\nNow, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer\\\\'s\\n`pad_token_id` must be ...\"],[\"```\\n\\nIn a final step, we define all parameters related to training. To give\\nmore explanation on some...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) To allow m...\"],[\"In case you want to use this google colab to fine-tune your model, you\\nshould make sure that your tr...\"],[\"```\\n\\n```python\\ntrainer.train()...\"],[\"```\\n\\nDepending on your GPU, it might be possible that you are seeing an `\\\"out-of-memory\\\"` error here...\"],[\"You can now upload the result of the training to the Hub, just execute this instruction:\\n\\n```python\\n...\"],[\"```\\n\\nYou can now share this model with all your friends, family, favorite pets: they can all load it...\"],[\"```\\n\\nNow, we will make use of the `map(...)` function to predict the\\ntranscription of every test sam...\"],[\"```\\n\\n| pred_str |  target_text |\\n|----------|:-------------:|\\n| am to balence your employe you benef...\"],[\"pred_ids = torch.argmax(logits, dim=-1)\\n\\n# convert ids to tokens\\n\\\" \\\".join(processor.tokenizer.conver...\"],[\"```\\n\\n**Print Output:**\\n```bash\\n[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] t t h e e | | b b [PAD] u u n n n...\"],[\"--\\ntitle: \\\"Hugging Face and AWS partner to make AI more accessible\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f131_aws...\"],[\"There have been significant advances in new Transformer and Diffuser machine learning models that pr...\"],[\"â€œGenerative AI has the potential to transform entire industries, but its cost and the required exper...\"],[\"--\\ntitle: \\\"Deploy Livebook notebooks as apps to Hugging Face Spaces\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f120_eli...\"],[\"## Hugging Face and Elixir\\n\\nThe Elixir community leverages the Hugging Face platform and its open so...\"],[\"--\\ntitle: \\\"Getting Started with Sentiment Analysis using Python\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f50_sentimen...\"],[\"- *\\\"dear @verizonsupport your service is straight ğŸ’© in dallas.. been with yâ€™all over a decade and th...\"],[\"```python\\npip install -q transformers\\nfrom transformers import pipeline\\nsentiment_pipeline = pipelin...\"],[\"```\\n\\nThis code snippet uses the [pipeline class](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_class...\"],[\"```\\n\\nYou can test these models with your own data using this [Colab notebook](https:\\u002f\\u002fcolab.research...\"],[\"Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or Germ...\"],[\"Let's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset ...\"],[\"```\\n\\nThen, install the libraries you will be using in this tutorial:\\n\\n```python\\n!pip install dataset...\"],[\"```\\n\\nNext, you will prepare the text inputs for the model for both splits of our dataset (training a...\"],[\"```\\n\\nThen, let's define the metrics you will be using to evaluate how good is your fine-tuned model ...\"],[\"```\\n\\nNow, it's time to fine-tune the model on the sentiment analysis dataset! ğŸ™Œ You just have to cal...\"],[\"```\\n\\nIn the IMDB dataset, `Label 1` means positive and `Label 0` is negative. Quite good! ğŸ”¥\\n\\n\\n### b....\"],[\"Next, let's create a [new project on AutoNLP](https:\\u002f\\u002fui.autonlp.huggingface.co\\u002fnew) to train 5 cand...\"],[\"After a few minutes, AutoNLP has trained all models, showing the performance metrics for all of them...\"],[\"```\\n!pip install -q transformers tweepy wordcloud matplotlib\\n```\\n\\n### 2. Set up Twitter API credenti...\"],[\"```\\n\\n### 3. Search for tweets using Tweepy\\nAt this point, you are ready to start using the Twitter A...\"],[\"```\\n\\n\\n### 4. Run sentiment analysis on the tweets\\nNow you can put our new skills to work and run sen...\"],[\"```\\n\\nOutput:\\n\\n```\\nTweet: @NFTGalIery Warm, exquisite and elegant palette of charming beauty Its pric...\"],[\"```\\n\\nInterestingly, most of the tweets about NFTs are positive (56.1%) and almost none are negative ...\"],[\"Finally, let's see what words stand out for each sentiment by creating a word cloud:\\n\\n```python\\nfrom...\"],[\"```\\n\\nSome of the words associated with positive tweets include Discord, Ethereum, Join, Mars4 and Sh...\"],[\"If you have questions, the Hugging Face community can help answer and\\u002for benefit from, please ask th...\"],[\"a href=\\\"https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fsanchit-gandhi\\u002fnotebooks\\u002fblob\\u002fmain\\u002ffine_tune_whispe...\"],[\"When scaled to 680,000 hours of labelled pre-training data, Whisper models \\ndemonstrate a strong abi...\"],[\"For demonstration purposes, we'll fine-tune the multilingual version of the \\n[`\\\"small\\\"`](https:\\u002f\\u002fhug...\"],[\"```\\n\\nNext, we need to update the Unix package `ffmpeg` to version 4:\\n\\n\\n```python\\n!add-apt-repository...\"],[\"```\\n\\n## Load Dataset\\n\\nUsing ğŸ¤— Datasets, downloading and preparing data is extremely simple. \\nWe can ...\"],[\"```\\n\\n## Prepare Feature Extractor, Tokenizer and Data\\n\\nThe ASR pipeline can be de-composed into thre...\"],[\"We'll load the feature extractor from the pre-trained checkpoint with the default values:\\n\\n\\n```pytho...\"],[\"```\\n\\n### Load WhisperTokenizer\\n\\nThe Whisper model outputs a sequence of _token ids_. The tokenizer m...\"],[\"```\\n\\n### Prepare Data\\n\\nLet's print the first example of the Common Voice dataset to see \\nwhat form t...\"],[\"```\\n\\nRe-loading the first audio sample in the Common Voice dataset will resample \\nit to the desired ...\"],[\"```\\n\\n## Training and Evaluation\\n\\nNow that we've prepared our data, we're ready to dive into the trai...\"],[\"We can leverage the `WhisperProcessor` we defined earlier to perform both the \\nfeature extractor and...\"],[\"```\\n\\nLet's initialise the data collator we've just defined:\\n\\n\\n```python\\ndata_collator = DataCollator...\"],[\"```\\n\\nOverride generation arguments - no tokens are forced as decoder outputs (see [`forced_decoder_i...\"],[\"```\\n\\n**Note**: if one does not want to upload the model checkpoints to the Hub, \\nset `push_to_hub=Fa...\"],[\"```\\n\\nOur best WER is 32.0% - not bad for 8h of training data! We can submit our checkpoint to the [`...\"],[\"```\\n\\nThe training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` comman...\"],[\"--\\ntitle: How to train a Language Model with Megatron-LM\\nthumbnail: \\u002fblog\\u002fassets\\u002f100_megatron_traini...\"],[\"We will try to break down the different steps for training a GPT2 model in this framework, this incl...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f100_megatron_training\\u002fkernel_fusion.png\\\" width=\\\"600\\\" \\u002f\\u003e\\n\\u003c\\u002fp\\u003e...\"],[\"So after having installed Docker, you can run the container with the following command (`xx.xx` deno...\"],[\"```\\n\\nYou also need to add the vocabulary file `vocab.json` and merges table `merges.txt` of your tok...\"],[\"```\\n\\nThe data is then tokenized, shuffled and processed into a binary format for training using the ...\"],[\"```\\nThe `workers` and `chunk_size` options refer to the number of workers used in the preprocessing ...\"],[\"### Training\\nYou can configure the model architecture and training parameters as shown below, or put...\"],[\"```\\nWith this setting, the training takes roughly 12 hours.\\n\\nThis setup uses Data Parallelism, but i...\"],[\"```\\nBe careful, you will need to replace the generated vocabulary file and merges table after the co...\"],[\"```\\nThis will use [accelerate](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002findex) library behind the scen...\"],[\"--\\ntitle: \\\"Accelerating Hugging Face Transformers with AWS Inferentia2\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f140...\"],[\"However, for all their greatness, Transformers can be challenging to deploy in production. On top of...\"],[\"## Introducing AWS Inferentia2\\n\\nAWS Inferentia2 is the next generation to Inferentia1 launched in 20...\"],[\"Speaking of, letâ€™s show you how several Hugging Face models run on Inferentia 2. Benchmarking time!\\n...\"],[\"### Results\\n\\nThe benchmark confirms that the performance improvements claimed by AWS can be reproduc...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"--\\ntitle: \\\"Introducing SafeCoder\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f159_safecoder\\u002fthumbnail.jpg\\nauthors:\\n- us...\"],[\"However, relying on closed-source Code LLMs to create internal code assistants exposes companies to ...\"],[\"Note: While StarCoder is the inspiration and model powering the initial version of SafeCoder, an imp...\"],[\"BigCode expanded upon this work by implementing novel techniques for the code domain and building Th...\"],[\"### Deploying SafeCoder\\n\\nDuring the setup phase, SafeCoder customers and Hugging Face design and pro...\"],[\"â€œOur collaboration with Hugging Face around SafeCoder fully aligns to VMwareâ€™s goal of enabling cust...\"],[\"--\\ntitle: \\\"The Reformer - Pushing the limits of language modeling\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f03_reform...\"],[\"The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a mill...\"],[\"## 1. Reformer Self-Attention Layer\\n\\nReformer uses two kinds of special self-attention layers: *loca...\"],[\"In short, a global self-attention layer projects \\\\\\\\(\\\\mathbf{X}\\\\\\\\) to the query, key and value matric...\"],[\"Important to remember is that for each output vector \\\\\\\\(\\\\mathbf{z}_{i}\\\\\\\\), the whole input sequence ...\"],[\"Assuming \\\\\\\\(l_{c} = 4, n_{c} = 4\\\\\\\\), chunked attention can be illustrated as follows:\\n\\n![alt text](h...\"],[\"$$\\\\mathbf{Z}^{\\\\text{loc}} = \\\\left[\\\\mathbf{Z}_{1:l_{c}}^{\\\\text{loc}}, \\\\ldots, \\\\mathbf{Z}_{(n_{c} - 1)...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"This enhanced local self-attention is better than the vanilla local self-attention architecture but ...\"],[\"Let's explain this in more detail.\\nLet \\\\\\\\(\\\\mathbf{k}_{i} \\\\in \\\\mathbf{K} = \\\\left[\\\\mathbf{k}_1, \\\\ldots...\"],[\"First, the authors of Reformer notice that sharing the query and key projections: \\\\\\\\(\\\\mathbf{Q} = \\\\m...\"],[\"For each set of indices \\\\\\\\(C_{m}\\\\\\\\), the softmax function on the corresponding bucket of query vecto...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"All in all for all chunks \\\\\\\\( k \\\\in \\\\{1, \\\\ldots, n_{c}\\\\} \\\\\\\\), LSH self-attention can be noted down a...\"],[\"One important feature to mention here as well is that the accuracy of LSH self-attention can be impr...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Let's recap quickly what we have gone through above:\\n\\n1. We want to approximate global attention usi...\"],[\"### Benchmark\\n\\nBenchmark tools were recently added to Transformers - see [here](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\n\\n\\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1279.0, style=Progr...\"],[\"For this a `google\\u002freformer-enwik8` model using global attention, a sequence length of over 16K resu...\"],[\"```\\n  config = ReformerConfig.from_pretrained(\\\"google\\u002freformer-enwik8\\\")\\n  benchmark_args = PyTorchBe...\"],[\"```\\n\\n    1 \\u002f 1\\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 ...\"],[\"As expected using local and LSH self-attention is much more memory efficient for longer input sequen...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Let's illustrate the feed forward layers for \\\\\\\\( \\\\mathbf{\\\\overline{z}}_1, \\\\ldots, \\\\mathbf{\\\\overline{...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Assuming \\\\\\\\( c_{f}=1 \\\\\\\\) for our example we can illustrate the incremental computation of the output...\"],[\"\\\\\\\\( {}^3 \\\\\\\\) As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake ...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\n\\n    1 \\u002f 2\\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 ...\"],[\"Interesting, chunked feed forward layers do not seem to help here at all. The reason is that `config...\"],[\"```\\nconfig_no_chunk = ReformerConfig.from_pretrained(\\\"google\\u002freformer-enwik8\\\", chunk_size_feed_forwa...\"],[\"```\\n\\n    1 \\u002f 2\\n    2 \\u002f 2\\n    \\n    ====================      INFERENCE - MEMORY - RESULT       ======...\"],[\"### Reversible Residual Layers in Reformer\\n\\nLet's start by investigating why training a model requir...\"],[\"Using the same notation as before, the input of a transformer layer *i.e.* \\\\\\\\( \\\\mathbf{X} \\\\\\\\) is fir...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Here, reversible residual layers come to our help. The idea is relatively straight-forward. The resi...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"If we assume to know \\\\\\\\( \\\\mathbf{\\\\overline{Y}}^{(1)}, \\\\mathbf{\\\\overline{Y}}^{(2)} \\\\\\\\), it can easily...\"],[\"\\\\\\\\) and \\\\\\\\( F \\\\\\\\) during the backward pass and passing \\\\\\\\( \\\\mathbf{X}^{(1)} \\\\\\\\) and \\\\\\\\( \\\\mathbf{X}^{...\"],[\"**Note**: Since recently, major deep learning frameworks have released code that allows to store onl...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\n\\n\\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=Progre...\"],[\"```\\nconfig_4_layers_reformer = ReformerConfig.from_pretrained(\\\"google\\u002freformer-enwik8\\\", num_hidden_l...\"],[\"```\\n\\n    1 \\u002f 3\\n    2 \\u002f 3\\n    3 \\u002f 3\\n    \\n    ====================        TRAIN - MEMORY - RESULTS    ...\"],[\"**Important:** *Axial Position Encodings were not explained in the official paper, but can be well u...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Such positional encodings would use an unnecessarily large amount of memory both when loading the mo...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"whereas \\\\\\\\( n_\\\\text{max}^1 = 7 \\\\\\\\) and \\\\\\\\( n_\\\\text{max}^2 = 7 \\\\\\\\) in our example.\\nThese new encoding...\"],[\"To demonstrate the drastic reduction in size, \\nlet's assume we would have set `config.axial_pos_shap...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\nconfig_no_pos_axial_embeds = ReformerConfig.from_pretrained(\\\"google\\u002freformer-crime-and-punishmen...\"],[\"```\\n\\n\\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1151.0, style=Progr...\"],[\"```\\n\\n    1 \\u002f 2\\n    2 \\u002f 2\\n    \\n    ====================      INFERENCE - MEMORY - RESULT       ======...\"],[\"--\\ntitle: \\\"Chat Templates: An End to the Silent Performance Killer\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fchat-te...\"],[\"```\\nBy loading the tokenizer and model from the same checkpoint, you ensure that inputs are tokenize...\"],[\"```\\nOr you could add special tokens to indicate the roles:\\n```\\n[USER] Hey there! [\\u002fUSER]\\n[ASST] Nice...\"],[\"```\\nThere are lots of ways to do this, and none of them is obviously the best or correct way to do i...\"],[\"```\\n```jinja\\n{% for message in messages %}\\n    {% if message['role'] == 'user' %}\\n        {{ \\\"[USER]...\"],[\"```\\n\\nIf you're unfamiliar with Jinja, I strongly recommend that you take a moment to look at these t...\"],[\"```\\n\\nThere's also a second reason not to hardcode a standard format, though, beyond the proliferatio...\"],[\"For information about how to set and apply chat templates, please see the [technical documentation](...\"],[\"If you'd like to use a checkpoint for chat but you can't find any documentation on the chat format i...\"],[\"--\\ntitle: \\\"Make your llama generation time fly with AWS Inferentia2\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002finferen...\"],[\"Alternatively, you can use the [Hugging Face Neuron SDK DLC](https:\\u002f\\u002fgithub.com\\u002faws\\u002fdeep-learning-co...\"],[\"```\\n\\u003e\\u003e\\u003e from optimum.neuron import NeuronModelForCausalLM\\n\\n\\u003e\\u003e\\u003e compiler_args = {\\\"num_cores\\\": 24, \\\"au...\"],[\"```\\n\\u003e\\u003e\\u003e from optimum.neuron import NeuronModelForCausalLM\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer...\"],[\"```\\n\\u003e\\u003e\\u003e from optimum.neuron import pipeline\\n\\n\\u003e\\u003e\\u003e p = pipeline('text-generation', 'aws-neuron\\u002fLlama-2...\"],[\"```\\n\\n## Benchmarks\\n\\nBut how much efficient is text-generation on Inferentia2?  Let's figure out!\\n\\nWe...\"],[\"| Model type                 | num cores | batch_size | Hugging Face Hub model                    |\\n...\"],[\"*Note: all models are compiled with a maximum sequence length of 2048.*\\n\\nThe `llama2 7B` \\\"budget\\\" mo...\"],[\"Encoding time is expressed in **seconds**.\\n\\n|   input tokens  |   Llama2 7B-L  |   Llama2 7B-T  |   ...\"],[\"Latency is expressed in **seconds**.\\n\\n|   new tokens  |   Llama2 7B-L  |   Llama2 7B-T  |   Llama2 1...\"],[\"Throughput is expressed in **tokens\\u002fsecond**.\\n\\n|   new tokens  |   Llama2 7B-L  |   Llama2 7B-T  |  ...\"],[\"Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the w...\"],[\"--\\ntitle: \\\"Introduction to 3D Gaussian Splatting\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002fthumbnail...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f124_ml-for-ga...\"],[\"### 3. Training\\n\\nThe training procedure uses Stochastic Gradient Descent, similar to a neural networ...\"],[\"Why has there been so much attention on 3D Gaussian Splatting? The obvious answer is that the result...\"],[\"So far, the original CUDA implementation has not been adapted to production rendering pipelines, lik...\"],[\"--\\ntitle: \\\"Train your ControlNet with diffusers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f136_train-your-controlnet\\u002ft...\"],[\"3. **Training the model**: Once your dataset is ready, it is time to train the model. This is the ea...\"],[\"The `FaceSynthetics` dataset sounded like a great start: it contains ground truth images of faces, a...\"],[\"Now, with the ground truth `image` and the `conditioning_image` on the dataset, we are missing one s...\"],[\"### Our training experience\\nWe trained the model for 3 epochs (this means that the batch of 100K ima...\"],[\"```\\n\\nAnd then run the [train_controlnet.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fexamp...\"],[\"Let's break down some of the settings, and also let's go over some optimisation tips for going as lo...\"],[\"- `num_train_epochs`: Each epoch corresponds to how many times the images in the training set will b...\"],[\"### Fitting on a 16GB VRAM GPU\\n```shell \\npip install bitsandbytes\\n\\n--train_batch_size=1 \\\\\\n--gradient...\"],[\"```\\n\\nThe combination of a batch size of 1 with 4 gradient accumulation steps is equivalent to using ...\"],[\"--\\ntitle: \\\"Spread Your Wings: Falcon 180B is here\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f162_falcon_180b\\u002fthumbnai...\"],[\"In this blog post, we explore what makes Falcon 180B so good by looking at some evaluation results a...\"],[\"â€¼ï¸ Commercial use: \\nFalcon 180b can be commercially used but under very restrictive conditions, excl...\"],[\"![open_llm_leaderboard.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"### Hardware requirements\\n\\nWe ran several tests on the hardware needed to run the model for differen...\"],[\"```\\n\\n### Transformers\\n\\nWith the release of Transformers 4.33, you can use Falcon 180B and leverage a...\"],[\"```\\n\\n#### 8-bit and 4-bit with `bitsandbytes`\\n\\nThe 8-bit and 4-bit quantized versions of Falcon 180B...\"],[\"```\\n\\nAs you can see, interactions from the user and responses by the model are preceded by `User: ` ...\"],[\"## Acknowledgments\\n\\nReleasing such a model with support and evaluations in the ecosystem would not b...\"],[\"--\\ntitle: Swift ğŸ§¨Diffusers - Fast Stable Diffusion for Mac\\nthumbnail: \\u002fblog\\u002fassets\\u002ffast-mac-diffuser...\"],[\"## What exactly is ğŸ§¨Diffusers for Mac anyway?\\n\\nThe Diffusers app ([App Store](https:\\u002f\\u002fapps.apple.com...\"],[\"Why would you want to run a native Mac app then? There are many reasons:\\n- It uses Core ML models, i...\"],[\"Come check out our benchmarks. All the combinations use the CPU in addition to either the GPU or the...\"],[\"We found that the amount of memory does not seem to play a big factor on performance, but the number...\"],[\"## Other Improvements in Version 1.1\\n\\nIn addition to the performance optimization and fixing a few b...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_ml_director_insights\\u002fth...\"],[\"ğŸš€  Letâ€™s meet some top Machine Learning Directors and hear what they have to say about Machine Learn...\"],[\"_Tightened testing:_ In a capital intensive media venture, there is a need to shorten the time betwe...\"],[\"**Background:** Li is an AI\\u002fML veteran with 15+ years of experience leading high-profile Data Scienc...\"],[\"#### **1. How has ML made a positive impact on Pharmaceuticals?**\\nAI\\u002fML applications have exploded i...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f61_ml_director...\"],[\"#### **2. What are the biggest ML challenges within Scientific research?**\\nThere are many challenges...\"],[\"**Background:** Nathan is a passionate machine learning leader with 7 years of experience in researc...\"],[\"#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into Logistics?**\\nI thi...\"],[\"At BEN, Nic innovates intelligent technologies that scale human capabilities to reach people. See hi...\"],[\"#### **1. How has ML made a positive impact on Marketing?**\\nIn so many ways! Itâ€™s completely changin...\"],[\"#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into Marketing?**\\nI don...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f61_ml_director...\"],[\"**E Source:** Provides independent market intelligence, consulting, and predictive data science to u...\"],[\"---\\n\\nğŸ¤—   Thank you for joining us in this first installment of ML Director Insights. Stay tuned for ...\"],[\"--\\ntitle: \\\"My Journey to a serverless transformers pipeline on Google Cloud\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"Below is the [official example](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers#quick-tour) from the Tra...\"],[\"```\\n\\n\\n## Deploy transformers to Google Cloud\\n\\u003e GCP is chosen as it is the cloud environment I am usi...\"],[\"### Step 4 - Test on Cloud Run\\n\\nLastly, I moved to [Cloud Run](https:\\u002f\\u002fcloud.google.com\\u002frun) with a ...\"],[\"```python\\nimport os\\nfrom flask import Flask, jsonify, request\\nfrom transformers import pipeline\\n\\napp...\"],[\"```\\n\\nThen the `DockerFile` which will be used to create a docker image of the service. We specify th...\"],[\"```\\n\\n\\n## Deployment instructions\\n\\nFirst, you will need to meet some requirements such as having a pr...\"],[\"```\\n\\nAfter a few minutes, you will also need to upgrade the memory allocated to your Cloud Run insta...\"],[\"For my micro-service, I am planning to near 1,000 requests per month, optimistically. 500 may more l...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Margaret Mitchell\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f57_meg_mitchell_int...\"],[\"**Meg:** I did heavy statistical work as a postdoc at Johns Hopkins and then went to Microsoft Resea...\"],[\"But the thing is, when youâ€™re learning from images people donâ€™t tend to take photos of terrible thin...\"],[\"Since women are often much more familiar with discrimination women are focusing a lot more on ethics...\"],[\"### Diversity in AI - Isnâ€™t there proof that having a more diverse set of people on an ML project re...\"],[\"Timnitâ€™s paper was called [â€˜Data Sheets for Datasetsâ€™](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1803.09010). So we call...\"],[\"### Decision thresholds & model transparency\\n\\n**Meg:** When Amazon first started putting out facial ...\"],[\"### What are you working on at Hugging Face?\\n\\n- Working on a few different tools designed for engine...\"],[\"### Rapid Fire Questions:\\n\\n### Best piece of advice for someone looking to get into AI?\\n\\n**Meg:** De...\"],[\"### Should people be afraid of AI taking over the world?\\n\\n**Meg:** There are a lot of things to be a...\"],[\"Earlier papers that Iâ€™m interested in are more reflective of what I was doing at that time. Really l...\"],[\"*â€œThe most pressing problem is the diversity and inclusion of whoâ€™s at the table from the start. All...\"],[\"--\\ntitle: \\\"Accelerating PyTorch distributed fine-tuning with Intel technologies\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"Running a text classification job, we will fine-tune a [BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-cased...\"],[\"All three major cloud providers offer virtual machines powered by Intel Ice Lake CPUs:\\n\\n- Amazon Web...\"],[\"When it comes to distributed training, the main performance bottleneck is often networking. Indeed, ...\"],[\"From a networking perspective, we will need the following setup: \\n\\n* Open port 22 for ```ssh``` acce...\"],[\"It looks like a lot, but there's nothing complicated. Here we go!\\n\\n__Installing Intel toolkits__\\n\\nFi...\"],[\"```\\nwget https:\\u002f\\u002fregistrationcenter-download.intel.com\\u002fakdlm\\u002firc_nas\\u002f18236\\u002fl_BaseKit_p_2021.4.0.3422...\"],[\"```\\nsudo yum -y update\\nsudo yum install -y git cmake3 gcc gcc-c++\\n```\\n\\nNext, we clone the oneCCL rep...\"],[\"```\\n\\n\\u003ckbd\\u003e\\n\\u003cimg src=\\\"assets\\u002f36_accelerating_pytorch\\u002f02_single_node.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nThis job takes __7 ...\"],[\"```\\nfor nic in eth0 eib0 hib0 enp94s0f0; do\\n  master_addr=$(ifconfig $nic 2\\u003e\\u002fdev\\u002fnull | grep netmask...\"],[\"```\\n+import torch_ccl\\n+\\n import datasets\\n import numpy as np\\n from datasets import load_dataset, loa...\"],[\"```\\n\\nSetup is now complete. Let's scale our training job to 2 nodes and 4 nodes.\\n\\n### Running a dist...\"],[\"```\\n\\nWithin seconds, a job starts on the first two nodes. The job completes in __4 minutes and 39 se...\"],[\"--\\ntitle: \\\"Introducing new audio and vision documentation in ğŸ¤— Datasets\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f87_...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"Also new in the Quickstart is the `to_tf_dataset` function which takes care of converting a dataset ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg style=\\\"border:none;\\\" alt=\\\"An overview of ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg style=\\\"border:none;\\\" alt=\\\"A table of...\"],[\"dataset = load_dataset(\\\"imagefolder\\\", data_dir=\\\"\\u002fpath\\u002fto\\u002ffolder\\\", split=\\\"train\\\")\\ndataset[0][\\\"objects...\"],[\"```\\n\\nYou can use `ImageFolder` to load an image dataset for nearly any type of image task if you hav...\"],[\"--\\ntitle: \\\"Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fmixt...\"],[\"## Table of Contents\\n\\n- [What is Mixtral 8x7b](#what-is-mixtral-8x7b)\\n  - [About the name](#about-th...\"],[\"For more details on MoEs, see our accompanying blog post: [hf.co\\u002fblog\\u002fmoe](https:\\u002f\\u002fhuggingface.co\\u002fbl...\"],[\"| Model                                                                             | License       ...\"],[\"For instruct and chat models, evaluating on benchmarks like MT-Bench or AlpacaEval is better. Below,...\"],[\"| Model                                                                                             ...\"],[\"| [HuggingFaceH4\\u002fzephyr-7b-beta](https:\\u002f\\u002fhuggingface.co\\u002fHuggingFaceH4\\u002fzephyr-7b-beta)               ...\"],[\"Impressively, Mixtral Instruct outperforms all other open-access models on MT-Bench and is the first...\"],[\"```\\n\\nThis format has to be exactly reproduced for effective use. Weâ€™ll show later how easy it is to ...\"],[\"- training and inference scripts and examples\\n- safe file format (`safetensors`)\\n- integrations with...\"],[\"```\\n\\nIn the following code snippet, we show how to run inference with ğŸ¤— Transformers and 4-bit quant...\"],[\"```\\n\\n\\u003e \\\\\\u003cs\\u003e[INST] Explain what a Mixture of Experts is in less than 100 words. [\\u002fINST] A\\nMixture of ...\"],[\"*Note: You might need to request a quota upgrade via email toÂ **[api-enterprise@huggingface.co](mail...\"],[\"```\\n\\n## Fine-tuning with ğŸ¤—Â TRL\\n\\nTraining LLMs can be technically and computationally challenging. In...\"],[\"```\\n\\nThis takes about 48 hours to train on a single A100, but can be easily parallelised by tweaking...\"],[\"prompt = \\\"[INST] Explain what a Mixture of Experts is in less than 100 words. [\\u002fINST]\\\"\\ninputs = toke...\"],[\"```\\n\\nThis 4-bit quantization technique was introduced in the [QLoRA paper](https:\\u002f\\u002fhuggingface.co\\u002fpa...\"],[\"```\\n\\nYou also need to install transformers from source:\\n\\n```bash\\npip install -U git+https:\\u002f\\u002fgithub.c...\"],[\"```\\n\\nNote that for both QLoRA and GPTQ you need at least 30 GB of GPU VRAM to fit the model. You can...\"],[\"--\\ntitle: Hyperparameter Search with Transformers and Ray Tune\\nthumbnail: \\u002fblog\\u002fassets\\u002f06_ray_tune\\u002fr...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n   \\u003ctd\\u003e\\u003cstrong\\u003eAlgorithm\\u003c\\u002fstrong\\u003e\\n   \\u003c\\u002ftd\\u003e\\n   \\u003ctd\\u003e\\u003cstrong\\u003eBest Val Acc.\\u003c\\u002fstrong\\u003e\\n   \\u003c...\"],[\"![alt_text](\\u002fblog\\u002fassets\\u002f06_ray_tune\\u002fray-hf.jpg \\\"image_tooltip\\\")\\n\\n\\nIn the Transformers 3.1 release, ...\"],[\"def encode(examples):\\n    outputs = tokenizer(\\n        examples['sentence1'], examples['sentence2'],...\"],[\"```\\n\\nBy default, each trial will utilize 1 CPU, and optionally 1 GPU if available.\\nYou can leverage ...\"],[\"```\\n\\n\\nIt also works with [Weights and Biases](https:\\u002f\\u002fwandb.ai\\u002f) out of the box!\\n\\n![alt_text](\\u002fblog\\u002f...\"],[\"--\\ntitle: \\\"Accelerate Large Model Training using DeepSpeed\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f83_accelerate_de...\"],[\"d. **Optimizer Offload**: Offloads the gradients + optimizer states to CPU\\u002fDisk building on top of Z...\"],[\"The code is available here [run_cls_no_trainer.py](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002faccelerate-deepspeed...\"],[\"```\\n\\nNow, run below command for training:\\n```bash\\naccelerate launch run_cls_no_trainer.py \\\\\\n  --mode...\"],[\"```\\n\\nIn our Single-Node Multi-GPU setup, the maximum batch size that DDP supports without OOM error ...\"],[\"# Accelerate ğŸš€:  Leverage a DeepSpeed Config file to tweak more options\\n\\nFirst, We will look at the ...\"],[\"We will leverage the DeepSpeed Zero Stage-2 config [zero2_config_accelerate.json](https:\\u002f\\u002fgithub.com...\"],[\"```\\n\\nTo enable DeepSpeed ZeRO Stage-2 with above config, please run `accelerate config` and provide ...\"],[\"Now, run below command for training:\\n```bash\\naccelerate launch run_seq2seq_no_trainer.py \\\\\\n    --dat...\"],[\"```\\n\\nWhen using DeepSpeed config, if user has specified `optimizer` and `scheduler` in config, the u...\"],[\"```\\n\\n---\\n| Method | Batch Size Max | Eval Size Max | Train time per epoch (seconds) | Eval time  per...\"],[\"![Chatbot](.\\u002fassets\\u002f83_accelerate_deepspeed\\u002fchatbot.png)\\n\\n---\\n## CPU\\u002fDisk Offloading to enable train...\"],[\"We will leverage the DeepSpeed Zero Stage-3 CPU offload config [zero3_offload_config_accelerate.json...\"],[\"\\\"stage3_max_reuse_distance\\\": 1e9,\\n        \\\"stage3_gather_16bit_weights_on_model_save\\\": true\\n    },\\n ...\"],[\"```\\n\\n**ZeRO Stage-3 CPU Offload DeepSpeed Config File Example**\\n```bash\\ncompute_environment: LOCAL_M...\"],[\"```\\n\\n---\\n| Method | Batch Size Max | Train time per epoch (seconds) | Notes |\\n| --- | --- | --- | --...\"],[\"--\\ntitle: \\\"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\\\"\\nthumbnail: \\u002f...\"],[\"The main bottleneck is the latency of predictions which can make large deployments expensive to run ...\"],[\"You can find more information about Hugging Face Infinity at [hf.co\\u002finfinity](https:\\u002f\\u002fhuggingface.co...\"],[\"These two metrics will be used to benchmark Hugging Face Infinity across different setups to underst...\"],[\"In this blog post, we will highlight a few results of the benchmark including the best latency and t...\"],[\"```\\n\\n### Throughput\\n\\nBelow you can find the throughput comparison for running infinity on 2 physical...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co\\u002finfinity-...\"],[\"--\\ntitle: \\\"Introducing âš”ï¸ AI vs. AI âš”ï¸ a deep reinforcement learning multi-agents competition system...\"],[\"## How does AI vs. AI works?\\n\\nAI vs. AI is an open-source tool developed at Hugging Face **to rank t...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e \\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fr...\"],[\"The process generally uses several Hugging Face Datasets to provide data persistence (here, matches ...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e \\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fr...\"],[\"In addition to being a useful tool for hosting multi-agent competitions, we think that this tool can...\"],[\"```\\n@article{cochet-simonini2023,\\n  author = {Cochet, Carl and Simonini, Thomas},\\n  title = {Introdu...\"],[\"--\\ntitle: \\\"SetFit: Efficient Few-Shot Learning Without Prompts\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f103_setfit\\u002fi...\"],[\"\\u003cp\\u003eğŸ \\u003cstrong\\u003eFast to train\\u003c\\u002fstrong\\u003e: SetFit doesn't require large-scale models like T0 or GPT-3 to a...\"],[\"And just by switching out the base Sentence Transformer model to a multilingual one, SetFit can func...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f103_setfit\\u002fthree-tasks.png\\\" width=700\\u003e\\n\\u003c\\u002fp\\u003e\\n\\u003cp align=\\\"center...\"],[\"```\\nNext, we import `SetFitModel` and `SetFitTrainer`, two core classes that streamline the SetFit t...\"],[\"```\\nThe last step is to train and evaluate the model:\\n```python\\n# Train and evaluate!\\ntrainer.train(...\"],[\"--\\ntitle: \\\"Creating a Coding Assistant with StarCoder\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fstarchat_alpha\\u002fthumbn...\"],[\"- How LLMs can be prompted to act like conversational agents.\\n- OpenAIâ€™s [Chat Markup Language](http...\"],[\"To get started, letâ€™s take a look at how language models can be turned into conversational agents wi...\"],[\"```\\nBelow are a series of dialogues between various people and an AI assistant.\\nThe AI tries to be h...\"],[\"```\\n\\nAs we can see, the first part of the prompt â€œBelow are a series...â€ corresponds to the system m...\"],[\"```\\nBelow are a series of dialogues between various people and an AI technical assistant.\\nThe assist...\"],[\"Human: Modify the function so that it returns all input elements when the lists have uneven length. ...\"],[\"```\\n\\nHere we can see how a well crafted prompt can induce coding behaviour similar to that observed ...\"],[\"Letâ€™s start by downloading the processed dataset from the Hub:\\n\\n```python\\nfrom datasets import load_...\"],[\"```\\n\\n```\\nDatasetDict({\\n    train: Dataset({\\n        features: ['messages'],\\n        num_rows: 19034\\n...\"],[\"```\\n{\\n    \\\"messages\\\": [\\n        {\\n            \\\"content\\\": \\\"Is it possible to imagine a society withou...\"],[\"\\\"role\\\": \\\"assistant\\\",\\n        },\\n        {\\n            \\\"content\\\": \\\"Yeah, but laws are complicated. Mo...\"],[\"```\\n\\nOK, this looks like an interesting dialogue about moral philosophy, with each turn involving a ...\"],[\"```\\n\\nAlthough this works fine for training, it isnâ€™t ideal for inference because the model will natu...\"],[\"```\\n\\n```\\n\\u003c|system|\\u003e\\nBelow is a dialogue between a human and AI assistant called StarChat.\\n\\u003c|end|\\u003e\\n\\u003c|...\"],[\"```\\n\\n```\\n{\\\"input_ids\\\": [49153], \\\"attention_mask\\\": [1]}\\n```\\n\\nGreat, it works!\\n\\n### Masking user label...\"],[\"```\\n\\nOK, we can see that all the user input IDs have been masked in the labels as desired. These spe...\"],[\"```\\n\\nWe need to be logged into both Hugging Face. To do so, run:\\n\\n```shell\\nhuggingface-cli login\\n```...\"],[\"```\\n\\nResponse:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nplt...\"],[\"```\\nDraw me a map of the world using geopandas. Make it so that only Germany and Spain are colored r...\"],[\"```\\nThere was a basketball game with the following stats. player, points, rebounds and assists: J. H...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"So what can be done instead of relying on automatic metrics on benchmarks? To date, two main methods...\"],[\"```\\nGenerate a bunch of instructions for coding questions in python (in the format of {\\\"prompt\\\": ins...\"],[\"```\\nWrite a Python function called reverse_string that takes a string as its argument and returns th...\"],[\"```\\n\\nBase-model completion (Assistant 1):\\n\\n```\\n\\\"Sure thing! Let's start by writing out the docstring...\"],[\"```\\n\\nWe can compare this to ChatGPTâ€™s response, which seems to miss the fact that the Assistant 1 do...\"],[\"```\\n\\nThis shows us that while there is extremely valuable signal in AI evaluations, we have a lot to...\"],[\"## Links\\n\\n- Code: [https:\\u002f\\u002fgithub.com\\u002fbigcode-project\\u002fstarcoder\\u002ftree\\u002fmain\\u002fchat](https:\\u002f\\u002fgithub.com\\u002fb...\"],[\"```\\n@article{Tunstall2023starchat-alpha,\\n  author = {Tunstall, Lewis and Lambert, Nathan and Rajani,...\"],[\"--\\ntitle: \\\"AI Policy @ğŸ¤—: Response to the U.S. NTIA's Request for Comment on AI Accountability\\\"\\nthumb...\"],[\"Hugging Faceâ€™s mission is to [â€œdemocratize good machine learningâ€](https:\\u002f\\u002fhuggingface.co\\u002fabout). We...\"],[\"Concretely, we make the following recommendations for accountability mechanisms:\\n\\n* Accountability m...\"],[\"--\\ntitle: \\\"Creating Privacy Preserving AI with Substra\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f139_owkin-substra\\u002ft...\"],[\"As the data never leaves its source, federated learning is naturally a privacy-first approach. Not o...\"],[\"![Substra diagram](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblo...\"],[\"--\\ntitle:  Deploy Embedding Models with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f168...\"],[\"Before we start, let's refresh our knowledge about Inference Endpoints.\\n\\n## 1. What is Hugging Face ...\"],[\"You can get started with Inference Endpoints at:Â https:\\u002f\\u002fui.endpoints.huggingface.co\\u002f\\n\\n## 2. What is...\"],[\"![Performance](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f16...\"],[\"![Select Instance](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblo...\"],[\"```\\n\\n## Conclusion\\n\\nTEI on Hugging Face Inference Endpoints enables blazing fast and ultra cost-effi...\"],[\"--\\ntitle: \\\"What Makes a Dialog Agent Useful?\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fdialog-agents\\u002fthumbnail.png\\na...\"],[\"The following table compares these AI chatbots based on the details of their public access, training...\"],[\"| &nbsp;| LaMDA | BlenderBot 3 |Sparrow | ChatGPT\\u002f InstructGPT | Assistant|\\n| --- | --- | --- | --- ...\"],[\"| **Crowdsourcing platform used for data labeling**| U.S. based vendor | Amazon MTurk | Unknown | Up...\"],[\"We observe that albeit there are many differences in the training data, model, and fine-tuning, ther...\"],[\"![Instruction and instance example](assets\\u002fdialog-agents\\u002fift.png)\\n\\nData for IFT is usually a collect...\"],[\"![IFT spectrum](assets\\u002fdialog-agents\\u002fift-spectrum.png)\\n\\nOn one end is the purely model-generated IFT...\"],[\"### Safely following instructions\\n\\nInstruction fine-tuned LMs, however, may not always generate resp...\"],[\"**Chain-of-thought (CoT)** prompting ([Wei et al., â€˜22](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2201.11903)) is a spec...\"],[\"![Comparing CoT and RLHF](assets\\u002fdialog-agents\\u002frlhf.png)\\n\\n## Takeaways:\\n\\n1. You only need a very tin...\"],[\"```\\n@article{rajani2023ift,\\n  author = {Rajani, Nazneen and Lambert, Nathan and Sanh, Victor and Wol...\"],[\"--\\ntitle: \\\"AI for Game Development: Creating a Farming Game in 5 Days. Part 1\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"#### Locally \\u003ca name=\\\"locally\\\"\\u003e\\u003c\\u002fa\\u003e\\n\\nWe'll be running Stable Diffusion locally using the [Automatic1...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fAUTOMATIC1111\\u002fstable-diffusion-webui.git...\"],[\"```\\n4. Download the [Stable Diffusion 1.5 weights](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-...\"],[\"### Generating Concept Art \\u003ca name=\\\"generating\\\"\\u003e\\u003c\\u002fa\\u003e\\n\\nLet's generate some concept art. The steps are...\"],[\"I settled on the prompt: *isometric render of a farm by a river, simple, solid shapes, james gillear...\"],[\"4. Set up your [Lighting](https:\\u002f\\u002fdocs.unity3d.com\\u002fManual\\u002fLighting.html). I'm using a warm sun (#FFE...\"],[\"That's it! A simple but appealing scene, made in less than a day! Have questions? Want to get more i...\"],[\"--\\ntitle: \\\"From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease\\\"\\nthu...\"],[\"def forward(self, x):\\n        x = self.act(self.conv1(x))\\n        x = self.act(self.conv2(x))\\n      ...\"],[\"```\\n\\nWe define the training device (`cuda`):\\n\\n```python\\ndevice = \\\"cuda\\\"\\n```\\n\\nBuild some PyTorch Data...\"],[\"```\\n\\nTypically from here, one could either throw all of this into a python script or run it on a Jup...\"],[\"```\\n\\nThe last piece of the puzzle is *how do I send my data and model to another GPU?*\\n\\nThis is wher...\"],[\"```\\n\\nThe above will run the training script on two GPUs that live on a single machine and this is th...\"],[\"# Build optimizer\\n    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\\n\\n    # Train for a si...\"],[\"```\\n\\nNext let's talk about how Accelerate can help. There's a few issues with the above code:\\n\\n1. Th...\"],[\"# Send everything through `accelerator.prepare`\\n    train_loader, test_loader, model, optimizer = ac...\"],[\"```\\n\\nWith this your PyTorch training loop is now setup to be ran on any distributed setup thanks to ...\"],[\"```\\n\\nOr:\\n\\n```python\\nnotebook_launcher(train_ddp_accelerate, args=(), num_processes=2)\\n```\\n\\n## Using ...\"],[\"```\\n\\n```python\\ntrainer.train()\\n```\\n\\n```python out\\n    ***** Running training *****\\n      Num example...\"],[\"```\\n\\n## Resources\\n\\nTo learn more about PyTorch Distributed Data Parallelism, check out the documenta...\"],[\"--\\ntitle: \\\"Probabilistic Time Series Forecasting with ğŸ¤— Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f118_ti...\"],[\"Some classical methods are point-valued (meaning, they just output a single value per time step) and...\"],[\"To begin with, the use of an Encoder-Decoder architecture is helpful at inference time where typical...\"],[\"A drawback of the Transformer architecture is the limit to the sizes of the context and prediction w...\"],[\"```\\n\\n## Load Dataset\\n\\nIn this blog post, we'll use the `tourism_monthly` dataset, which is available...\"],[\"```\\n\\nThe `start` simply indicates the start of the time series (as a datetime), and the `target` con...\"],[\"```\\n\\nLet's visualize this:\\n\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\nfigure, axes = plt.subplots(...\"],[\"```\\n\\n## Define the Model\\n\\nNext, let's instantiate a model. The model will be trained from scratch, h...\"],[\"Let's use the default lags provided by GluonTS for the given frequency (\\\"monthly\\\"):\\n\\n\\n```python\\nfrom...\"],[\"```\\n\\n\\nThis means that we'll look back up to 37 months for each time step, as additional features.\\n\\nL...\"],[\"```\\n\\nNote that, similar to other models in the ğŸ¤— Transformers library, [`TimeSeriesTransformerModel`...\"],[\"```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level...\"],[\"# a bit like torchvision.transforms.Compose\\n    return Chain(\\n        # step 1: remove static\\u002fdynami...\"],[\"AddAgeFeature(\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n  ...\"],[\"```\\n\\nWe have a test dataloader helper for completion, even though we will not use it here. This is u...\"],[\"```\\n\\nLet's check the first batch:\\n\\n\\n```python\\nbatch = next(iter(train_dataloader))\\nfor k, v in batch...\"],[\"```\\n\\n\\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the...\"],[\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts t...\"],[\"model, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_datalo...\"],[\"```\\n\\n\\n## Inference\\n\\nAt inference time, it's recommended to use the `generate()` method for autoregre...\"],[\"```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values pr...\"],[\"```\\n\\nWe can also plot the individual metrics of each time series in the dataset and observe that a h...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002ftime-...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002ftime-...\"],[\"## Next Steps\\n\\nWe would encourage the readers to try out the [notebook](https:\\u002f\\u002fcolab.research.googl...\"],[\"Finally, the NLP\\u002fVision domain has benefitted tremendously from [large pre-trained models](https:\\u002f\\u002fa...\"],[\"--\\ntitle: Image Classification with AutoTrain \\nthumbnail: \\u002fblog\\u002fassets\\u002f105_autotrain-image-classific...\"],[\"## How can you train your own image classifier?\\n\\nIf you havenâ€™t [created a Hugging Face account](htt...\"],[\"\\u003cdiv class=\\\"grid grid-cols-2 gap-4\\\"\\u003e\\n  \\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003c\\u002ffigur...\"],[\"Once the data has been added, simply choose the number of model candidates that youâ€™d like AutoModel...\"],[\"In the screenshots above you can see that my project started 5 different models, which each reached ...\"],[\"--\\ntitle: \\\"Fine-Tune XLSR-Wav2Vec2 for low-resource ASR with ğŸ¤— Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"XLSR\\\\'s successor, simply called **XLS-R** (refering to the\\n[*\\\\'\\\\'XLM-R*](https:\\u002f\\u002fai.facebook.com\\u002fbl...\"],[\"![wav2vec2\\\\_structure](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fx...\"],[\"```python\\n!pip install datasets==1.18.3\\n!pip install transformers==4.11.3\\n!pip install huggingface_h...\"],[\"```\\n\\nWe strongly suggest to upload your training checkpoints directly to the\\n[Hugging Face Hub](http...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\( {}^1 \\\\\\\\) In the [...\"],[\"Let\\\\'s start by creating the tokenizer to decode the predicted output\\nclasses to the output transcri...\"],[\"Great, now we can use ğŸ¤— Datasets\\\\' simple API to download the data. The\\ndataset name is `\\\"common_voi...\"],[\"```\\n\\nMany ASR datasets only provide the target text, `'sentence'` for each\\naudio array `'audio'` and...\"],[\"```\\n\\n**Print Output:**\\n\\n| Idx |  Sentence |\\n|----------|:-------------:|\\n|\\t1  | Jonuz, kÄ±sa sÃ¼reli g...\"],[\"```python\\nimport re\\nchars_to_remove_regex = '[\\\\,\\\\?\\\\.\\\\!\\\\-\\\\;\\\\:\\\\\\\"\\\\â€œ\\\\%\\\\â€˜\\\\â€\\\\ï¿½\\\\']'\\n\\ndef remove_special_cha...\"],[\"```\\n\\n```python\\ncommon_voice_train = common_voice_train.map(remove_special_characters)\\ncommon_voice_t...\"],[\"```\\n\\n**Print Output:**\\n\\n| Idx |  Transcription     |\\n|----------|:-------------:|\\n| 1   | birisi bey...\"],[\"Let\\\\'s write another short mapping function to further simplify the text\\nlabels. Remember, the simpl...\"],[\"```\\n\\n```python\\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\\ncommon_voice_t...\"],[\"```\\n\\n```python\\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\\nvocab_dict\\n```\\n\\n**Print...\"],[\"```\\n\\nCool, we see that all letters of the alphabet occur in the dataset\\n(which is not really surpris...\"],[\"```\\n\\nCool, now our vocabulary is complete and consists of 39 tokens, which\\nmeans that the linear lay...\"],[\"```\\n\\nGreat, you can see the just created repository under\\n`https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-username\\u003e\\u002fwa...\"],[\"A `Wav2Vec2FeatureExtractor` object requires the following parameters to\\nbe instantiated:\\n\\n-   `feat...\"],[\"```\\n\\nGreat, XLS-R\\\\'s feature extraction pipeline is thereby fully defined!\\n\\nFor improved user-friend...\"],[\"```\\n\\nGreat, we can see that the audio file has automatically been loaded.\\nThis is thanks to the new ...\"],[\"```\\n\\nThis seemed to have worked! Let\\\\'s listen to a couple of audio files to\\nbetter understand the d...\"],[\"```\\n\\nGood! Everything looks fine - the data is a 1-dimensional array, the\\nsampling rate always corre...\"],[\"```python\\ndef prepare_dataset(batch):\\n    audio = batch[\\\"audio\\\"]\\n\\n    # batched output is \\\"un-batche...\"],[\"```\\n\\nLet\\\\'s apply the data preparation function to all examples.\\n\\n```python\\ncommon_voice_train = com...\"],[\"```\\n\\nAwesome, now we are ready to start training!\\n\\nTraining\\n--------\\n\\nThe data is processed so that ...\"],[\"Without going into too many details, in contrast to the common data\\ncollators, this data collator tr...\"],[\"processor: Wav2Vec2Processor\\n    padding: Union[bool, str] = True\\n\\n    def __call__(self, features: ...\"],[\"```\\n\\n```python\\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\\n```\\n\\nNe...\"],[\"```\\n\\nThe model will return a sequence of logit vectors:\\n\\\\\\\\( \\\\mathbf{y}_1, \\\\ldots, \\\\mathbf{y}_m \\\\\\\\) w...\"],[\"```\\n\\nNow, we can load the pretrained checkpoint of\\n[Wav2Vec2-XLS-R-300M](https:\\u002f\\u002fhuggingface.co\\u002fface...\"],[\"```\\n\\nThe first component of XLS-R consists of a stack of CNN layers that are\\nused to extract acousti...\"],[\"```\\n\\nIn a final step, we define all parameters related to training. To give\\nmore explanation on some...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\( {}^1 \\\\\\\\) To allow...\"],[\"```\\n\\n**Print Output:**\\n\\n| Training Loss | Epoch | Step | Validation Loss | Wer    |\\n|:-------------:...\"],[\"```\\n\\nFor more examples of how XLS-R can be fine-tuned, please take a look at the official \\n[ğŸ¤— Transf...\"],[\"```\\n\\n**Print Output:**\\n\\n| pred_str |  target_text |\\n|----------|:-------------:|\\n| hatta kÃ¼Ã§Ã¼k ÅŸeyle...\"],[\"--\\ntitle: \\\"Advantage Actor Critic (A2C)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f89_deep_rl_a2c\\u002fthumbnail.gif\\nauthor...\"],[\"We saw that Reinforce worked well. However, because we use Monte-Carlo sampling to estimate return (...\"],[\"Sounds exciting? Let's get started!\\n  \\n- [The Problem of Variance in Reinforce](https:\\u002f\\u002fhuggingface....\"],[\"This return \\\\\\\\(R(\\\\tau)\\\\\\\\) is calculated using a *Monte-Carlo sampling*. Indeed, we collect a traject...\"],[\"However, increasing the batch size significantly **reduces sample efficiency**. So we need to find a...\"],[\"On the other hand, your friend (Critic) will also update their way to provide feedback so it can be ...\"],[\"Let's see the training process to understand how Actor and Critic are optimized:\\n- At each timestep,...\"],[\"The idea is that the Advantage function calculates **how better taking that action at a state is com...\"],[\"The leaderboard to compare your results with your classmates ğŸ† ğŸ‘‰Â **[https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fch...\"],[\"--\\ntitle: \\\"Faster Training and Inference: Habana GaudiÂ®2 vs Nvidia A100 80GB\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"One of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Inte...\"],[\"8. You can copy the SSH command to access your Gaudi2 instance remotely!\\n\\n\\u003e If you terminate the ins...\"],[\"Since Gaudi2 has roughly 3 times more memory per device compared to first-gen Gaudi, it is possible ...\"],[\"The following table displays the throughputs we got for first-gen Gaudi, Gaudi2 and Nvidia A100 80GB...\"],[\"[This script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-habana\\u002ftree\\u002fmain\\u002fexamples\\u002fstable-diffusion) was...\"],[\"### Fine-tuning T5-3B\\n\\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. F...\"],[\"\\u003c\\u002fcenter\\u003e\\n\\n*BS* is the batch size per device. Gaudi2 and A100 runs were performed in fp32 with gradi...\"],[\"---\\n\\nThanks for reading! If you have any questions, feel free to contact me, either through [Github]...\"],[\"--\\ntitle: \\\"Parameter-Efficient Fine-Tuning using ğŸ¤— PEFT\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f130_peft\\u002fthumbnail....\"],[\"It also helps in portability wherein users can tune models using PEFT methods to get tiny checkpoint...\"],[\"## Use Cases\\n\\nWe explore many interesting use cases [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft#use-c...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"```\\n\\n2. Creating config corresponding to the PEFT method\\n```py\\npeft_config = LoraConfig(\\n    task_ty...\"],[\"```\\n\\nThis will only save the incremental PEFT weights that were trained. For example, you can find t...\"],[\"```\\n\\n## Next steps\\nWe've released PEFT as an efficient way of tuning large LLMs on downstream tasks ...\"],[\"--\\ntitle: \\\"MTEB: Massive Text Embedding Benchmark\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f110_mteb\\u002fthumbnail.png\\na...\"],[\"## Why Text Embeddings?\\n\\nText Embeddings are vector representations of text that encode semantic inf...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f110_mteb\\u002fmteb_diagram_white_background.png\\\" alt=\\\"MTEB Taxono...\"],[\"**ğŸ’ª Maximum performance** Multi-billion parameter models like [ST5-XXL](https:\\u002f\\u002fhuggingface.co\\u002fsente...\"],[\"```\\n\\nNext, benchmark a model on a dataset, for example [komninos word embeddings](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\nNow add the metadata to the top of a `README.md` of any model on the Hub, like this [SGPT-5.8B-...\"],[\"--\\ntitle: Deploying ğŸ¤— ViT on Vertex AI\\nthumbnail: \\u002fblog\\u002fassets\\u002f97_vertex_ai\\u002fimage1.png\\nauthors:\\n- us...\"],[\"- Authentication\\n\\n- Autoscaling based on traffic\\n\\n- Model versioning\\n\\n- Traffic splitting between di...\"],[\"```bash\\nThe given SavedModel SignatureDef contains the following input(s):\\n  inputs['string_input'] ...\"],[\"```\\n\\nThe model will accept [\\u003cu\\u003ebase64 encoded\\u003c\\u002fu\\u003e](https:\\u002f\\u002fwww.base64encode.org\\u002f) strings of images,...\"],[\"- Version of a model\\n\\n- Specification of VM in terms of CPU, memory, and accelerators\\n\\n- Min\\u002fMax num...\"],[\"```\\n\\nLetâ€™s unpack the code piece by piece:\\n\\n- `GCS_BUCKET` denotes the path of your GCS bucket where...\"],[\"```\\n\\nHere youâ€™re using an `endpoint_service_client` which is an\\n[`EndpointServiceClient`](https:\\u002f\\u002fcl...\"],[\"```\\n\\nHere, youâ€™re chaining together the model you uploaded to the Vertex AI\\nModel Registry and the E...\"],[\"```\\n\\nNotice how youâ€™re defining the traffic split for the model. If you had\\nmultiple versions of the...\"],[\"```\\nfrom google.protobuf import json_format\\nfrom google.protobuf.struct_pb2 import Value\\n\\ndef predic...\"],[\"```\\n\\nNote, however, this is not the only way to obtain predictions using a\\nVertex AI Endpoint. If yo...\"],[\"![](.\\u002fassets\\u002f97_vertex_ai\\u002fimage5.png)\\n\\nAmong all the different statistics shown in the table, `Avera...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n\\n| **Machine Type**            | **Hourly Pricing (USD)** |\\n|:-----------------...\"],[\"# Conclusion\\n\\nIn this post, you learned how to deploy a Vision Transformer model with\\nthe Vertex AI ...\"],[\"--\\ntitle: 'Few-shot learning in practice: GPT-Neo and the ğŸ¤— Accelerated Inference API'\\n# thumbnail: ...\"],[\"Few-Shot NLP examples consist of three main components: \\n\\n- **Task Description**: A short descriptio...\"],[\"Let's now take a look at how at how GPT-Neo and the ğŸ¤— Accelerated Inference API can be used to gener...\"],[\"```python\\nimport json\\nimport requests\\n\\nAPI_TOKEN = \\\"\\\"\\n\\ndef query(payload='',parameters=None,options=...\"],[\"```\\n\\n---\\n## Practical Insights\\n\\nHere are some practical insights, which help you get started using `...\"],[\"\\u003e ###  \\n\\u003e Tweet: \\\"I'm a disabled happy person\\\"  \\n\\u003e Sentiment: Negative  \\n\\nWhat could go wrong? Imagi...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 3: Finance Edition]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"### [Ioannis Bakagiannis](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fbakagiannisioannis\\u002f\\u002f) - Director of Machine Le...\"],[\"#### **2. What are the biggest ML challenges within finance?**\\nI canâ€™t speak for companies but estab...\"],[\"#### **4. What excites you most about the future of ML?**\\nIt is difficult not to get excited with ev...\"],[\"#### **1. How has ML made a positive impact on finance?**\\nMachine learning (ML) has made a significa...\"],[\"3. Legacy infrastructure and databases - Many financial institutions still carry legacy infrastructu...\"],[\"#### **4. What excites you most about the future of ML?**\\nI am really blown away by how modern ML mo...\"],[\"**U.S. Bank:** The largest regional bank in the United States, U.S. Bank blends its relationship tea...\"],[\"#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into financial applicat...\"],[\"--\\ntitle: \\\"Instruction-tuning Stable Diffusion with InstructPix2Pix\\\" \\nthumbnail: assets\\u002finstruction_...\"],[\"Our code, pre-trained models, and datasets can be found [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002finstru...\"],[\"With this approach, one can create exemplars covering many different tasks, which makes instruction-...\"],[\"| ![cartoonization_results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"**(1)** training methodology of  InstructPix2Pix and\\n**(2)** the flexibility of creating instruction...\"],[\"Our final dataset for cartoonization can be found [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002finstruction...\"],[\"Datasets mentioned above typically come as input-output pairs, so we do not have to worry about the ...\"],[\"- [Cartoonization](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002finstruction-tuning-sd\\u002fruns\\u002fwszjpb1b) ([hyperparameters...\"],[\"### Cartoonization results\\n\\nFor testing the [instruction-tuned cartoonization model](https:\\u002f\\u002fhugging...\"],[\"Our model, however, [fails to produce](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002finstruction-tuning-sd\\u002fruns\\u002fg6cvggw...\"],[\"However, for low-light image enhancement, it leaves a lot to be desired: \\n\\n| ![image_enhancement_res...\"],[\"\\u003cgradio-app theme_mode=\\\"light\\\" src=\\\"https:\\u002f\\u002finstruction-tuning-sd-instruction-tuned-sd.hf.space\\\"\\u003e\\u003c\\u002fg...\"],[\"## Open questions\\n\\nWe acknowledge that our experiments are preliminary. We did not go deep into abla...\"],[\"## Conclusion\\n\\nIn this post, we presented our exploration of â€œinstruction-tuningâ€ of Stable Diffusio...\"],[\"## Citation\\n\\nTo cite this work, please use the following citation:\\n\\n```bibtex\\n@article{\\n  Paul2023in...\"],[\"--\\ntitle: \\\"Deploying the AI Comic Factory using the Inference API\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f165_ai_co...\"],[\"## Duplicating the Space\\n\\nTo duplicate the AI Comic Factory, go to the Space and [click on \\\"Duplicat...\"],[\"You can find more information about alternative engines and vendors in the project's [README](https:...\"],[\"--\\ntitle: What's new in Diffusers? ğŸ¨\\nthumbnail: \\u002fblog\\u002fassets\\u002f102_diffusers_2nd_month\\u002finpainting.png\\n...\"],[\"## Image to Image pipeline\\n\\nOne of the most requested features was to have image to image generation...\"],[\"```\\n\\nDon't have time for code? No worries, we also created a [Space demo](https:\\u002f\\u002fhuggingface.co\\u002fspa...\"],[\"## Experimental inpainting pipeline\\n\\nInpainting allows to provide an image, then select an area in t...\"],[\"```\\n\\nPlease note this is experimental, so there is room for improvement.\\n\\n## Optimizations for small...\"],[\"```\\n\\n## Experimental ONNX exporter and pipeline\\n\\nThe new experimental pipeline allows users to run S...\"],[\"```\\n\\n## New docs\\n\\nAll of the previous features are very cool. As maintainers of open-source librarie...\"],[\"```\\n\\n\\n### Diffusers Interpret\\n\\n[Diffusers interpret](https:\\u002f\\u002fgithub.com\\u002fJoaoLages\\u002fdiffusers-interpre...\"],[\"```\\n\\n### Japanese Stable Diffusion\\n\\nThe name says it all! The goal of JSD was to train a model that ...\"],[\"## Thanks for reading!\\n\\nI hope you enjoy reading this! Remember to give a Star in our [GitHub Reposi...\"],[\"--\\ntitle: \\\"Announcing the Hugging Face Fellowship Program\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f62_fellowship\\u002ffel...\"],[\"- **MarÃ­a Grandury** - Created the [largest Spanish-speaking NLP community](https:\\u002f\\u002fsomosnlp.org\\u002f) a...\"],[\"- **Christopher Akiki** - Contributed to sprints, workshops, [Big Science](https:\\u002f\\u002ft.co\\u002foIRne5fZYb),...\"],[\"Additionally, there are strategic areas where Hugging Face is looking for open-source contributions....\"],[\"* **Where and how can I contribute?**\\n  \\nIt depends on your interests. Here are some ideas of areas ...\"],[\"* **Will I receive benefits during the Fellowship?**\\n  \\nYes, the benefits will depend on the particu...\"],[\"--\\ntitle: \\\"How ğŸ¤— Accelerate runs very large models thanks to PyTorch\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f104_ac...\"],[\"```\\n\\nWe'll explain what each of those arguments do in a moment, but first just consider the traditio...\"],[\"For instance, the following code will crash on Colab:\\n\\n```python\\nimport torch\\n\\nlarge_tensor = torch....\"],[\"```\\n\\nas this large tensor requires `4 * 10**10` bytes (the default precision is FP32, so each elemen...\"],[\"```\\n\\nThis works on any model, but you get back a shell you can't use directly: some operations are i...\"],[\"```\\n\\nThis will return a dictionary mapping modules or weights to a device. On a machine with one Tit...\"],[\"```\\n\\nAccelerate evaluated that the embeddings and the decoder up until the 9th block could all fit o...\"],[\"```\\n\\nNow, each layer is always on the same device.\\n\\nIn Transformers, when using `device_map` in the ...\"],[\"```\\n\\nIn this precision, we can fit the model up to layer 21 on the GPU:\\n\\n```python out\\n\\n\\n{'model.dec...\"],[\"```\\n\\nThis works pretty well for models with less than 1 billion parameters, but for larger models, t...\"],[\"To load such a sharded checkpoint into a model, we just need to loop over the various shards. Accele...\"],[\"```\\n\\nIf the device map computed automatically requires some weights to be offloaded on disk because ...\"],[\"```\\n\\nThis will fit in Colab, but will be so close to using all the RAM available that it will go out...\"],[\"```\\n\\n## Running a model split on several devices\\n\\nOne last part we haven't touched is how Accelerate...\"],[\"To learn more about Accelerate big model inference, see the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"--\\ntitle: 'The Partnership: Amazon SageMaker and Hugging Face'\\nthumbnail: \\u002fblog\\u002fassets\\u002f17_the_partne...\"],[\"---\\n\\n## **Features & Benefits ğŸ”¥**\\n\\n## One Command is All you Need\\n\\nWith the new Hugging Face Deep Le...\"],[\"---\\n\\n## **Resources, Documentation & Samples ğŸ“„**\\n\\nBelow you can find all the important resources to ...\"],[\"## Documentation\\n\\n- [Hugging Face documentation for Amazon SageMaker](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fsa...\"],[\"- [all Notebooks](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002ftree\\u002fmaster\\u002fsagemaker)\\n- [Getting Started...\"],[\"- [Image Classification with Vision Transformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmaste...\"],[\"---\\n\\n## **Getting started: End-to-End Text Classification ğŸ§­**\\n\\nIn this getting started guide, we wil...\"],[\"```\\n\\nTo run training on SageMaker we need to create a sagemaker Session and provide an IAM role with...\"],[\"```\\n\\n## Create the training script `train.py`\\n\\nIn a SageMaker `TrainingJob` we are executing a pytho...\"],[\"# Data, model, and output directories\\n    parser.add_argument(\\\"--output-data-dir\\\", type=str, default...\"],[\"# download model from model hub\\n    model = AutoModelForSequenceClassification.from_pretrained(args....\"],[\"```\\n\\n## Preprocess our data and upload it to s3\\n\\nWe use the `datasets` library to download and prepr...\"],[\"# set format for pytorch\\ntrain_dataset = train_dataset.rename_column(\\\"label\\\", \\\"labels\\\")\\ntrain_datase...\"],[\"```\\n\\n## Create a HuggingFace Estimator and train our model\\n\\nIn order to create a SageMaker `Training...\"],[\"```\\n\\nTo start our training we call the .fit() method and pass our S3 uri as input.\\n\\n```python\\n# star...\"],[\"```\\n\\nThe \\\"Getting started: End-to-End Text Classification ğŸ§­\\\" example can be used for distributed tra...\"],[\"distribution={\\n    \\\"smdistributed\\\": {\\\"modelparallel\\\": smp_options},\\n    \\\"mpi\\\": mpi_options\\n}\\n\\n # cre...\"],[\"```\\n\\n## Spot instances\\n\\nWith the creation of HuggingFace Framework extension for the SageMaker Pytho...\"],[\"huggingface_estimator = HuggingFace(\\n        entry_point='train.py',\\n        source_dir='.\\u002fscripts',...\"],[\"```\\n\\n## Git Repositories\\n\\nWhen you create an `HuggingFace` Estimator, you can specify a [training sc...\"],[\"```\\n\\n## SageMaker Metrics\\n\\n[SageMaker Metrics](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemaker\\u002flatest\\u002fdg\\u002ftrain...\"],[\"```\\n\\n---\\n\\n## **FAQ ğŸ¯**\\n\\nYou can find the complete [Frequently Asked Questions](https:\\u002f\\u002fhuggingface.c...\"],[\"A: The DLCs are fully tested, maintained, optimized deep learning environments that require no insta...\"],[\"_Q: How is my data and code secured by Amazon SageMaker?_\\n\\nA: Amazon SageMaker provides numerous sec...\"],[\"A: No - the Hugging Face DLCs are open source and licensed under Apache 2.0.\\n\\n_Q: How can I run infe...\"],[\"_Q: I use Hugging Face with Azure Machine Learning or Google Cloud Platform, what does this partners...\"],[\"--\\ntitle: \\\"Introducing Optimum: The Optimization Toolkit for Transformers at Scale\\\"\\nauthors:\\n- user:...\"],[\"### ğŸ­ Optimum puts Transformers to work\\n\\nTo get optimal performance training and serving models, the...\"],[\"1. The model needs to be edited: some ops need to be replaced by their quantized counterparts, new o...\"],[\"### ğŸ’¡ How Intel is solving quantization and more with Neural Compressor\\n\\nIntelÂ® [Neural Compressor](...\"],[\"### ğŸŒŸ A journey of collaboration: join us, follow our progress\\n\\nEvery journey starts with a first st...\"]],\"hovertemplate\":\"source=blog\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"blog, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"blog, circle\",\"showlegend\":true,\"x\":[-7.2242274,-4.6144056,-5.331593,-5.8337584,-6.067604,-5.6872907,3.5186985,3.2185066,-4.119815,-4.3950176,3.561498,3.8486247,3.5294104,-7.543781,3.1593475,3.085836,3.2687025,-5.601083,-5.8132825,6.1916637,10.1518755,10.063928,10.015364,9.642871,-5.4620442,-5.5834293,-5.2963886,-5.5162177,-5.1748085,-5.3462915,-4.7313128,-8.956962,-9.266585,-9.335829,-9.514492,-4.89505,-8.145563,8.268367,8.259842,-9.444156,-9.656838,-9.643818,-9.196277,-9.231799,-9.152713,-9.028235,-8.997072,-8.91908,-2.0402853,-1.0328575,-1.0384128,-9.719098,0.17126653,0.2997838,0.047196887,-0.24859025,-0.15163238,0.13141787,0.05674522,0.19134727,-0.1149362,-0.0589893,-0.024947286,0.03412304,0.0725176,-0.16907355,-0.0022098145,-0.0407554,0.016125979,-0.40765688,-1.9480448,-1.5467113,-1.0478139,10.881339,-0.0073651676,0.07474501,0.6081936,0.178664,0.42080724,-9.619601,-5.7219234,3.655343,3.5032861,3.6314921,3.6392522,3.5082734,3.336043,3.635259,3.5939164,-4.8289585,-4.825512,-4.317642,-7.242167,-3.812444,-7.778826,-4.3168015,-4.4670167,-4.0271273,-1.0695499,8.171579,8.223037,-4.353418,-3.941473,-3.898015,7.714553,7.740474,-3.5460932,7.6378193,-3.880792,7.679559,7.5197806,-3.91085,-8.398871,-3.829315,-3.6673753,-8.485242,-4.3399186,-4.9449053,-6.5256805,-2.5390952,-5.7321377,-4.5149317,-4.5963264,-4.2651486,-4.417872,-4.890808,-8.007138,-7.659727,-7.5949883,-5.6459794,-7.8718953,-8.331699,1.9152527,-8.03047,-8.286286,-8.375238,-8.313394,-8.064558,-8.336338,7.6573067,-7.94483,-7.6162753,3.2761877,3.2943177,1.5618745,3.03177,2.1360834,1.925544,1.5549886,0.8981316,-1.7960448,-2.1926916,-6.07312,-6.1625423,-5.7659383,-4.966686,-3.533446,-6.00275,-0.9949321,-5.84887,-2.3108077,-3.907078,-5.53448,-0.59731627,-3.9823277,-5.383855,-5.586967,-6.363327,-4.836255,3.5871124,3.669766,3.7483187,3.9150996,-3.658276,9.0622425,3.713729,4.047858,8.916102,7.9689517,-4.026684,-3.4592113,-4.274499,-4.4445553,-6.2394853,-5.722491,2.3086498,3.016884,-5.568821,3.2154036,-5.5428786,-4.5917616,2.314307,1.3203628,-2.415247,-2.4401083,-0.04936735,-2.5610044,-2.5958698,0.31916943,0.77470773,-1.1021184,8.437613,8.422706,8.0621805,-0.15734582,0.68892616,-0.3571751,-0.009925589,-1.1775514,0.5522081,1.2435721,-5.2615857,4.4124846,-9.32494,-9.3763075,-9.618012,-9.639916,-9.630633,-9.604358,-9.524671,-9.671207,-9.356362,-9.456175,6.1782236,-9.597953,-8.157733,-9.468727,7.3913183,7.6425824,7.5459576,-9.6124,8.979552,-6.6604843,-5.7316976,-4.8138685,4.148145,-7.8967085,-7.3136916,-7.517464,-5.3701324,-5.7470174,-2.292481,-4.213553,8.129596,-4.2005377,-3.8181567,-2.7161326,-3.470089,7.950437,-4.139127,-3.926371,7.959139,-3.5643723,7.87582,-3.8626103,-2.7185864,8.001908,-3.5394645,-3.6803625,-3.3491173,7.8472023,-4.8490734,-4.665585,-5.124939,-4.8623466,-3.7821927,0.37881184,-3.9060311,-4.174803,-4.994668,-4.5958114,-3.9345493,-4.179964,-2.9966328,-3.7589188,-3.822508,-2.6875374,-3.7487178,-3.8131492,-3.9139295,-3.615723,-3.7307494,-1.2736965,-3.9655168,8.619063,8.994638,3.5656247,3.7065387,3.8898647,4.129507,9.979117,9.736606,-4.171143,9.500725,9.123178,9.714807,-6.977115,-5.463334,-5.6400437,-6.208288,-6.035664,-5.9515386,7.532369,12.736509,12.848155,-6.4797006,-5.3290753,-2.9384785,-1.3637167,-3.4532144,-3.9954479,-3.6383054,-2.1010473,-2.3905268,-3.500278,-2.6153886,-2.3356895,-2.805902,-2.698232,-5.603434,-4.281611,-8.026486,-7.5881057,-7.654895,-7.4638762,-7.806845,-6.3051443,-6.075695,-1.9666378,-2.1279151,-2.2829375,-5.7982793,-6.7960205,-0.09069155,-0.84747136,-0.67564327,-0.8718018,-0.26933363,-0.028634975,1.222431,-6.204362,-6.172719,-6.2613654,-6.323393,-6.1197124,-5.6613474,-5.697304,7.7127557,-6.005397,-6.1638618,7.688894,7.708099,-6.328113,-6.1538043,-5.8518085,-6.1281805,-5.891582,-5.769796,4.3412833,5.545847,-7.9157944,-7.4378533,-4.6503553,-3.5889285,-4.7586994,-3.750294,-2.822469,-2.329815,-2.399054,-1.9261211,-0.4263438,-3.4516866,-9.864662,-9.872257,-9.874784,-9.898017,-9.908014,-9.865623,-9.8808775,-9.928574,-9.910726,-9.593021,-9.824689,-9.939171,-9.721755,-9.621688,-9.7792015,7.688419,-9.212655,-9.840077,-1.4333307,-1.4520446,-10.210333,-9.998798,-9.803855,-9.739127,-9.43647,-8.075775,-7.0363445,-5.957128,-2.2042987,-5.87079,0.7270037,-6.556628,-6.3299313,1.7898867,0.42132336,-1.9012134,-1.9078583,-1.4073796,-2.1927085,-2.807509,-1.7719578,-1.507719,-1.5987518,0.7749562,-1.4092242,-0.83200645,-1.5528455,-2.3571439,-0.6433428,-5.6084723,-1.4702015,4.3846564,4.609304,-3.982491,-3.9995973,2.4412262,3.4193087,1.0110947,3.2230105,-1.3829253,-1.7091541,-1.2298558,-4.123166,3.6768038,-4.711056,-4.898867,-4.5080214,-4.4875607,-4.650638,-4.2969236,2.9856565,-4.548911,-1.9378912,-1.6601281,-4.407853,-4.573566,0.14714761,-0.9718184,0.20242412,-0.7488334,0.5727478,0.5522336,-0.26305357,2.613179,-6.950956,-5.835409,-2.307052,-2.949944,0.62763166,-0.9267853,-5.332872,-7.7203784,-7.553487,4.7209496,9.543235,9.92852,9.407951,9.401269,9.980759,-8.004777,-7.7975235,3.6668859,-5.0196776,3.6860619,-8.989492,-7.4823384,-4.6531825,-4.061991,-4.144806,7.585469,-2.3567975,-4.5056243,3.6232197,4.1126785,4.0509977,1.9259603,9.277118,3.8961906,3.9091585,-6.5268674,6.1150727,5.7768474,-5.1749067,-5.169928,5.94745,5.70312,-4.5218563,-4.202859,-7.2799025,-5.4885907,-4.849996,-2.5247216,-4.954746,-2.1627684,-5.4728804,-1.8327916,-2.2552862,-1.9698304,-1.5316709,-2.0708294,-5.5089617,-5.682336,-1.1841501,-5.1933894,-4.9218636,-7.7545624,-7.5136847,-7.454962,-7.3951325,-7.3292933,-7.571227,-7.6134353,-4.1867824,-4.0792756,-4.103022,-3.2272696,-3.8144853,1.2602329,-2.1590776,-3.4552097,-4.125512,-6.256772,2.9972913,0.9817487,2.7216969,0.3398426,0.43715838,-0.44089752,-0.84434193,-0.7928362,-0.5370179,-10.437189,-10.590595,-10.588564,-10.419538,-10.480868,1.8057827,1.4043354,1.7817427,2.1234765,0.27308723,-3.9443228,-0.28958398,0.58769333,0.5690058,-0.78893554,2.8628793,-1.5033017,0.1325529,-10.392761,-4.236178,-3.9543076,-3.4664083,-2.3094776,-2.0794148,-1.1243343,-3.4749582,-9.366891,-9.23767,-7.7215943,-7.704221,-8.237129,-9.1638,-7.966269,-7.993749,-7.944999,-7.583977,-8.036621,-8.650952,-7.7382283,-7.5995855,1.6551628,8.203321,-7.0819454,-7.865886,-7.5020804,-8.147112,-8.634592,-8.573132,-8.578209,-8.509731,8.887788,-7.8905196,-8.550002,-8.39551,-8.256048,-8.254571,-8.199189,-8.348947,-8.663883,-8.654466,-8.591672,-7.773287,-8.438228,-8.606221,-3.846338,7.4423714,-4.6862597,-8.491036,-8.448188,-8.628894,-8.850344,-9.106979,-8.824599,-8.856745,-8.984146,-8.5928955,-8.43969,-7.8651423,-2.1744635,-2.429472,-8.405447,-7.432146,-8.518832,-8.121007,-8.541372,-8.509407,-8.5718355,-8.814504,-9.158507,-8.706933,-8.695785,-8.638527,-8.862187,-8.847755,-8.487949,-8.651858,-7.915977,-2.1041915,-2.5401583,-1.7876889,-8.344856,-7.191645,-2.206229,-2.42061,-4.651834,-6.3000317,-0.96368706,-3.3201022,-3.6800132,-9.299778,-9.559167,-9.554446,-9.669733,-9.4401,-1.1846554,-9.494561,-9.19527,-1.0173929,-9.223691,-9.03339,-1.171825,-1.1152999,-9.525886,-9.435347,-9.549682,0.58213973,-0.058028467,0.17468746,-0.036755383,-0.110053524,-0.049117804,0.082077354,0.32184628,0.25660798,0.2572778,-0.8369506,0.018832907,-9.5357485,-9.654515,7.668324,-9.117167,-1.299618,1.337651,8.855981,8.954068,9.036315,9.014041,9.144471,8.968086,9.022188,8.998363,8.857506,-9.151878,-9.137999,1.6121382,1.6810013,3.1768963,2.9942718,-0.4444397,-0.46208447,-0.22154792,-1.2263855,-8.982814,-9.1819315,-7.4814634,-7.591043,-8.57875,-7.466603,-7.469538,-1.3338605,-3.0463853,-7.232248,-6.7987747,-6.759683,-7.3901353,-5.2325306,-5.8473687,13.08308,-3.780827,-3.9150758,6.203041,-7.229084,-7.4237227,-7.6178465,5.4678693,8.054895,8.110448,-8.045479,-7.565045,-7.5123315,5.867799,4.909033,-7.557322,-7.6477566,-6.2670937,-4.0685987,-3.6573768,-3.8574562,-6.4408827,-6.3810444,-3.6384237,-3.3586876,-4.9335847,-7.1296363,-4.8153768,-6.058524,-4.384927,-2.6782482,-5.990375,-7.1057467,-4.1130548,-4.251298,0.4175622,-4.563243,-3.4556613,-4.4199877,-3.8578846,-3.5656934,-3.1622186,-3.4817119,-2.935748,-3.9668028,-7.6334605,6.420898,-6.280482,-9.574635,-7.910646,-9.177247,-6.9933996,-2.7863,-10.278054,-6.6402874,-6.1376386,3.4779854,-6.270677,-5.486127,3.6703157,-8.842364,-6.286599,-6.12127,-6.4854755,4.596166,4.6135535,4.6286817,4.6877704,4.561312,-7.0516095,-8.176692,-8.643685,-7.895625,-7.7797394,-5.252562,-4.0650864,-3.1761916,-4.0843,1.064456,-6.4464374,-7.4876485,-4.231103,-3.5450175,-2.876937,-0.96347415,-2.8089688,-2.6298625,-1.6063423,-4.3950367,-4.5451713,-4.755124,-4.4625206,-4.001437,-3.7277458,-2.3971436,-2.6952403,-2.0414696,-3.3004217,-4.2239394,3.07937,2.901011,2.8456404,-2.1126049,2.7119927,2.481414,2.503032,2.402019,2.8833184,2.8761816,3.2289348,3.852584,-6.8982954,-7.2980027,-3.4695745,-3.3632443,2.971725,7.862812,3.7336988,9.319019,7.9988756,8.744309,8.692264,8.647084,8.366032,8.682601,8.673078,8.6951,8.685072,8.677659,8.687336,8.690224,8.245998,8.659739,-6.687334,-6.2441545,-6.7680154,-6.140969,1.7655272,-4.4036856,-3.692586,-3.706499,-4.055183,-3.2600875,-4.2325253,-4.1685452,-4.9104643,-3.6973429,-3.2235374,-3.8221436,-4.072684,-2.0422838,-2.0978622,-0.93031114,-3.2029073,-2.5448172,-3.2154512,-0.49931896,-2.73603,-2.2126672,-3.8391812,-5.337202,-7.624867,-8.411955,-9.540321,-9.801009,-9.449552,-1.7675945,-9.865662,-10.063897,-9.596887,-1.0238559,-0.08582975,-9.944433,-9.957358,-1.2034588,-10.034859,-10.11361,-10.149296,-10.004542,-9.953897,-10.057375,-10.080764,-9.942956,-9.500883,-9.580527,-6.1545215,-5.758659,-8.112685,-5.798882,-6.1155195,-6.021622,-6.6826506,-6.1466246,1.0842483,8.011548,-0.7373298,-5.909087,-0.93870133,0.3414905,0.45705736,-0.5931276,0.8984046,-0.09473006,0.13542776,-0.027154973,0.9797265,1.3991973,-4.377242,0.739104,1.2198015,-5.4281497,-5.535209,-6.457351,-6.59561,-6.167222,-7.3802924,-6.1402164,-5.747374,2.6049056,3.6387346,-0.2683001,3.113944,3.1735623,2.499788,-1.5010262,-0.48407862,-5.3622055,-5.399707,3.4654155,3.265211,3.155049,-4.905108,-4.6903496,-6.593823,3.3591015,3.0311458,-3.9904442,-3.7825923,-3.8461852,-4.34469,-1.2232473,-4.161527,-4.002256,-4.1977067,-5.1702747,-5.618388,3.6321201,-4.6433644,-5.8049407,-5.7013164,-6.0287385,-5.5334187,-4.6658745,1.9865932,2.4365585,3.4707434,-6.897169,-0.69803673,-1.7037923,-1.4518137,-1.5643717,-2.5719209,-1.3344502,-2.5222654,0.019367946,-0.24349245,-1.2885406,0.8077301,-4.3917127,-0.0896811,-3.0731964,-1.2520131,-0.2669039,0.2515411,0.18544768,-0.5458026,-0.9315894,-0.16239326,0.45194843,-0.28782108,0.3313178,-0.14615391,-5.8665376,-5.441103,-5.4537454,-3.5423777,-3.5156271,-5.032294,-4.450581,-4.932266,-4.2863975,-5.330681,-5.7011,-3.4433877,-5.9898415,-3.3829024,-4.6502714,-3.4499798,-1.7195677,-2.2425926,0.4386848,-1.2115098,-0.5127761,-6.609189,-6.1550117,5.909811,5.9131575,5.671287,4.8050685,-5.647943,-5.6617026,-5.6330266,-5.1664276,-4.8537154,-5.0499935,-5.1132426,-4.921924,-4.9854255,-5.2519546,-7.3988423,1.5024961,1.8708662,1.6937865,1.3601062,1.4299766,-6.262235,1.6078408,-7.1657515,-7.1971884,-7.2976003,-6.919578,-7.12436,-6.9215717,-7.498532,-7.568099,-8.213574,-7.5314703,-6.790802,-7.3788037,-7.1967425,-7.655443,-7.982868,-8.542125,-7.368369,-7.900134,-7.9214063,-7.467913,-7.1667576,-7.613536,-8.284972,-8.64335,-7.4244,-6.9005127,-6.627834,-6.4593806,-5.7041597,-1.8403517,-5.979161,-5.7712007,-1.652854,-7.0084276,-6.9790945,-4.752254,-7.093579,-6.8226037,-6.947763,-6.739987,-7.0133405,8.863554,-6.9980025,-5.3521986,-0.20891987,-0.28775412,7.850464,7.790838,7.768731,7.7288904,7.8530946,7.933333,-2.5652738,-1.8272184,-0.17119795,-2.9736757,-2.7073832,-1.2846042,-1.9679921,-3.360096,-2.9825172,-3.7200007,-4.5858727,-4.715919,-4.390141,-4.542694,-4.499874,-4.512485,-5.4786205,-2.9716759,-2.6666148,-4.3403873,-4.5660825,-4.6176896,-4.5162992,-2.650334,-2.558165,-2.6793156,-3.286249,-2.9052331,-3.1153097,-4.6400843,-3.0341122,0.07246211,-3.6274362,-3.4939804,-0.7189977,-7.945688,-7.4329386,-7.3608236,-7.3873096,-7.3859854,-7.324113,-7.6816635,-5.930603,-5.8691072,-5.8694897,-5.8035464,-7.485573,3.6638288,4.6505427,-6.2838697,-6.5030136,3.723456,9.434288,3.6618779,-5.592099,-3.03524,-2.0056136,-4.4880424,-2.306213,10.423394,8.977247,3.6407986,-6.660008,-6.7508583,-7.8494105,-5.410495,-8.572071,-8.88394,-3.6255429,-6.079139,-6.529658,-6.518191,-4.1601996,-4.3082557,-4.116998,-4.1416583,-3.988422,-7.7273335,4.0215898,5.938152,3.703274,3.721474,3.7548277,3.8514748,4.060518,3.7354722,4.0849996,5.607142,-7.970244,-7.596649,-7.5121565,-6.701167,-5.5023327,-6.7263613,-6.5054574,-6.555798,-8.036539,-8.1737585,-8.201599,-7.918761,-6.5749164,-8.016614,-7.958213,-8.035663,-7.960281,-7.5815144,-5.3055,-7.588452,-7.622696,-7.127895,-3.261658,-3.8966463,-3.7687752,-3.090681,-2.06061,-2.731568,-2.1624777,-3.4372077,-7.980106,-7.511591,8.061754,-4.8953023,6.9715743,-5.0134587,7.2381024,-6.9432306,-7.2274823,-5.8794312,0.76341814,3.9179702,0.6168407,10.132029,1.4931284,2.058755,1.0047458,1.1565815,0.34369785,0.43617377,0.82399446,2.154801,-5.6685715,-1.675485,0.25209343,0.43977085,-0.085450046,-1.3110204,-0.08836809,-1.529401,-5.5416207,-5.9035497,-6.2717695,8.062095,-6.5485916,-6.1076927,-6.42883,-6.5787363,-5.8620133,-3.4186003,-3.1208704,-3.4766123,-5.3674936,-1.5437341,-2.0045786,-2.3879454,-0.50014347,-1.462851,-5.8992863,-2.5101435,-4.7284875,-7.333541,13.0916815,-6.7851133,-6.097295,-5.6540523,-7.268038,-7.5084896,-7.2928185,-7.4525747,-7.151031,-3.5639384,-6.5387454,-6.6319633,-6.9349923,-3.1325011,-6.854469,-2.8810952,-0.32845166,-3.5380378,-6.5054493,-7.0258427,-6.568157,-2.4689841,-2.9255266,-6.8762584,-4.9342165,-3.8637717,-2.7741249,-3.2518148,-2.835675,-3.7916763,-3.0043578,-2.2087126,-3.1079783,-3.6463444,-7.026412,3.4043305,2.6151986,1.4351289,1.0500172,-1.0492997,-6.446207,-6.4849296,-6.4366093,-6.2268925,-6.416842,-6.733719,-6.605554,2.4804099,2.3971453,-6.9462256,-7.359173,-7.646073,-6.9731855,-7.276611,-7.2655067,-7.149471,-7.272847,-7.273548,-7.2955894,-6.991124,-4.7853403,-4.433012,0.98261285,-4.320961,-4.0026584,-3.829562,-3.9232259,-3.8870103,-4.1218367,-3.923799,-3.897568,-3.7648432,-13.7687645,-0.26115173,-2.6691208,-13.912068,-1.0526317,-8.766466,-1.4151434,-1.1524806,-8.209187,-14.093245,-1.1269497,-0.941482,-0.7811839,-3.2363863,-0.6208152,-0.8334504,-2.2564788,-2.302619,-1.8337721,-1.455468,-1.8048733,-1.961746,0.22165583,-2.8540406,-1.4299709,-3.0156558,-0.7001857,-0.40956524,-1.0685982,-2.3947473,-5.0539265,-4.992042,-4.740231,-8.040227,-6.2402573,-7.672139,-6.9647036,1.263167,1.8145498,-6.1367316,-5.8920984,8.168212,8.401412,-7.0170336,-5.846515,-6.224618,-6.296149,-7.2437243,-2.5458474,-6.936468,-7.0833526,-6.434381,-6.2214913,2.5170445,-6.7799287,-5.7952776,8.59101,4.994465,4.9283485,4.971549,4.8703403,4.961575,4.7807136,5.0429807,8.828333,4.4298515,8.73435,8.877838,8.829387,-5.2584667,8.558164,9.113766,5.1115174,5.062613,8.88228,4.5017157,8.7507105,-2.4141374,8.949985,8.889806,3.617961,-4.855491,-5.3084707,-4.1147065,-4.821169,-3.7018263,3.5373185,-5.4885464,-4.7661004,-3.7372692,-2.3352287,-2.4033153,-2.7685223,0.7593221,-4.007621,-2.9388137,-2.5095234,-2.2407596,-2.6216762,-3.7194004,-0.8820429,-2.6551085,-1.8573307,-3.006407,-2.0729814,-2.4561336,-1.7276546,-5.7149987,-7.9085207,-7.0028133,-7.008199,-7.4431543,-7.9148984,-6.431174,0.014394291,0.12735699,-2.9125063,-8.620361,3.162972,-3.8766415,-3.9467583,-7.900211,-6.7560196,0.09616856,0.066799566,-10.2153425,-10.016041,3.2952106,-6.8495164,-4.8209486,-3.981733,-3.5328069,8.16941,8.123598,-1.1495073,-2.921554,3.9665337,2.1290333,-1.7048422,-2.052637,-2.2305038,-2.4520822,-3.6850257,5.890623,13.613191,14.215806,13.884053,13.953239,13.908399,12.980672,13.3716545,14.265131,13.863332,-6.2448554,-6.33428,-5.9765124,-6.4317007,9.056903,8.061863,4.9970226,4.7235665,6.611135,4.680848,4.8431244,4.6211395,4.951522,5.390734,5.1382046,5.4160643,8.75163,8.870639,8.434707,8.446463,8.820791,8.716531,8.999469,-6.674987,-7.807605,-6.306686,-6.9985075,-5.3267517,-6.042189,-4.9631104,-5.903776,-3.4486015,-0.60960394,-0.65068555,-5.688151,-4.6689377,-4.9739666,8.983246,-5.7561207,-7.9856877,-7.3809566,-7.35654,-5.1912093,-4.3874593,-1.0983218,-5.1491256,8.602696,4.951728,6.4648857,5.009894,5.5325956,5.0870204,5.027088,4.24533,9.254069,8.381411,7.8155837,8.837767,8.69793,8.9249325,9.353647,9.226099,-5.0642433,-5.2503467,-5.065392,-7.6327085,-0.57397157,-7.328033,-7.4019322,-7.1076427,-7.3873224,-7.29174,-7.457256,-7.481253,-7.6385775,-5.923452,-5.8282456,-5.807781,-0.20832303,0.89833814,-5.747392,-6.0032034,-5.3998404,-7.9975324,-7.268829,-7.397035,-5.010482,-4.6369424,0.7975813,-4.4384856,-5.2328753,-7.9613085,-5.246134,-4.69271,-4.781679,3.299291,-0.5666444,-1.210097,1.240127,-6.63856,-6.140857,-6.261997,-6.350045,7.9295645,-5.8809724,7.598022,7.6354275,-6.3313518,-6.0922656,-6.0009212,-6.286463,-5.9579673,-6.4175277,-6.1099052,2.5066957,-6.678919,-5.375441,-4.2931232,-3.8875837,-3.4868572,-3.4383671,-3.5005374,-3.5752645,-3.2299304,-3.0961049,-3.4314456,-3.5467691,-3.5570717,-3.518179,-3.3986533,-3.0028532,-3.4069047,-4.7334127,-4.058632,-4.4398937,-5.8527627,-5.7884374,-4.94756,-4.7807918,-2.6035163,-5.1910143,-3.9956958,-4.1755543,-3.8675747,-4.458086,-5.4226494,-5.3382034,-3.6403916,-4.779202,-5.0667324,-4.155086,-4.1773915,-4.2190557,-4.571277,-4.4394455,-7.1015997,-4.505951,1.2651341,-4.3432035,-3.569782,-4.1251564,-5.361919,-5.365179,4.3204937,4.2815027,-4.9387197,8.601399,8.545283,-4.5683537,1.8817161,-2.7902493,-4.1213593,-4.1190505,-4.9581947,-6.9214,-7.169295,-7.0081754,-7.0365634,-6.8782864,8.318866,-6.86753,2.2776544,-7.0356693,-6.4218535,-5.8088093,-6.9926476,-7.092479,-6.734158,-6.470535,-5.6130867,-6.1248956,-6.090368,-6.54438,-6.3801312,-6.481053,-6.737387,-1.5566362,-2.0786424,0.28226492,-6.053121,-0.185616,-1.0536232,-0.19262192,-2.4926457,-6.5209227,0.81592304,-6.8329782,-6.9889765,-8.132709,-7.5810966,-7.410666,-7.5919733,-7.500872,-7.3823867,-7.2823763,-7.4359093,-7.6346083,-7.525713,-7.7601843,-7.214,-7.374482,-7.4523425,2.3934615,2.2667696,2.4163747,2.262093,-7.364545,-7.3245163,2.3548217,-0.14731723,1.1157318,-7.470351,2.4231079,-4.5874557,7.2564783,-7.50737,2.7578003,2.762375,1.4863272,2.675373,2.2187362,-0.32893345,-0.5067628,1.5764383,-0.79020834,-0.59208214,1.3105738,1.1120338,2.9384534,1.0730497,3.347252,3.649737,-6.8100934,-5.736881,-5.427404,-5.5101137,-5.429181,-5.7070637,-5.7064333,7.579102,-6.104245,-4.498797,1.7227637,-4.5268464,-1.1937582,0.2138646,-5.9608736,-6.9774284,-7.1727357,-7.1872425,-6.5106463,-5.991045,-6.430405,-5.5755715,-6.849436,-7.185139,-7.083452,-2.9821444,3.3891287,-7.026383,-5.541029,-2.9087305,-0.7275972,8.206459,-4.593484,-4.059958,-0.9751129,-2.2773867,-0.8073508,-3.031436,0.40753484,3.1703176,0.10932355,9.278795,3.8170836,-1.5920892,0.18558139,-2.5437903,-7.3560586,-7.0848074,-5.873134,-5.849537,-5.9347506,-5.8996387,-6.0299892,-6.207565,-5.777835,-5.936495,-5.6969447,-5.981651,-6.014316,-5.94689,8.553075,-6.092142,-6.035202,-6.0330725,8.528061,-2.8002157,2.2950048,2.0434198,-3.458994,-2.3024237,-2.2104027,-2.3084373,-1.8728,-2.519882,-2.2931352,-5.0448294,-5.41488,-6.462304,-4.736438,-4.7050953,-4.081386,-2.1503835,-1.8799222,-3.6498694,-2.8653417,-0.3643699,2.7060373,1.1086532,-4.5146275,-4.322238,-5.221812,-8.355818,-8.157634,-8.379711,-8.333718,-8.254119,-7.941436,-8.406175,-8.291947,-8.010707,-8.317433,-8.345085,-8.081991,-8.369807,-8.376601,-7.808648,-8.311019,-8.266011,-8.2315035,-8.30609,-7.9655395,-8.385014,-8.30258,-8.310965,-8.322473,-8.324685,-8.277686,-8.388234,-8.206154,-7.701556,-8.208515,-7.8284993,-5.2147117,-5.5458083,-4.0710273,-2.8090053,-4.4822946,-4.9426856,-3.6337695,-4.628412,-3.8230155,-2.293792,-1.6281033,-7.9498057,-8.554025,-1.389499,-1.0572221,-7.1976147,-7.344529,-4.3902164,-4.452475,-3.945178,-4.7259007,-3.9112318,-5.068987,-3.882635,-4.707469,-3.7898383,-2.38069,-4.0960393,-5.9785247,-5.0542493,-5.0825725,-4.723513,-5.210824,-5.0942583,-5.295053,-5.636962,-4.667953,-4.3482065,-4.987747,-4.5337586,-3.1639378,-3.4297576,-1.7291176,-1.3785365,-1.5629195,-1.9066708,-1.7237593,-3.5426235,-3.7802763,1.4260135,-4.444175,-4.5293,-7.4263787,-7.1659937,-7.2419868,-6.3871546,-6.542739,-6.36804,-7.0061398,-6.39544,-6.3482037,-7.115298,-7.2750254,-6.065486,-7.2930837,-6.5535645,-6.5367794,-6.3093295,-4.2675643,-1.2774277,-0.22770977,-3.5188386,-3.4683218,-2.6520941,-2.4819593,-3.9056075,-5.604504,-6.391956,-5.731705,15.751629,-6.07409,-6.856521,-4.883726,-6.185925,-5.847788,-4.755746,8.0092535,10.552084,10.445114,10.453106,0.8153807,10.408453,10.308842,9.483113,8.037648,-6.9300437,-6.3732243,-6.289001,-1.9473085,-1.5305734,2.8514676,0.26554903,2.5296085,12.7840395,12.710265,7.4648366,-7.195936,-4.114657,-4.716659,-3.9810402,-3.8570335,-4.179048,-4.2723584,-4.208211,9.972208,-4.0607576,2.9127681,1.5553913,-4.138999,-4.528573,-2.345842,-1.6535082,-1.7141378,-0.9493357,-1.6875143,-1.0975605,-0.97596294,-7.317669,-6.552251,1.8478552,2.552185,-0.76089597,0.511303,4.4070253,3.397695,-7.7286544,3.5994577,3.724873,4.78261,6.039838,-4.1151834,-2.9266284,-3.0019403,-0.50664425,-1.9586082,-0.2653507,-0.5605632,0.22665724,-1.772185,-1.3283391,-0.8022184,-0.45883414,-4.621519,0.5753958,1.5851066,-3.878608,-6.734438,-7.049536,-0.49359536,-6.647577,-2.8526745,-6.608145,-7.1169887,-3.7662365,-6.728657,-7.0442805,-6.8330655,-6.78866,-6.8906345,-4.205401,-4.0775204,-4.102852,-8.20023,-8.201031,-8.200736,-8.178782,-8.209386,-8.233689,-8.196443,-8.338187,-8.28663,-8.236694,-7.4773088,5.8497343,-5.165231,-5.0951815,-4.65005,-5.1019955,-7.178674,-6.359501,-5.9490414,-5.9262986,-1.3899802,-1.8785866,-4.894948,-1.9221919,-1.04494,-5.1033683,-6.3700643,-6.3072104,0.18610533,-2.3046598,3.984032,8.573362,8.495177,8.340966,-4.030127,-1.4530169,-0.43257594,-1.1294726,-1.1375505,8.278104,-0.99234885,8.494586,-6.9098206,-6.8321414,-7.405465,-7.737002,-6.9849715,-5.415503,7.8174663,-7.1766205,-6.490602,-7.7979693,3.02591,-6.548037,-3.6608295,-2.8013146,-3.9255824,-0.6840385,-3.3453293,-2.8529482,-3.3617966,-2.965456,-3.6942894,-3.5876582,-3.2714305,-2.799462,-4.4017496,0.7563773,0.60590285,-5.001186,-0.5438632,-5.0682936,-4.04567,-3.947433,5.8553796,-8.141061,-7.5134907,-4.4537034,-4.2769327,-4.4632096,-3.7456677,-2.4924254,-4.0106287,-4.191321,-3.9208238,-3.3638568,-2.248535,-3.147764,-3.0262234,-2.8759778,-3.096536,-2.606879,-2.2050788,-2.6769955,-3.6325438,-3.267984,-10.055127,-6.628538,1.9218894,0.90293556,10.928299,0.059276477,0.18215357,0.29790652,0.30790257,0.6342805,-1.1511091,0.21634318,-4.0394745,-4.992393,-5.981384,-3.6447065,-2.5254383,-4.355529,-4.619602,-4.3053837,12.659547,-3.5573516,-3.5387275,-3.2463012,-3.1847904,-3.2783377,-5.552649,-3.6709528,-0.5968738,-0.79808277,-1.2245413,-1.0956377,-0.8241396,-2.7665725,-2.2958617,-2.2276363,-0.8100065,-0.9948567,-10.541205,-10.825882,-10.6587,-10.784557,-10.563186,-10.788751,-10.854081,-10.758421,-10.74543,-10.723408,-10.723228,-10.749114,-10.83366,-10.746489,-10.76451,-10.783401,0.023546988,-10.627831,-10.597391,-10.1933565,-10.698479,-10.663501,-10.733106,-6.3095403,-5.939656,-6.4912553,-5.720204,-5.90576,-3.0513327,-3.3896432,-4.4093146,-5.8568745,-5.6736865,-5.7254186,-5.545238,-6.893824,-1.0390425,-6.0149703,15.163576,13.0179615,12.583963,13.319646,-7.1772203,-5.906594,-5.7830606,-5.1744037,-5.181478,-1.0929729,-5.1091604,-7.1280284,-3.9282427,-3.433665,-3.6085858,-3.8683383,-5.6460147,-6.127993,-2.3790247,-4.993231,-2.004256,-5.353121,7.993042,7.9606576,-10.1569805,-3.2727568,-2.7842536,-1.2051815,10.381564,8.029367,7.9299116,3.0226743,2.7409708,-6.543588,-6.4329786,-1.8257707,-6.7072525,-6.477707,8.025752,8.220413,8.030572,8.054864,8.120357,8.08215,8.08848,-3.6697032,-3.8468907,-3.9476442,-3.7280095,-3.4375975,-3.4964056,-3.6316714,-3.592996,-3.3704271,-3.4813313,-2.9727192,-2.5561287,-2.447438,-2.234981,-2.1207402,-4.055402,-3.103425,-6.547435,-4.461483,2.5446775,-3.6984408,-2.1746202,2.4058654,-1.5891907,2.482932,2.54436,-4.447858,-4.3305745,-5.3589344,-6.834347,-5.081439,-1.2249248,-6.680509,13.049618,-6.5075064,-8.124535,-5.6832976,3.4148252,-5.805472,-5.9305906,3.72459,-7.944296,-4.234894,8.637931,-4.5572686,-3.8962862,-9.474791,-3.8976686,-3.7963417,-2.5929666,-2.8039656,-3.5674744,-3.1272283,-1.6715848,-2.4888937,-3.2636726,-1.5085281,-2.6881769,-2.3153436,-2.4934368,-3.9930882,-3.6324704,-3.3059554,-10.591856,-10.647232,-10.602006,-10.626153,-10.379061,0.00070286327,0.12357016,11.026817,-10.556366,-9.876766,-5.684308,-5.6246276,-5.833147,-5.8036337,-5.6028566,-5.715901,-5.8456674,-5.6830654,13.250171,-2.018102,-5.5789857,-8.492934,-4.8140073,-4.476189,-4.587651,-5.050449,-5.2154946,-5.169466,-4.46684,-4.4992514,-4.401311,-4.560247,8.86619,8.650915,8.392267,8.761849,8.516937,8.686935,8.549784,8.147465,8.581917,8.557991,8.441955,8.477899,8.780649,8.828033,8.823334,8.974632,8.750531,8.491976,8.791898,-5.2623053,-4.2349734,-3.9063425,-4.46901,-3.8157165,-3.8477683,-3.9032178,-4.3453774,-4.0511312,-7.4983344,-4.0363007,-4.121286,-3.8424911,-4.0705476,-4.042612,-3.5157034,-3.9429898,-4.017441,-4.131529,-4.059576,-4.026068,-4.144442,-3.8109367,-3.7129393,-3.6965792,-4.1621966,-4.3350153,-4.109569,-3.9788113,-5.163312,4.0442076,3.3986986,-0.7496401,-1.4272467,-0.86312556,4.5865946,1.0917888,1.8881519,-5.054566,-3.7127934,-3.4827251,-2.7433507,-3.9321458,1.2754453,-4.185235,-3.46554,-3.0249956,-2.9347098,-3.992065,-3.4306383,-4.2014823,-3.1822665,-3.4132776,-3.7613542,-1.0622764,-3.0251296,-2.7485013,-3.1725445,-3.8148823,-3.1516056,-3.046664,-7.078279,-6.5612235,8.90515,4.893533,0.1965059,0.21008234,0.40813422,5.0970135,4.9526525,5.1061964,5.19989,4.3110733,8.741437,8.383823,8.351388,8.373525,8.903338,2.3007812,1.9912837,3.5604198,-5.7827744,0.46784413,1.5960606,4.4070945,-1.4913722,-1.8141986,-1.4651638,0.14451644,1.7098771,-3.3950846,-3.610341,-3.6344678,-3.5763068,-1.8626115,-3.470936,-3.0144064,-3.0286384,-1.2105277,0.031223195,-0.5210073,-3.441504,-3.4311821,-3.0558028,-1.1910727,-1.2541679,-1.4173443,-1.3780103,-1.2932281,-1.1236445,-1.6963503,-3.735384,-2.6465125,2.88244,-1.4811152,-2.9274688,0.34087142,-1.3636965,-1.6069139,-1.7932378,0.22882749,0.0008527607,-4.7971773,0.13994089,-2.2798514,4.5534625,3.2502706,-4.164877,0.79197335,1.4221389,-8.4846525,-8.532743,-3.3254526,-3.130666,-2.885098,-8.99684,-8.989615,-9.0674095,-8.898176,-2.1375446,-9.012746,-9.010542,-8.953043,-9.062927,-9.028445,-9.050856,-6.4869924,-9.07862,-3.2013988,-2.4410243,-1.6874703,-8.46793,-8.644201,-7.0156164,8.429843,8.524817,1.8686485,-0.29857883,0.14827274,-0.12690958,-2.810075,-1.0480672,-1.9847127,-1.1436105,8.496064,-3.8616438,-3.8066761,-3.7045283,-3.3820245,-2.0481136,-1.9798396,-3.681233,-1.3325226,2.1868935,-3.8877206,-6.536691,-6.1531954,-6.261496,-5.8681164,-6.599576,7.142674,-6.72207,-6.67399,2.0752604,5.2154336,-6.779212,-6.742651,-6.5672464,-5.952662,-6.148726,-4.128702,3.3436816,1.3449359,-4.665097,-1.9074339,2.4268446,2.1425247,6.7634244,3.3580413,3.1133816,8.903043,9.127784,9.085129,9.1153555,9.08857,9.275453,9.190352,9.002883,9.111339,9.074559,8.6461115,-7.1993055,-6.37008,8.015198,-10.282917,8.41288,-10.052115,-4.2919755,-9.905642,-9.78558,-9.65675,8.25422,1.3930569,8.442132,8.228372,2.628277,-9.618898,8.306561,2.7124903,2.438666,-0.001294787,8.319376,8.431832,8.463387,-3.358865,-5.1439953,-4.917903,-2.6116605,-3.6957157,-4.9208317,-5.844573,7.4192863,8.037944,-5.546058,-8.974583,-8.19239,-9.12057,-9.165398,-9.154372,-8.734517,-8.636915,-6.280389,8.913765,-9.024368,-6.328581,-6.2129083,15.745344,-5.90696,0.2325047,-5.677669,-5.123511,-3.985873,-6.011421,-5.1679354,-5.8953915,-6.2341533,-7.297319,-6.7039437,-5.870106,-2.161848,-4.1818876,-4.7248216,-5.1166773,-6.5704656,-5.7636695,-5.8838186,-5.28933,-5.6172385,-5.646451,-4.3014674,-1.4146472,-3.8841493,-1.7010905,-3.9021084,-4.9838223,-5.297039,-4.709508,-4.746577,8.138745,8.221957,-5.1589737,-4.6421742,-3.0098507,-2.382613,-5.9601884,-4.102022,-3.8211346,-2.2779393,-4.466806,-4.523142,-6.8912725,-6.631759,-6.6448975,4.784927,-6.723033,-7.818765,-4.6945944,-4.4423304,-3.5144584,-2.938103,-2.6119933,-2.6742196,-1.4770412,-5.0011806,-5.2477922,-3.4065144,-3.4314902,0.51530105,-4.951913,-8.122949,-8.040098,-8.100705,-1.4943206,-0.5442013,-2.9051957,-3.014914,-2.725221,-7.157317,-8.324685,-8.463369,-8.5453615,-8.236739,-8.282192,-2.1334453,-0.8374927,-7.6041517,-1.5631815,-4.9265037,7.7460604,-6.3061967,-7.863409,-0.35042217,-7.832439,-7.8047204,-6.163111,3.3814237,-3.71205,1.1207441,-1.7251174,-1.840118,-1.7916394,2.550191,2.8568194,2.5160637,1.598894,-3.190759,1.6670688,6.861751,13.77218,13.274533,9.679267,10.009619,13.246678,-8.019422,-7.5998354,-6.97322,-7.4088373,-7.008666,-7.0928473,-6.819122,-7.1687465,1.2561264,2.1860237,-6.990448,-6.8286467,-5.839593,-5.4785166,-5.4997435,-5.6564264,-4.5220227,-5.537126,-1.1898744,-5.3925786,-3.1843166,-2.3134658,-2.4041545,-6.948957,-6.250196,-6.6999073,-6.5285935,-6.2264047,-6.553845,-6.003196,-5.932749,-6.615449,-1.6671084,-1.6169993,0.2735841,-0.23926151,-6.4446645,-2.1704025,-1.1079704,-6.0983996,-4.506568,-0.5022634,1.1688453,-6.3503532,-5.9091525,-6.2368217,-6.745132,8.921426,8.841368,8.788505,8.665149,8.789531,8.732475,8.719079,8.62083,3.8603652,4.9408655,4.4042826,5.755397,7.3070803,7.135746,7.016453,7.143227,-5.480436,-5.304513,-5.419386,-5.6016603,-5.475535,-2.1054091,-5.253373,-5.237381,-3.7063785,-5.2912545,-4.9662,-5.2454295,-5.2775745,-5.2007313,2.9149346,3.1903288,3.356822,2.8465953,2.45641,-0.93813986,3.211077,-8.223137,-8.662299,-8.336526,-9.055889,-10.233634,-9.558355,2.9821985,2.3258448,1.2226669,-9.503252,-1.1330237,-0.003354341,-9.275816,-9.574337,-9.509403,-9.456067,-9.529612,-9.149864,-5.8809705,-4.9277053,-5.4106154,-5.3534517,-5.5645638,-5.5428224,-5.5304146,-6.6864767,-4.7922254,4.506338,3.62368,6.4284897,-3.3927217,-3.0362182,-3.5493028,-2.2347069,-3.2448092,-2.2136726,-2.5118432,-1.9909028,-3.1229765,-1.403084,-1.1746881,-3.490799,-3.1765375,-0.28639036,-7.2007484,8.496028,-7.759866,-7.363157,-7.472827,-6.639107,-7.00649,-7.3564506,4.6762295,4.979021,-8.005989,5.931752,-7.0850244,-8.671507,-8.6218405,-7.9366713,-8.323763,-8.24217,-8.020499,-8.686845,-8.720639,-8.788987,-8.744846,-7.944072,-8.574218,-6.537601,-4.038432,-4.528307,-4.810239,-4.9000216,-10.442839,-10.324274,-4.8954377,-4.5483856,-10.3795,-10.4354,-9.965998,-10.083391,-10.3392515,-1.8099405,0.3375611,-1.2668037,-0.35543653,-0.14671083,-0.6000859,-0.3734054,-10.428779,1.4530193,0.14160682,-0.0147105595,1.5065352,-10.404143,-10.011561,-5.9649143,-5.9348965,-5.864952,-5.7405562,3.0879655,-5.717644,-5.4577875,-5.6773195,-5.7255783,-5.488113,-5.815158,-5.880257,-5.741187,-5.591688,-5.9454927,-5.8682847,-5.9016023,5.771322,3.6071675,3.5188215,3.5929298,3.539186,3.567442,3.6034937,3.9274724,3.9089122,4.816473,-7.8369966,-7.4412107,8.086965,-6.4750934,6.787765,-6.648155,-6.4652863,-6.3034344,8.067498,-7.144297,-7.178203,-6.7560625,-6.5328298,-6.9997554,-6.617762,-6.958678,-7.0483994,-7.1355224,-7.118137,-6.901592,-6.816082,-3.424344,-6.4796114,-6.3833365,-6.0866513,-5.6040235,-5.5498414,-6.080022,-6.2173376,-6.3551087,-5.8746133,-6.005393,-5.7567487,-6.1004887,-6.3128843,-6.2231956,-5.4531727,-6.2792797,-6.834554,-5.6716967,-5.244626,-4.956163,-5.666627,-6.597041,-5.7142725,-5.389081,-5.8033767,-4.8248587,-4.986574,-4.5441437,-3.841793,-3.1625156,-1.7133135,-3.4768035,8.305639,-2.0044065,8.334156,-4.5602174,-2.6479692,-2.795525,-4.6013994,-2.8342133,-4.906401,-4.4688616,-4.4720135,-4.702322,-4.461467,-4.2560086,-4.4465046,-4.7831597,8.552526,-4.551633,-4.745307,-5.682483,-3.6097186,-5.2588224,-1.7650183,-1.0892175,-1.4084954,-4.2608404,-3.5434349,-3.669123,-1.2058915,-0.36806348,8.676499,-0.5168187,-3.6470623,-2.606006,-3.2841983,-3.1474843,-3.1080172,-3.3765478,-3.4067569,-3.7551866,-3.4650955,-4.753613,-5.0677004,-3.6410012,-3.669074,-3.54245,-1.3987094,-3.4751475,-3.392615,-1.980202,-2.2475708,-3.1554794,-2.4288654,-2.7161798,-2.4942875,-2.9400408,-3.05693,-3.8786223,-7.633115,11.191459,0.5153377,10.752015,-7.3240404,-5.2872033,-5.5976686,-5.3719378,-6.0022984,-5.6355643,-5.599257,-5.484806,-5.5287685,-5.2930393,-5.6074843,-5.7890596,-5.6252575,-4.8716044,-4.288935,-4.807657,-3.9121506,-5.875429,-5.977793,6.775878,-7.965945,8.879629,9.008666,8.738371,8.828905,8.846057,8.830607,8.671117,8.666607,8.647853,8.721614,8.647457,8.641034,8.793868,8.7666445,8.895303,8.657149,3.2842877,-6.077443,3.2660763,2.532176,2.334161,3.005304,2.429,2.6949644,2.0885947,-4.69419,13.217668,13.238595,3.2559865,3.617145,3.25269,2.8530307,3.2201297,3.4366703,-6.970385,-4.486418,-4.4490733,-4.2453904,-3.9727242,-4.4964304,-5.2103496,7.685528,-3.4019835,-4.455543,-4.3600416,-4.506457,-5.88458,0.39566523,2.5770319,0.5119827,1.5792255,0.5523575,0.6084326,3.204331,2.5082562,3.3242114,2.6970654,2.2882674,1.7011448,2.2873135,2.218149,2.4290135,2.425036,0.6023995,-2.7683842,-0.0109004835,-7.462071,8.104351,2.635699,8.1931095,3.849507,-8.191509,-8.280334,-8.367274,-8.2813635,-8.080481,-8.293577,-8.329221,-8.11603,-8.306885,-8.068546,-6.9498925,-4.684643,-6.3360868,1.2667869,1.4011934,1.4812045,1.9666157,0.966949,-7.6134033,1.1191405,-0.22916137,-0.9579312,-1.1975653,1.3897247,-2.7238615,-7.711973,3.749016,3.2658336,3.0848017,3.980069,3.9492283,3.6371396,-5.498676,-4.823148,-5.0700684,-2.4501824,-3.6746192,-2.556319,-4.8283854,-5.668622,-4.349177,-2.6216373,-5.6422586,-5.1218605,-5.0749497,7.8339143,-7.8316054,4.1341276,4.283777,3.70424,4.242371,2.893684,5.7184315,4.5757313,-6.7541914,2.5543113,-5.5961876,-5.5095544,-5.8052797,2.7408016,3.3081405,-7.2836595,-6.7656345,-6.6072917,-4.876133,-6.3833985,-6.5755224,-6.5655546,-2.0578425,-6.4882717,-0.40286294,-6.166122,-5.945708,-6.0577626,-6.406438,-6.5897326,-6.3592153,-5.8929415,-6.538028,-6.3977594,-7.9649215,3.5105882,3.5978827,3.619827,8.951625,9.030062,9.040408,8.985421,9.018683,8.868105,8.850483,8.827912,8.844432,8.770173,8.980811,8.712025,-5.3128533,-5.4743266,-5.345881,-5.342084,-5.2276073,-4.891541,-4.7462163,-4.934604,-5.104174,-5.1531396,-5.203467,-6.117798,-5.954482,-5.6560397,-5.624585,-5.1625633,-4.4620585,-5.4866443,-1.3376012,0.3363994,-1.800177,-4.2485127,-2.1306696,-4.5795507,1.257752,-5.7476745,-3.5613978,-3.7622645,-3.425082,-2.688516,-2.817041,-3.1045618,-1.6498306,-3.142857,-0.93882716,-3.6395235,-4.291232,-4.1828012,-3.2097843,-0.85568297,-3.6187549,-3.620473,-3.6748939,-7.402438,1.9078071,3.7349434,2.2718632,1.0158675,3.08107,3.497478,-6.3072925,-5.736287,-5.1403484,3.0647373,1.5923809,-4.716229,-4.130311,-5.141955,-4.81294,-5.758172,-5.2869506,-4.7732635,-9.257987,-9.899379,-9.504187,-9.862106,-1.2150072,-1.6508561,-3.7675233,-2.6060555,-9.713408,-4.941326,-5.2885194,-4.6675057,-4.125947,-3.7362757,-3.980987,-3.9462967,-3.9213119,-3.209294,-2.9174747,-3.773633,-3.713548,-3.5621738,-3.1022174,-2.984192,-3.752408,-3.8717225,-3.5800679,-2.8174486,-12.633092,-2.6804826,-7.5200877,-6.0427423,1.9177318,-6.91168,-9.199434,-9.341199,-9.748921,-0.17929742,-9.345013,-0.6188792,-0.700775,0.015909396,-9.156606,-0.49617395,-9.28213,-7.4976482,-4.9447417,-4.7533092,-4.886694,-6.0040126,-3.3421645,-4.98758,-4.892527,-7.0912094,-7.1311727,-6.5042844,2.672407,-7.0294046,-6.968719,-5.6490245,-5.7525134,-4.4643745,-5.5426445,-1.321389,-4.838936,-2.1626325,-6.970889,-6.8290358,-6.730292,-6.3058796,-6.215634,-6.557085,-0.13721575,-6.8123736,-6.614673,-1.5216967,-1.5070267,0.3222716,-0.53695595,-6.82664,-1.0465242,-6.7301745,0.14704141,-0.95192957,-2.2189662,1.0653317,-0.34086165,-5.9108615,-3.5886502,-1.9701431,-6.544115,-7.8722963,3.2945676,3.4054282,-7.1847053,4.325255,-5.783626,-5.8688703,-3.1457744,-4.003271,-5.77931,-5.734186,-5.2519293,1.0668887,-2.487808,0.75858396,-4.807399,-5.8176084,-5.2424536,-5.775949,1.426032,0.40866,-5.430025,-5.670358,-5.5878396,-5.437226,-5.665098,5.78685,-6.941882,-7.0094423,-6.790235,2.1273816,-6.847671,-6.750366,-1.814718,-6.162273,-6.464376,-6.373856,-6.330938,-1.6085349,0.09062515,-3.5342917,-0.65498585,-0.28266394,-6.262216,-5.429837,-3.6152096,-3.388496,2.3684654,-2.3500197,-2.8234134,1.4815716,-1.6141508,-3.1361976,-2.5895643,-4.41338,-7.219804,-5.3485646,-4.819055,-5.2447863,-4.753298,-5.1555915,-7.1677003,-6.7390037,-7.027746,-6.827113,-6.562261,-6.8429236,-6.8339868,-8.495285,-8.815098,-9.045195,-8.873152,-9.10264,-9.097995,-9.024095,-8.716099,-9.071642,-8.758617,-9.2647085,-9.251922,-9.001089,-9.202473,-9.134886,-8.828955,-9.250189,-8.560123,-2.0906746,-2.4330633,-8.584715,-2.4337556,-2.160194,-8.710094,-8.695725,-8.652161,-8.344629,-8.643258,-8.699661,-1.5285591,-2.2123787,-8.262983,-2.3931828,-8.304944,-8.57274,-8.563763,-8.474909,-8.68597,-8.643422,-8.7431965,-8.699753,-8.507605,-3.1920753,-7.9436626,-2.165717,-8.198907,-8.504791,-8.414623,-8.5102625,-8.50058,-8.558794,-8.547708,-8.455253,-8.022249,-1.230795,-8.131101,-8.257748,-5.056355,-4.374695,-4.569122,-4.614742,-4.1715417,-4.5900164,-4.785761,-4.8155336,-5.010149,-5.81003,2.8262427,-4.157052,-2.7623072,-1.8053279,-5.4308333,-5.0803165,-4.939893,-5.1035247,-5.0092397,-4.9458857,-4.082954,6.9696136,6.97173,-9.717415,-5.400225,-3.8580651,-4.293844,-3.8194256,-9.239478,-3.8331413,-3.9421408,-1.1057439,-3.3076189,-2.9430187,-2.4858937,-3.8896294,-6.308499,-5.8690166,-5.8268914,-5.3678517,-5.482974,-3.2264624,-5.413063,-5.678358,-6.1111503,-4.56584,-4.200934,-4.332382,-3.975631,-4.051327,-4.453522,-8.126878,-8.201403,-8.352025,-7.98921,-8.336573,-8.035638,-8.318492,-8.34858,-8.344942,-7.786991,-8.369278,-8.370044,-7.9889994,-8.2645,-8.176086,-6.9504914,-3.3425403,0.3312381,1.5363197,-2.8259015,2.2503743,3.0975902,2.1297863,-4.2484765,-8.192196,-8.179502,-7.6311665,-7.497328,-8.301089,4.7198496,-7.889553,-8.035181,-8.309774,-8.266035,-8.259236,-7.367475,-4.6563787,-5.240359,-4.0454392,-3.273505,3.247229,-3.7504494,1.0723732,1.3168294,-1.4696186,-1.0588739,0.7436215,-1.6595557,-3.3096693,-7.3628716,12.855162,-0.6539256,2.6457212,2.2887132,1.8428276,2.7530637,-6.983023,-5.96952,-5.7717185,-5.751144,-5.842127,-5.788567,15.743604,-5.7718534,-6.069511,-0.49028328,-3.7422926,-5.8097653,-5.8584833,-5.110815,-3.8104763,-2.6823516,-5.3158092,-1.3578428,-5.369933,-6.6245112,-4.5697794,-4.8830285,-1.787439,-1.9988543,-5.8663535,-3.7165418,-3.279997,-2.0048702,-1.0747558,-3.2994666,-6.014061,-2.1015942,-1.6264155,-1.0317209,-1.9088868,-3.7775798,-2.9344547,-2.1177552,-1.554754,-1.4879501,-3.4016669,-5.394423,-5.157832,-5.0028954,-4.9569187,-4.534786,-4.188482,-4.8675375,5.837884,8.3159,8.366673,8.318276,8.399275,8.417956,8.364635,8.41845,-5.991108,-6.1324816,-6.0053916,-3.9530919,-3.659683,-4.4686675,-6.556615,-6.363202,-6.2528524,-6.47302,-5.798768,-5.64622,-0.16997086,-6.2747393,2.9800682,0.32139698,7.287613,7.4069986,-6.2221985,-4.2846217,-3.4577384,-3.8296254,-2.392782,1.4844979,11.221582,11.149869,11.150091,-6.1325283,-5.605316,-5.5078735,-1.0456908,-0.62417984,-0.9691089,-6.3428507,-6.5229025,-6.4535804,-8.105784,-7.5265794,-7.4576654,-8.005761,-7.2672606,-7.4065127,3.697015,3.6965036,-5.56327,3.6888518,3.7721024,3.6550882,-6.4443393,-6.3274784,-6.571791,-7.3369484,-6.3041835,-6.1857867,-6.257137,-6.3080335,-6.1670713,-6.123161,7.1216364,7.994063,-3.0300796,3.6187465,-4.1084967,-4.1078486,7.057543,7.058092,8.129618,-2.007998,-1.1779377,-1.1159786,-1.4509305,-2.0751362,-1.9098417,-1.1362779,-2.0399582,-0.93410975,-1.6804377,-0.9004729,-1.1230401,-2.2898195,-9.648932,-9.627512,-9.09888,-9.438125,0.5440489,0.12480625,0.594364,-0.16339959,-0.06337121,0.01569596,-0.49762753,-0.028581161,-0.029425032,-0.038440634,0.036965933,-0.02825371,0.07449289,-0.07279049,-0.8525519,-1.4768615,-1.4144396,-1.0871962,0.019702775,0.501079,0.41084608,0.33176753,-9.461021,-9.641761,-7.982219,-8.889055,-8.056134,-4.569055,-4.975837,-5.746338,-7.1164746,-7.329161,-7.133964,1.1995412,2.3878415,-6.881352,-7.2156873,-6.014249,-5.87394,-4.9530625,-2.2549503,-5.6234927,-3.738182,-2.2494483,-5.5165753,-1.0380708,-5.233294,-3.087069,-6.958257,-6.890552,-6.782525,-6.4700193,-6.0821714,-6.6640587,-5.9993124,-6.3124394,-6.7352304,-1.6708287,-1.4816651,0.31448278,-0.3107058,-6.7331247,-6.7921453,-0.8453293,-6.234317,-0.69424826,-6.4417815,-4.3945303,8.7426,8.701445,8.719097,8.628224,8.64272,8.800597,8.746573,8.724774,8.740372,-4.655269,3.2038732,-4.3715796,-3.9028926,-4.1931562,-4.3368893,-4.3327665,-4.4998546,6.3395023,-4.964272,-4.2112336,-4.0306144,-3.0401325,-1.2293222,-0.9936182,-4.1585712,-7.054057,-6.414078,-6.2100544,-5.7792974,0.7283989,4.0878806,-6.437854,-0.20226705,-1.2209649,0.40930593,0.59974736,0.68804026,0.97869456,0.6233863,0.41847998,0.09724456,0.1378711,-4.511427,-4.078838,0.024118701,-6.8016076,-6.1311355,-5.9146338,2.5286093,-5.7990055,-6.806762,-8.281381,-8.243187,-8.347138,-8.36565,-8.268861,-8.238498,-8.391856,-8.330991,-8.288398,-4.4029837,-6.098141,-6.1455503,-4.4917946,-4.62518,-4.62257,-4.460285,-4.4774857,-4.448243,-4.4960923,-4.402899,-4.5985885,-4.5194564,-4.3574715,-4.1940446,3.619071,5.915201,3.4494784,-4.5507936,-1.7952995,-3.853225,-2.863683,-3.197794,-2.5168796,-3.7850294,-3.274014,-4.770612,-4.1720357,-7.99008,-7.353194,-7.2358675,-7.2503266,-7.3486104,5.8034916,-4.5569406,-1.9124576,-0.34652674,-2.2565932,-2.0372043,-1.4494537,-2.2725303,-2.3958058,-1.4384366,-2.5379922,-0.8215526,-1.5066726,-1.900894,-2.0150054,-4.751348,3.4860327,2.967426,3.1251767,2.9156933,1.9054239,2.8525186,2.6929853,2.3043962,-0.4068124,0.024593828,-0.39611503,1.7425113,-0.32384965,1.9824735,1.7740338,1.8416159,1.2606719,1.7312915,1.315162,1.7349116,0.17901817,3.1023862,3.1612642,3.7268581,3.3955574,3.7156944,-7.229609,-4.9797177,-4.4175224,-4.9102635,-5.26706],\"xaxis\":\"x\",\"y\":[-1.0510684,-1.7596548,0.44502985,2.0036309,-1.5266111,-0.45218953,-2.0608308,-1.8645102,-2.386605,-2.2438242,-1.637846,1.2275776,-1.6984948,-1.2260041,3.8364584,3.718363,4.0215974,-5.697853,-5.9251037,1.7435029,3.487605,3.4369626,3.6264935,3.330909,-6.1461515,-6.1269546,-6.145134,-6.0967517,-6.0072136,-6.0609064,-6.1752677,-0.4054399,-0.25763085,-0.008601552,0.048759386,-5.6374626,0.1548031,4.9457245,4.742103,-0.022954939,2.8792675,2.8864102,2.7909286,2.61585,2.6951866,2.653582,2.620503,2.6856232,2.723318,1.5103595,3.4066257,2.9379253,4.540271,4.712252,4.324438,3.9297378,4.2426414,4.5234237,4.340047,4.198826,4.139251,4.215963,3.4722464,3.7218258,4.0857162,3.091336,3.6938128,3.494676,3.609818,3.0167258,3.0298367,1.7401325,-0.5758346,4.1102777,4.652916,4.513758,6.283472,6.161385,5.1515827,2.8374188,0.2727852,5.8758154,5.9615126,5.9332986,5.9525146,5.866202,6.012531,5.917841,5.7172956,4.1333117,3.8179488,4.4349084,-0.7706699,4.307372,-1.9493312,4.4517045,4.339269,4.059016,4.715534,-9.703901,-9.957959,4.2818985,4.234621,4.443912,3.481324,3.2212725,4.115916,3.4283812,4.2031183,3.4615157,3.4112024,4.0922103,0.12885892,3.9463468,4.3761983,0.08582571,4.2782316,4.088446,-1.3315934,3.0267308,-1.3165268,-1.745313,-1.4913182,-1.3904305,-1.8138402,-1.4906019,-2.1751256,-1.7049197,-1.7368897,1.8544061,-2.0248873,-2.78117,5.9505363,-2.3859744,-2.7236946,-2.787633,-2.6651132,-2.2944345,-2.869627,3.4192348,-2.0150573,-1.4481698,0.87121046,0.62600905,0.008950749,0.5483741,0.40490443,0.2573799,-0.14381064,1.4254129,5.419148,4.907851,0.2987208,-0.03030976,1.5279062,-0.87323433,-1.9192519,1.0253046,-1.3875233,0.18328933,1.9205002,-1.4203144,1.0611748,-0.06047788,-1.1073279,1.0438664,0.6573804,0.19953074,3.942517,-1.8540435,-1.8155985,-1.3173119,-1.1222527,3.9650652,3.3654747,-0.77299404,-0.59458244,3.2203047,-9.460871,-6.6337104,-6.476705,-6.476345,-6.480411,1.9605464,2.6951487,5.3001347,4.638157,2.6804578,2.3188825,1.6036688,0.19046485,1.2902303,4.4207344,3.5372622,4.475836,4.496753,3.3385873,1.4931214,6.7413554,6.3831186,-0.12336541,4.1886735,4.079158,3.3515048,0.32069904,6.3502264,4.556179,6.1732354,5.6480145,6.2126746,-0.17637414,-0.41307792,-0.9908998,0.9040258,0.88872015,0.9552445,0.9364062,0.92791444,1.027717,1.0777131,0.8687847,0.8731275,0.7903067,2.8265646,0.936678,0.57217616,0.8018869,0.30597883,0.4981287,0.12208678,0.8150144,2.362095,0.9770526,-0.85215807,-1.3987966,-1.2459284,-1.5272025,-2.3772748,-2.286165,3.36897,4.293087,2.5815408,4.533336,3.8479707,4.6633677,4.8092914,5.771342,4.940166,3.7607048,4.557551,4.157381,3.7162135,4.562126,3.8213615,4.280763,4.126067,3.8912401,4.9014974,4.476145,4.8514614,3.771966,4.056404,3.4313784,-1.1814036,-1.5687,-0.4448184,-6.2402706,-0.40571252,-0.43099228,-1.7175959,4.0869174,4.8935413,4.861786,3.3642108,4.302711,4.758811,4.222121,5.0702133,5.095617,5.1514225,5.2791414,4.9755073,3.5225742,4.712165,2.6700945,3.0686636,-1.3652409,-0.67409337,-0.8594991,-0.5407327,3.1997426,3.1890142,-6.78586,3.1665833,3.1231089,3.217614,0.77134764,3.0368702,2.8145068,2.381382,2.5631304,2.7792375,3.1839323,4.14799,3.9847422,1.7878525,-2.047905,-1.9449776,-0.80944103,-2.2711914,-2.3834832,-2.192895,-1.7470738,-0.84330475,-2.1436849,-2.3543801,-2.116247,-2.572792,-1.5480654,-1.8594158,-2.147928,-1.9785733,-2.3948662,-2.290704,-2.2140229,-2.4042814,0.27197942,0.29094264,6.1677723,5.683068,5.596638,0.017906718,0.5212581,6.77489,6.0781865,7.0622406,6.1583247,6.9022813,7.0133204,6.419233,0.7173657,0.3515224,0.7440492,0.6330406,0.23286392,0.37250742,0.63240355,3.6580958,0.4041212,0.4417079,3.5394142,3.6177492,0.342342,0.45989364,0.8075088,0.70690227,0.68880105,1.3473926,4.119953,3.2633805,-1.774813,-2.2852108,1.238375,0.3737276,1.4812133,1.5188382,1.3707833,-0.46827552,1.6121185,1.2194297,5.750745,-0.6744914,0.4915332,0.5774134,0.48087612,0.28730112,0.25200182,0.52122384,0.5698736,0.45931563,0.47122034,0.42003492,0.39492202,0.3780165,0.4464735,0.5036123,0.44680372,0.2337944,0.22049792,0.3753084,0.28148884,2.6416695,-0.45143175,-0.2765777,0.30835238,0.32810393,0.5228599,-0.6550042,0.24442984,0.0038963268,-0.14658128,0.8392602,0.65735996,2.634115,0.68474406,3.1081052,4.998972,4.6515083,4.726137,4.3142996,3.9348345,3.2346523,-1.4974321,0.44812495,-1.1639161,6.4153256,0.54015595,0.12735365,-0.43870398,-1.0813532,4.846138,-0.009549258,0.11595121,2.8706088,2.7384708,-2.0339067,-2.0483403,-0.17489552,0.4451655,0.59114116,0.09371079,-1.2738177,-1.7254263,-1.6857169,-1.9341439,-1.8641026,-5.758919,-1.7336189,-5.393697,-5.893098,-5.984559,-5.9936714,1.1490766,-5.7635193,-6.895178,-6.5488567,-5.744992,-5.5906134,-0.060495753,1.3289324,-6.265873,2.8246632,0.16800565,0.28391394,1.2865132,-0.7900539,0.7899278,2.4041286,-1.6813309,-1.9402521,0.7407784,-0.215221,-0.41198936,-1.0397557,-1.6201112,1.1407442,3.1910374,3.2615547,3.1237996,3.3174295,3.362798,-1.7724477,0.18584743,-1.8308536,1.5166897,-1.3368915,-0.4731211,-1.4470747,-1.9517715,-2.0236552,0.59247226,3.6577063,1.2560304,-1.7140597,-1.9041868,-1.0378244,-0.9101168,2.9453418,3.3111734,-0.7706765,-0.97993857,0.19677942,1.710087,1.6190056,-0.24802601,-0.20292503,1.8322426,-0.053970106,-7.837453,-7.462308,-0.26644963,2.1692293,1.6692812,2.2154915,1.8842591,7.100388,3.3331823,7.1800146,6.930887,7.112993,7.3568444,7.1368656,1.6057985,2.2977545,4.436115,2.1274323,1.7978781,-1.9311473,-1.993206,-2.028203,-2.0724003,-1.5878087,-2.0882137,-2.1589756,-4.1287956,-5.955985,-3.1429758,-3.670935,-4.0846434,0.4159924,-5.856613,-5.170698,-4.4539933,1.21612,2.9238553,4.962298,3.880805,5.203496,4.4808087,3.9593651,3.20839,-0.08651661,-0.42953327,-0.9007237,-0.8031832,-1.0247303,-0.8767274,-1.0861048,2.433387,3.0469065,2.897983,2.4768748,-6.2864103,-3.94958,-0.48693863,6.5948195,6.935394,-0.3569267,0.39442956,1.8088993,-6.187794,-0.8386648,-5.7915573,-6.2383394,-6.2637486,-6.934744,-7.151947,-6.6159916,-6.263308,-0.4585509,-0.70510554,0.76360625,0.7056791,1.016506,0.047398705,0.99148214,1.420047,1.3042898,0.9610304,1.5430117,1.6778803,1.4790719,0.56401205,5.0574346,2.5377262,0.7908875,3.363921,3.3805118,3.3419576,3.672599,3.7315423,3.719363,3.7409565,2.6086197,3.4758105,3.5276918,3.6979067,3.7019875,2.9073782,3.3965096,3.4163435,3.7442613,3.5866683,3.6731997,3.570044,3.625582,3.6918056,4.6670246,3.4129426,4.4159865,3.3520377,2.94761,3.3300471,3.154274,3.0195956,3.2781427,3.2930732,2.6655238,3.224234,3.3638802,2.6972668,3.4453037,3.0685787,3.1074386,3.3523953,3.642362,3.1137488,3.6551828,3.435146,3.694138,3.2348685,2.8791943,3.256814,3.296414,3.4522705,3.2630155,3.2623658,3.4017134,3.315948,2.5849307,3.4587898,3.6245801,2.8778305,3.1629965,3.3496995,3.4774852,3.6697433,4.560369,0.2858497,1.604675,-1.5194567,-2.292786,2.3031147,2.9396288,2.947978,2.9055896,2.9443479,2.4765828,2.5121057,2.78051,1.795434,2.6859348,2.7207978,2.01915,2.350066,2.9071836,2.9510155,3.0274193,4.659649,4.158823,3.6935966,3.449306,3.519988,3.6076448,4.3519607,4.9097877,5.000517,4.9692035,1.7760923,4.6769347,2.8665328,2.8971732,2.6975214,-0.0086280685,4.077269,3.5886571,2.7246192,2.6444979,2.7181628,2.7858043,2.8309677,2.7010894,2.7824984,2.7637086,2.747974,-0.051364996,-0.051284317,3.8273973,3.8234677,2.8587277,2.63844,4.0388007,4.496127,4.117129,3.551959,-0.21237044,-0.034441445,-0.08068112,-0.29039,-0.008865174,-2.330644,-1.7254384,-6.1305814,4.1974998,-0.7562829,2.995242,2.8351095,5.9431996,-1.2742354,-0.807178,3.6982343,2.8536074,2.435085,1.737676,-0.9225323,-1.6445411,-1.4101107,-0.27327734,-9.481304,-9.658739,-1.995219,-2.2245684,-2.3236063,1.3459847,1.723908,-2.2595065,-2.365024,0.8298693,-1.5831684,-1.8433737,-1.6418833,1.860842,2.9732025,6.357818,5.7390423,2.1101897,-1.291532,-1.0597897,2.3969593,0.6899206,3.13303,1.0960515,-1.1062272,-2.1928186,-2.386522,3.36304,-1.9458032,-2.4576926,-2.389835,-2.0269141,-2.439843,-2.3002303,-2.3183918,-2.4634447,-2.4761117,-1.5706253,1.79092,6.049666,-0.70913756,-0.15720011,-0.14984205,0.09846428,0.894773,-0.42599118,1.9676263,0.9178493,2.5708973,0.6490518,-1.310648,-2.1278622,-0.8606857,0.55368406,0.19643204,0.417892,3.7771564,3.844279,3.8014717,3.8634355,3.5965862,0.517124,1.8024521,2.5608892,1.2627133,0.902532,-0.59315634,1.1095793,3.919293,3.1953144,0.57700217,1.6492486,-1.0687238,-6.8868475,-6.642454,-6.7694993,-6.6659193,-6.4059587,-6.6353188,-6.8591666,-6.8500257,-7.0550594,-6.770576,-7.042132,-7.061883,4.388953,-6.9874716,3.3335943,-6.892274,-6.9522076,-7.239889,-2.2890408,-2.3817592,-2.3240566,3.906799,-2.2745354,-2.042867,-1.4111067,-1.6200643,-0.6791543,-2.2587104,-2.2449172,-1.5554467,-1.5103155,-0.9379911,-0.18129764,1.9099363,2.1840272,-9.295374,-0.80535513,3.4883761,-9.341065,-11.5607195,-11.739999,-11.755655,-10.470855,-12.054831,-12.09798,-12.068672,-12.017489,-11.992601,-12.040397,-11.992269,-10.30777,-10.507157,0.8161677,1.5320886,-0.5585422,1.422468,6.09098,-6.5684075,-3.6424913,-4.542241,-4.821784,-4.8048325,-4.925562,-5.4454966,3.1322799,4.015798,3.7332706,4.3413076,2.9779866,1.0380465,2.582923,3.152337,2.4263258,0.77273357,3.2310853,3.4481204,2.7554524,3.2202377,2.1905982,1.6261889,0.77341294,1.0932391,1.2504373,1.0688362,1.3377675,2.6004643,1.0258427,0.92138606,1.0790517,1.7085241,-5.8288226,0.963024,0.5927754,1.7238269,0.9396168,0.8398056,0.42257583,0.5162197,0.9508189,0.89360565,0.70718145,0.5961197,1.0670339,0.92009956,2.6337361,3.062398,2.4975402,2.7098327,2.1835158,2.673418,0.82889616,-0.45488775,4.54881,3.1612885,6.0393605,0.7118845,5.33438,5.100748,5.2808123,3.5125713,4.2260284,5.3227663,5.1925282,5.3597775,4.932851,4.368122,-2.1319454,5.0100203,4.288648,3.257546,1.8595268,1.8882614,1.9765683,2.4730802,1.5551553,2.462162,2.95611,0.4248164,-1.0590092,4.639336,3.106841,2.8408434,3.0519423,4.571927,4.9977565,2.9353867,1.2142402,0.40889168,-0.6520816,0.69704604,1.7718006,-1.7654805,-0.15140441,2.4518788,2.37262,-1.6315808,-1.7566862,-2.0583868,-0.25425485,-2.8783057,-1.5392115,-1.6309396,-1.2299428,1.0606452,-0.4256969,-1.5832273,-1.1270294,0.92019403,0.96741986,-0.05206091,-0.21517555,-0.28818217,0.6972348,0.53853524,-0.6730514,-0.5326891,0.70883673,-1.2123483,-0.5497192,0.08562303,-0.8242689,1.188011,-0.29142702,5.862238,6.291492,0.08741733,0.6116943,0.30476487,0.22934036,0.33663064,1.4034679,1.176422,-6.195943,-6.277161,1.5452158,1.8038017,1.0865198,0.57542044,1.2489783,0.6032357,0.97157013,0.55986255,-0.0031193881,0.5719471,-1.8309271,-1.8578533,-1.531002,-1.6767519,-1.4325238,-0.7331363,1.4962008,1.1884354,-1.8442758,2.710149,3.7642145,3.6274574,3.533214,2.545344,3.4468668,7.842713,3.250966,3.1643288,0.16859432,1.0669311,1.7179675,1.6133236,1.4987185,1.118311,-0.85731614,-0.532664,-0.5955209,-1.5593548,-1.8709906,-2.0249856,-1.8555796,-1.9002777,-1.7781785,-1.4464952,-1.2033566,6.090845,5.7059135,6.002724,5.987138,6.0519633,1.3242146,6.0726657,0.17989416,3.709065,3.6544077,2.6210208,3.262817,3.0193224,3.2790043,3.429997,3.6343145,3.375188,3.5403469,3.680016,3.775196,3.662373,3.505451,3.674622,3.5907555,3.5024958,3.5122018,3.3043554,3.761288,3.6072097,3.3967671,3.3015316,3.7276483,3.7635646,3.4668422,3.177927,3.377091,7.2860727,2.8652916,3.3857098,6.7677794,3.6360633,3.6325572,1.3688384,3.6810112,3.3319097,3.5872483,3.8565164,3.753505,0.65495825,2.3827684,3.088325,4.4264584,4.057729,3.2318964,3.0687706,3.1484556,3.2919807,3.1548123,3.021566,5.5672727,5.437743,5.7572045,4.9035296,4.201788,3.6963375,3.2804904,2.2663627,1.7624674,1.8144203,2.4407442,2.5067918,2.3006907,2.498931,2.5151808,2.3129005,1.0747982,2.141734,3.2853646,2.6537,2.5385647,2.3801143,2.3247855,1.6297585,3.5795894,3.1835928,3.4097931,2.069401,3.6320193,2.8806345,1.6063604,6.783189,1.0195167,1.8678983,6.461874,-1.8241249,-1.8397572,-1.5898876,-1.7064782,-1.6947204,-2.173134,-1.141998,2.2986846,2.6655393,1.7654159,1.665887,-1.9969935,-1.8192906,2.4662478,0.42102957,0.008202389,-1.3377817,3.3455136,0.3960039,1.6227672,4.6007195,-6.5836577,3.5986893,-6.0978837,3.473193,3.1676097,-1.7242113,0.50657356,-0.032986667,1.2998667,-1.3309302,2.1888027,2.4985945,-0.003243615,-0.49010837,0.1283577,0.1895051,-4.309268,-5.579512,-5.860692,-3.596946,-4.2640743,-1.564761,1.2771819,1.6940103,2.0548546,2.948275,2.9874067,3.1481576,1.5621921,2.0289116,1.688539,0.0070173084,-2.035875,-2.3316853,-2.2420046,-0.07368133,0.6986068,-0.23377797,-0.2879354,-0.47673783,-1.8279138,-2.570675,-2.568917,-2.2817833,-0.06365872,-2.3649914,-2.275484,-2.531302,-2.507449,-2.4197724,-5.41064,-2.3513596,-2.386857,-0.4319749,0.009160099,-0.044872686,0.3693926,0.8875199,0.9957716,0.6783712,0.80811197,0.09901959,-1.8106457,-2.1045644,-9.548642,-6.6867623,-9.352192,-6.502848,-9.329789,0.89227307,-0.5303128,2.9842575,4.033467,1.3300804,3.8812678,3.3975801,4.197677,4.225617,4.3847084,3.5675757,4.2739897,4.000997,4.3789706,2.635567,0.52190244,-0.325588,1.8826271,3.8955085,1.1939608,4.6072245,3.9837215,-0.15126626,2.1861134,2.9425566,1.298742,-9.489349,0.62297297,0.72925776,0.5532097,1.7611126,2.338438,5.846206,5.768607,5.86223,2.666571,3.5339503,3.9707203,-0.13746887,5.62301,5.6350985,2.477612,5.2909317,0.6084314,-0.82032055,3.6975648,-0.8995488,-0.9157509,-0.7843777,7.053791,7.0839443,2.5109465,6.9915047,7.078044,-7.5571446,7.205205,7.4952316,7.262028,-5.597204,7.4133086,-4.556674,3.9335878,-8.152839,7.4716964,7.304818,7.2113667,-2.0747118,-2.3327887,7.206738,-2.8698125,-6.838852,-5.6101174,-3.9264412,-4.953673,-3.5062993,-2.288464,-4.146446,-5.040541,-4.633772,6.9511604,3.2336977,4.2058015,4.542412,4.60108,5.252021,6.9741282,7.7876124,7.845316,7.543429,7.7047935,7.330603,7.474438,4.221916,4.371749,6.842535,6.711261,6.9352417,6.7673135,6.861458,6.6819396,6.651189,6.6372533,6.389197,6.617578,6.797551,-5.844935,-7.0721574,0.93929535,-7.4141936,-7.8740582,-7.9654026,-7.9120026,-8.202982,-8.057464,-8.007213,-7.9471703,-7.9957976,-1.7154474,3.0932584,2.134379,-1.7999774,1.9548988,2.1918213,2.0982614,2.0937998,1.1678085,-1.8053852,1.942148,2.040216,2.3771062,-7.223582,3.0204248,2.7197876,-6.913832,-6.930065,-6.670897,-6.6783557,-6.9148407,-6.8928227,-6.3421683,-7.2144237,-6.6519365,-6.3234773,-0.44327733,2.961535,-6.4906654,-6.70603,-6.645179,-6.726454,-7.1736298,-1.9936101,1.3224604,-2.1032176,-0.89248675,4.306437,4.067356,-1.3117583,-1.304754,-9.7216425,-9.96367,0.9079098,1.7675301,2.1063612,1.5130076,0.4076439,5.712522,0.81858236,0.37037882,1.6239078,1.896587,0.80360746,1.3127686,2.0565314,2.7551801,3.9000022,3.7622576,3.850759,3.9144392,3.9238667,3.9032586,3.983055,3.0140443,-0.49409994,2.8150308,3.0970533,2.9928074,3.245935,2.487079,2.830511,3.9329839,3.8623238,2.9784667,-0.34204194,2.8946166,6.477184,3.0924006,2.9872775,-0.83323103,2.600832,1.799108,-6.427628,-6.489279,-6.712922,0.21697234,-0.9743374,-1.756968,-5.065438,-2.028568,-1.9745129,-2.1008458,0.4241066,-5.594479,-6.273679,-4.850191,-6.8921747,-6.6229124,-6.541629,-6.5220203,-6.830686,-6.8616986,-6.668864,-7.2638764,-6.7570624,-6.9041696,0.56212664,-1.8059012,-1.8891865,-1.5387923,-2.406473,-0.77566826,0.6390605,-6.137034,-6.1759787,0.37256366,-0.56262714,3.307008,-6.881184,-6.8094726,-0.019275026,0.7568944,-5.9085107,-6.040135,-0.23867045,-0.34525898,-1.9753991,-0.87599987,-5.4372315,-2.430896,-2.2779152,2.6968207,2.4626317,-1.946013,-5.384965,1.8437247,-0.37911388,-2.1730437,-3.7774243,-4.119012,-0.27027062,-5.9334397,-0.215042,3.6687446,3.841992,3.5094988,4.019693,4.0629873,4.3726425,3.7160485,3.7539263,3.6509886,2.6584516,2.6977577,2.225268,2.841017,2.7639418,2.8004336,3.8070319,3.8507078,2.520452,3.5169175,3.8802989,3.9482791,3.8555627,3.420724,3.812717,3.4462771,2.7659814,2.998993,4.177357,4.1943603,2.8114703,2.8534024,2.9963086,2.778982,2.0556853,1.3007083,1.0034964,1.80035,1.6158301,1.7662342,2.540181,4.3324685,5.7174077,5.758882,2.366874,2.1474063,1.5726974,2.3052611,2.1949277,-1.9475751,-2.021812,-2.1136765,3.4288476,3.604473,4.498365,2.8955336,2.8628821,3.8050497,2.7566934,3.9548259,3.8201158,3.7221558,3.8500082,-0.8236918,3.030074,4.2411385,3.1112227,2.9367855,2.800988,2.9931006,2.9374268,2.7816916,3.420486,3.2968843,3.3546314,-1.779944,7.5510435,-2.3000546,-2.3265169,-2.7420897,-2.53856,-1.3599484,-2.3485112,-2.3568435,-2.2342105,0.9247111,2.6217132,1.6696154,3.6814075,4.420276,1.444367,1.4068632,0.7263051,-1.8499695,-1.9878671,-1.1755431,-1.1835853,-1.1218566,5.7083917,-0.9842288,-1.5355101,1.6563423,-1.1852067,-1.53708,-1.1769872,2.8325095,2.7246418,1.0136405,2.135764,0.29111052,0.11058955,0.32969624,0.37756816,3.7668264,1.5906371,3.6571183,3.5924459,0.28927666,0.550523,0.3575335,0.38474435,0.38836834,0.31801167,0.21768177,1.3612882,-1.2410575,-1.6416676,-2.0346997,-1.978872,-2.156394,-2.0682998,-1.7603892,-1.8074161,-2.0927706,-2.3228724,-2.4105427,-2.1181474,-2.1739573,-2.050739,-1.4533029,-2.245481,-2.1277308,-0.6925992,-1.6187452,-1.0410587,0.3574814,-0.016588962,-6.268869,-6.4861655,-6.4667993,-6.362786,-6.472665,-7.3371553,-4.878041,-6.562559,-5.398488,-5.452191,-6.745718,-6.480152,-6.4154325,-6.161013,-7.6729918,-7.7866416,-6.9636083,-7.025503,-0.4125059,-1.0810611,0.50331444,1.2647158,0.72511804,1.1177372,1.9583086,1.6219935,0.6957983,0.93514174,-6.6412854,-11.749043,-11.7254715,-6.534305,-0.07138354,-1.3859056,-3.8552382,-4.491637,-6.6619496,5.544187,6.2812877,6.1092806,6.0370474,5.81811,5.196707,6.1443796,-0.25437653,6.7924705,7.0736256,7.1978297,6.7232466,6.8585505,7.0104656,6.07147,6.7646503,7.136945,7.3444057,7.886868,7.7779803,7.9046845,6.425986,4.694208,4.3688474,6.5346966,5.9683647,-0.4445782,-0.055765145,-0.5409642,-0.7181886,6.0925264,-0.46981224,6.141246,5.774555,-2.1884606,-2.447172,-2.0984902,-2.323277,-2.41738,-2.4401293,-2.2947311,-2.3228877,-2.492347,-2.4305608,-2.3194656,-2.0314136,-2.3315096,-2.355011,5.8744125,5.8413844,5.7786098,5.8470054,-2.3916564,-2.3975472,5.7939196,6.9681473,6.2479587,-2.4765801,5.7218337,-5.426542,2.57796,-2.3300295,-2.3704004,-2.4067407,-1.9883468,-2.3808594,-1.443475,0.8150564,-1.434462,-2.0607727,-2.2156775,-1.8670987,2.6024864,2.3312566,-2.3791947,3.1676335,0.56096214,-0.8411602,0.6380891,2.2379,2.6363416,2.8367743,2.8528416,2.4566712,2.2466362,3.481264,1.3732296,-1.379235,0.87477434,2.683399,2.8248532,0.04954004,1.9127276,6.0623593,6.6144276,6.4809303,7.1498265,6.8748946,6.629131,5.9876976,6.427492,6.393741,6.631654,3.562717,0.22344103,6.399763,4.657629,5.199722,4.8174067,4.298679,-2.2348125,-2.1433375,4.5240374,5.071229,4.6869164,5.027777,1.25688,1.7067455,2.6983907,2.3687115,2.5337481,4.829032,2.81921,4.656048,-1.3612708,-1.9568459,0.91737455,0.4906519,0.79643196,0.6788312,0.73805445,0.8476146,0.9176819,0.8455349,0.47730106,0.7198223,0.6028041,0.87409145,-11.368753,0.67710674,0.6205473,0.69920737,-11.21755,-4.4549265,1.4396034,0.942425,-7.7750206,-4.5300727,-4.796145,-4.4363675,-6.8321157,-4.615769,-4.66871,1.8203679,1.8629879,2.8947337,2.1737947,1.988102,2.9096682,4.4730225,4.733401,0.8228864,-0.18275219,-1.2266223,1.1333488,-1.2672254,0.7352598,-1.5882876,1.7877723,-2.7343197,-2.4838507,-2.7951913,-2.7807598,-2.8195884,-2.5249922,-2.853083,-2.780204,-2.6062093,-2.850247,-2.807832,-2.8153615,-2.8358612,-2.844363,-2.3019316,-2.7637815,-2.7788777,-2.7129312,-2.772164,-2.5118854,-2.7949028,-2.7246747,-2.7356668,-2.6202404,-2.8298135,-2.771876,-2.8420935,-2.6014285,-1.926614,-2.5787177,-1.9069402,0.29537106,2.8960617,0.41860038,1.2757419,-0.7615684,0.53015107,-1.9117626,-1.0935934,-2.3793793,-1.9924407,-2.2585769,1.5872045,1.884881,-2.2903333,-2.6247802,0.055750806,-0.77500015,-1.8211535,-5.819339,-6.1655216,-4.78765,-5.316119,-2.1002455,-4.4220686,-2.9953368,-4.259887,-4.504789,-5.9621882,-0.88226295,-1.4704854,-1.292518,-1.6192418,-1.6371304,-1.566403,-1.5981228,-1.0063386,-2.1013122,-2.2047467,-1.5339807,-2.3339236,0.85860777,-0.50361437,1.6228136,2.646993,1.669152,0.14126118,1.0441667,-0.52710396,-1.5364234,0.8084002,-1.82547,-1.4946074,6.3749714,6.194484,6.2402263,6.73186,7.1672277,7.3293204,6.568372,6.5686107,7.090993,6.5242143,6.4112277,6.9517765,6.401938,-0.20228703,-0.07203196,-0.07619325,-0.20915967,4.564151,4.1340456,4.233825,5.758554,3.6872427,4.0125217,-0.7596916,-0.13435404,-0.010846825,0.17735486,-16.044146,0.29589605,-1.0575751,-1.4051868,-1.3278972,-1.0278462,-1.1895094,-9.463499,4.004113,4.0119505,3.9934573,3.5851476,3.8778167,3.8639386,3.9113295,-9.490616,0.35134676,1.8477069,2.2990115,4.20291,4.7234797,3.3073108,4.850927,3.7356684,4.214622,3.9904969,1.1785167,-0.5887794,-1.6900585,-2.138482,-2.3791,-2.49345,-2.6011615,-2.573864,-2.8080816,3.2180107,-2.4142501,0.570294,-0.3063561,-2.0814888,-2.125366,-1.655524,-1.3278003,-1.4326458,-1.2728345,-1.2816948,-0.63809675,-2.3347113,-0.8709168,6.821361,4.4243712,4.6731176,2.4908304,4.525265,2.888758,4.0648537,-1.4515709,-1.9093736,-1.7976271,0.85501,2.0665843,1.9618013,2.784166,2.1147952,-0.47731403,1.7207078,4.3863344,4.186686,4.2281265,-0.66177416,-0.22541323,-0.49309236,-0.16202354,1.585593,8.146636,4.0388603,3.5333588,2.4676552,6.404637,0.3154353,6.6768265,-1.818864,7.49081,7.212975,-1.6413537,7.431194,7.356078,7.4969306,7.570621,7.42997,-2.3609078,-2.506235,-2.420336,-2.4970968,-2.9822998,-3.0780253,-2.921471,-3.0012217,-3.1043086,-2.9307425,-3.1374059,-3.0512335,-2.9216766,-1.2956994,2.0625813,-5.6747613,-5.93399,-6.4991865,-6.0724573,-0.6015895,-1.2953815,0.37820122,0.8384309,4.3976293,0.7506912,-0.4958956,-1.6647849,-0.5139908,-0.5915701,0.15011334,2.1602776,0.9354889,4.1939564,0.585848,-11.172227,-11.461097,-11.864944,0.36389473,0.43419513,4.0053625,2.3201377,2.2249274,-10.555299,2.1725721,-11.675859,1.1243477,-0.4273952,-1.7204138,-2.4038002,-1.0319571,0.8559373,2.463069,-1.5003792,-1.4088014,-2.477269,-2.434272,0.6378806,0.022764785,2.045015,0.5324627,0.21396077,0.2721319,1.7837254,0.22062558,0.7359533,-0.46478304,-0.2427289,0.9532719,1.1111027,1.0421052,5.4926896,7.9150443,1.8839214,3.2227492,-1.4106368,0.51707244,-0.25770965,-0.084441096,-1.734104,-2.2479386,-6.0419827,-6.048391,-5.46341,-5.4826527,-4.748298,-5.5574665,-5.6250525,-5.6591034,-4.662277,-5.90382,-4.3544106,-4.502351,-4.3141932,-4.881268,-5.0349674,-5.8320556,-5.0605273,-3.3228292,-7.687813,-0.31406972,0.81676614,3.3482091,3.3895137,4.077919,-6.1995873,-6.266374,-6.277263,-6.2102265,6.597067,-0.7798479,-0.5382817,1.6865622,1.1875148,1.7188784,1.6218241,1.6523317,1.2110436,-6.0051875,-5.5579405,4.148557,-5.1656437,-5.6985974,-4.6504283,-4.873098,-4.566193,-0.25303596,-1.3081287,1.240586,1.2876251,2.6083052,2.3219154,1.8413788,1.5664507,1.7406288,2.5195045,1.5783459,1.5574986,-0.6070661,-0.7085474,-0.7027116,-0.7820144,-0.75561726,-0.7579738,-0.8434099,-0.7022551,-0.7206193,-0.72666943,-0.6861789,-0.7062893,-0.7319645,-0.7193161,-0.74769807,-0.72732353,-6.1042686,-0.60288775,-0.47025174,-0.52989167,-0.67799675,-0.6975646,-0.7498617,-0.2193524,-0.4990878,1.2029278,-0.39869782,0.20943758,-0.21828042,3.921297,-1.274263,-0.081480175,-0.650263,0.361364,-0.59691024,0.4280132,3.1619716,-0.36679083,3.1334503,3.8763192,4.223164,3.7326362,-0.79839945,0.3118879,0.20473635,0.8106829,-0.8383365,0.9308601,0.26059243,1.7853463,5.537858,6.2622,6.145678,3.4248226,0.80243343,0.02917954,1.0849047,0.70071495,1.4689732,0.8575126,-9.353871,-9.777318,-0.44514573,0.3395489,0.86418927,0.92138565,3.4100764,-9.920405,-9.45802,-0.8664075,0.36110577,7.567666,7.6414604,3.3648171,7.4036136,7.7457957,-9.448299,-9.525436,-9.614899,-9.706756,-9.830901,-9.5900755,-9.629157,3.6268978,4.1013556,4.0908313,4.2296543,2.243505,3.7270622,3.897743,3.6704144,-5.055269,-2.9260416,-4.232362,-4.3837924,-4.756065,-4.6006255,-4.535501,-2.7335653,-4.94299,0.24396682,-1.1136571,-2.3696847,2.4049563,1.8750525,-2.0542588,1.0815196,-1.7201043,-2.0152779,0.1765745,1.1539665,-1.1322905,0.41636038,0.5386076,1.934467,0.5565035,3.8727417,0.5149853,-2.338245,1.3906893,-2.1911335,-0.047724575,-0.66380614,-1.7337987,-1.8742077,-6.733109,2.920358,-6.9022827,-6.4991503,-0.47827217,-6.7656536,-7.3060465,-6.632852,-7.0417256,-7.5292954,-5.5488873,-6.7129264,-6.8161135,-5.825997,-6.169801,-6.219501,-7.1364617,-7.049356,-7.0213428,-6.3885794,-6.9971476,-0.7467633,-0.7604376,-0.7413702,-0.7798086,-0.9665701,-6.2500668,-6.231238,4.1618896,-0.8068333,-0.7285069,-6.3886256,-6.470129,-6.4573936,-6.409426,-6.393068,-6.431011,-6.3420854,-5.778327,3.5383353,-6.6441813,-6.445381,-0.77836514,-5.6767807,-5.772669,-5.7324624,-2.1257725,-2.331222,-2.2217824,-5.717581,-5.6712723,-5.9195414,-5.762687,-11.0996275,-10.833553,-10.31244,-11.406494,-11.149867,-11.479814,-11.19012,-10.429182,-11.380855,-11.268335,-11.00969,-10.982765,-11.593177,-11.834457,-11.699734,-11.357284,-11.338256,-10.460972,-10.916227,0.47791103,-2.1233506,-2.2340624,-1.7812992,-2.4565244,-2.5039473,-2.2251627,-1.6755207,-2.4161775,0.2754084,-3.2134702,-3.2119753,-2.5189583,-3.1316931,-3.3193038,-2.303995,-2.5942883,-3.0762935,-3.2764065,-2.561382,-3.1602309,-3.2111902,-2.1776261,-2.702631,-2.3621693,-1.9174831,-1.9904882,-2.4275708,-2.405977,-1.2815046,1.5675335,1.333596,3.1981738,1.9637494,4.060731,3.4785068,3.1777215,1.8850515,-2.1784065,-1.5570438,-1.622875,-2.251805,-2.372492,2.1340237,-2.387319,-2.324916,-1.793938,-3.1509001,-2.3348818,-1.5429528,-1.9815245,-2.2418206,-1.2053185,-2.2578723,2.0711248,-2.3680477,-2.3808696,-1.9636705,2.6100485,-1.5316855,-1.1652125,-1.5121928,0.96753615,2.630936,3.9575956,6.718744,6.6145105,6.8911877,3.7909226,3.8760517,3.62348,3.7364888,-0.43295234,3.7472708,4.2624145,4.2527604,4.110371,3.1141284,0.5588,0.62139726,0.6965275,2.2223585,1.3114321,0.8245627,2.4029238,1.4646298,2.7961438,1.0622156,0.95007324,1.4858497,5.026721,5.6351366,6.3650837,6.087756,3.2875211,5.683504,5.5032063,5.5883975,2.811051,3.4502256,3.4015057,5.2963576,5.9273205,3.539116,0.377309,2.1738098,1.3042955,1.5104821,1.5730451,1.2905016,1.276404,4.719921,4.012143,1.4888664,1.508346,2.8514218,2.411244,1.5352051,1.5828447,1.6237814,3.5840094,7.160362,0.3926782,4.4515977,1.3798938,2.5345879,1.2586976,1.2174782,2.201145,1.1633495,2.6160247,2.7538428,5.0307765,4.9067097,4.5180006,2.7971268,2.5573072,2.675187,2.7734869,5.123816,2.8514237,2.6827888,2.770951,2.727727,2.7568896,2.8887594,2.0996745,2.6550574,2.4401095,0.5251091,0.3079504,2.6079497,2.691611,-0.6753451,-11.877943,-11.536719,3.452604,4.082784,4.3574343,4.05753,2.3302004,2.132765,1.7751077,-0.21600361,-11.703987,-3.4868486,-3.4152741,-2.987966,-3.2064698,-5.5874505,-5.773499,-3.0026994,-2.947198,-0.0811813,-2.8632429,0.57760036,1.045827,-0.11374552,2.0679116,-0.4716673,1.8841785,-0.76038766,-0.56313294,3.2468445,3.293518,1.3996178,2.5090404,-0.28284505,0.13908094,0.0018902125,-1.4014637,-2.0976348,-0.09967564,-6.188501,-1.7494031,0.34976673,0.4549062,2.2792487,0.90085775,-1.6711624,-11.030293,-11.2579975,-11.367664,-11.389185,-11.491516,-11.4715605,-11.485298,-11.439965,-11.425463,-11.33695,-10.478714,-0.7404255,1.4666582,0.16480893,0.103233084,2.7929115,-0.24742481,-1.3049268,-0.29905006,-0.5373239,0.1607369,2.6743767,0.57497776,2.6685576,2.8400276,3.454582,-0.594819,3.224909,3.5847228,2.6931214,-5.8516583,3.0611095,2.5935543,2.7571058,-0.09512067,-0.81441355,-0.40404603,-0.8302505,-0.54778886,-0.5597089,-0.36455685,2.0401216,2.8890803,-1.316023,2.318258,2.1412935,2.722964,2.7641017,2.4110675,1.8969566,2.3663547,3.369463,3.3671553,2.1917534,-0.28515062,0.05804315,-16.045467,-0.36212975,1.0244362,-0.26310813,-0.7026124,2.9161894,1.1307455,2.1195927,1.5224968,-0.2174064,-0.09198844,-0.26133785,-0.5075107,1.8709085,-0.8950669,-2.0778463,-1.3352406,-0.77573246,-0.7198134,-0.7955121,-1.5735776,-1.2889911,-0.50203604,-0.58741665,0.5997942,-0.68641984,1.31742,-0.657648,-1.4880494,1.724779,-1.2559971,-2.1639085,2.5654967,2.8673592,-0.45193347,-0.7718951,-0.42757007,-1.6523192,-1.1845733,-0.74051434,-0.51293725,-0.12585853,-1.0893847,-0.7613689,-0.5101523,-0.2500596,-0.52567565,0.6267688,0.88092816,1.6636542,-1.6584681,-1.6039943,-2.189325,-0.91724795,-2.1314466,-1.7167833,-1.230543,-1.6069144,-1.50959,-0.77990806,-1.230927,2.9229589,-1.4541337,1.9582268,1.8795593,1.8419343,4.1329613,4.2779408,-1.8553189,-2.3941262,-2.2932913,1.4846704,2.7830987,2.5822635,2.5755403,2.213807,2.462271,3.5484455,3.6223438,2.1843054,3.4251692,2.6152024,3.644969,2.0081437,1.9993473,4.203627,1.8119705,1.6189454,-0.3961433,-2.2353399,-1.766898,-1.0520484,0.67632914,1.932796,1.852977,-1.993746,0.9152352,-1.881767,-0.8428437,3.8085806,-1.3017902,1.8460643,3.959299,4.083427,3.1937428,3.1097796,3.9228776,-1.9097648,-2.2849972,5.5537295,6.356355,5.7708106,6.1437664,5.8403177,6.143851,0.9032379,-0.33827874,6.621494,6.8713765,7.283893,7.1479774,6.95332,7.257084,6.406077,6.965545,5.758329,6.6742187,5.7554364,4.7108393,4.376247,6.871496,6.762068,7.146726,7.8583965,7.489909,7.517902,7.46528,7.1582074,6.761938,4.626577,4.5193667,6.610974,5.5081315,6.064286,0.76660645,-0.6290436,5.961459,-1.0440493,0.30292448,-0.33827245,6.037289,6.654813,7.1948466,6.2603045,-11.337919,-11.8324995,-11.655203,-11.432425,-11.716429,-11.815116,-11.7818365,-10.492994,-1.782725,-0.15973961,-0.43557948,-0.010160752,-9.376874,-9.406041,-9.408601,-9.382311,-1.3654978,-1.5249474,-1.3732439,-1.4746785,-1.253818,2.3832548,-1.3999729,-1.4578003,-0.31599343,-1.4270685,-1.4364132,-1.5050412,-1.5537647,-1.3937109,5.444964,5.1909595,5.26461,5.3086524,5.7409363,7.1196623,5.242914,-0.27773136,-0.02425178,-0.42523354,-0.1551914,-0.13660745,0.19404629,2.9396796,4.0572257,4.370286,-0.5767044,2.6396153,-5.793334,-0.1437179,-0.17569908,0.04332396,-0.39860106,-0.6468731,-0.048978332,-0.4567514,-1.4207102,-1.4906253,-1.2711216,-0.6587676,-0.9006223,-0.6004137,-1.112686,2.3337326,0.39063537,-1.5648158,1.6749415,0.582742,1.8733076,1.771438,4.113213,1.1222326,1.7869337,1.7763788,2.3529758,1.9004365,4.284432,2.2915745,2.0772119,4.248933,0.2769196,-1.7582088,2.5285678,-1.4876865,-1.6411432,-2.1125522,1.5693333,-1.5566897,-1.9320751,1.1589468,0.96381044,-1.8759364,0.06320574,-0.37610736,2.474518,2.5624123,2.8093343,2.7163546,2.5414746,2.8138914,2.5648375,2.553638,2.409709,2.5258327,2.8190858,2.488346,-0.39701268,-6.8645663,-6.735628,-7.1096997,-7.0982165,0.10173038,0.15629919,-6.7837787,-6.641871,-0.4861613,-0.5432044,-0.18352541,-0.42537352,-0.44628143,-6.0904164,1.823822,2.7778132,-6.1711535,-6.161134,-6.292823,-6.337516,-0.82003224,3.0529315,-6.219453,-6.113766,2.8305676,-0.6866063,-0.99536467,3.4288995,3.5620003,3.6770556,3.6736577,2.3963656,3.3876166,3.9015994,3.796437,3.491917,3.9993432,3.6994889,3.579987,3.5648441,3.31008,3.593575,3.844977,3.6513965,-0.12004809,5.9464254,5.9163485,5.9196005,5.845509,6.0081754,5.908482,2.006025,0.8267576,1.5180607,-1.7429843,-2.0123467,-9.344822,0.58659023,2.7621658,0.9585412,0.7647924,0.6773726,-9.582365,6.4214435,6.5007873,6.850283,6.913427,6.451469,0.24552897,-0.5684361,-0.596936,-1.0845739,-1.0392284,-0.6904849,0.19433592,5.8509636,-0.10958546,0.38670686,-0.04159302,-0.47617143,-0.44969857,-0.12613593,-0.55492663,0.40356502,0.9241471,0.79878944,0.057540506,0.5991557,-0.0013643435,0.26244593,0.48645627,0.07349156,-0.6367988,-0.09145644,-0.17075536,-1.6889986,-1.1851481,-0.35462892,0.71604687,0.8228888,0.77403367,-1.434901,1.764802,2.3158965,3.159063,3.817377,3.6258528,2.7984164,4.162491,3.2710829,4.1994057,2.5050461,2.9846776,3.413626,1.5894519,-1.8709247,1.7864118,2.0006847,1.7491171,1.8650932,1.9329795,1.9087462,1.8357662,1.6686788,-11.287089,1.7270864,1.8246609,0.9957976,3.6710138,1.4698733,-0.09912037,0.7995264,1.3559815,1.8247687,1.5955173,1.7753245,2.5481715,3.2110417,-12.0165615,1.7013069,0.6553571,0.9388795,-0.006737422,0.85154706,0.4722772,-0.14929433,-0.3340811,0.43912083,-0.2994441,-0.6213428,0.15228082,-1.9944412,-2.0092227,-2.1221688,-2.3100045,-2.0667744,-2.0149925,-1.8677279,-1.7211689,-2.167279,-1.9368047,-1.9264612,-1.3986001,-1.7807178,-1.9625646,-1.9321339,-0.081974216,3.4553385,4.6148014,4.3565373,-0.8243039,-0.91071624,-0.5662181,-0.79057074,-0.16482213,-0.4421834,-0.53288144,-0.861366,-0.63694566,-0.7680321,-0.17415357,-0.37977973,-0.60352194,-0.84931606,-1.5911869,-1.3098481,-1.8100207,-0.32938808,0.2386093,1.7167838,-2.321087,-10.870641,-11.184612,-11.488961,-11.691251,-11.78943,-11.697329,-11.450877,-11.462,-11.522496,-11.465104,-11.357757,-11.433535,-11.470195,-11.424403,-11.432227,-10.634461,-2.12917,-0.07849489,-1.8878019,-0.85327107,-1.5181903,-2.219913,-1.6664749,-1.9337484,-1.4164791,2.3917613,3.5995805,3.7296803,-2.183116,-2.1695013,-2.3565922,-2.33903,-2.3984334,-2.344973,-0.3681489,-2.0799665,-2.1138978,-2.4066458,-2.190689,-2.2967486,2.5755534,3.49893,-2.3005836,-2.413798,-2.1923988,-2.1288471,1.6168559,0.02180152,0.61001295,0.47909483,1.0900348,0.59603554,0.22477515,0.7499691,0.4623974,0.50908715,0.63909435,0.5719962,0.55775774,0.8116155,0.55976796,0.9472215,0.5988644,0.29621664,-1.0265496,0.04251704,-1.1120186,-9.88704,-0.09245885,-9.935098,0.52643126,-2.416942,-3.0040693,-3.0781896,-3.08161,-2.5568264,-3.0449831,-3.0429459,-2.925152,-2.98462,-2.7108476,-1.179765,-1.1485274,-0.3523437,1.0593874,0.98312986,0.9702298,0.9096703,1.0704538,0.2503475,3.19902,3.984987,-0.11412044,1.158847,2.6045496,0.20148285,-1.664587,2.4787447,3.5247276,4.3119607,1.5381705,2.521504,2.7724583,2.7086506,3.6137865,1.4935753,2.8902838,-1.8792088,3.0945458,3.3908274,3.0581973,3.3810225,2.374027,1.3814621,2.77894,3.3981032,2.707917,-1.8213003,1.095468,1.0157958,1.4043344,2.8517551,4.0763574,1.4279133,0.7914809,-1.4565847,0.6232225,2.0123026,1.2128949,1.0428637,-1.104186,-0.93669,-1.9243428,5.6673074,5.3782945,3.3966973,5.304164,5.1643424,6.5833473,1.2843449,7.2909718,5.6691184,6.0788937,5.5614223,6.141672,7.0362134,5.8371415,7.0160055,6.230084,5.5400558,5.6259713,-1.6792576,-1.9586861,-1.8544554,-1.8781185,-11.122407,-11.350672,-11.348628,-11.43036,-11.393485,-11.493499,-11.583975,-11.456839,-11.292247,-11.428905,-11.330686,-10.507703,-1.0174057,-1.3598478,-1.2718072,-1.393733,-1.2779891,-2.6737921,-2.281462,-1.5318148,-1.4934405,-1.535304,-1.2770631,-0.3809281,2.411988,2.658077,2.7835867,2.8636436,3.4489646,2.9829001,4.310957,4.762632,4.297418,3.2350588,4.0523243,2.759861,-0.22528754,2.5552042,-5.6224017,-4.5823274,-5.427394,-6.001275,-6.10254,-5.980769,-5.296221,-6.0174212,-7.0562654,-5.64191,-2.6044343,-2.9852433,-5.8564057,-7.1108003,-5.433118,-5.839979,-5.583729,-1.1682398,0.72034234,2.5929158,2.545363,2.1875806,0.29009256,2.5929554,0.18219715,-0.952251,-1.2326899,0.6812972,0.50598615,-0.6758752,-0.7078481,-1.3410821,-1.2343519,-0.70605266,-0.34505814,-1.1857961,0.31120926,0.45490202,0.44300595,-0.3581616,-5.7925467,-6.176431,3.2073226,3.7858155,0.43380934,0.59632355,-0.4930635,-1.2724841,-1.4816531,-1.9809769,-2.5941103,-2.5899403,-2.6292253,-1.8823876,-1.273548,-2.4973178,-2.3809402,-1.9762164,-1.8921049,-1.9739794,-2.0871372,-2.1491783,-1.7079232,-1.1836538,-18.41863,-10.749516,-1.3162311,1.0263451,0.39402652,-0.80463856,-0.036234077,-0.025428275,-0.41539416,-5.786783,-0.23570822,3.6522129,4.5507946,4.663589,-0.0708263,4.792863,-0.23226993,-0.5954609,-6.5343366,-6.5618644,-6.318604,3.4238393,5.309293,-6.3140545,-6.3138814,5.811908,6.5011525,6.9845643,-0.20752072,6.5583844,6.2331085,6.9852166,7.2477183,6.419792,7.1291184,6.045105,6.4942575,4.561301,6.821241,6.943889,7.0725713,7.7153406,7.478264,7.5502887,4.0368013,7.002806,6.6775613,4.5777807,4.393458,6.6620264,5.185068,6.559153,-0.39738902,6.417704,0.5884631,-0.23792261,-1.2176379,-0.37695834,0.53558105,7.1187387,4.944327,3.8541062,6.696716,-1.7537835,-2.2746859,-2.2431571,-1.0396899,0.2531202,3.5336757,3.433309,3.624346,3.8959727,2.1712718,2.898081,1.7977262,0.82393444,3.54028,6.223877,2.553135,2.429499,0.28798932,3.5549784,0.70464855,4.911329,3.649098,3.8215892,3.805315,3.9320102,3.7080963,-0.050406016,6.117595,4.710079,6.1333995,0.4345719,6.911761,6.893883,0.77015704,6.6035786,7.8219857,7.8932314,6.619018,4.70165,6.3003607,2.8340857,-0.631295,-0.6611353,5.238255,0.70611143,-1.8549228,-1.8359741,0.5660421,4.413557,5.014047,3.9772403,-1.6723795,-1.5535318,3.0168846,0.1731417,-0.65529525,-0.85428,-1.60385,-1.6186233,-1.734514,-1.3274461,-0.32511714,-0.6220529,-1.3115859,-0.5918204,-0.89408016,-1.0531487,0.9295846,2.5014417,2.405095,2.7194,2.4959443,2.556024,2.6884418,2.4915085,2.6040344,2.6917706,2.4129574,2.61993,2.6472151,2.580742,2.64489,2.7059112,2.3705475,2.6819081,1.9799777,0.20914547,-1.6948291,2.2771873,-0.33182436,-2.0600197,2.410437,2.6588547,2.5758135,2.5069017,2.5392509,2.1905272,0.5207193,-1.9885784,1.7799748,-0.42094958,1.5734582,2.3895633,2.6338303,2.2765758,2.6779966,2.4857757,2.895408,2.6810005,2.223737,0.2958748,1.4433838,-0.103285566,1.7807075,2.503329,2.3595753,2.406918,2.4108658,2.4637008,2.2427862,2.3518453,2.2008877,2.2249627,2.079046,1.93011,2.8084114,3.4998345,4.0005693,3.4376693,4.3874297,3.9268744,3.2729356,3.3021996,2.340649,-0.572079,-2.120649,0.0023052802,2.5724328,4.229954,-0.78388596,-2.031826,-1.1972725,-1.3697393,-1.7539134,-1.6771603,-1.8394691,-9.46149,-9.500858,-0.14367592,-6.4176927,-2.4094837,-6.673897,-5.295481,-0.89592594,-5.785818,-4.850135,-3.6230664,-3.9483778,-2.943507,-2.2304487,-3.9877312,-0.08200458,-0.4304848,-0.12619162,-0.76356757,-0.11890221,0.6842427,-0.6456773,1.4644623,-0.58592814,-6.1328444,-6.583833,-2.6483366,-2.5787725,-2.4452703,-6.2536683,-2.3418546,-2.858883,-3.077816,-2.8916538,-3.1110961,-2.5569324,-3.039113,-3.1563447,-3.1366656,-2.6500356,-3.192687,-3.1284342,-2.7751136,-3.0313005,-2.7642827,-0.32059342,3.7305098,0.16346465,0.13427638,3.773858,0.6374289,0.49659035,0.58618104,-1.4921783,-2.5777123,-2.6272647,-2.3924356,-2.1437614,-2.7578177,3.9188337,-2.3330536,-2.3518515,-2.768668,-2.7319393,-2.7487335,-1.72607,-2.0881288,1.3900131,-2.2117667,-1.9812257,-0.028419815,-2.159367,0.44050658,0.6275325,-1.734801,-1.3947202,1.2189461,-1.9960504,-1.9412363,-0.39976862,3.8852158,2.9294243,3.519444,3.4088445,3.888037,3.7785726,-0.7090708,-0.15947396,-0.14722423,0.17752512,0.5395766,-0.1677066,-16.058119,0.18786038,0.07239411,0.61015326,-0.97563595,0.1919572,-0.17052215,-0.63126117,-1.5707783,4.0151396,-1.3132037,1.6019808,-1.1801336,0.5601168,-1.4630182,0.8965552,4.1315947,-1.3140942,1.1900262,-2.2163277,-2.1915734,-2.2632673,-2.203935,-2.4067807,0.32721254,-2.6418781,-2.4155025,-2.2233365,-2.0769393,-2.1681373,-2.4397366,-2.475334,-0.6089255,-2.2446127,-2.1919572,-1.4583303,-1.3178964,-1.6850622,-1.7920326,-2.1698012,-2.327366,-2.0442386,-0.12319237,-10.045018,-10.224561,-10.054863,-10.187617,-9.977227,-10.256196,-10.249899,1.8286333,2.19805,2.1424289,-1.079817,2.7648318,2.4595695,-0.020189032,0.7303554,1.0807853,0.37897292,1.3552716,1.0105842,4.1979246,0.9514595,3.000794,4.94626,2.7567818,3.0061553,0.8539355,3.4335573,5.3722777,3.7201383,-0.16169806,0.015706811,4.4781113,4.3996673,4.3869224,0.59579974,0.800463,1.1009787,4.179184,2.8303711,3.9720595,0.57456744,0.3105985,-0.09911699,-1.8578923,-2.2245982,-2.1487727,-1.8648953,-0.7896222,-1.9705266,-1.9299527,-1.8798431,0.046706714,-1.6347551,-1.0314889,-1.8660669,0.49410412,0.61161405,0.05256975,-2.456655,0.7903983,1.3103843,1.2944002,0.62013876,0.62374103,0.6607801,2.755129,-9.5227785,-5.702127,1.0085503,-6.814417,-6.733193,-9.346307,-9.416349,-9.690316,-1.4838341,1.9818323,-0.482776,-1.7520767,-1.7095479,-1.8002971,-0.39455536,-1.9693304,-0.5508581,-1.6885122,-0.39838237,-0.4033695,-1.3118098,2.8965528,2.9595964,3.0187666,2.927696,4.5577354,4.087353,4.694947,4.370816,4.165521,4.377536,4.1923203,4.1503468,4.171464,3.931092,3.75614,3.472263,3.414602,3.5476038,2.9691849,3.194076,-0.09122074,-0.60083055,4.704859,6.2273145,5.322173,5.2096467,2.8051088,2.861846,2.19652,-0.58318824,-0.47653043,-0.2726796,0.1780253,0.35750896,5.951197,6.1596227,6.4469156,1.1261325,-0.29829997,6.6400433,6.457589,6.9065633,7.188984,6.62717,5.1149435,7.2566314,6.179106,4.546001,6.962146,5.7558827,6.649804,5.4636574,6.8154354,6.9809556,7.063998,7.8702703,7.646096,7.405069,7.4870877,7.431385,6.641434,4.447176,4.3967834,6.8809795,5.538399,6.3189607,6.620805,-0.4681998,6.4030957,2.9404385,6.816671,5.896756,-11.657571,-11.813249,-11.795552,-11.806758,-11.768958,-11.659734,-11.669919,-11.726148,-10.518488,-1.8661506,0.28357986,-1.1677248,-1.9690425,-2.9898474,-2.8169408,-2.0864086,-2.2012384,-0.102827035,-0.60189575,-1.423447,-1.9528128,-1.962365,-1.4835947,-0.8243646,-1.780123,0.57644767,2.6076005,1.960647,0.7989778,4.5309978,2.579292,-0.5914786,0.14441364,2.2992065,0.3410024,0.15471645,0.22841953,0.25697353,0.15112865,0.54696035,0.75448346,0.024511129,-1.7724415,-1.9052104,-0.05708754,1.2980399,1.7065318,0.719138,2.7012284,1.5748206,-1.1899638,-2.556642,-2.964877,-3.1944125,-3.115836,-3.0718787,-3.0493584,-3.1193714,-3.0595906,-3.064364,-5.608857,1.092996,1.7749469,-4.9679756,-5.0124974,-4.9462357,-4.8451796,-5.148624,-4.7660213,-4.554728,-5.148358,-5.755228,-4.862367,-5.5765157,-5.8304124,-1.7359332,1.609834,-1.335551,-6.423214,-7.0888405,-6.602919,-7.499907,-5.281185,-6.792131,-6.970723,-7.300388,-6.5040503,-7.6097727,-1.8887311,-1.294305,-1.8625342,-2.4896872,-2.1592994,-0.13223346,-1.8236027,-0.34702337,0.821138,-0.18834074,-0.28916705,1.654411,-0.40416887,-0.91611546,0.51678306,-0.926076,0.054621212,-0.23699272,0.20189384,-1.3682053,-0.7450628,-2.2947118,-2.419589,-2.1043415,-2.3588054,-2.3009932,-2.3599396,-2.290703,-1.4788424,-0.22003423,0.99388874,-0.09323129,3.824177,3.408977,-2.0294414,-2.049487,-2.1047795,-0.24518465,-2.007497,-1.3945751,0.19465113,0.16114937,-2.3646405,-2.374593,-1.8929975,-2.133879,-2.011643,-0.3782179,-1.0160248,-1.2462316,-1.2140025,-1.1506546],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"hat is dynamic padding? In the \\\"Batching Inputs together\\\" video, we have seen that to be able to gro...\"],[\"a data collator. Those classes in the Transformers library are responsible for applying all the fina...\"],[\"ow to slice and dice a dataset. Most of the time, the data you work with wonâ€™t be perfectly prepared...\"],[\"we've created a small lambda function that checks whether the title starts with the letter \\\"L\\\". Once...\"],[\"he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library t...\"],[\"As long as the function returns a dictionary-like object, the map method will add new columns as nee...\"],[\"efore diving in character-based tokenization, understanding why this kind of tokenization is interes...\"],[\"Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\\n\\n\\u003cCourseFloatingBanner chap...\"],[\"## Normalization[[normalization]]\\n\\n\\u003cYoutube id=\\\"4IIC2jI9CaU\\\"\\u002f\\u003e\\n\\nThe normalization step involves some...\"],[\"```\\n\\n```python out\\n\\u003cclass 'tokenizers.Tokenizer'\\u003e\\n```\\n\\nThe `normalizer` attribute of the `tokenizer`...\"],[\"```\\n\\nNotice how the tokenizer is already keeping track of the offsets, which is how it can give us t...\"],[\"```\\n\\nLike the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), ...\"],[\"Model | BPE | WordPiece | Unigram\\n:----:|:---:|:---------:|:------:\\nTraining | Starts from a small v...\"],[\"ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them to...\"],[\"tokens from the second sentence, and a final SEP token. If we have several pairs of sentences, we ca...\"],[\"The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face co...\"],[\"| Language                                                                      | Source            ...\"],[\"|:------------------------------------------------------------------------------|:------------------...\"],[\"---------------------------------------------------|...\"],[\"| [English](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fen\\u002fchapter1\\u002f1)                        | [`chapters\\u002fen`](ht...\"],[\"| [Spanish](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fes\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fes`](ht...\"],[\"| [Hebrew](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fhe\\u002fchapter1\\u002f1) (WIP)                   | [`chapters\\u002fhe`](ht...\"],[\"| [Japanese](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fja\\u002fchapter1\\u002f1) (WIP)                 | [`chapters\\u002fja`](ht...\"],[\"| [Russian](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fru\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fru`](ht...\"],[\"| [Chinese (simplified)](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fzh-CN\\u002fchapter1\\u002f1)  | [`chapters\\u002fzh-CN`](https...\"],[\"### Translating the course into your language\\n\\nAs part of our mission to democratise machine learnin...\"],[\"```\\n\\n**ğŸ“‹ Copy-paste the English files with a new language code**\\n\\nThe course files are organised und...\"],[\"```\\n\\n\\u003e ğŸš¨ Make sure the `_toctree.yml` file only contains the sections that have been translated! Oth...\"],[\"```\\npip install -r requirements.txt\\nmake style\\n```\\n\\nOnce that's run, commit any changes, open a pull...\"],[\"```\\n\\nThen run the following script:\\n\\n```bash\\npython utils\\u002fgenerate_notebooks.py --output_dir nbs\\n```...\"],[\"What to do when you get an error[[what-to-do-when-you-get-an-error]]\\n\\n\\u003cCourseFloatingBanner chapter=...\"],[\"```\\n\\nor the following in your favorite terminal:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nThis will promp...\"],[\"```\\n\\nNow when you call `copy_repository_template()`, it will create a copy of the template repositor...\"],[\"```\\n\\nOh no, something seems to have gone wrong! If you're new to programming, these kind of errors c...\"],[\"```python out\\n\\\"\\\"\\\"\\nMake sure that:\\n\\n- 'lewtun\\u002fdistillbert-base-uncased-finetuned-squad-d5716d28' is a...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ If you encounter an error message that is difficult to understand, just copy and paste...\"],[\"Okay, this got a hit. Now let's try to download the model again with the correct model ID:\\n\\n```pytho...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nOSError: Can't load config for 'lewtun\\u002fdistilbert-base-uncased-finetuned-squa...\"],[\"```\\n\\nInteresting -- there doesn't seem to be a *config.json* file in the repository! No wonder our `...\"],[\"```\\n\\nNow we can test if this worked by loading the model from the latest commit on the `main` branch...\"],[\"```\\n\\nWoohoo, it worked! Let's recap what you've just learned:\\n\\n- The error messages in Python are kn...\"],[\"```\\n\\nNext we need a question, so let's see if our favorite frameworks are supported:\\n\\n```python\\nques...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\n---------------------------------------------------------------------------\\nA...\"],[\"~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodeling_di...\"],[\"```\\n\\nOh dear, it looks like we have a bug in our code! But we're not afraid of a little debugging. Y...\"],[\"```\\n~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodelin...\"],[\"```\\n\\nIt looks like our code tried to call `input_ids.size()`, but this clearly won't work for a Pyth...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nQuestion: Which frameworks can I use?\\nAnswer: pytorch, tensorflow, and jax\\n\\\"\\\"...\"],[\"hat is transfer learning? The idea of Transfer Learning is to leverage the knowledge acquired by a m...\"],[\"English Wikipedia and 11,000 unpublished books. In practice, transfer learning is applied on a given...\"],[\"ow to batch inputs together? In this video, we will see how to batch input sequences together. In ge...\"],[\"along with the input ids will give us the same results as when we sent the two sentences individuall...\"],[\"upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on whi...\"],[\"the line that places the batch on the proper device, and just before passing your predictions and la...\"],[\"Building your first demo[[building-your-first-demo]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  classNames...\"],[\"```\\n\\nLet's walk through the code above:\\n\\n- First, we define a function called `greet()`. In this cas...\"],[\"Try using this GUI right now with your own name or some other input!\\n\\nYou'll notice that in this GUI...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-hello-world-custom.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"300\\\" tit...\"],[\"```\\n\\nThis function completes prompts that you provide, and you can run it with your own input prompt...\"],[\"Natural Language Processing[[natural-language-processing]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n  ...\"],[\"## Why is it challenging?[[why-is-it-challenging]]\\n\\nComputers don't process information in the same ...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={8}\\n    classNames=\\\"absolute z-10 ri...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Sharing pretrained models[[sharing-pretrained-models]]\\n\\n{#if fw ===...\"],[\"There are three ways to go about creating new model repositories:\\n\\n- Using the `push_to_hub` API\\n- U...\"],[\"```\\n\\nIn a terminal, you can run:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIn both cases, you should be pr...\"],[\"```\\n\\nWhen you call `trainer.train()`, the `Trainer` will then upload your model to the Hub each time...\"],[\"```\\n\\nThen you should add `callbacks=[callback]` in your call to `model.fit()`. The callback will the...\"],[\"```\\n\\nThis will create the new repository `dummy-model` in your profile, and populate it with your mo...\"],[\"```\\n\\nNow head to the Model Hub to find your newly uploaded model: *https:\\u002f\\u002fhuggingface.co\\u002fuser-or-or...\"],[\"The `push_to_hub()` method is backed by the [`huggingface_hub`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggi...\"],[\"```\\n\\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpo...\"],[\"```\\n\\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belo...\"],[\"After creating your model repository, you should see a page like this:\\n\\n\\u003cdiv class=\\\"flex justify-cen...\"],[\"In the next section, we go over three different ways of uploading files to the Hub: through `hugging...\"],[\"```\\n\\nThis will upload the file `config.json` available at `\\u003cpath_to_file\\u003e` to the root of the reposi...\"],[\"```\\n\\nAnd others! We recommend taking a look at the `Repository` documentation available [here](https...\"],[\"```\\n\\n```bash\\nUpdated git hooks.\\nGit LFS initialized.\\n```\\n\\nOnce that's done, the first step is to clo...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import TFAutoModelForMaskedLM, AutoTokenizer\\n\\ncheckpoint = \\\"came...\"],[\"```\\n\\nIf you look at the file sizes (for example, with `ls -lh`), you should see that the model state...\"],[\"```\\n{\\u002fif}\\n\\nSimilarly, we can make sure that git-lfs is tracking the correct files by using its `stat...\"],[\"```\\n\\n{#if fw === 'pt'}\\n```bash\\n[main b08aab1] First model version\\n 7 files changed, 29027 insertions...\"],[\"```\\n\\n{#if fw === 'pt'}\\nIf we take a look at the model repository when this is finished, we can see a...\"],[\"n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a masked language model[[fine-tuning-a-masked-language-...\"],[\"However, there are a few cases where you'll want to first fine-tune the language models on your data...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-distilbert-base-uncased-finetuned-imdb.hf.space\\\" frameBorder=\\\"0\\\" h...\"],[\"Although the BERT and RoBERTa family of models are the most downloaded, we'll use a model called [Di...\"],[\"```\\n\\nWe can see how many parameters this model has by calling the `num_parameters()` method:\\n\\n```pyt...\"],[\"```python out\\nModel: \\\"tf_distil_bert_for_masked_lm\\\"\\n________________________________________________...\"],[\"Total params: 66,985,530\\nTrainable params: 66,985,530\\nNon-trainable params: 0\\n______________________...\"],[\"```\\n\\n{\\u002fif}\\n\\nWith around 67 million parameters, DistilBERT is approximately two times smaller than th...\"],[\"```\\n\\nWith a tokenizer and a model, we can now pass our text example to the model, extract the logits...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n'\\u003e\\u003e\\u003e This is a great deal.'\\n'\\u003e\\u003e\\u003e This is a great success.'\\n'\\u003e\\u003e\\u003e This is a ...\"],[\"```\\n\\nWe can see that the `train` and `test` splits each consist of 25,000 reviews, while there is an...\"],[\"```\\n\\n```python out\\n\\n'\\u003e\\u003e\\u003e Review: This is your typical Priyadarshan movie--a bunch of loony character...\"],[\"'\\u003e\\u003e\\u003e Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, an...\"],[\"```\\n\\nYep, these are certainly movie reviews, and if you're old enough you may even understand the co...\"],[\"```python\\ndef tokenize_function(examples):\\n    result = tokenizer(examples[\\\"text\\\"])\\n    if tokenizer...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['attention_mask', 'input_id...\"],[\"```\\n\\n```python out\\n512\\n```\\n\\nThis value is derived from the *tokenizer_config.json* file associated w...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e\\u003e Review 0 length: 200'\\n'\\u003e\\u003e\\u003e Review 1 length: 559'\\n'\\u003e\\u003e\\u003e Review 2 length: 192'\\n...\"],[\"```\\n\\nAs you can see in this example, the last chunk will generally be smaller than the maximum chunk...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['attention_mask', 'input_id...\"],[\"```\\n\\n```python out\\n\\\".... at.......... high. a classic line : inspector : i'm here to sack one of you...\"],[\"```\\n\\nTo see how the random masking works, let's feed a few examples to the data collator. Since it e...\"],[\"```\\n\\nNice, it worked! We can see that the `[MASK]` token has been randomly inserted at various locat...\"],[\"wwm_probability = 0.2\\n\\n\\ndef whole_word_masking_data_collator(features):\\n    for feature in features:...\"],[\"```\\n\\n{:else}\\n\\n```py\\nimport collections\\nimport numpy as np\\n\\nfrom transformers.data.data_collator impo...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e\\u003e [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as so...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Try it out!** Run the code snippet above several times to see the random masking ha...\"],[\"```\\n\\nwhich will display a widget where you can enter your credentials. Alternatively, you can run: \\n...\"],[\"```\\n\\nNext, we set up our training hyperparameters and compile our model. We use the `create_optimize...\"],[\"# Train in mixed-precision float16\\ntf.keras.mixed_precision.set_global_policy(\\\"mixed_float16\\\")\\n\\nmode...\"],[\"```\\n\\nWe're now ready to run `model.fit()` -- but before doing so let's briefly look at _perplexity_,...\"],[\"```\\n\\nHere we tweaked a few of the default options, including `logging_steps` to ensure we track the ...\"],[\"```\\n\\nWe're now ready to run `trainer.train()` -- but before doing so let's briefly look at _perplexi...\"],[\"```\\n\\n{:else}\\n\\nAssuming our test set consists mostly of sentences that are grammatically correct, the...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n\\u003e\\u003e\\u003e Perplexity: 11.32\\n```\\n\\nNice -- this is quite a reduction in perplexity...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Your turn!** Run the training above after changing the data collator to the ...\"],[\"```\\n\\nNext, we'll apply this function to our test set and drop the unmasked columns so we can replace...\"],[\"```\\n\\nWith these objects, we can now prepare everything for training with the `Accelerator` object:\\n\\n...\"],[\"```\\n\\nWith that done, it's just a simple matter of writing out the full training and evaluation loop:...\"],[\"```\\n\\nCool, we've been able to evaluate perplexity with each epoch and ensure that multiple training ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Using pretrained models[[using-pretrained-models]]\\n\\n{#if fw === 'pt...\"],[\"Let's say we're looking for a French-based model that can perform mask filling.\\n\\n\\u003cdiv class=\\\"flex ju...\"],[\"```\\n\\n```python out\\n[\\n  {'sequence': 'Le camembert est dÃ©licieux :)', 'score': 0.49091005325317383, '...\"],[\"```\\n\\nAs you can see, loading a model within a pipeline is extremely simple. The only thing you need ...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM\\n\\ntokenizer = C...\"],[\"Understanding the Interface class[[understanding-the-interface-class]]\\n\\n\\u003cCourseFloatingBanner chapte...\"],[\"Let's take a look at another example, this time with an `Audio` component.\\n\\n## A simple example with...\"],[\"```\\n\\nThe code above will produce an interface like the one below (if your browser doesn't\\nask you fo...\"],[\"The code snippet below shows how three input components line up with the three arguments of the `gen...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-generate-tone.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"450\\\" title=\\\"G...\"],[\"Let's build an interface that allows you to demo a **speech-recognition** model.\\nTo make it interest...\"],[\"```\\n\\nIf your browser doesn't ask you for microphone permissions, \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fspa...\"],[\"ou are at the right place if you want to understand what the Byte pair Encoding subword tokenization...\"],[\"of tokens composed of the tokens \\\"l\\\" and \\\"e\\\". And now we just have to reproduce the same steps with ...\"],[\"ome bugs in your code are very straightforward. You try running it, you get a syntax error somewhere...\"],[\"So what do we get when we inspect that batch? We see that we're not getting any gradient because we'...\"],[\"of 2! This gives us a very strong clue - if we check the model, with model.config.num_labels, we see...\"],[\"How to write a good issue[[how-to-write-a-good-issue]]\\n\\n\\u003cCourseFloatingBanner chapter={8}\\n  classNam...\"],[\"\\u003cTip\\u003e\\n\\nğŸš¨ Many issues in the ğŸ¤— Transformers repository are unsolved because the data used to reproduc...\"],[\"```\\ntransformers-cli env\\n```\\n\\nand you should get something like this:\\n\\n```out\\nCopy-and-paste the tex...\"],[\"```\\n```python\\n```\\n\\nthen paste in your minimal reproducible example and type a new line with three ba...\"],[\"oading a custom dataset. Although the Hugging Face Hub hosts over a thousand public datasets, you'll...\"],[\"in the dataset. For JSON files, there are two main formats to know about. The first one is called JS...\"],[\"Summary[[summary]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-10 right-0 top-...\"],[\"What if my dataset isn't on the Hub?[[what-if-my-dataset-isnt-on-the-hub]]\\n\\n\\u003cCourseFloatingBanner ch...\"],[\"ğŸ¤— Datasets provides loading scripts to handle the loading of local and remote datasets. It supports ...\"],[\"The training and test splits are hosted on GitHub, so we can download them with a simple `wget` comm...\"],[\"```\\n\\nThis will download two compressed files called *SQuAD_it-train.json.gz* and *SQuAD_it-test.json...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['title', 'paragraphs'],\\n   ...\"],[\"```\\n\\nThis is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up ...\"],[\"```\\n\\nThis can be useful if you don't want to manually decompress many GZIP files. The automatic deco...\"],[\"```\\n\\nThis returns the same `DatasetDict` object obtained above, but saves us the step of manually do...\"],[\"rite your own training loop in PyTorch. In this video, we will look at how we can do the same fine-t...\"],[\"there is no error. If the labels are provided, the models of the Transformers library always return ...\"],[\"mode, then go through all the data in the evaluation data loader. As we have seen in the Trainer vid...\"],[\"atasets and DataFrames equals love. Although the processing functions of Datasets will cover most th...\"],[\"method if you want to do the format conversion and slicing of the dataset in one go. And once you ha...\"],[\"n this video, we'll study the decoder architecture. An example of a popular decoder-only architectur...\"],[\"representation, they can also be used in a wide variety of tasks. However, the strength of a decoder...\"],[\"like. It is based off of the masked self-attention layer, which allows to have word embeddings which...\"],[\"n our other videos, and as always, there'll be links below if you want to check those out, we showed...\"],[\"GLUE benchmark. Each of the GLUE datasets, as well as many of our other datasets, has some predefine...\"],[\"Unigram tokenization[[unigram-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6}\\n  classNames=\\\"absolu...\"],[\"This is all a very costly operation, so we don't just remove the single symbol associated with the l...\"],[\"```\\n(\\\"hug\\\", 10), (\\\"pug\\\", 5), (\\\"pun\\\", 12), (\\\"bun\\\", 4), (\\\"hugs\\\", 5)\\n```\\n\\nand for this example, we will...\"],[\"```\\n\\nSo, the sum of all frequencies is 210, and the probability of the subword `\\\"ug\\\"` is thus 20\\u002f210...\"],[\"```\\n\\nSo, `\\\"pug\\\"` would be tokenized as `[\\\"p\\\", \\\"ug\\\"]` or `[\\\"pu\\\", \\\"g\\\"]`, depending on which of those s...\"],[\"```\\n\\nThus `\\\"unhug\\\"` would be tokenized as `[\\\"un\\\", \\\"hug\\\"]`.\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Now your turn!** Determine t...\"],[\"```\\n\\nNow we need to compute how removing each token affects the loss. This is rather tedious, so we'...\"],[\"```\\n\\nLike for BPE and WordPiece, we begin by counting the number of occurrences of each word in the ...\"],[\"```\\n\\nWe group the characters with the best subwords to arrive at an initial vocabulary of size 300:\\n...\"],[\"```\\n\\nNow the main function is the one that tokenizes words using the Viterbi algorithm. As we saw be...\"],[\"segmentation = best_segmentations[-1]\\n    if segmentation[\\\"score\\\"] is None:\\n        # We did not fin...\"],[\"```\\n\\nWe can already try our initial model on some words:\\n\\n```python\\nprint(encode_word(\\\"Hopefully\\\", m...\"],[\"```\\n\\nWe can try it on a given token:\\n\\n```python\\nscores = compute_scores(model)\\nprint(scores[\\\"ll\\\"])\\np...\"],[\"```\\n\\nThen, to tokenize some text, we just need to apply the pre-tokenization and then use our `encod...\"],[\"n this video, we're going to go over the HuggingFace Model Hub navigation. This is the huggingface.c...\"],[\"other users to leverage your model in their applications. On the right of the model card is the infe...\"],[\"n this video, we're going to see how to load and fine-tune a pre-trained model. It's very quick, and...\"],[\"encourages the network to output large values for the right class, and low values for the wrong clas...\"],[\"to the classes for our examples, and thatâ€™s it. If you're following along with the data from our dat...\"],[\"Part 2 Release Event[[part-2-release-event]]\\n\\nFor the release of part 2 of the course, we organized ...\"],[\"**Jay Alammar:** *A gentle visual intro to Transformers models*\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003c...\"],[\"Margaret Mitchell is a researcher working on Ethical AI, currently focused on the ins and outs of et...\"],[\"**Mark Saroufim:** *How to Train a Model with Pytorch*\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube i...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"u--UVvH-LIQ\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\nLewis is a machine learning en...\"],[\"Lucile is a machine learning engineer at Hugging Face, developing and supporting the use of open sou...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"O2e3pXO4aRE\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"How do Transformers work?[[how-do-transformers-work]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    cla...\"],[\"- **February 2019**: [GPT-2](https:\\u002f\\u002fcdn.openai.com\\u002fbetter-language-models\\u002flanguage_models_are_unsup...\"],[\"This type of model develops a statistical understanding of the language it has been trained on, but ...\"],[\"## Transformers are big models[[transformers-are-big-models]]\\n\\nApart from a few outliers (like Disti...\"],[\"This is why sharing language models is paramount: sharing the trained weights and building on top of...\"],[\"This pretraining is usually done on very large amounts of data. Therefore, it requires a very large ...\"],[\"Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also q...\"],[\"Each of these parts can be used independently, depending on the task: \\n\\n* **Encoder-only models**: G...\"],[\"The same concept applies to any task associated with natural language: a word by itself has a meanin...\"],[\"The original Transformer architecture looked like this, with the encoder on the left and the decoder...\"],[\"For example, BERT is an architecture while `bert-base-cased`, a set of weights trained by the Google...\"],[\"n this video we'll take a look at how you upload your very own dataset to the Hub. The first you'll ...\"],[\"n this video we take a look at the mysterious sounding metric called Perplexity. You might have enco...\"],[\"Decoder models[[decoder-models]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-1...\"],[\"n this video, we'll study the encoder architecture. An example of a popular encoder-only architectur...\"],[\"Let's dive in this representation. It contains one vector per word that was passed through the encod...\"],[\"the task of predicting a hidden word in a sequence of words. Here, for example, we have hidden the w...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Translation[[translation]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanne...\"],[\"\\u003cYoutube id=\\\"1JvfrvZgi6c\\\"\\u002f\\u003e\\n\\nIf you have a big enough corpus of texts in two (or more) languages, yo...\"],[\"Once we're finished, we will have a model able to make predictions like this one:\\n\\n\\u003ciframe src=\\\"http...\"],[\"As in the previous sections, you can find the actual model that we'll train and upload to the Hub us...\"],[\"```\\n\\nIf you want to work with a different pair of languages, you can specify them by their codes. A ...\"],[\"```\\n\\nNow let's take a look at one element of the dataset:\\n\\n```py\\nsplit_datasets[\\\"train\\\"][1][\\\"transla...\"],[\"```\\n\\nOur pretrained model, however, sticks with the compact and familiar English word:\\n\\n```py\\ntransl...\"],[\"```\\n\\nYou can also replace the `model_checkpoint` with any other model you prefer from the [Hub](http...\"],[\"```\\n\\nAs we can see, the output contains the input IDs associated with the English sentence, while th...\"],[\"```\\n\\nNote that we set the same maximum length for our inputs and outputs. Since the texts we're deal...\"],[\"```\\n\\nNow that the data has been preprocessed, we are ready to fine-tune our pretrained model!\\n\\n{#if ...\"],[\"```\\n\\n\\u003cTip warning={false}\\u003e\\n\\nğŸ’¡ The `Helsinki-NLP\\u002fopus-mt-en-fr` checkpoint only has PyTorch weights, ...\"],[\"{#if fw === 'pt'}\\n\\n```py\\nfrom transformers import DataCollatorForSeq2Seq\\n\\ndata_collator = DataCollat...\"],[\"```\\n\\n{:else}\\n\\n```py\\nfrom transformers import DataCollatorForSeq2Seq\\n\\ndata_collator = DataCollatorFor...\"],[\"```\\n\\nHere are the labels for the first and second elements in our dataset:\\n\\n```py\\nfor i in range(1, ...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\n### Metrics[[metrics]]\\n\\n\\u003cYoutube id=\\\"M05L1DhFqcw\\\"\\u002f\\u003e\\n\\n{#if fw === 'pt'}\\n\\nThe feature tha...\"],[\"One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult...\"],[\"```\\n\\nWe can then load it via `evaluate.load()` like we did in [Chapter 3](\\u002fcourse\\u002fchapter3):\\n\\n```py\\n...\"],[\"```\\n\\nThis gets a BLEU score of 46.75, which is rather good -- for reference, the original Transforme...\"],[\"```\\n\\n```py\\npredictions = [\\\"This plugin\\\"]\\nreferences = [\\n    [\\n        \\\"This plugin allows you to aut...\"],[\"```\\n\\nThe score can go from 0 to 100, and higher is better.\\n\\n{#if fw === 'tf'}\\n\\nTo get from the model...\"],[\"def compute_metrics():\\n    all_preds = []\\n    all_labels = []\\n\\n    for batch, labels in tqdm(tf_gene...\"],[\"```\\n\\n{:else}\\n\\nTo get from the model outputs to texts the metric can use, we will use the `tokenizer....\"],[\"```\\n\\nThis will display a widget where you can enter your Hugging Face login credentials.\\n\\nIf you are...\"],[\"```\\n\\nNext, we define a `PushToHubCallback` to upload our model to the Hub during training, as we saw...\"],[\"```\\n\\n```\\n{'bleu': 57.334066271545865}\\n```\\n\\nAt this stage, you can use the inference widget on the Mo...\"],[\"```\\n\\nApart from the usual hyperparameters (like learning rate, number of epochs, batch size, and som...\"],[\"```\\n\\nBefore training, we'll first look at the score our model gets, to double-check that we're not m...\"],[\"```\\n\\nThat's a nearly 14-point improvement, which is great.\\n\\nFinally, we use the `push_to_hub()` meth...\"],[\"```\\n\\nAt this stage, you can use the inference widget on the Model Hub to test your model and share i...\"],[\"```\\n\\nThen we will need an optimizer:\\n\\n```py\\nfrom transformers import AdamW\\n\\noptimizer = AdamW(model....\"],[\"```\\n\\nLastly, to push our model to the Hub, we will need to create a `Repository` object in a working...\"],[\"```\\n\\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. ...\"],[\"```\\n\\nThe training loop looks a lot like the ones in [section 2](\\u002fcourse\\u002fchapter7\\u002f2) and [Chapter 3](...\"],[\"predictions_gathered = accelerator.gather(generated_tokens)\\n        labels_gathered = accelerator.ga...\"],[\"```\\n\\n```python out\\nepoch 0, BLEU score: 53.47\\nepoch 1, BLEU score: 54.24\\nepoch 2, BLEU score: 54.44\\n...\"],[\"```\\n\\nAnother great example of domain adaptation!\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Your turn!** What does the model retur...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={6}\\n    classNames=\\\"absolute z-10 ri...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={5}\\n    classNames=\\\"absolute z-10 ri...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Question answering[[question-answering]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCours...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-bert-finetuned-squad.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"450\\\" title=...\"],[\"\\u003cTip\\u003e\\n\\nğŸ’¡ Encoder-only models like BERT tend to be great at extracting answers to factoid questions l...\"],[\"```\\n\\nWe can then have a look at this object to learn more about the SQuAD dataset:\\n\\n```py\\nraw_datase...\"],[\"```\\n\\nThe `context` and `question` fields are very straightforward to use. The `answers` field is a b...\"],[\"```\\n\\nWe won't dive into the evaluation script as it will all be wrapped up by a ğŸ¤— Datasets metric fo...\"],[\"```\\n\\nAs mentioned previously, we'll be fine-tuning a BERT model, but you can use any other model typ...\"],[\"```\\n\\nThe labels will then be the index of the tokens starting and ending the answer, and the model w...\"],[\"```\\n\\n```python out\\n'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [S...\"],[\"```\\n\\nAs we can see, our example has been in split into four inputs, each of them containing the ques...\"],[\"```\\n\\nAs we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as ...\"],[\"```\\n\\nAs we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave ...\"],[\"# Find the start and end of the context\\n    idx = 0\\n    while sequence_ids[idx] != 1:\\n        idx +=...\"],[\"```\\n\\n```python out\\n([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\\n [85, 53, 21,...\"],[\"```\\n\\n```python out\\n'Theoretical answer: a Marian place of prayer and reflection, decoded example: [C...\"],[\"```\\n\\nIndeed, we don't see the answer inside the context.\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Your turn!** When using the XL...\"],[\"# If the answer is not fully inside the context, label is (0, 0)\\n        if offset[context_start][0]...\"],[\"```\\n\\nNote that we defined two constants to determine the maximum length used as well as the length o...\"],[\"```\\n\\nAs we can see, the preprocessing added roughly 1,000 features. Our training set is now ready to...\"],[\"for i in range(len(inputs[\\\"input_ids\\\"])):\\n        sample_idx = sample_map[i]\\n        example_ids.app...\"],[\"```\\n\\nWe can apply this function on the whole validation dataset like before:\\n\\n```py\\nvalidation_datas...\"],[\"```\\n\\nIn this case we've only added a couple of hundred samples, so it appears the contexts in the va...\"],[\"- We masked the start and end logits corresponding to tokens outside of the context.\\n- We then conve...\"],[\"```\\n\\nNow that the preprocessing is done, we change the tokenizer back to the one we originally picke...\"],[\"```\\n\\n{:else}\\n\\n```python\\nimport tensorflow as tf\\nfrom transformers import TFAutoModelForQuestionAnswe...\"],[\"```\\n\\nWith this in hand, we can really get to work by looping through all the examples and, for each ...\"],[\"best_answer = max(answers, key=lambda x: x[\\\"logit_score\\\"])\\n    predicted_answers.append({\\\"id\\\": examp...\"],[\"```\\n\\nThe final format of the predicted answers is the one that will be expected by the metric we wil...\"],[\"```\\n\\nAgain, that's rather good considering that according to [its paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1910....\"],[\"predicted_answers = []\\n    for example in tqdm(examples):\\n        example_id = example[\\\"id\\\"]\\n       ...\"],[\"```\\n\\nWe can check it works on our predictions:\\n\\n```python\\ncompute_metrics(start_logits, end_logits, ...\"],[\"```\\n\\nIf you aren't working in a notebook, just type the following line in your terminal:\\n\\n```bash\\nhu...\"],[\"```\\n\\nWe've seen most of these before: we set some hyperparameters (like the learning rate, the numbe...\"],[\"```\\n\\nNext, we set up our training hyperparameters and compile our model:\\n\\n```python\\nfrom transformer...\"],[\"```\\n\\nFinally, we're ready to train with `model.fit()`. We use a `PushToHubCallback` to upload the mo...\"],[\"```\\n\\n{\\u002fif}\\n\\nNote that while the training happens, each time the model is saved (here, every epoch) i...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}\\n```\\n\\nGreat! As...\"],[\"```\\n\\nThe `Trainer` also drafts a model card with all the evaluation results and uploads it.\\n\\n{\\u002fif}\\n\\n...\"],[\"```\\n\\nNext we reinstantiate our model, to make sure we're not continuing the fine-tuning from before ...\"],[\"```\\n\\nTo push our model to the Hub, we will need to create a `Repository` object in a working folder....\"],[\"```\\n\\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. ...\"],[\"start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\\n        end_logits.appen...\"],[\"```\\n\\nIn case this is the first time you're seeing a model saved with ğŸ¤— Accelerate, let's take a mome...\"],[\"```\\n\\nThe first line is self-explanatory: it tells all the processes to wait until everyone is at tha...\"],[\"```\\n\\n```python out\\n{'score': 0.9979003071784973,\\n 'start': 78,\\n 'end': 105,\\n 'answer': 'Jax, PyTorch...\"],[\"ow to write a good issue on GitHub? GitHub is the main place for the Hugging Face open source librar...\"],[\"this. Just execute it in your notebook or in a terminal, and copy paste the results. There are two l...\"],[\"The Hugging Face Hub[[the-hugging-face-hub]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames=\\\"...\"],[\"The video below shows how to navigate the Hub.\\n\\n\\u003cYoutube id=\\\"XvSGPZFEjDY\\\"\\u002f\\u003e\\n\\nHaving a huggingface.co...\"],[\"n this video, we will study together \\\"the Unigram Language Model subword tokenization algorithm\\\".\\n\\nT...\"],[\"Before going further in the explanation of the training algorithm, I need to explain what is an Unig...\"],[\"by the total number of appearance of all the tokens. We could use this vocabulary to tokenize our wo...\"],[\"there is only one possible tokenization left for hug. The tokenization of the other words of the voc...\"],[\"Creating your own dataset[[creating-your-own-dataset]]\\n\\n\\u003cCourseFloatingBanner chapter={5}\\n  classNam...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-course\\u002fdocum...\"],[\"```\\n\\nOnce the library is installed, you can make GET requests to the `Issues` endpoint by invoking t...\"],[\"```python out\\n[{'url': 'https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002f2792',\\n  'repositor...\"],[\"'following_url': 'https:\\u002f\\u002fapi.github.com\\u002fusers\\u002fbhavitvyamalik\\u002ffollowing{\\u002fother_user}',\\n   'gists_url...\"],[\"'html_url': 'https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets\\u002fpull\\u002f2792',\\n   'diff_url': 'https:\\u002f\\u002fgithub.com\\u002f...\"],[\"```\\n\\nWhoa, that's a lot of information! We can see useful fields like `title`, `body`, and `number` ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nâš ï¸ Do not share a notebook with your `GITHUB_TOKEN` pasted in it. We reco...\"],[\"all_issues.extend(batch)\\n    df = pd.DataFrame.from_records(all_issues)\\n    df.to_json(f\\\"{issues_pat...\"],[\"```\\n\\nNow when we call `fetch_issues()` it will download all the issues in batches to avoid exceeding...\"],[\"```\\n\\nGreat, we've created our first dataset from scratch! But why are there several thousand issues ...\"],[\"```\\n\\n```python out\\n\\u003e\\u003e URL: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets\\u002fpull\\u002f850\\n\\u003e\\u003e Pull request: {'url':...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Try it out!** Calculate the average time it takes to close issues in ğŸ¤— Datasets. Yo...\"],[\"```py\\nissue_number = 2792\\nurl = f\\\"https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002f{issue_nu...\"],[\"```python out\\n[{'url': 'https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002fcomments\\u002f897594128'...\"],[\"'subscriptions_url': 'https:\\u002f\\u002fapi.github.com\\u002fusers\\u002fbhavitvyamalik\\u002fsubscriptions',\\n   'organizations_...\"],[\"'updated_at': '2021-08-12T12:31:17Z',\\n  'author_association': 'CONTRIBUTOR',\\n  'body': \\\"@albertvilla...\"],[\"```\\n\\nWe can see that the comment is stored in the `body` field, so let's write a simple function tha...\"],[\"```\\n\\nThis looks good, so let's use `Dataset.map()` to add a new `comments` column to each issue in o...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '...\"],[\"```\\n\\nCool, we've pushed our dataset to the Hub and it's available for others to use! There's just on...\"],[\"2. Read the [ğŸ¤— Datasets guide](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets\\u002fblob\\u002fmaster\\u002ftemplates\\u002fREADME_...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. Which of the following is an example of subword tokenization?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\tt...\"],[\"{#if fw === 'pt'}\\n### 5. What is an AutoModel?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A model that aut...\"],[\"{\\u002fif}\\n\\n### 6. What are the techniques to be aware of when batching sequences of different lengths to...\"],[\"### 8. What method is most of the tokenizer API centered around?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext:...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A list of strings, each string being a token\\\",\\n\\t\\t\\texplain: ...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"No, it seems correct.\\\",\\n\\t\\t\\texplain: \\\"Unfortunately, couplin...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"No, it seems correct.\\\",\\n\\t\\t\\texplain: \\\"Unfortunately, couplin...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Processing the data[[processing-the-data]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCou...\"],[\"```python\\nimport torch\\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassifica...\"],[\"```\\n{:else}\\nContinuing with the example from the [previous chapter](\\u002fcourse\\u002fchapter2), here is how w...\"],[\"```\\n{\\u002fif}\\n\\nOf course, just training the model on two sentences is not going to yield very good resul...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['sentence1', 'sentence2', '...\"],[\"```\\n\\n```python out\\n{'sentence1': Value(dtype='string', id=None),\\n 'sentence2': Value(dtype='string',...\"],[\"```\\n\\nHowever, we can't just pass two sequences to the model and get a prediction of whether the two ...\"],[\"```\\n\\nSo we see the model expects the inputs to be of the form `[CLS] sentence1 [SEP] sentence2 [SEP]...\"],[\"```\\n\\nAs you can see, the parts of the input corresponding to `[CLS] sentence1 [SEP]` all have a toke...\"],[\"```\\n\\nThis works well, but it has the disadvantage of returning a dictionary (with our keys, `input_i...\"],[\"```\\n\\nThis function takes a dictionary (like the items of our dataset) and returns a new dictionary w...\"],[\"```\\n\\nThe way the ğŸ¤— Datasets library applies this processing is by adding new fields to the datasets,...\"],[\"```\\n\\nYou can even use multiprocessing when applying your preprocessing function with `map()` by pass...\"],[\"{:else}\\n\\nThe function that is responsible for putting together samples inside a batch is called a *c...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import DataCollatorWithPadding\\n\\ndata_collator = DataCollatorWith...\"],[\"```\\n\\n{:else}\\n\\n```python out\\n{'attention_mask': torch.Size([8, 67]),\\n 'input_ids': torch.Size([8, 67]...\"],[\"```\\n\\nLooking good! Now that we've gone from raw text to batches our model can deal with, we're ready...\"],[\"```\\n\\nAnd that's it! We can take those datasets forward into the next lecture, where training will be...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Models[[models]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanner chapter=...\"],[\"The `AutoModel` class and all of its relatives are actually simple wrappers over the wide variety of...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import BertConfig, TFBertModel\\n\\n# Building the config\\nconfig = B...\"],[\"```\\n{\\u002fif}\\n\\nThe model can be used in this state, but it will output gibberish; it needs to be trained...\"],[\"```\\n\\nAs you saw earlier, we could replace `TFBertModel` with the equivalent `TFAutoModel` class. We'...\"],[\"```\\nls directory_on_my_computer\\n\\nconfig.json pytorch_model.bin\\n```\\n{:else}\\n```\\nls directory_on_my_co...\"],[\"```\\n\\nThe tokenizer converts these to vocabulary indices which are typically called *input IDs*. Each...\"],[\"ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo...\"],[\"we have the configuration, we can create a model that has the same architecture as our checkpoint bu...\"],[\"Introduction to Gradio[[introduction-to-gradio]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={9}\\n    classNam...\"],[\"* An extractive **question answering** model that takes in a context paragraph and a quest and outpu...\"],[\"This chapter is broken down into sections which include both _concepts_ and _applications_. After yo...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Handling multiple sequences[[handling-multiple-sequences]]\\n\\n{#if fw...\"],[\"- How do we handle multiple sequences?\\n- How do we handle multiple sequences *of different lengths*?...\"],[\"```\\n\\n```python out\\nIndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1...\"],[\"```\\n{:else}\\n```py\\ntokenized_inputs = tokenizer(sequence, return_tensors=\\\"tf\\\")\\nprint(tokenized_inputs...\"],[\"```\\n{:else}\\n```py\\nimport tensorflow as tf\\nfrom transformers import AutoTokenizer, TFAutoModelForSequ...\"],[\"```\\nbatched_ids = [ids, ids]\\n```\\n\\nThis is a batch of two identical sequences!\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Try it ou...\"],[\"```\\n\\nThe padding token ID can be found in `tokenizer.pad_token_id`. Let's use it and send our two se...\"],[\"```\\n\\n```py out\\ntf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)\\ntf.Tensor([[ 0.5803...\"],[\"```\\n\\n```python out\\ntensor([[ 1.5694, -1.3895],\\n        [ 0.5803, -0.4125]], grad_fn=\\u003cAddmmBackward\\u003e)...\"],[\"```\\n{\\u002fif}\\n\\nNow we get the same logits for the second sentence in the batch.\\n\\nNotice how the last val...\"],[\"Advanced Interface features[[advanced-interface-features]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  clas...\"],[\"iface = gr.Interface(\\n    chat,\\n    [\\\"text\\\", \\\"state\\\"],\\n    [\\\"chatbot\\\", \\\"state\\\"],\\n    allow_screensho...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-Chatbot-Demo.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"350\\\" title=\\\"Gr...\"],[\"# Download human-readable labels for ImageNet.\\nresponse = requests.get(\\\"https:\\u002f\\u002fgit.io\\u002fJJkYN\\\")\\nlabel...\"],[\"```\\n\\nTest the interpretation function by submitting an input then clicking Interpret under the outpu...\"],[\"n this video we will see together what is the purpose of training a tokenizer, what are the key step...\"],[\"to train a new tokenizer it is first necessary to build a training corpus composed of raw texts. The...\"],[\"is finished, we just have to save our new tokenizer locally or send it to the hub to be able to reus...\"],[\"he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune ...\"],[\"the model predictions), label_ids (which contains the labels if your dataset had them) and metrics (...\"],[\"he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers tha...\"],[\"are the special tokens. The special tokens are added by the prepare_for_model method, which knows th...\"],[\"Sharing demos with others[[sharing-demos-with-others]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  classNam...\"],[\"To add additional content to your demo, the `Interface` class supports some optional parameters:\\n   ...\"],[\"article = \\\"Check out [the original Rick and Morty Bot](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fkingabzpro\\u002fRick...\"],[\"```\\n\\nUsing the options above, we end up with a more complete interface. Try the interface below:\\n\\n\\u003ci...\"],[\"```\\n\\nThis generates a public, shareable link that you can send to anybody! When you send this link, ...\"],[\"We can load the labels from [class_names.txt](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdawood\\u002fSketch-Recognitio...\"],[\"def predict(im):\\n    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) \\u002f 255.0\\n   ...\"],[\"```\\n\\nNow that we have a `predict()` function. The next step is to define and launch our gradio inter...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fast tokenizers in the QA pipeline[[fast-tokenizers-in-the-qa-pipel...\"],[\"{#if fw === 'pt'}\\n\\n\\u003cYoutube id=\\\"_wxyB3j3mk4\\\"\\u002f\\u003e\\n\\n{:else}\\n\\n\\u003cYoutube id=\\\"b3u8RzBCX9Y\\\"\\u002f\\u003e\\n\\n{\\u002fif}\\n\\n## Usin...\"],[\"```\\n\\n```python out\\n{'score': 0.97773,\\n 'start': 78,\\n 'end': 105,\\n 'answer': 'Jax, PyTorch and Tensor...\"],[\"```\\n\\nUnlike the other pipelines, which can't truncate and split texts that are longer than the maxim...\"],[\"ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and Tensor...\"],[\"```\\n\\n```python out\\n{'score': 0.97149,\\n 'start': 1892,\\n 'end': 1919,\\n 'answer': 'Jax, PyTorch and Ten...\"],[\"```\\n\\n{\\u002fif}\\n\\nNote that we tokenize the question and the context as a pair, with the question first.\\n\\n...\"],[\"```\\n\\n{:else}\\n\\n```python out\\n(1, 66) (1, 66)\\n```\\n\\n{\\u002fif}\\n\\nTo convert those logits into probabilities, ...\"],[\"```\\n\\n{:else}\\n\\n```py\\nstart_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()\\nend_prob...\"],[\"```\\n\\n{:else}\\n\\nThen we'll mask the values where `start_index \\u003e end_index` by setting them to `0` (the...\"],[\"```\\n\\nNow we just have to format everything to get our result:\\n\\n```py\\nresult = {\\n    \\\"answer\\\": answer...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\n[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Tran...\"],[\"```\\n\\nThis means the model will have a hard time picking the correct answer. To fix this, the `questi...\"],[\"```\\n\\n```python out\\n[0, 0, 0, 0, 0, 0, 0]\\n```\\n\\nThis is more useful when we tokenize several sentences...\"],[\"```\\n\\nThose `inputs` will contain the input IDs and attention masks the model expects, as well as the...\"],[\"```\\n\\n{:else}\\n\\n```python out\\n(2, 384) (2, 384)\\n```\\n\\n{\\u002fif}\\n\\nLike before, we first mask the tokens that...\"],[\"```\\n\\n{:else}\\n\\n```py\\nstart_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()\\nend_probabi...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n[(0, 18, 0.33867), (173, 184, 0.97149)]\\n```\\n\\nThose two candidates correspo...\"],[\"```\\n\\nIf we ignore the first result, we get the same result as our pipeline for this long context -- ...\"],[\"he tokenization pipeline involves several steps that convert raw text into numbers. In this video, w...\"],[\"et's see how to preprocess a dataset for summarization. This is the task of well summarizing a long ...\"],[\"he post-processing step in a question answering task. When doing question answering, the processing ...\"],[\"at more of the logits. Note that in the question-answering pipeline, we attributed score to each ans...\"],[\"A full training[[a-full-training]]\\n\\n\\u003cCourseFloatingBanner chapter={3}\\n  classNames=\\\"absolute z-10 ri...\"],[\"```\\n\\n### Prepare for training[[prepare-for-training]]\\n\\nBefore actually writing our training loop, we...\"],[\"```\\n\\nTo quickly check there is no mistake in the data processing, we can inspect a batch like this:\\n...\"],[\"```\\n\\n```python out\\ntensor(0.5441, grad_fn=\\u003cNllLossBackward\\u003e) torch.Size([8, 2])\\n```\\n\\nAll ğŸ¤— Transform...\"],[\"```\\n\\n```python out\\n1377\\n```\\n\\n### The training loop[[the-training-loop]]\\n\\nOne last thing: we will wan...\"],[\"```\\n\\nYou can see that the core of the training loop looks a lot like the one in the introduction. We...\"],[\"```\\n\\nAgain, your results will be slightly different because of the randomness in the model head init...\"],[\"```\\n\\nAnd here are the changes:\\n\\n```diff\\n+ from accelerate import Accelerator\\n  from transformers imp...\"],[\"```\\n\\nThe first line to add is the import line. The second line instantiates an `Accelerator` object ...\"],[\"progress_bar = tqdm(range(num_training_steps))\\n\\nmodel.train()\\nfor epoch in range(num_epochs):\\n    fo...\"],[\"```\\n\\nPutting this in a `train.py` script will make that script runnable on any kind of distributed s...\"],[\"Transformers, what can they do?[[transformers-what-can-they-do]]\\n\\n\\u003cCourseFloatingBanner chapter={1}\\n...\"],[\"\\u003cTip\\u003e\\nâš ï¸ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of mod...\"],[\"```\\n\\n```python out\\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\\n```\\n\\nWe can even pass severa...\"],[\"```\\n\\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for se...\"],[\"```\\n\\n```python out\\n{'sequence': 'This is a course about the Transformers library',\\n 'labels': ['educ...\"],[\"```\\n\\nYou can control how many different sequences are generated with the argument `num_return_sequen...\"],[\"```\\n\\nYou can refine your search for a model by clicking on the language tags, and pick a model that ...\"],[\"```\\n\\nThe `top_k` argument controls how many possibilities you want to be displayed. Note that here t...\"],[\"```\\n\\nHere the model correctly identified that Sylvain is a person (PER), Hugging Face an organizatio...\"],[\"```\\n\\n```python out\\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\\n`...\"],[\"```\\n\\nLike with text generation, you can specify a `max_length` or a `min_length` for the result.\\n\\n\\n#...\"],[\"sing the Python debugger in a notebook. In this video, we'll learn how to use the Python debugger in...\"],[\"input IDs, attention mask and token type IDs, so we have to pad the labels ourselves before trying t...\"],[\"ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo...\"],[\"28,996. Once we have the configuration, we can create a model that has the same architecture as our ...\"],[\"n this video, we're going to understand how to manage a model repository on the HuggingFace model hu...\"],[\"File\\\" button. The added files can be of any type: python, json, text, you name it! Alongside your ad...\"],[\"here for simplicity's sake, but we encourage you to add information relevant to how was the model tr...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Behind the pipeline[[behind-the-pipeline]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCou...\"],[\"Let's start with a complete example, taking a look at what happened behind the scenes when we execut...\"],[\"```\\n\\nand obtained:\\n\\n```python out\\n[{'label': 'POSITIVE', 'score': 0.9598047137260437},\\n {'label': 'N...\"],[\"```\\n\\nAs we saw in [Chapter 1](\\u002fcourse\\u002fchapter1), this pipeline groups together three steps: preproce...\"],[\"Since the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetu...\"],[\"```\\n\\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dicti...\"],[\"```\\n{\\u002fif}\\n\\nDon't worry about padding and truncation just yet; we'll explain those later. The main th...\"],[\"```\\n{:else}\\n\\nHere's what the results look like as TensorFlow tensors:\\n\\n```python out\\n{\\n    'input_id...\"],[\"```\\n{:else}\\nWe can download our pretrained model the same way we did with our tokenizer. ğŸ¤— Transform...\"],[\"```\\n\\n```python out\\ntorch.Size([2, 16, 768])\\n```\\n{:else}\\n```py\\noutputs = model(inputs)\\nprint(outputs....\"],[\"```\\n{\\u002fif}\\n\\nNote that the outputs of ğŸ¤— Transformers models behave like `namedtuple`s or dictionaries....\"],[\"{#if fw === 'pt'}\\nFor our example, we will need a model with a sequence classification head (to be a...\"],[\"```\\n{:else}\\nFor our example, we will need a model with a sequence classification head (to be able to...\"],[\"```\\n{\\u002fif}\\n\\nOur model predicted `[-1.5607, 1.6123]` for the first sentence and `[ 4.1692, -3.3464]` f...\"],[\"```\\n\\n```python out\\n{0: 'NEGATIVE', 1: 'POSITIVE'}\\n```\\n\\nNow we can conclude that the model predicted ...\"],[\"ğŸ¤— Datasets, check![[datasets-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={5}\\n    classNames=\\\"absolute...\"],[\"n this video we will see together what is the normalizer component that we find at the beginning of ...\"],[\"are transformed into a single \\\"code point\\\" by UTF-8. The unicode standard then allows us to find the...\"],[\"Building a tokenizer, block by block[[building-a-tokenizer-block-by-block]]\\n\\n\\u003cCourseFloatingBanner c...\"],[\"The ğŸ¤— Tokenizers library has been built to provide several options for each of those steps, which yo...\"],[\"\\u003cYoutube id=\\\"MR8tZm5ViWU\\\"\\u002f\\u003e\\n\\nMore precisely, the library is built around a central `Tokenizer` class...\"],[\"## Acquiring a corpus[[acquiring-a-corpus]]\\n\\nTo train our new tokenizer, we will use a small corpus ...\"],[\"```\\n\\nThe function `get_training_corpus()` is a generator that will yield batches of 1,000 texts, whi...\"],[\"```\\n\\nWe have to specify the `unk_token` so the model knows what to return when it encounters charact...\"],[\"```\\n\\n```python out\\nhello how are u?\\n```\\n\\n\\u003cTip\\u003e\\n\\n**To go further** If you test the two versions of th...\"],[\"```\\n\\nIf you only want to split on whitespace, you should use the `WhitespaceSplit` pre-tokenizer ins...\"],[\"```\\n\\nAs well as specifying the `vocab_size` and `special_tokens`, we can set the `min_frequency` (th...\"],[\"```\\n\\nThe `encoding` obtained is an `Encoding`, which contains all the necessary outputs of the token...\"],[\"```\\n\\n```python out\\n['[CLS]', 'let', \\\"'\\\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']...\"],[\"```\\n\\nTo use this tokenizer in ğŸ¤— Transformers, we have to wrap it in a `PreTrainedTokenizerFast`. We ...\"],[\"```\\n\\nYou can then use this tokenizer like any other ğŸ¤— Transformers tokenizer. You can save it with t...\"],[\"```\\n\\n```python out\\n[('Let', (0, 3)), (\\\"'s\\\", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (...\"],[\"```\\n\\nWe apply the byte-level post-processing for the GPT-2 tokenizer as follows:\\n\\n```python\\ntokenize...\"],[\"```\\n\\nor:\\n\\n```python\\nfrom transformers import GPT2TokenizerFast\\n\\nwrapped_tokenizer = GPT2TokenizerFas...\"],[\"```\\n\\nNext is the model, which needs training. XLNet has quite a few special tokens:\\n\\n```python\\nspeci...\"],[\"```\\n\\n```python out\\n['â–Let', \\\"'\\\", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']\\n```\\n\\nA peculiarit...\"],[\"```\\n\\n```python out\\n['â–Let', \\\"'\\\", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '\\u003csep\\u003e'...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 2. What part of the preprocessing for token classification differs from the other preprocessing ...\"],[\"### 4. What does \\\"domain adaptation\\\" mean?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's when we run a m...\"],[\"### 6. Which of these tasks can be seen as a sequence-to-sequence problem?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"{#if fw === 'pt'}\\n\\n### 8. Why is there a specific subclass of `Trainer` for sequence-to-sequence pro...\"],[\"{\\u002fif}\\n\\n### 10. When should you pretrain a new model?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"When there...\"],[\"### 12. What are the main challenges when preprocessing data for a question answering task?\\n\\n\\u003cQuesti...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. What can you do using the Hugging Face Hub web interface? \\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext...\"],[\"{#if fw === 'pt'}\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A tokenizer\\\",\\n\\t\\t\\texplain: \\\"Correct! All tokeni...\"],[\"correct: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \\\"All of the above with a dedicated callback\\\",\\n\\t\\t\\texplain: \\\"That's ri...\"],[\"### 6. What is the first step when using the `push_to_hub()` method or the CLI tools?\\n\\n\\u003cQuestion\\n\\tch...\"],[\"ext embeddings and semantic search. In this video weâ€™ll explore how Transformer models represent tex...\"],[\"want! And once we have our sentence embeddings, we can compute the cosine similarity for each pair o...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. Suppose you try to run the following code, which throws an error:\\n\\n```py\\nfrom transformers im...\"],[\"```\\n\\nWhich of the following might be a good choice for the title of a forum topic to ask for help?\\n\\n...\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"The optimization step where we compute gradients and perform bac...\"],[\"### 6. What is the best way to get an issue on GitHub fixed?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"Po...\"],[\"### 8. Why is it a good idea to include details on your compute environment with `transformers-cli e...\"],[\"et's study the transformer architecture. This video is the introductory video to the encoders, decod...\"],[\"a high-level representation of those inputs. These outputs are then passed to the decoder. The decod...\"],[\"Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  ...\"],[\"We will explore all of these concepts below.\\n\\n### Creating a simple demo using Blocks[[creating-a-si...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-flip-text.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"400\\\" title=\\\"Gradi...\"],[\"In the example above, we run the `flip_text()` function when the value in the `Textbox` named input ...\"],[\"Finally, you can also create tabs for your demo by using the `with gradio.Tabs()` context manager. W...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-flip-text-image.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"450\\\" title=...\"],[\"- `fn`: the function to run\\n- `inputs`: a (list of) component(s) whose values should supplied as the...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-blocks-gpt.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"300\\\" title=\\\"Grad...\"],[\"b1 = gr.Button(\\\"Recognize Speech\\\")\\n    b2 = gr.Button(\\\"Classify Sentiment\\\")\\n\\n    b1.click(speech_to_...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-blocks-multi-step.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"600\\\" titl...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-blocks-update-component-properties.hf.space\\\" frameBorder=\\\"0\\\" ...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. Where can you launch a Gradio demo from?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \\\"Standard ...\"],[\"### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?\\n\\n\\u003cQ...\"],[\"### 8. Which of the following are components included in the Gradio library?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n...\"],[\"### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spac...\"],[\"i, this is going to be a video about the push_to_hub API for Tensorflow and Keras. So, to get starte...\"],[\"saved to before they're uploaded to the Hub. The second argument is the tokenizer, and the third arg...\"],[\"So now if we drop over to my profile on HuggingFace, and you can get there just by clicking the prof...\"],[\"valid English grammar and hopefully the model will see that. It's going to reload here, so I'm going...\"],[\"et's take a look at word-based tokenization. Word-based tokenization is the idea of splitting the ra...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Putting it all together[[putting-it-all-together]]\\n\\n{#if fw === 'pt...\"],[\"```py\\nfrom transformers import AutoTokenizer\\n\\ncheckpoint = \\\"distilbert-base-uncased-finetuned-sst-2-...\"],[\"```\\n\\nHere, the `model_inputs` variable contains everything that's necessary for a model to operate w...\"],[\"```\\n\\nThe `tokenizer` object can handle the conversion to specific framework tensors, which can then ...\"],[\"```\\n\\nOne token ID was added at the beginning, and one at the end. Let's decode the two sequences of ...\"],[\"```\\n{:else}\\n```py\\nimport tensorflow as tf\\nfrom transformers import AutoTokenizer, TFAutoModelForSequ...\"],[\"n this video, I'm going to give you a very quick introduction to how our transformers models work to...\"],[\"those methods in other videos that I'll link below. For now the key thing to take away from this vid...\"],[\"Gradio, check![[gradio-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={9}\\n    classNames=\\\"absolute z-10 ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fast tokenizers' special powers[[fast-tokenizers-special-powers]]\\n\\n...\"],[\"\\u003cYoutube id=\\\"g8quOxoqhHQ\\\"\\u002f\\u003e\\n\\nIn the following discussion, we will often make the distinction between...\"],[\"Let's take a look at an example:\\n\\n```py\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoToke...\"],[\"```\\n\\nAs mentioned previously, we get a `BatchEncoding` object in the tokenizer's output:\\n\\n```python ...\"],[\"```\\n\\nWe can see that the tokenizer's special tokens `[CLS]` and `[SEP]` are mapped to `None`, and th...\"],[\"```py\\nstart, end = encoding.word_to_chars(3)\\nexample[start:end]...\"],[\"```\\n\\n```python out\\nSylvain...\"],[\"```\\n\\nAs we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of t...\"],[\"```py\\nfrom transformers import pipeline\\n\\ntoken_classifier = pipeline(\\\"token-classification\\\")\\ntoken_c...\"],[\"```\\n\\n```python out\\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'e...\"],[\"```\\n\\nThe model properly identified each token generated by \\\"Sylvain\\\" as a person, each token generat...\"],[\"```\\n\\nThe `aggregation_strategy` picked will change the scores computed for each grouped entity. With...\"],[\"example = \\\"My name is Sylvain and I work at Hugging Face in Brooklyn.\\\"\\ninputs = tokenizer(example, r...\"],[\"```\\n\\nSince we're using `AutoModelForTokenClassification` here, we get one set of logits for each tok...\"],[\"```\\n\\n```python out\\n(1, 19)\\n(1, 19, 9)\\n```\\n\\n{\\u002fif}\\n\\nWe have a batch with 1 sequence of 19 tokens and t...\"],[\"```\\n\\nAs we saw earlier, there are 9 labels: `O` is the label for the tokens that are not in any name...\"],[\"With this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we c...\"],[\"```\\n\\n```python out\\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\\n {'entity': 'I...\"],[\"```\\n\\n```python out\\n[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22...\"],[\"```\\n\\nThis is the same as what we got from the first pipeline!\\n\\n### Grouping entities[[grouping-entit...\"],[\"```\\n\\n```python out\\nHugging Face\\n```\\n\\nTo write the code that post-processes the predictions while gro...\"],[\"```\\n\\nAnd we get the same results as with our second pipeline!\\n\\n```python out\\n[{'entity_group': 'PER'...\"],[\"et's take a look at subword-based tokenization. Understanding why subword-based tokenization is inte...\"],[\"token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that izat...\"],[\"et's see together what is the training strategy of the WordPiece algorithm and how it performs the t...\"],[\"use it to tokenize a text.  Let's say we want to tokenize the word \\\"huggingface\\\".  WordPiece follows...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Summarization[[summarization]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingB...\"],[\"\\u003cYoutube id=\\\"yHnr5Dk2zCI\\\"\\u002f\\u003e\\n\\nAlthough there already exist various fine-tuned models for summarizatio...\"],[\"## Preparing a multilingual corpus[[preparing-a-multilingual-corpus]]\\n\\nWe'll use the [Multilingual A...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['review_id', 'product_id', ...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e Title: Worked in front position, not rear'\\n'\\u003e\\u003e Review: 3 stars because these ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Try it out!** Change the random seed in the `Dataset.shuffle()` command to explore ...\"],[\"```\\n\\nThe most popular products in the English dataset are about household items, clothing, and wirel...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e Title: I\\\\'m dissapointed.'\\n'\\u003e\\u003e Review: I guess I had higher expectations for ...\"],[\"```\\n\\nOkay, we can see that the reviews are not strictly about books and might refer to things like c...\"],[\"```\\n\\nThis certainly looks like a mix of English and Spanish reviews! Now that we have a training cor...\"],[\"```\\n\\nNow that we've prepared our corpus, let's take a look at a few possible Transformer models that...\"],[\"| Transformer model | Description                                                                   ...\"],[\"|    [BART](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-base)     | A novel Transformer architecture with b...\"],[\"As you can see from this table, the majority of Transformer models for summarization (and indeed mos...\"],[\"\\u003cTip\\u003e\\n\\nâœï¸ **Try it out!** Once you've worked through this section, see how well mT5 compares to mBAR...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ In the early stages of your NLP projects, a good practice is to train a class of \\\"smal...\"],[\"```\\n\\n```python out\\n['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '\\u003c\\u002fs\\u003e']\\n```\\n\\nTh...\"],[\"```\\n\\nLet's walk through this code to understand what's happening. The first thing we've done is defi...\"],[\"```\\n\\nNow that the corpus has been preprocessed, let's take a look at some metrics that are commonly ...\"],[\"```\\n\\nOne way to compare them could be to count the number of overlapping words, which in this case w...\"],[\"Applying this to our verbose summary gives a precision of 6\\u002f10  = 0.6, which is considerably worse t...\"],[\"```\\n\\nand then loading the ROUGE metric as follows:\\n\\n```python\\nimport evaluate\\n\\nrouge_score = evaluat...\"],[\"```\\n\\nWhoa, there's a lot of information in that output -- what does it all mean? First, ğŸ¤— Datasets a...\"],[\"```\\n\\n```python out\\nScore(precision=0.86, recall=1.0, fmeasure=0.92)\\n```\\n\\nGreat, the precision and re...\"],[\"```\\n\\nand then download the punctuation rules:\\n\\n```python\\nimport nltk\\n\\nnltk.download(\\\"punkt\\\")\\n```\\n\\nNe...\"],[\"```\\n\\n```python out\\n{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}\\n```\\n\\nWe can...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ If you're wondering why you don't see any warnings about fine-tuning the model ...\"],[\"```\\n\\nHere, the `predict_with_generate` argument has been set to indicate that we should generate sum...\"],[\"```python\\nimport numpy as np\\n\\n\\ndef compute_metrics(eval_pred):\\n    predictions, labels = eval_pred\\n ...\"],[\"```\\n\\n{\\u002fif}\\n\\nNext, we need to define a data collator for our sequence-to-sequence task. Since mT5 is ...\"],[\"```\\n\\n```python out\\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\"],[\"```\\n\\nThe main thing to notice here is that the first example is longer than the second one, so the `...\"],[\"```\\n\\nFrom the scores we can see that our model has handily outperformed our lead-3 baseline -- nice!...\"],[\"```\\n\\nThis will save the checkpoint and configuration files to `output_dir`, before uploading all the...\"],[\"```\\n\\nNow, we define our training hyperparameters and compile:\\n\\n```python\\nfrom transformers import cr...\"],[\"```\\n\\nWe got some loss values during training, but really we'd like to see the ROUGE metrics we compu...\"],[\"all_preds = []\\nall_labels = []\\nfor batch, labels in tqdm(tf_generate_dataset):\\n    predictions = gen...\"],[\"```\\n\\nOnce we have our lists of label and prediction strings, computing the ROUGE score is easy:\\n\\n```...\"],[\"```\\n\\nWe can then instantiate the data collator and use this to define our dataloaders:\\n\\n```python\\nfr...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸš¨ If you're training on a TPU, you'll need to move all the code above into a dedicated t...\"],[\"```\\n\\nThis should look familiar to you if you recall how we defined the `compute_metrics()` function ...\"],[\"```\\n\\nThis will allow us to push the artifacts back to the Hub by calling the `repo.push_to_hub()` me...\"],[\"generated_tokens = accelerator.pad_across_processes(\\n                generated_tokens, dim=1, pad_in...\"],[\"```\\n\\n```python out\\nEpoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.50...\"],[\"```\\n\\nAnd that's it! Once you run this, you'll have a model and results that are pretty similar to th...\"],[\"```\\n\\nThis is not too bad! We can see that our model has actually been able to perform _abstractive_ ...\"],[\"et's study how to preprocess a dataset for token classification! Token classification regroups any t...\"],[\"we write a function that shifts the labels for tokens that are inside a word (that you can customize...\"],[\"he fast tokenizers of the Transformers library are fast, but they also implement features that will ...\"],[\"into tokens,() before finally doing the post-processing, where special tokens are added. From the be...\"],[\"et's have a look inside the question answering pipeline. The question answering pipeline can extract...\"],[\"answer! Now, when the context is long, it might get truncated by the tokenizer. This might result in...\"],[\"Basic usage completed![[basic-usage-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={2}\\n    className...\"],[\"et's study how to preprocess a dataset for question answering! Question answering is the task of fin...\"],[\"how we can do it: using the sequence IDs of an input, we can determine the beginning and the end of ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={7}...\"],[\"Each section can be read independently.\\n\\n{\\u002fif}\\n\\n\\n\\u003cTip\\u003e\\n\\nIf you read the sections in sequence, you wi...\"],[\"hat happens inside the pipeline function? In this video, we will look at what actually happens when ...\"],[\"with 0s where the padding is applied. The second key, attention mask, indicates where padding has be...\"],[\"et's see how to preprocess a dataset for translation. This is the task of well translating a sentenc...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning, Check![[fine-tuning-check]]\\n\\n\\u003cCourseFloatingBanner\\n   ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\\n...\"],[\"The best way to debug an error that arises in `model.fit()` is to manually go through this whole pip...\"],[\"```\\n\\nIf you try to execute it, you might get some `VisibleDeprecationWarning`s when doing the datase...\"],[\"```\\n\\n`break` ends the loop after one iteration, so this grabs the first batch that comes out of `tra...\"],[\"```\\n\\nThis looks right, doesn't it? We're passing the `labels`, `attention_mask`, and `input_ids` to ...\"],[\"```\\n\\nNow we'll use the model's internal loss, and this problem should be resolved!\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Your...\"],[\"```\\n\\nOh no. \\n\\n`nan` is not a very encouraging loss value. Still, we've checked our data, and it look...\"],[\"```\\n\\nWell, this is tricky. Everything is `nan`! But that's strange, isn't it? How would all our logi...\"],[\"```\\n\\nWhen we run that, we get:\\n\\n```py out\\nTFSequenceClassifierOutput(loss=\\u003ctf.Tensor: shape=(16,), d...\"],[\"```\\n\\n*Now* we're getting somewhere! There are no `nan` values in our logits, which is reassuring. Bu...\"],[\"```python out\\narray([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,\\n        23212, ...\"],[\"2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,\\n         2110,  1998,  3977,  2000,  2...\"],[\"0,     0,     0,     0,     0,     0,     0,     0,     0,\\n            0,     0,     0,     0,     0...\"],[\"[  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,\\n         2252,  5689,  2013,  2010, ...\"],[\"```\\n\\nWell, there's a lot in here, but nothing stands out as unusual. Let's look at the labels:\\n\\n```p...\"],[\"```\\n\\nWe're training! No more `nan`s, and our loss is declining... sort of. If you watch it for a whi...\"],[\"Does anything stand out here? That's right -- the learning rate! When we just use the string `'adam'...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ You can also import the `create_optimizer()` function from ğŸ¤— Transformers, which will ...\"],[\"```\\n\\nNow our loss is really going somewhere! Training finally looks like it's working. There's a les...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Hungry Hungry TensorFlow ğŸ¦›[[hungry-hungry-tensorflow]]\\n\\nOne particular quirk of TensorFl...\"],[\"### Check your data (again!)[[check-your-data-again]]\\n\\nYour model will only learn something if it's ...\"],[\"```\\n\\nThen you can compare it with the first label, like so:\\n\\n```py\\nlabels = batch[\\\"labels\\\"].numpy()\\n...\"],[\"```\\n\\nOnce you can view your data like this, you can ask yourself the following questions:\\n\\n- Is the ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ If your training data is unbalanced, make sure to build a batch of training data conta...\"],[\"### Ask for help[[ask-for-help]]\\n\\nHopefully you will have found some advice in this section that hel...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\\n...\"],[\"The best way to debug an error that arises in `trainer.train()` is to manually go through this whole...\"],[\"```\\n\\nIf you try to execute it, you will be met with a rather cryptic error:\\n\\n```python out\\n'ValueErr...\"],[\"```\\n\\nDo you notice something wrong? This, in conjunction with the error message about `input_ids` mi...\"],[\"metric = evaluate.load(\\\"glue\\\", \\\"mnli\\\")\\n\\n\\ndef compute_metrics(eval_pred):\\n    predictions, labels = e...\"],[\"```\\n\\nThis new code will now give a different error (progress!):\\n\\n```python out\\n'ValueError: expected...\"],[\"```\\n\\n```python out\\ndict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'...\"],[\"```\\n\\nThat's good! Lastly, let's check our label:\\n\\n```py\\ntrainer.train_dataset[0][\\\"label\\\"]\\n```\\n\\n```py...\"],[\"```\\n\\nThis code creates the training dataloader, then iterates through it, stopping at the first iter...\"],[\"```\\n\\nSo this is the `default_data_collator`, but that's not what we want in this case. We want to pa...\"],[\"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\\n\\ntrainer = Trainer(\\n    model,\\n    args...\"],[\"```\\n\\nThe good news? We don't get the same error as before, which is definitely progress. The bad new...\"],[\"```\\n\\nIf you're running this code in a notebook, you may get a CUDA error that's similar to the one w...\"],[\"```\\n\\n```python out\\n~\\u002f.pyenv\\u002fversions\\u002f3.7.9\\u002fenvs\\u002fbase\\u002flib\\u002fpython3.7\\u002fsite-packages\\u002ftorch\\u002fnn\\u002ffunctional...\"],[\"```\\n\\n```python out\\n2\\n```\\n\\nWith two labels, only 0s and 1s are allowed as targets, but according to t...\"],[\"```\\n\\nWe aren't including the `trainer.train()` line yet, to take the time to check that everything l...\"],[\"```\\n\\nAgain, if you're using the default optimizer in the `Trainer`, you shouldn't get an error at th...\"],[\"```\\n\\n```python out\\nTypeError: only size-1 arrays can be converted to Python scalars\\n```\\n\\nYou will re...\"],[\"```\\n\\nThis tells us that the error originates in the `datasets\\u002fmetric.py` module -- so this is a prob...\"],[\"```\\n\\n```python out\\n{'accuracy': 0.625}\\n```\\n\\nNow our error is fixed! This was the last one, so our sc...\"],[\"```\\n\\nIn this instance, there are no more problems, and our script will fine-tune a model that should...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nAfter looking at your data, go through a few of the model's predictions and decode them too....\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ If your training data is unbalanced, make sure to build a batch of training data conta...\"],[\"```\\n\\n100% accuracy, now this is a nice example of overfitting (meaning that if you try your model on...\"],[\"Here are some additional resources that may prove helpful:\\n\\n- [\\\"Reproducibility as a vehicle for eng...\"],[\"Part 1 completed![[part-1-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames=\\\"absolut...\"],[\"Bias and limitations[[bias-and-limitations]]\\n\\n\\u003cCourseFloatingBanner chapter={1}\\n  classNames=\\\"absolu...\"],[\"```\\n\\n```python out\\n['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']\\n['nurse', 'waitress', 'te...\"],[\"n this video, we'll study the encoder-decoder architecture. An example of a popular encoder-decoder ...\"],[\"the numerical representation output by the encoder, can now be used to generate a second word. Pleas...\"],[\"and understanding what was said in the English language; extracting information from that language, ...\"],[\"et's have a look inside the token classification pipeline. In the pipeline video, we looked at the d...\"],[\"a token is in the same entity as the previous one.() Note that there are two ways of labelling used ...\"],[\"hat is the ROUGE metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but...\"],[\"precision, which in the ROUGE context measures how much of the generated summary was relevant. In th...\"],[\"WordPiece tokenization[[wordpiece-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6}\\n  classNames=\\\"ab...\"],[\"```\\nw ##o ##r ##d\\n```\\n\\nThus, the initial alphabet contains all the characters present at the beginni...\"],[\"```\\n\\nso the initial vocabulary will be `[\\\"b\\\", \\\"h\\\", \\\"p\\\", \\\"##g\\\", \\\"##n\\\", \\\"##s\\\", \\\"##u\\\"]` (if we forget a...\"],[\"```\\nVocabulary: [\\\"b\\\", \\\"h\\\", \\\"p\\\", \\\"##g\\\", \\\"##n\\\", \\\"##s\\\", \\\"##u\\\", \\\"##gs\\\", \\\"hu\\\"]\\nCorpus: (\\\"hu\\\" \\\"##g\\\", 10), ...\"],[\"```\\n\\nand we continue like this until we reach the desired vocabulary size.\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Now your tur...\"],[\"When the tokenization gets to a stage where it's not possible to find a subword in the vocabulary, t...\"],[\"```\\n\\nFirst, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece toke...\"],[\"```\\n\\nAs we saw before, the alphabet is the unique set composed of all the first letters of words, an...\"],[\"```\\n\\nNow that we are ready for training, let's write a function that computes the score of each pair...\"],[\"```\\n\\nNow, finding the pair with the best score only takes a quick loop:\\n\\n```python\\nbest_pair = \\\"\\\"\\nma...\"],[\"```\\n\\n```python out\\n['ab', '##o', '##u', '##t']\\n```\\n\\nNow we have everything we need to loop until we ...\"],[\"```\\n\\nWe can then look at the generated vocabulary:\\n\\n```py\\nprint(vocab)\\n```\\n\\n```python out\\n['[PAD]', ...\"],[\"```\\n\\nAs we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster.\\n\\n\\u003c...\"],[\"```\\n\\nWe can try it on any text:\\n\\n```python\\ntokenize(\\\"This is the Hugging Face course!\\\")\\n```\\n\\n```pyth...\"],[\"emory mapping and streaming. In this video we'll take a look at two core features of the Datasets li...\"],[\"To handle these large datasets, the Datasets library is built on two core features: the Apache Arrow...\"],[\"through a huge dataset without having to download it first. Tokenizing text with the map() method al...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={2}\\n    classNames=\\\"absolute z-10 ri...\"],[\"Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function...\"],[\"Introduction[[introduction]]\\n\\nWelcome to the Hugging Face course! This introduction will guide you t...\"],[\"The next step is to install the libraries that we'll be using in this course. We'll use `pip` for th...\"],[\"```\\n!pip install transformers\\n```\\n\\nYou can make sure the package was correctly installed by importin...\"],[\"```\\n!pip install transformers[sentencepiece]\\n```\\n\\nThis will take a bit of time, but then you'll be r...\"],[\"```\\npython -m venv .env\\n```\\n\\nYou should now have a directory called *.env* in your otherwise empty f...\"],[\"n this video we take a look at the data processing necessary to train causal language models. Causal...\"],[\"this example both sample 1 and 2 are shorter than the context size and would be discarded with the p...\"],[\"hat is the BLEU metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but ...\"],[\"perspective! For example, if our model just generates the word \\\"six\\\", we get a perfect unigram preci...\"],[\"that it assumes the human translations have already been tokenized and this makes it hard to compare...\"],[\"elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging ...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-10 ri...\"],[\"- Chapters 1 to 4 provide an introduction to the main concepts of the ğŸ¤— Transformers library. By the...\"],[\"## Who are we?[[who-are-we]]\\n\\nAbout the authors:\\n\\n[**Abubakar Abid**](https:\\u002f\\u002fhuggingface.co\\u002fabidlab...\"],[\"[**Dawood Khan**](https:\\u002f\\u002fhuggingface.co\\u002fdawoodkhan82) is a Machine Learning Engineer at Hugging Fac...\"],[\"- **Does taking this course lead to a certification?**\\nCurrently we do not have any certification fo...\"],[\"- **How can I contribute to the course?**\\nThere are many ways to contribute to the course! If you fi...\"],[\"```\\n@misc{huggingfacecourse,\\n  author = {Hugging Face},\\n  title = {The Hugging Face Course, 2022},\\n ...\"],[\"ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks pl...\"],[\"Question Answering video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a lo...\"],[\"Masked Language Modeling video\\n\\nWelcome to the Hugging Face tasks series! In this video weâ€™ll take a...\"],[\"Translation video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a look at t...\"],[\"as recorded adlib - need to generate transcript with Whisper :)...\"],[\"Big data? ğŸ¤— Datasets to the rescue![[big-data-datasets-to-the-rescue]]\\n\\n\\u003cCourseFloatingBanner chapte...\"],[\"## What is the Pile?[[what-is-the-pile]]\\n\\nThe Pile is an English text corpus that was created by [El...\"],[\"```\\n\\nNext, we can load the dataset using the method for remote files that we learned in [section 2](...\"],[\"```\\n\\nOkay, this looks like the abstract from a medical article. Now let's see how much RAM we've use...\"],[\"```\\n\\nNice -- despite it being almost 20 GB large, we're able to load and access the dataset with muc...\"],[\"Memory-mapped files can also be shared across multiple processes, which enables methods like `Datase...\"],[\"```\\n\\n```python out\\n'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB\\u002fs'\\n```\\n\\n...\"],[\"```\\n\\n```python out\\n{'meta': {'pmid': 11409574, 'language': 'eng'},\\n 'text': 'Epidemiology of hypoxae...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ To speed up tokenization with streaming you can pass `batched=True`, as we saw in the ...\"],[\"```\\n\\n```python out\\n[{'meta': {'pmid': 11409574, 'language': 'eng'},\\n  'text': 'Epidemiology of hypox...\"],[\"```\\n\\nLet's round out our exploration of dataset streaming with a common application: combining multi...\"],[\"```\\n\\nThis dataset is large enough to stress the RAM of most laptops, yet we've been able to load and...\"],[\"```\\n\\nHere we've used the `islice()` function from Python's `itertools` module to select the first tw...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Try it out!** Use one of the large Common Crawl corpora like [`mc4`](https:\\u002f\\u002fhuggin...\"],[\"et's see how we can preprocess our data for masked language modeling. As a reminder, masked language...\"],[\"the easy part. There is a data collator designed specifically for this in the Transformers library. ...\"],[\"Subtitles for the course videos\\n\\nThis folder contains all the subtitles for the course videos on You...\"],[\"```\\n1\\n00:00:05,850 --\\u003e 00:00:07,713\\næ¬¢è¿æ¥åˆ° Hugging Face è¯¾ç¨‹ã€‚\\n```\\n\\nTo handle this, we provide a script t...\"],[\"he pipeline function. The pipeline function is the most high-level API of the Transformers library. ...\"],[\"the pipeline to a given prompt, we can specify several arguments, such as the maximum length of the ...\"],[\"Sequence-to-sequence models[sequence-to-sequence-models]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    ...\"],[\"aving and reloading a dataset. In this video we'll take a look saving a dataset in various formats, ...\"],[\"case you'll need to loop over the splits of the DatasetDict object and save each dataset as an indiv...\"],[\"n our other videos we talked about the basics of fine-tuning a language model with Tensorflow (and a...\"],[\"constant rate until it hits zero right at the very end of training. So why do they call it polynomia...\"],[\"Tokenizers, check![[tokenizers-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={6}\\n    classNames=\\\"absolu...\"],[\"hat happens inside the pipeline function? In this video, we will look at what actually happens when ...\"],[\"0s where the padding is applied. The second key, attention mask, indicates where padding has been ap...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It will return classification scores for this sentence, wit...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"This pipeline requires that labels be given to classify thi...\"],[\"### 7. Select the sentence that best describes the terms \\\"model\\\", \\\"architecture\\\", and \\\"weights\\\".\\n\\n\\u003cQ...\"],[\"### 9. Which of those types of models would you use for summarizing texts?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"n this video we will see how you can create your own tokenizer from scratch! To create your own toke...\"],[\"at the level of spaces and the second one isolating the punctuation marks. Now, we can define the tr...\"],[\"Gradio Blocks Party[[gradio-blocks-party]]\\n\\nAlong with the release of the Gradio chapter of the cour...\"],[\"Training a new tokenizer from an old one[[training-a-new-tokenizer-from-an-old-one]]\\n\\n\\u003cCourseFloatin...\"],[\"\\u003cYoutube id=\\\"DJimQynXZsQ\\\"\\u002f\\u003e\\n\\n\\u003cTip warning={true}\\u003e\\n\\nâš ï¸ Training a tokenizer is not the same as traini...\"],[\"```\\n\\nWe can have a look at the training split to see which columns we have access to:\\n\\n```py\\nraw_dat...\"],[\"```\\n\\nThe first thing we need to do is transform the dataset into an _iterator_ of lists of texts -- ...\"],[\"```\\n\\nwe get them once and then an empty list:\\n\\n```python out\\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[]\\n```\\n\\n...\"],[\"```\\n\\nEven though we are going to train a new tokenizer, it's a good idea to do this to avoid startin...\"],[\"```\\n\\nThis tokenizer has a few special symbols, like `Ä ` and `ÄŠ`, which denote spaces and newlines, r...\"],[\"```\\n\\nThis command might take a bit of time if your corpus is very large, but for this dataset of 1.6...\"],[\"```py\\ntokens = tokenizer.tokenize(example)\\ntokens...\"],[\"```\\n\\n```python out\\n['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \\\"\\\"\\\"', 'Add',...\"],[\"```\\n\\n```python out\\n['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self'...\"],[\"```\\n\\nIn addition to the token corresponding to an indentation, here we can also see a token for a do...\"],[\"```\\n\\nYou're now all set for training a language model from scratch and fine-tuning it on your task a...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. How does the BERT model expect a pair of sentences to be processed?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"### 5. What does dynamic padding mean?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's when you pad the in...\"],[\"### 6. What is the purpose of a collate function?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It ensures al...\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"Nothing, but you get a warning.\\\",\\n\\t\\t\\texplain: \\\"You do get a warn...\"],[\"### 9. Why should you use the ğŸ¤— Accelerate library?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It provides...\"],[\"### 5. The TensorFlow models from `transformers` are already Keras models. What benefit does this of...\"],[\"et's have a look inside the token classification pipeline. In the pipeline video, we looked at the d...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a model with Keras[[fine-tuning-a-model-with-keras]]\\n\\n\\u003c...\"],[\"data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\\\"tf\\\")\\n\\ntf_train_dataset ...\"],[\"```\\n\\n### Training[[training]]\\n\\nTensorFlow models imported from ğŸ¤— Transformers are already Keras mode...\"],[\"```\\n\\nYou will notice that unlike in [Chapter 2](\\u002fcourse\\u002fchapter2), you get a warning after instantia...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nNote a very common pitfall here â€” you *can* just pass the name of the los...\"],[\"```py\\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\\n\\nbatch_size = 8\\nnum_epochs =...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThe ğŸ¤— Transformers library also has a `create_optimizer()` function that will create an ...\"],[\"```\\n\\nWe can convert these logits into the model's class predictions by using `argmax` to find the hi...\"],[\"he post-processing step in a question answering task. When doing question answering, the processing ...\"],[\"question, we will look at more of the logits. Note that in the question-answering pipeline, we attri...\"],[\"Asking for help on the forums[[asking-for-help-on-the-forums]]\\n\\n\\u003cCourseFloatingBanner chapter={8}\\n  ...\"],[\"Similarly, the [Intermediate](https:\\u002f\\u002fdiscuss.huggingface.co\\u002fc\\u002fintermediate\\u002f6) and [Research](https:...\"],[\"```\\n\\nNow suppose we try to embed a whole section of the [Wikipedia article](https:\\u002f\\u002fen.wikipedia.org...\"],[\"The Transformers TV series began around the same time. Produced by Sunbow\\nProductions and Marvel Pro...\"],[\"```\\n\\n```python output\\nIndexError: index out of range in self...\"],[\"```\\n\\nUh-oh, we've hit a problem -- and the error message is far more cryptic than the ones we saw in...\"],[\"Although this topic contains the error message we need help with, there are a few problems with the ...\"],[\"### Formatting your code snippets[[formatting-your-code-snippets]]\\n\\nReading source code is hard enou...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-course\\u002fdocum...\"],[\"n this video, we will see how to debug an error you encounter when running trainer.train(). As an ex...\"],[\"because for one, they put your kernel in a state that is not recoverable (so you have to restart you...\"],[\"Encoder models[[encoder-models]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-1...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Token classification[[token-classification]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cC...\"],[\"{\\u002fif}\\n\\nThe first application we'll explore is token classification. This generic task encompasses an...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-bert-finetuned-ner.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"350\\\" title=\\\"G...\"],[\"## Preparing the data[[preparing-the-data]]\\n\\nFirst things first, we need a dataset suitable for toke...\"],[\"```\\n\\nThis will download and cache the dataset, like we saw in [Chapter 3](\\u002fcourse\\u002fchapter3) for the ...\"],[\"```\\n\\n```python out\\n[3, 0, 7, 0, 0, 0, 7, 0, 0]\\n```\\n\\nThose are the labels as integers ready for train...\"],[\"```\\n\\nWe already saw these labels when digging into the `token-classification` pipeline in [Chapter 6...\"],[\"```\\n\\nAs we can see, entities spanning two words, like \\\"European Union\\\" and \\\"Werner Zwingmann,\\\" are a...\"],[\"```\\n\\n```python out\\nTrue\\n```\\n\\nTo tokenize a pre-tokenized input, we can use our `tokenizer` as usual ...\"],[\"```\\n\\n```python out\\n[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\\n```\\n\\nWith a tiny bit of work, we can t...\"],[\"```\\n\\nAs we can see, our function added the `-100` for the two special tokens at the beginning and th...\"],[\"```\\n\\nNote that we haven't padded our inputs yet; we'll do that later, when creating the batches with...\"],[\"```\\n\\nWe've done the hardest part! Now that the data has been preprocessed, the actual training will ...\"],[\"```\\n\\n{\\u002fif}\\n\\nTo test this on a few samples, we can just call it on a list of examples from our tokeni...\"],[\"```\\n\\n```python out\\n[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\\n[-100, 1, 2, -100]\\n```\\n\\n{#if fw === 'p...\"],[\"```\\n\\n\\n Next stop: the model itself.\\n\\n{\\u002fif}\\n\\n{#if fw === 'tf'}\\n\\n### Defining the model[[defining-the-...\"],[\"```\\n\\n```python out\\n9\\n```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nâš ï¸ If you have a model with the wrong number of labe...\"],[\"```\\n\\nThis will display a widget where you can enter your Hugging Face login credentials.\\n\\nIf you are...\"],[\"```\\n\\nNote also that we don't supply a `loss` argument to `compile()`. This is because the models can...\"],[\"```\\n\\nYou can specify the full name of the repository you want to push to with the `hub_model_id` arg...\"],[\"```\\n\\nWe can then load it via the `evaluate.load()` function like we did in [Chapter 3](\\u002fcourse\\u002fchapt...\"],[\"```\\n\\nNote that the metric takes a list of predictions (not just one) and a list of labels. Here's th...\"],[\"```\\n\\n{#if fw === 'pt'}\\n\\nThis is sending back a lot of information! We get the precision, recall, and...\"],[\"```\\n\\nNow that this is done, we are almost ready to define our `Trainer`. We just need a `model` to f...\"],[\"```\\n\\n\\n```python out\\n{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},\\n 'MISC'...\"],[\"```\\n\\nNow we can just pass them to the `AutoModelForTokenClassification.from_pretrained()` method, an...\"],[\"```\\n\\nThis will display a widget where you can enter your Hugging Face login credentials.\\n\\nIf you are...\"],[\"```\\n\\nYou've seen most of those before: we set some hyperparameters (like the learning rate, the numb...\"],[\"```\\n\\nNote that while the training happens, each time the model is saved (here, every epoch) it is up...\"],[\"```\\n\\nThe `Trainer` also drafts a model card with all the evaluation results and uploads it. At this ...\"],[\"```\\n\\nOnce we have all those objects, we can send them to the `accelerator.prepare()` method:\\n\\n```py\\n...\"],[\"```\\n\\n```python out\\n'sgugger\\u002fbert-finetuned-ner-accelerate'\\n```\\n\\nThen we can clone that repository in...\"],[\"```\\n\\nThen we can write the training loop. After defining a progress bar to follow how training goes,...\"],[\"predictions_gathered = accelerator.gather(predictions)\\n        labels_gathered = accelerator.gather(...\"],[\"```\\n\\nThe first line is self-explanatory: it tells all the processes to wait until everyone is at tha...\"],[\"```\\n\\n```python out\\n[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end...\"],[\"n this video we take a look at setting up a custom loss function for training. In the default loss f...\"],[\"to combine the loss with the weight per sample. We do this with element wise multiplication and then...\"],[\"Time to slice and dice[[time-to-slice-and-dice]]\\n\\n\\u003cCourseFloatingBanner chapter={5}\\n  classNames=\\\"ab...\"],[\"First we need to download and extract the data, which can be done with the `wget` and `unzip` comman...\"],[\"```\\n\\nSince TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can lo...\"],[\"```\\n\\n```python out\\n{'Unnamed: 0': [87571, 178045, 80482],\\n 'drugName': ['Naproxen', 'Duloxetine', 'M...\"],[\"```\\n\\nNote that we've fixed the seed in `Dataset.shuffle()` for reproducibility purposes. `Dataset.se...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['patient_id', 'drugName', '...\"],[\"```\\nlambda \\u003carguments\\u003e : \\u003cexpression\\u003e\\n```\\n\\nwhere `lambda` is one of Python's special [keywords](http...\"],[\"```\\n\\n```python out\\n['left ventricular dysfunction', 'adhd', 'birth control']\\n```\\n\\nIt works! Now that...\"],[\"```\\n\\nAs expected, we can see a `review_length` column has been added to our training set. We can sor...\"],[\"```\\n\\n```python out\\n{'train': 138514, 'test': 46108}\\n```\\n\\nAs you can see, this has removed around 15%...\"],[\"```\\n\\nAs you can see, the `Dataset.map()` method is quite useful for processing data -- and we haven'...\"],[\"```\\n\\nIf you're running this code in a notebook, you'll see that this command executes way faster tha...\"],[\"```\\n\\nYou can also time a whole cell by putting `%%time` at the beginning of the cell. On the hardwar...\"],[\"`Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by ...\"],[\"```\\n\\nYou can experiment a little with timing to determine the optimal number of processes to use; in...\"],[\"\\u003cTip\\u003e\\n\\nğŸ’¡ In machine learning, an _example_ is usually defined as the set of _features_ that we feed ...\"],[\"```\\n\\nLet's test this on one example before using `Dataset.map()` on the whole dataset:\\n\\n```py\\nresult...\"],[\"```\\n\\n```python out\\nArrowInvalid: Column 1 named condition expected length 1463 but got length 1000\\n`...\"],[\"```\\n\\n```python out\\n(206772, 138514)\\n```\\n\\nWe mentioned that we can also deal with the mismatched leng...\"],[\"```\\n\\nWe get the same number of training features as before, but here we've kept all the old fields. ...\"],[\"```\\n\\n\\u003ctable border=\\\"1\\\" class=\\\"dataframe\\\"\\u003e\\n  \\u003cthead\\u003e\\n    \\u003ctr style=\\\"text-align: right;\\\"\\u003e\\n      \\u003cth\\u003e\\u003c\\u002f...\"],[\"Let's create a `pandas.DataFrame` for the whole training set by selecting all the elements of `drug_...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸš¨ Under the hood, `Dataset.set_format()` changes the return format for the dataset's `__...\"],[\"```\\n\\n\\u003ctable border=\\\"1\\\" class=\\\"dataframe\\\"\\u003e\\n  \\u003cthead\\u003e\\n    \\u003ctr style=\\\"text-align: right;\\\"\\u003e\\n      \\u003cth\\u003e\\u003c\\u002f...\"],[\"```\\n\\n## Creating a validation set[[creating-a-validation-set]]\\n\\nAlthough we have a test set we could...\"],[\"```\\n\\nGreat, we've now prepared a dataset that's ready for training some models on! In [section 5](\\u002fc...\"],[\"```\\n\\nwhere we can see that each split is associated with its own *dataset.arrow* table, and some met...\"],[\"```\\n\\n```python out\\n{\\\"patient_id\\\":141780,\\\"drugName\\\":\\\"Escitalopram\\\",\\\"condition\\\":\\\"depression\\\",\\\"review\\\":...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={3}...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"```\\n\\nWhich of the following commands will produce a random sample of 50 elements from `dataset`?\\n\\n\\u003cQ...\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"\\u003ccode\\u003epets_dataset.filter(lambda x : x['name'].startswith('L'))\\u003c...\"],[\"### 5. Which of the following are the main benefits of memory mapping?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It tries to stream a dataset that's too large to fit in RAM...\"],[\"### 7. Which of the following are the main benefits of creating a dataset card?\\n\\n\\u003cQuestion\\n\\tchoices=...\"],[\"### 9. For asymmetric semantic search, you usually have:\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A shor...\"],[\"hy are fast tokenizers called fast? In this video we will see exactly how much faster the so-called ...\"],[\"Part 2 completed![[part-2-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={8}\\n    classNames=\\\"absolut...\"],[\"n a lot of our examples, you're going to see DataCollators popping up over and over. They're used in...\"],[\"These are DefaultDataCollator and DataCollatorWithPadding. These are the ones you should use if your...\"],[\"your labels are more or less just a copy of your inputs, and the collator handles that and ensures y...\"],[\"Integrations with the Hugging Face Hub[[integrations-with-the-hugging-face-hub]]\\n\\n\\u003cCourseFloatingBan...\"],[\"```py\\nimport gradio as gr\\n\\ntitle = \\\"GPT-J-6B\\\"\\ndescription = \\\"Gradio Demo for GPT-J 6B, a transformer...\"],[\"```\\n    \\nThe code above will produce the interface below:\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-gpt-j-6...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-remove-bg-original.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"650\\\" tit...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-Remove-bg.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"550\\\" title=\\\"Gradi...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a model with the Trainer API[[fine-tuning-a-model-with-...\"],[\"def tokenize_function(example):\\n    return tokenizer(example[\\\"sentence1\\\"], example[\\\"sentence2\\\"], tru...\"],[\"```\\n\\n### Training[[training]]\\n\\nThe first step before we can define our `Trainer` is to define a `Tra...\"],[\"```\\n\\nYou will notice that unlike in [Chapter 2](\\u002fcourse\\u002fchapter2), you get a warning after instantia...\"],[\"```\\n\\nThis will start the fine-tuning (which should take a couple of minutes on a GPU) and report the...\"],[\"```\\n\\n```python out\\n(408, 2) (408,)\\n```\\n\\nThe output of the `predict()` method is another named tuple ...\"],[\"```\\n\\n```python out\\n{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}\\n```\\n\\nThe exact results...\"],[\"```\\n\\nNote that we create a new `TrainingArguments` with its `evaluation_strategy` set to `\\\"epoch\\\"` a...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Tokenizers[[tokenizers]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanner ...\"],[\"```\\nJim Henson was a puppeteer\\n```\\n\\nHowever, models can only process numbers, so we need to find a w...\"],[\"```\\n\\nThere are also variations of word tokenizers that have extra rules for punctuation. With this k...\"],[\"- The vocabulary is much smaller.\\n- There are much fewer out-of-vocabulary (unknown) tokens, since e...\"],[\"Here is an example showing how a subword tokenization algorithm would tokenize the sequence \\\"Let's d...\"],[\"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading ...\"],[\"```\\n\\n{#if fw === 'pt'}\\nSimilar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokeni...\"],[\"```\\n\\nSaving a tokenizer is identical to saving a model:\\n\\n```py\\ntokenizer.save_pretrained(\\\"directory_...\"],[\"```\\n\\nThe output of this method is a list of strings, or tokens:\\n\\n```python out\\n['Using', 'a', 'trans...\"],[\"```\\n\\n```python out\\n'Using a Transformer network is simple'\\n```\\n\\nNote that the `decode` method not on...\"],[\"Building a model card[[building-a-model-card]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames...\"],[\"### Intended uses & limitations[[intended-uses-limitations]]\\n\\nHere you describe the use cases the mo...\"],[\"## Example[[example]]\\n\\nCheck out the following for a few examples of well-crafted model cards:\\n\\n- [`...\"],[\"```\\n---\\nlanguage: fr\\nlicense: mit\\ndatasets:\\n- oscar\\n---\\n```\\n\\nThis metadata is parsed by the Hugging ...\"],[\"Byte-Pair Encoding tokenization[[byte-pair-encoding-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6...\"],[\"```\\n\\\"hug\\\", \\\"pug\\\", \\\"pun\\\", \\\"bun\\\", \\\"hugs\\\"\\n```\\n\\nThe base vocabulary will then be `[\\\"b\\\", \\\"g\\\", \\\"h\\\", \\\"n\\\", \\\"...\"],[\"```\\n(\\\"hug\\\", 10), (\\\"pug\\\", 5), (\\\"pun\\\", 12), (\\\"bun\\\", 4), (\\\"hugs\\\", 5)\\n```\\n\\nmeaning `\\\"hug\\\"` was present 1...\"],[\"```\\n\\nNow we have some pairs that result in a token longer than two characters: the pair `(\\\"h\\\", \\\"ug\\\")...\"],[\"```\\n(\\\"u\\\", \\\"g\\\") -\\u003e \\\"ug\\\"\\n(\\\"u\\\", \\\"n\\\") -\\u003e \\\"un\\\"\\n(\\\"h\\\", \\\"ug\\\") -\\u003e \\\"hug\\\"\\n```\\n\\nThe word `\\\"bug\\\"` will be tokeniz...\"],[\"```\\n\\nThen we compute the frequencies of each word in the corpus as we do the pre-tokenization:\\n\\n```p...\"],[\"```\\n\\n```python out\\n[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k',...\"],[\"```\\n\\n```python out\\n('T', 'h'): 3\\n('h', 'i'): 3\\n('i', 's'): 5\\n('Ä ', 'i'): 2\\n('Ä ', 't'): 7\\n('t', 'h'):...\"],[\"```\\n\\n```python out\\n['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']\\n```\\n\\nNow we have everything we need to loop ...\"],[\"```\\n\\nAs a result, we've learned 19 merge rules (the initial vocabulary had a size of 31 -- 30 charac...\"],[\"```\\n\\nAnd the vocabulary is composed of the special token, the initial alphabet, and all the results ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ Using `train_new_from_iterator()` on the same corpus won't result in the exact same vo...\"],[\"Live sessions and workshops[[live-sessions-and-workshops]]\\n\\nFor the release of parts 1 and 2 of the ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"ExUR7w6xe94\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e...\"],[\"sing the Python debugger in a terminal. In this video, we'll learn how to use the Python debugger in...\"],[\"This is because the pad method only takes care of the tokenizer outputs: input IDs, attention mask a...\"],[\"n this video, we will learn the first things to do when you get an error. Let's say we want to use t...\"],[\"for question answering. The difference is that it's spelled distilbert with one l, and we used two. ...\"],[\"Mastering NLP[[mastering-nlp]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={7}\\n    classNames=\\\"absolute z-10 ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Semantic search with FAISS[[semantic-search-with-faiss]]\\n\\n{#if fw =...\"],[\"\\u003cYoutube id=\\\"OATCgQtNX2o\\\"\\u002f\\u003e\\n\\n## Using embeddings for semantic search[[using-embeddings-for-semantic-...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['html_url', 'title', 'comments', 'body'],\\n    num_rows: ...\"],[\"```\\n\\nIf we inspect the first row in this `DataFrame` we can see there are four comments associated w...\"],[\"\\u003ctable border=\\\"1\\\" class=\\\"dataframe\\\" style=\\\"table-layout: fixed; word-wrap:break-word; width: 100%;\\\"\\u003e...\"],[\"\\u003ctd\\u003ecannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚\\u003c\\u002ftd\\u003e\\n      \\u003ctd\\u003eHe...\"],[\"Great, we can see the rows have been replicated, with the `comments` column containing the individua...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['html_url', 'title', 'comments', 'body'],\\n    num_rows: ...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['html_url', 'title', 'comments', 'body', 'comment_length...\"],[\"```\\n\\nWe're finally ready to create some embeddings! Let's take a look.\\n\\n## Creating text embeddings[...\"],[\"```\\n\\n{:else}\\n\\n```py\\nfrom transformers import AutoTokenizer, TFAutoModel\\n\\nmodel_ckpt = \\\"sentence-tran...\"],[\"```\\n\\nWe can test the function works by feeding it the first text entry in our corpus and inspecting ...\"],[\"```\\n\\n```python out\\nTensorShape([1, 768])\\n```\\n\\nGreat, we've converted the first entry in our corpus i...\"],[\"```\\n\\nWe can now perform queries on this index by doing a nearest neighbor lookup with the `Dataset.g...\"],[\"```\\n\\nNow we can iterate over the first few rows to see how well our query matched the available comm...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nCOMMENT: Requiring online connection is a deal breaker in some cases unfortun...\"],[\"\\u003e @mandubian's second bullet point suggests that there's a workaround allowing you to use your offli...\"],[\"```\\n\\u003e\\n\\u003e import datasets\\n\\u003e\\n\\u003e data = datasets.load_dataset(...)\\n\\u003e\\n\\u003e data.save_to_disk(\\u002fYOUR\\u002fDATASET\\u002fDI...\"],[\"```\\n\\nNot bad! Our second hit seems to match the query.\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Try it out!** Create your own qu...\"],[\"he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with you...\"],[\"interpreted by the Hugging Face website in the model card. On top of informations about the training...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 2. What is the advantage of using a generator of lists of texts compared to a list of lists of t...\"],[\"### 4. How does the `token-classification` pipeline handle entities that span over several tokens?\\n\\n...\"],[\"### 6. What is normalization?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's any cleanup the tokenizer pe...\"],[\"### 8. Select the sentences that apply to the BPE model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n...\"],[\"### 9. Select the sentences that apply to the WordPiece model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={...\"],[\"### 10. Select the sentences that apply to the Unigram model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={[...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Training a causal language model from scratch[[training-a-causal-la...\"],[\"{\\u002fif}\\n\\nUp until now, we've mostly been using pretrained models and fine-tuning them for new use case...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-codeparrot-ds.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"300\\\" title=\\\"Gradio...\"],[\"However, training on the full corpus is time- and compute-consuming, and we only need the subset of ...\"],[\"```\\n\\nLet's test it on two examples:\\n\\n```py\\nfilters = [\\\"pandas\\\", \\\"sklearn\\\", \\\"matplotlib\\\", \\\"seaborn\\\"]\\n...\"],[\"```\\n\\n```python out\\n3.26% of data after filtering.\\n```\\n\\nThis leaves us with about 3% of the original ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nPretraining the language model will take a while. We suggest that you first run the trai...\"],[\"```\\n\\nWe can see that the `content` field contains the code that we want our model to train on. Now t...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n```python out\\nInput IDs length: 34\\nInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128...\"],[\"```\\n\\nWe can see that we get 34 segments in total from those two examples. Looking at the chunk lengt...\"],[\"```\\n\\nWe now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion ...\"],[\"{#if fw === 'pt'}\\n\\n```py\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\\n\\nconfig...\"],[\"```\\n\\nWith that configuration, we can load a new model. Note that this is the first time we don't use...\"],[\"```\\n\\n```python out\\n_________________________________________________________________\\nLayer (type)   ...\"],[\"```\\n\\n{\\u002fif}\\n\\nOur model has 124M parameters that we'll have to tune. Before we can start training, we ...\"],[\"```\\n\\n{:else}\\n\\n```python out\\ninput_ids shape: (5, 128)\\nattention_mask shape: (5, 128)\\nlabels shape: (...\"],[\"```\\n\\nThis will display a widget where you can enter your Hugging Face login credentials.\\n\\nIf you are...\"],[\"```\\n\\nAfter training completes, we can push the model and tokenizer to the Hub:\\n\\n```py\\ntrainer.push_t...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\u003cTip\\u003e\\n\\nâœï¸ **Try it out!** It only took us about 30 lines of code in addition to the `Tra...\"],[\"```\\n\\n{:else}\\n\\n```py\\nfrom transformers import pipeline\\n\\ncourse_model = TFGPT2LMHeadModel.from_pretrai...\"],[\"```\\n\\nNice, that's the correct answer -- although it then inserts the column `x` again. Since the num...\"],[\"```\\n\\n{#if fw === 'tf'}\\n\\nLooking at these few examples, it seems that the model has learned some of t...\"],[\"\\u003cYoutube id=\\\"Hm8_PgVTFuc\\\"\\u002f\\u003e\\n\\nSince we are mainly interested in sensible autocompletion for the the d...\"],[\"```\\n\\n```python out\\n'Keyword has not single token: testtest'\\n```\\n\\nGreat, that seems to work nicely! W...\"],[\"```\\n\\nBefore we can start training with this awesome new loss function, we need to prepare a few thin...\"],[\"```\\n\\nSince we want to evaluate the model regularly on the validation set during training, let's writ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸš¨ If you're training on a TPU, you'll need to move all the code starting at the cell abo...\"],[\"```\\n\\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. ...\"],[\"```\\n\\nThose are very high values for loss and perplexity, but that's not surprising as we haven't tra...\"],[\"```py\\nfrom tqdm.notebook import tqdm\\n\\ngradient_accumulation_steps = 8\\neval_steps = 5_000\\n\\nmodel.trai...\"],[\"```\\n\\nAnd that's it -- you now have your own custom training loop for causal language models such as ...\"],[\"ow to ask a question on the Hugging Face forums?\\n\\nIf you have a general question or are looking to d...\"],[\"For this example, we will use the following code,\\n\\nthat produces an error, as we saw in the \\\"What to...\"],[\"hat is domain adaptation? When fine-tuning a pretrained model on a new dataset, the fine-tuned model...\"]],\"hovertemplate\":\"source=course\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"course, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"course, circle\",\"showlegend\":true,\"x\":[-1.9798723,-1.3930416,1.0022696,0.58431995,2.8998241,-0.62076586,-3.5405986,-3.6494186,-3.6706896,-3.6010892,-3.666161,-3.56694,-3.3398995,-3.6118498,-3.3981352,-6.0173182,-1.9434403,9.109147,8.592023,-7.3520727,-7.3144817,-7.3460374,-7.3486443,-7.299669,-7.3589497,-6.7479897,5.496148,5.1297355,5.1609516,4.544838,0.08495198,3.7402406,2.2196238,0.74897605,0.9303959,6.0137987,0.10168803,1.6239158,0.7648244,-3.4080932,0.2665503,-2.6412153,0.9119833,-1.0915989,-0.88087255,-1.1147343,-0.8196807,-2.4659154,-5.813932,-5.4877653,-2.8271632,-3.5853803,-1.9206011,-0.8077525,13.2032585,12.987768,12.914992,12.898319,12.844636,-5.8738875,-5.445516,5.4418488,2.0405016,3.1832871,3.0991316,0.8893911,1.1044395,2.7700782,3.0818698,2.6673543,3.9441376,3.8312206,4.1529775,3.8719242,3.3477747,3.7101877,3.451884,-2.375739,3.4442148,3.3979301,1.7456027,4.515157,-3.6331108,-3.7636414,-6.000066,-6.5640764,-4.3115087,-2.3434644,-4.0778475,8.258764,-5.088783,-2.6217864,-0.5969723,-0.13077484,7.593996,7.726887,-5.95879,-1.5159886,-1.8502144,-2.2951236,-0.7872763,-1.9236537,-1.1462846,-3.0089495,-2.6534636,-3.047862,-2.551377,-1.5289624,7.557642,-2.921127,1.8919545,-0.50861835,-0.8338451,-1.0303354,-1.2093651,-1.4846443,-1.2236729,-0.3882711,-2.831427,-1.3671136,-0.13615806,-0.8667594,-5.48216,-0.7950214,-1.9296021,-0.637158,-1.5915874,-1.9747695,13.2328,12.96583,12.593313,12.651194,12.824587,11.776358,11.83749,-3.4455035,-3.24383,-2.2604916,-2.4687092,-2.3203802,4.899385,4.783391,2.4476202,0.7865305,2.8575053,2.6723623,-6.2071233,3.1304488,2.5290825,0.71884114,2.571533,1.5730523,2.5190222,2.613399,2.8686125,-1.4472095,-2.0063891,0.40008503,1.3013244,0.4016645,-8.337028,-7.3848124,-8.430258,-1.6397716,0.6760704,-3.374766,-3.1626341,-3.3313007,-3.092116,-3.0595555,-3.269019,-3.2532167,-2.2557123,-2.1880705,-2.2740333,-1.7271997,-1.5291301,-2.1763284,-3.377059,3.695626,-1.2437325,-3.158744,-2.575471,-3.0781744,-7.36555,-6.951731,-7.810586,-6.311543,-7.0786405,-7.1741095,3.2437675,-6.249421,-6.5944624,-6.959656,-5.785256,-5.6807733,-5.7222795,-4.8499584,-7.3960204,-8.18474,-7.393286,-4.2468596,3.5728216,-1.1596177,-7.5542526,-7.8898945,-7.492066,-6.8973737,-4.3703017,-6.522203,-5.860638,-3.921564,1.8856127,-3.348975,-3.4277127,-3.9650352,-2.7792387,-1.7686931,-1.4355685,-2.1548686,-1.5548848,-1.5141454,-1.0757982,-2.1921895,-0.74286795,0.2180768,-0.36795118,0.6270476,-2.6035419,0.33547837,-0.25693458,2.2208333,0.6720799,-1.1923214,0.05757891,-0.8525087,0.7664841,-2.2208066,-0.9903315,2.663191,0.6766248,-1.0403286,-0.31077203,-3.522038,-6.027191,-3.3451788,1.9250878,-5.195291,-6.3093634,-5.5274787,-0.8059112,-0.46360925,-0.6189569,-3.6421633,-2.6189835,7.8366594,-2.3722806,-1.4943057,-2.2467036,-1.5595468,-2.1601813,7.8456354,-2.4350784,-1.9428988,-0.70216084,-1.8607854,-0.6106565,0.60552675,-2.3425012,-1.9789577,-1.7649122,-1.5503277,-0.9677071,-0.98704064,0.25017744,0.14709556,0.01832186,-0.35265452,-0.4997459,-1.1659847,-1.529981,0.36948565,-0.68543595,-0.7355321,-1.1041584,-1.4966346,2.7998035,0.33862442,-0.2796083,-0.99648684,-1.6805687,-0.5166952,3.9499612,4.387217,3.6867554,4.5978465,-3.404779,-3.360637,-3.3236425,-3.3237753,3.1887283,4.0975194,3.865061,3.8477373,4.4996567,3.8416169,4.323348,3.1736646,1.9612451,3.4743311,4.1107874,3.7056663,3.8367596,3.5204682,3.8293815,4.899392,2.3621933,2.5540686,3.0114958,1.5266954,3.6864,3.5762405,-3.9151516,-3.9551194,-4.014113,-2.816648,-3.4742172,-3.397658,-3.4764082,-3.907853,-1.9413952,-2.2988536,-2.1749058,-4.1253304,0.35092625,-2.051615,-3.0798159,-2.128093,-3.4583125,-1.4393189,-1.3615762,-0.6623845,-1.0607631,-1.0341642,-1.4618738,-1.432259,-1.1674068,-5.5387077,-1.1782862,-2.8322394,-2.7966232,-2.9765754,-2.9794886,-1.5616317,-2.319249,-2.8860457,-2.446022,13.318033,-5.7880545,-7.4135423,-2.8491747,-2.3898222,-2.0238156,-2.3411915,-2.2212818,-2.555767,-2.404383,-2.1683996,-1.695373,-2.7118974,12.64594,12.334887,12.830138,12.09201,12.794995,-3.5193636,-3.30097,-3.3296573,-2.139165,0.44632182,-3.631307,-3.3722117,13.211086,10.448079,10.682068,12.777045,12.857482,-0.59020585,-1.0445068,12.759931,-4.678684,-4.4684634,-0.42876282,-5.280849,-4.201607,-2.8540714,-2.50164,-1.9977443,-1.6953897,-1.7407261,-2.3339436,-6.0336275,-2.4064705,-2.5031145,-1.4824961,-1.9394499,-0.80822045,-1.7055902,-3.4852178,-3.6109762,-2.1887183,-2.59687,-1.9114834,-3.408002,-1.0590502,-1.3119707,-1.602519,-0.9216397,0.3471069,-1.9835497,-1.255055,-1.1079147,-0.9229191,-1.0286921,-4.917388,-5.489172,-0.7161943,-3.71483,-2.6109207,-2.8704815,-4.6299906,-3.361444,-3.994805,-1.2176894,-3.861378,-1.3453283,-1.4647932,-2.9208477,-2.287124,3.6755245,3.5114198,2.158056,-1.1157587,-3.0886436,-0.42058662,-4.3413143,-3.4688857,-2.2712471,-1.9222472,-2.0575829,-2.4120624,-1.4433092,-7.3053293,-2.2746022,-1.976742,-0.8782603,-4.364094,2.3893838,-3.611395,-3.5740979,-3.6707897,-3.5537925,-3.6360528,-3.24537,-3.3676229,-3.5951302,-3.6807973,-3.4961388,-3.4788725,-3.2959912,-3.2348602,-3.3577764,-3.5428817,-3.3922253,-3.4192405,-3.4423568,-3.383966,-3.33534,-3.3668618,-4.266307,-3.7151458,-4.206038,-3.7657142,-3.6386683,-4.0969286,-3.8263984,3.2777011,3.7535152,5.0969515,5.161122,3.8664746,-5.6534553,-5.787913,0.16107138,0.7672118,0.41026667,-2.488986,3.6956568,2.9583685,-8.240021,-8.448944,13.240569,13.044374,12.807222,12.960357,12.43858,12.798454,12.254736,12.642075,11.082383,12.984746,12.79116,13.259526,13.454413,12.727517,13.825139,6.477087,0.5019573,2.1362257,-4.7921143,-3.762014,-3.6110563,-3.7925975,-2.97269,-2.953405,-1.9477897,-2.9831226,-2.1538002,-3.3888412,-6.3986306,13.118228,-4.6048136,-2.8581312,-3.34804,-2.9723182,-3.5510101,-0.85669905,0.121830426,-4.143701,-3.0900707,-0.5815413,-3.822017,-1.55734,-2.626819,-2.2841547,-1.1530666,-4.9618363,-1.0865804,-0.9704481,-1.8320137,-3.0563858,-1.0261209,-2.5310044,-3.5136492,-3.3921916,-3.417055,-3.409373,-5.0222163,-0.8792914,-0.073281705,0.6156494,7.97091,-0.24314785,0.10085004,7.694719,-0.52290183,-0.5869661,-7.0087357,-6.783243,-6.34851,-6.8702793,-3.0550482,-3.5071063,-2.8579912,-1.891297,-0.6842799,-0.43495023,-0.035995692,0.46497273,-0.34097418,-0.47896066,-1.1467234,-2.6070094,-1.5348963,-1.0248656,0.31349817,-1.8545123,-1.7631309,-0.56595975,0.08717574,1.4304543,-0.9594485,-2.6883392,-1.0065712,-1.6599054,-0.8862584,-1.2436807,1.6500309,0.2733523,-1.7152803,-0.319805,-0.11903728,-1.1990438,-3.6264966,-1.5358638,-3.5101545,-3.7681673,-3.0847218,-2.9301949,-4.7627726,-2.812736,-2.3525777,-3.9882948,-2.3682172,-4.010961,-3.752667,-2.2772813,-2.3978705,-1.5788888,-1.5270852,-1.5127013,-0.9144965,-2.3680122,-2.3571372,-2.1884203,-1.6339265,-1.4365852,-1.0778371,-0.40829077,8.457199,8.473444,8.514491,-1.6982327,-2.3248873,-2.1784208,-2.058624,-2.9160285,-2.0322502,-1.7427056,-0.9244887,-2.4013834,-2.7135043,-3.610551,-1.4918599,-1.5720977,-1.4114563,-1.5924108,0.62983847,-1.2940336,-1.4841628,-1.5059428,-0.83891726,-1.5307215,-1.2316737,-0.9839291,-1.1833979,-1.4222927,-1.2454273,-1.2292029,-1.5158186,-1.085853,0.65489465,-0.7974718,-2.4627006,-2.379547,0.25517774,-2.2985916,-4.130883,-6.105841,-2.3149235,-3.1632676,-8.087044,-7.385319,-7.76831,-4.2187734,-3.3643415,-0.42248452,-0.29063812,-3.445751,-3.0947857,-2.761477,-2.5491982,-3.2703264,-3.3945165,-3.2362728,-1.9347367,-0.31914827,-1.7335465,-1.9074516,-2.7830358,-3.3533723,-3.1422195,1.7038659,1.6928703,0.49140698,-4.6923337,-4.0531325,4.0618744,1.27185,0.8522464,1.2657576,1.4691268,-2.7570143,-2.5278828,-0.62531954,-0.55895483,-0.85215676,-7.0604196,-6.3751626,-6.325276,-7.313716,-7.338823,8.691468,-6.824983,-5.687224,-5.502557,-5.6631784,-6.675224,-7.01051,10.001756,2.6064236,-5.32239,2.0617146,1.2881359,1.4224197,1.5393841,1.5383883,0.055557083,-0.77244955,0.054564036,1.2004849,1.3745286,1.759287,2.9035895,-2.8955595,-0.83751935,6.407479,7.3559422,-4.292292,-5.9642773,-7.3303537,2.2049959,2.1791363,-2.4923801,-2.3423855,-3.610235,-4.115804,-3.8847823,-4.7486815,-3.571461,-3.914953,-3.9867966,-4.171978,-3.5558174,-3.4220185,13.097015,-3.902783,-3.2009542,0.21270837,-1.9293581,-2.6033454,-3.2096963,-3.190983,-3.269646,-3.0598266,-3.0101311,-0.46783587,-3.205175,-3.341842,-4.868237,-3.5107594,-3.2147784,-2.5141635,-4.1640425,-4.535134,-3.0716872,-4.1854415,-2.1843562,-1.0970814,-1.8003215,-2.6363187,-2.2909238,-1.8270987,-1.5589566,0.32154936,-2.5020235,-2.087801,4.5723906,-6.2059846,8.127519,8.159633,0.124930486,5.4841294,4.6117473,5.96588,6.3832436,-2.0742164,-1.758265,-7.3603277,-3.4861062,-5.0594225,-5.9790325,-0.67193216,-0.09736996,-0.18633047,-2.4501002,-3.4821389,-3.0297728,-2.195117,-2.0044062,-0.6826429,-1.5157719,-1.4142386,-1.1576092,-1.5777478,0.11535504,2.127319,-1.1650703,0.8839747,0.34734684,0.5721776,0.497337,-0.040844478,-1.3327982,-0.6453981,2.779894,0.7945996,0.71347135,-1.4679136,-0.12874894,0.05670919,-0.60556066,-0.09800777,-1.6366371,-0.63869834,-2.3820193,-1.4966466,1.2740687,2.0663707,1.9161248,7.6839333,0.29092243,0.6232361,0.3080891,0.111947365,0.22679737,-0.38562977,0.3901518,-1.8977166,-1.8778365,-0.5436797,-1.7307658,-1.3031198,-0.3775303,0.46221116,0.032684278,0.41603008,0.46443656,0.52749187,0.6010417,0.80755514,0.99300003,2.7438946,1.9094151,-0.40220693,-0.96313304,2.7337852,0.5991079,0.5942624,-0.07663797,0.76049745,-6.0979905,-5.3067274,-2.9232025,-6.749787,-0.846541,-1.9893134,-3.07143,12.120849,12.855202,12.605125,12.816732,12.831094,-2.0583289,-1.7197862,-0.79643756,-2.7812548,0.37642306,0.5777078,0.60376716,-1.5585884,-4.076991,-3.3402278,-3.6301808,-3.534907,-3.565726,-3.1184273,-2.587402,-3.1315236,-3.2545252,-3.3568993,4.3906,2.0886528,4.3361926,5.418466,-3.522921,-3.3151956,-3.0861936,-3.2175593,-3.2777393,-2.4306748,-2.1707888,-2.186576,-1.983022,-1.8680844,-2.0263958,-3.1517584,-6.104974,9.142763,-1.3271677,-1.6273873,-0.075590506,0.066552125,-6.3119297,-4.941035,-6.222202,1.3989049,0.79959273,0.9200858,2.296654,4.7128687,4.3902698,1.243848,0.5859364,0.24320021,-3.5640724,-2.5025854,-2.0386324,-0.6645138,-0.3721024,0.23009329,2.4825122,2.4753423,2.5840359,-0.2109442,1.4293126,1.214419,-3.7761726,-3.0341976,-4.0150194,-3.7317605,-4.051415,-4.016022,-3.941135,-1.8488983,-4.14241,12.794671,0.56047916,1.025662,1.642041,-2.9432712,-2.1805968,-0.05826273,-0.81367487,-0.754197,-2.6389399,-2.5335402,-1.517387,-1.1891174,-1.9507413,-0.5666229,1.7535108,-0.27583137,-2.6021376,-0.25821966,-0.5362676,-1.356872,-1.3757027,-1.5862012,-1.2952647,-1.0016842,-0.0041882596,1.3115735,-1.095508,-0.68614763,-2.515448,5.7567205,4.428968,-5.68869],\"xaxis\":\"x\",\"y\":[4.4610505,4.1487722,4.4564776,4.687894,3.5088503,4.6296935,6.51843,6.088203,6.4540505,6.3535676,6.3731594,6.456571,6.7274847,5.044487,5.15345,1.7480059,7.1602955,1.7346072,1.5898468,4.7011747,4.6805744,4.8784623,4.7236395,5.0800743,5.012492,2.8391395,2.7078772,2.4710581,2.3034961,2.15308,1.5478275,0.6769519,1.2232319,2.1987724,0.7308545,2.4426124,1.0517938,0.81006473,0.84581596,2.637731,1.6796693,3.9942563,2.6115253,2.1011176,2.7355196,2.2434742,3.4462063,3.8444932,1.2718843,1.4111651,4.3828716,4.915884,-1.924123,-1.172162,3.7207162,3.9274104,4.06202,4.1073194,3.9120212,2.731714,2.7101963,0.3758764,0.92433035,0.5036858,-0.076843955,-0.35607868,-0.09195823,0.34126225,0.59348655,0.13695507,1.033102,1.0861901,1.3745096,0.90832,1.089571,0.78388894,0.87836766,3.1666338,1.4305601,1.0459042,1.74068,1.6310289,6.150202,1.4946985,2.1077821,0.8578145,1.9899346,1.4827662,2.27338,3.9216728,2.7082057,3.5970469,5.038935,5.2941437,3.3408778,3.3178792,2.5440328,5.073606,4.904912,4.8291907,5.86323,5.085912,4.986533,4.542055,4.8823557,4.1848373,4.4430532,4.1779065,3.3122778,4.5097423,1.7199981,0.07248107,0.025883017,0.48152444,0.001979712,6.1990514,5.821563,0.6375715,4.1260467,3.6310334,-0.46681118,-0.06864911,2.0682056,1.6201069,1.4006932,5.779784,1.0297335,1.8992686,3.8279672,4.43996,3.992259,4.442662,3.869443,4.762106,4.157842,6.673392,6.9999876,1.6145471,1.5498303,1.4194001,1.4952686,0.49057433,1.1799124,2.063038,3.6538725,4.0043035,2.3634925,3.388665,4.0030537,-0.1747325,3.9254394,4.4888887,3.810085,3.4573848,3.4852242,0.44268146,0.98681647,6.0164213,4.4003587,4.427551,3.263855,3.4878376,3.2491498,2.3225143,6.8873243,6.864806,6.7595005,6.9594665,6.736751,6.83699,6.9837937,6.725457,6.201657,5.9592385,6.0567145,5.749972,5.4870677,5.6021643,6.214245,1.2942874,0.82809955,1.682615,1.2682474,2.089963,-1.0072347,-0.52413166,-2.424717,-0.39728466,-1.3622802,-1.8234501,-2.5042493,1.9794052,2.1668398,2.690678,0.5100217,0.9393,1.1415664,-0.15236272,3.2507908,3.0149686,2.006631,1.5032631,2.965261,6.927848,3.1923919,3.1713212,3.1609397,2.9341247,3.7834525,2.9000878,1.34315,2.3734286,4.540827,3.972053,3.876959,2.9284978,4.5714445,4.41263,0.76303244,2.5854428,3.378349,4.141905,4.126282,3.3105419,8.197657,7.4639745,8.30496,8.047293,3.1106324,6.8095055,5.8414407,0.14192048,-0.114742346,0.6506344,-0.07631574,0.8858551,-0.11443551,0.9674314,-0.5172889,0.18820393,-0.09649085,0.787752,-0.22468682,2.6385477,2.5495977,5.6613297,4.187631,3.0594254,1.1158041,2.7798815,4.8433785,5.395557,6.312158,4.1613936,4.7536077,3.1887252,4.9861603,4.8597336,4.872775,4.952069,5.2583694,3.1913953,4.842229,4.978611,4.350339,4.1907535,4.5525813,4.428909,2.6877625,4.2990794,3.327955,3.1895616,5.493988,5.4536185,7.80349,6.2445283,7.2662964,0.7891217,0.4378552,1.0060208,0.81683874,-0.15144467,1.2051767,7.5922995,0.37934232,-0.2724809,0.2704241,-0.21900386,0.0696721,-0.34161893,0.37556776,4.59238,1.1121734,1.6490747,0.883422,0.35770515,6.8019843,6.929158,7.0792637,6.9945006,4.559483,2.41627,2.572924,2.3151243,1.9806606,2.3588383,1.9877516,2.4631617,3.8695314,2.6784031,2.231171,2.8044574,2.455912,2.609072,2.3259628,1.9970579,3.750117,3.3057587,2.7082598,4.415086,2.945097,4.1409807,4.855243,5.1942635,4.308918,4.69559,4.7787895,4.3676624,4.3839817,4.685209,1.8660861,3.0873601,3.8175223,2.7325819,4.480386,4.4956217,5.075684,5.2387843,4.693174,4.724436,4.9107285,4.3386307,4.448425,3.8131456,4.244316,3.737796,3.2117548,1.2651206,1.6848775,1.5483416,1.8948507,1.6219891,1.6952045,1.2067391,3.9492493,1.550503,1.4804081,3.689123,2.7838297,-1.6727564,4.116707,4.0167637,3.547272,3.377547,3.7237701,4.4206986,4.1503024,3.7312865,3.2653956,4.5464873,3.633801,3.4079406,3.9179304,4.4508634,3.9753268,6.305055,5.819552,6.0840435,0.5500549,6.5029755,5.548562,5.2467704,3.5348961,3.329629,3.6023824,3.7786865,3.638225,1.1843028,2.939037,3.8894372,3.730996,3.430583,5.5546436,2.815497,0.9015554,3.2802036,4.0332623,3.6959233,4.0478296,3.9087296,5.025009,1.6416472,5.1542835,5.0071135,2.9583807,3.7927742,5.445363,5.116304,5.1424685,6.332324,4.6701665,4.7376165,5.2114463,2.785366,3.882622,3.2027354,0.08590966,-0.44703853,6.066105,-1.2262946,-0.7898776,-1.2475916,-0.39959252,-1.970443,1.3979406,1.550143,5.765778,3.6080992,3.8712962,4.1739097,2.407827,4.035414,3.9197116,6.263163,3.4999907,2.3132107,3.1134558,1.5286129,1.3668215,1.0681268,1.0148672,2.2658203,1.903502,3.7553773,5.8181224,4.120251,3.5116456,3.6284504,3.808484,3.6129668,2.1097896,2.9453878,2.2567973,2.5665796,3.2291734,3.2357306,3.902279,4.0866737,6.3000126,6.1877394,6.154953,6.268953,6.081541,5.819068,6.025179,6.194645,6.363412,6.3888135,5.7498612,5.7886276,5.792072,5.063368,5.601763,5.7396703,5.874144,6.0867805,5.785952,5.7460628,5.7489424,4.496496,5.117078,5.117814,4.9253163,4.4561615,4.798463,5.0597467,0.7208284,1.414568,1.515177,1.1604377,1.2327802,3.0335493,2.934328,2.1174643,1.7447325,2.1136253,4.1332955,1.621161,1.4158663,2.972257,3.2452016,3.7514036,4.2500863,3.968344,3.7007456,4.082273,3.9019468,3.9011662,4.1858764,3.7245796,3.8852417,3.8579736,3.664606,3.7403228,3.619611,3.775426,1.7359942,0.08742314,0.054074403,1.9826657,1.9218589,6.27093,3.2388258,3.555161,4.4948916,3.7174468,4.434472,3.3841898,1.0126785,0.5072655,3.6822777,4.056164,5.5580373,4.436202,5.5910554,5.6418467,4.440891,3.1720288,4.7710476,3.5513346,5.645869,4.1378274,5.0793953,3.7240129,3.539938,3.512177,3.199584,4.2799788,5.7008553,5.3621936,5.999905,5.1323543,5.273939,6.4568686,6.432336,6.686348,6.509766,3.7240045,5.7094316,5.574499,4.6880703,2.9454882,5.368018,5.159026,3.5217679,5.503082,5.393671,3.0701773,2.8511338,2.776025,2.7497663,4.615767,4.793259,5.1016965,4.8569765,7.0137815,7.6196423,7.621886,7.3178763,7.5173545,7.451269,6.006371,2.4720519,0.5007775,1.5088085,7.101359,4.211053,3.7129421,5.2514024,0.46056694,0.23328315,0.1880222,1.60518,4.4512935,3.7800517,0.5320959,-0.14015071,0.28821772,-0.012935939,3.7955508,5.1509547,1.3662426,5.878028,4.9267654,4.3375616,5.8094397,5.1043115,4.5911536,5.095112,3.8990865,4.7842364,4.5858912,1.9991555,5.5695095,4.0463696,4.3607154,4.4925957,0.7567018,1.3813852,1.7848178,2.6523511,3.0783994,1.4597086,1.3949735,1.6380336,1.7565062,2.313503,3.094268,5.0169387,3.1710935,3.1696925,3.2714982,2.5435598,1.1441498,0.87658083,0.29264808,-0.3974681,-1.064087,2.70705,4.2334743,1.6737673,0.9435899,0.9018178,0.7293927,1.2637033,2.431304,2.7866855,6.6238184,3.0531955,2.6298764,2.920377,2.4506454,4.253152,0.7122456,1.5488399,0.61241096,2.8659744,3.9352965,-0.03788617,-0.096445456,1.8127149,7.1741447,2.779056,1.2667546,1.4480627,5.799,1.4465262,0.504756,2.1896422,4.570912,4.503326,3.338857,3.5691378,3.3973944,4.1942897,5.48797,7.684243,7.7364564,6.524635,6.945056,6.959896,6.9498153,6.986173,6.615292,5.826472,6.344104,6.6386824,6.4583898,6.30131,6.3061333,6.245312,6.540297,4.1223807,4.2736187,4.444411,0.87712246,4.699044,0.58765614,0.84684324,0.90919083,0.8693247,0.93257636,4.100443,4.3447046,8.197485,8.1945095,8.210425,-1.1634946,2.2713656,1.5162147,-1.6845771,-1.8273501,-9.866596,3.532622,3.1830046,3.136527,2.626413,2.9814463,2.957042,4.022552,4.610155,2.657703,4.037529,4.2825174,4.3577566,4.157456,4.3373737,4.527864,4.691453,5.352593,4.615234,4.6839523,4.233553,5.9171014,4.2596836,2.3075664,2.4106333,1.3322868,3.4085588,3.1531067,3.226245,3.9267747,4.4127803,0.4982582,0.73033935,6.128977,4.1302094,4.381441,3.89349,4.2226534,4.7400985,4.9975815,4.889118,5.9543576,6.103137,3.6490192,5.3724046,5.6308537,4.460402,4.983377,4.4908543,6.132221,6.1682277,5.6035314,5.8782253,6.1290903,3.1020691,6.048885,5.529823,3.38914,4.8442492,4.8444486,4.590713,4.1653233,3.0820642,1.1624569,4.282266,1.4755633,3.6635761,1.5473783,1.7578244,0.9423325,0.3591725,0.6979867,6.230676,4.6172004,5.2427316,1.4840635,2.7356849,0.74516463,1.5931926,3.660845,1.7689018,2.6692257,1.9706019,2.1627412,1.3519889,-0.2913061,3.1070561,3.4784544,3.5741525,1.7257892,4.3894315,4.2272,4.2301407,5.329673,5.0093102,5.8348684,5.1701603,5.1655793,4.4751167,1.0049864,3.9503999,3.9006197,1.8232015,0.96914816,-0.30100533,0.9513629,0.019668018,6.778319,7.806721,7.4371333,5.644048,3.5794961,0.5090012,0.12826717,-0.47627982,-0.28725642,0.44251275,-0.46017328,0.00277486,-0.19359554,0.00040753608,0.40713897,5.775627,1.5975077,1.0097845,4.2900367,3.6538122,4.2373543,3.3686829,4.7055073,4.5504,4.2725463,5.1130476,4.9411263,5.317006,4.443476,5.212369,4.9700184,4.514054,5.1464176,4.692606,4.6101904,4.624314,4.739618,4.2584567,4.828387,4.2766542,4.319292,4.6272674,4.511468,3.0608177,4.3780994,5.044612,0.38557348,3.2808077,4.3893805,4.057857,3.8037198,4.069849,2.5385082,3.4387286,5.6622925,1.5977169,3.7860322,4.18746,3.8834548,3.2182984,3.9907408,3.5993211,3.757175,3.5673351,1.1357365,5.0860133,-0.11376408,2.1808653,6.013311,6.842574,6.6633925,0.4966063,4.4482017,5.887282,6.367225,6.536154,6.325116,2.7221656,3.112461,4.523025,5.307944,5.4699802,3.6906369,4.2591324,3.164224,2.0809743,6.4128976,6.8466816,7.0583925,6.957844,6.4963994,6.2535443,6.2283297,6.527691,6.3467436,6.4381113,6.3288927,6.205158,0.6784243,2.8090742,2.6464136,3.0401742,2.4710002,3.5025783,1.7762272,3.755815,2.7468703,4.467959,4.702948,4.65595,3.8544784,2.3137293,2.3062863,4.562915,4.7703876,4.831669,3.6094651,3.3817294,3.583592,4.6308174,4.791827,4.9489613,3.8820832,3.9677076,3.7314742,4.878804,-0.022038326,0.29524264,4.776313,5.1078453,5.0926623,5.1196074,5.236655,5.126186,5.3431897,1.8819749,2.8364384,3.7999098,4.437765,4.3722267,4.2968535,2.1710403,4.715758,-6.099103,4.354474,4.83231,4.934513,2.7870896,1.0653102,2.9253535,3.7509716,2.526666,-0.41301036,0.2582141,-0.18233001,4.183123,4.512166,0.63735396,4.8864694,3.5951872,0.376227,0.28294465,-0.4931171,-0.17071192,0.8186061,0.076462105,0.75974905,0.21519202,1.7307473,1.5097495],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [ai-forever\\u002fKandinsky-3](https:\\u002f\\u002fgithub.com\\u002fai-forever\\u002fKandins...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nLoRA is very versatile and supported for [DreamBooth](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffuser...\"],[\"```\\n\\nNavigate to the example folder with the training script and install the required dependencies f...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```py\\nlora_attn_procs = {}\\nfor name in unet.attn_processors.keys():\\n    cross_attention_dim = None i...\"],[\"```\\n\\nThe [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fdd9a5caf61f04d11c0fa9f3947b69ab00...\"],[\"```\\n\\nAside from setting up the LoRA layers, the training script is more or less the same as train_te...\"],[\"accelerate launch --mixed_precision=\\\"fp16\\\"  train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_...\"],[\"```\\n\\nOnce training has been completed, you can use your model for inference:\\n\\n```py\\nfrom diffusers i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Text-to-audio (TTA) system has recently gained attention for its a...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"LCM-LoRAs are available for [stable-diffusion-v1-5](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion...\"],[\"```\\n\\n## Text-to-image\\n\\nYou'll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMSchedul...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# load LCM-LoRA\\npipe.load_lora_weights(\\\"latent-consistency\\u002flcm-lora-sdxl\\\")\\n\\nprompt = \\\"face focus, cu...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# Combine LoRAs\\npipe.set_adapters([\\\"lcm\\\", \\\"papercut\\\"], adapter_weights=[1.0, 0.8])\\n\\nprompt = \\\"paperc...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# load LCM-LoRA\\npipe.load_lora_weights(\\\"latent-consistency\\u002flcm-lora-sdv1-5\\\")\\n\\ngenerator = torch.manu...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion-xl-base-1...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"pipe.set_adapters([\\\"lcm\\\", \\\"motion-lora\\\"], adapter_weights=[0.55, 1.2])\\n\\nprompt = \\\"best quality, mast...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2...\"],[\"```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramati...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be i...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## UNet1DModel\\n[[autodoc]] UNet1DModel\\n\\n## UNet1DOutput\\n[[autodoc]] models.unet_1d.UNet1DOutput...\"],[\"Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](http...\"],[\"## Available Pipelines:\\n\\n| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_stable_diffusion.py]...\"],[\"## Examples:\\n\\n### Using Stable Diffusion without being logged into the Hub.\\n\\nIf you want to download...\"],[\"```\\n\\nThis however can make it difficult to build applications on top of `diffusers` as you will alwa...\"],[\"```\\n\\n### Text-to-Image with K-LMS scheduler\\n\\n```python\\n# make sure you're logged in with `huggingfac...\"],[\"```\\n\\n### CycleDiffusion using Stable Diffusion and DDIM scheduler\\n\\n```python\\nimport requests\\nimport ...\"],[\"image.save(\\\"horse_to_elephant.png\\\")\\n\\n# let's try another example\\n# See more samples at the original ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [ai-forever\\u002fKandinsky-2](https:\\u002f\\u002fgithub.com\\u002fai-forever\\u002fKandins...\"],[\"[[autodoc]] KandinskyV22Img2ImgCombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetImg2Img...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\nWhile calling [`StableDiffusionPanoramaPipeline`], it's possible to specify the `view_batch...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## StableDiffusionPanoramaPipeline\\n[[autodoc]] StableDiffusionPanoramaPipeline\\n\\t- __call__\\n\\t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## ShapEPipeline\\n[[autodoc]] ShapEPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ShapEImg2ImgPipeline\\n[[aut...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"- **Self-contained**: An example script shall only depend on \\\"pip-install-able\\\" Python packages that...\"],[\"We provide **official** examples that cover the most popular tasks of diffusion models.\\n*Official* e...\"],[\"Training examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Current...\"],[\"## Community\\n\\nIn addition, we provide **community** examples, which are examples added and maintaine...\"],[\"```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt\\n```...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is gene...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe `apply_patch` function exposes a number of [arguments](https:\\u002f\\u002fgithub.com\\u002fdbolya\\u002ftomesd#usa...\"],[\"```bash\\n- `diffusers` version: 0.15.1\\n- Python version: 3.8.16\\n- PyTorch version (GPU?): 1.13.1+cu11...\"],[\"```\\n\\nTo reproduce this benchmark, feel free to use this [script](https:\\u002f\\u002fgist.github.com\\u002fsayakpaul\\u002f2...\"],[\"| **GPU**  | **Resolution** | **Batch size** | **Vanilla** | **ToMe**       | **ToMe + xFormers** |\\n...\"],[\"|          |                |              2 |        3.14 | 2.43 (+22.61%) |      2.27 (+27.71%) |\\n...\"],[\"As seen in the tables above, the speed-up from `tomesd` becomes more pronounced for larger image res...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline cla...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"response = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\\\"RGB\\\")\\nimage.thum...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Use multiple pipelines\\n\\nFor some workflows or if you're loading many pipelines, it is more m...\"],[\"```\\n\\nIf you passed an optional argument - like disabling the safety checker - to the original pipeli...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Free-form inpainting is the task of adding new content to an image...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo further speed-up inference, statically reshape the model. If you change any parameters such ...\"],[\"```\\n\\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in th...\"],[\"--\\n{{ card_data }}\\n---\\n\\n\\u003c!-- This model card has been generated automatically according to the infor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Attend-and-Excite on the [project page](https:\\u002f\\u002fattendande...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by the community contributor [HimariO](https:\\u002f\\u002fgithub.com\\u002fHimariO) â¤ï¸ .\\n\\n...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fcolor_ref...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fcolor_out...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadapter\\u002fresolve\\u002fmain\\u002fsketch.png)\\n\\nThen, create the ada...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadapter\\u002fresolve\\u002fmain\\u002fsketch_output.png)\\n\\n## Available ...\"],[\"| Model Name | Control Image Overview| Control Image Example | Generated Image Example |\\n|---|---|--...\"],[\"|[TencentARC\\u002ft2iadapter_canny_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_canny_sd14v1)\\u003cbr\\u002f...\"],[\"|[TencentARC\\u002ft2iadapter_sketch_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_sketch_sd14v1)\\u003cb...\"],[\"|[TencentARC\\u002ft2iadapter_depth_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_depth_sd14v1)\\u003cbr\\u002f...\"],[\"|[TencentARC\\u002ft2iadapter_openpose_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_openpose_sd14v...\"],[\"|[TencentARC\\u002ft2iadapter_keypose_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_keypose_sd14v1)...\"],[\"|[TencentARC\\u002ft2iadapter_seg_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_seg_sd14v1)\\u003cbr\\u002f\\u003e*Tr...\"],[\"|[TencentARC\\u002ft2iadapter_sketch_sd15v2](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_sketch_sd15v2)||...\"],[\"## Combining multiple adapters\\n\\n[`MultiAdapter`] can be used for applying multiple conditionings at ...\"],[\"```\\n\\nThe two control images look as such:\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fkeypose_d...\"],[\"Adapt a model to a new task\\n\\nMany diffusion systems share the same components, allowing you to adapt...\"],[\"```\\n\\nTo adapt your text-to-image model for inpainting, you'll need to change the number of `in_chann...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Text-to-3D\\n\\nTo generate a gif of a 3D object, pass a text prompt to the [`ShapEPipeline`]. T...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"prompt = \\\"A cheeseburger, white background\\\"\\n\\nimage_embeds, negative_image_embeds = prior_pipeline(pr...\"],[\"```\\n\\nPass the cheeseburger to the [`ShapEImg2ImgPipeline`] to generate a 3D representation of it.\\n\\n`...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nUse the [`~utils.export_to_ply`] function to save the mesh output as a `ply` file:\\n\\n\\u003cTip\\u003e\\n\\nYou ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Available Pipelines\\n\\n| Pipeline | Tasks | Demo\\n|---|---|:---:|\\n| [AnimateDiffPipeline](https:\\u002f\\u002fgi...\"],[\"# enable memory savings\\npipe.enable_vae_slicing()\\npipe.enable_model_cpu_offload()\\n\\noutput = pipe(\\n  ...\"],[\"```\\n\\nHere are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestq...\"],[\"```\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestquality, sunset.\\n        \\u003cbr\\u003e\\n  ...\"],[\"```\\n\\nThen you can use the following code to combine Motion LoRAs.\\n\\n```python\\nimport torch\\nfrom diffu...\"],[\"```\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestquality, sunset.\\n        \\u003cbr\\u003e\\n  ...\"],[\"!--Copyright 2023 The GLIGEN Authors and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the [paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2301.07093) is:\\n\\n*Large-scale text-to-im...\"],[\"## StableDiffusionGLIGENTextImagePipeline\\n\\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\\n\\t- all...\"],[\"WÃ¼rstchen text-to-image fine-tuning\\n\\n## Running locally with PyTorch\\n\\nBefore running the scripts, ma...\"],[\"```\\n\\n## Prior training\\n\\nYou can fine-tune the WÃ¼rstchen prior model with the `train_text_to_image_pr...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n## Training with LoRA\\n\\nLow-Rank Adaption of Large Language Mode...\"],[\"```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip-captions\\\"\\n\\naccelerate launch train_text_to_imag...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Self-Attention Guidance on the [project page](https:\\u002f\\u002fku-c...\"],[\"# [Deprecated] Multi Token Textual Inversion\\n\\n**IMPORTART: This research project is deprecated. Mult...\"],[\"## Running locally with PyTorch\\n### Installing the dependencies\\n\\nBefore running the scripts, make su...\"],[\"```\\n\\nThen cd in the example folder  and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initial...\"],[\"```\\n\\nIf you have already cloned the repo, then you won't need to go through these steps.\\n\\n\\u003cbr\\u003e\\n\\nNow ...\"],[\"```\\n\\nA full training run takes ~1 hour on one V100 GPU.\\n\\n### Inference\\n\\nOnce you have trained a mode...\"],[\"```\\nIt should be at least 70% faster than the PyTorch script with the same configuration.\\n\\n### Train...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [CompVis\\u002flatent-diffusion](https:\\u002f\\u002fgithub.com\\u002fCompVis\\u002flatent-d...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\nNext, load a LoRA checkpoint with the [`~diffusers.loaders.StableDiffusionXLLoraLoaderMixin.lo...\"],[\"```\\n\\nLet's now generate an image with the second adapter and check the result:\\n\\n```python\\nprompt = \\\"...\"],[\"```\\n\\nNow that we have set these two adapters, let's generate an image from the combined adapters!\\n\\n\\u003c...\"],[\"```\\n\\n![toy-face-pixel-art](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002f...\"],[\"```\\n\\n![no-lora](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffus...\"],[\"```\\n\\n## Fusing adapters into the model\\n\\nYou can use PEFT to easily fuse\\u002funfuse multiple adapters dir...\"],[\"Consistency Decoder\\n\\nConsistency decoder can be used to decode the latents from the denoising UNet i...\"],[\"# Amused training\\n\\nAmused can be finetuned on simple datasets relatively cheaply and quickly. Using ...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"```\\n\\n#### Full finetuning + 8 bit adam\\n\\nNote that this training config keeps the batch size low and ...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"```\\n\\n#### Full finetuning + lora\\n\\nBatch size: 16, Learning rate: 8e-4, Gives decent results in 1000-...\"],[\"```\\n\\n### Finetuning the 512 checkpoint\\n\\nThese examples finetune on this [minecraft](https:\\u002f\\u002fhuggingf...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"```\\n\\n#### Full finetuning + 8 bit adam\\n\\nBatch size: 8, Learning rate: 5e-6, Gives decent results in ...\"],[\"```\\n\\n#### Full finetuning + lora \\n\\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 500-1...\"],[\"```\\n\\n### Styledrop\\n\\n[Styledrop](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2306.00983) is an efficient finetuning method ...\"],[\"```\\n\\n#### 256\\n\\nExample results:\\n\\n![glowing_256_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"```\\n\\n#### 512\\n\\nExample results:\\n\\n![glowing_512_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"# Diffusers examples with Intel optimizations\\n\\n**This research project is not actively maintained by...\"],[\"ğŸ§¨ Diffusers Experimental\\n\\nWe are adding experimental code to support novel applications and usages o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\ncd examples\\u002fdreambooth\\npip install -r requirements_fl...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nSome basic and important parameters to know and specify are:\\n\\n- `--pretrained_model_name_or_pat...\"],[\"```\\n\\n### Prior preservation loss\\n\\nPrior preservation loss is a method that uses a model's own genera...\"],[\"```\\n\\n## Training script\\n\\nDreamBooth comes with its own dataset classes:\\n\\n- [`DreamBoothDataset`](htt...\"],[\"```\\n\\nNext is the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f072e00897a7cf4302c347a63ec...\"],[\"if model_has_vae(args):\\n    vae = AutoencoderKL.from_pretrained(\\n        args.pretrained_model_name_...\"],[\"```\\n\\nThen, it's time to [create the training dataset](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f...\"],[\"```\\n\\nLastly, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f072e00897a7cf4302c347...\"],[\"```\\n\\nOne more thing before you launch the script! Depending on the GPU you have, you may need to ena...\"],[\"```\\n\\nDuring configuration, confirm that you want to use DeepSpeed. Now it should be possible to trai...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\u003chfoptions id=\\\"training-inference\\\"\\u003e\\n\\u003chfoption id=\\\"PyTorch\\\"\\u003e\\n\\n```py\\nfrom diffusers impor...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## LoRA\\n\\nLoRA is a training technique for significantly reducing the ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n2. Set the number of timesteps to run the denoising process for:\\n\\n```py\\n\\u003e\\u003e\\u003e scheduler.set_times...\"],[\"```\\n\\n5. Now write a loop to iterate over the timesteps. At each timestep, the model does a [`UNet2DM...\"],[\"```\\n\\nIn the next section, you'll put your skills to the test and breakdown the more complex Stable D...\"],[\"```py\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import CLIPTextModel, CLIPTok...\"],[\"```\\n\\nInstead of the default [`PNDMScheduler`], exchange it for the [`UniPCMultistepScheduler`] to se...\"],[\"```\\n\\nTokenize the text and generate the embeddings from the prompt:\\n\\n```py\\n\\u003e\\u003e\\u003e text_input = tokenize...\"],[\"```\\n\\n### Create random noise\\n\\nNext, generate some initial random noise as a starting point for the d...\"],[\"```\\n\\nThe last step is to create the denoising loop that'll progressively transform the pure noise in...\"],[\"```\\n\\nLastly, convert the image to a `PIL.Image` to see your generated image!\\n\\n```py\\n\\u003e\\u003e\\u003e image = (ima...\"],[\"Overview\\n\\nThese examples show how to run [Diffuser](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.09991) in Diffusers. ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"There are two options for converting a `.ckpt` file: use a Space to convert the checkpoint or conver...\"],[\"```\\n\\nTo use the script:\\n\\n1. Git clone the repository containing the `.ckpt` file you want to convert...\"],[\"```\\n\\n## Keras .pb or .h5\\n\\n\\u003cTip warning={true}\\u003e\\n\\nğŸ§ª This is an experimental feature. Only Stable Diffu...\"],[\"The Convert KerasCV Space allows you to input the following:\\n\\n* Your Hugging Face token.\\n* Paths to ...\"],[\"```\\n\\nThen, you can generate an image like:\\n\\n```py\\nfrom diffusers import DiffusionPipeline\\n\\npipeline ...\"],[\"```\\n\\nLoad the LoRA checkpoint into the pipeline with the [`~loaders.LoraLoaderMixin.load_lora_weight...\"],[\"InstructPix2Pix SDXL training example\\n\\n***This is based on the original InstructPix2Pix training exa...\"],[\"You will also need to get access of SDXL by filling the [form](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fst...\"],[\"```\\n\\nNow, we can launch training:\\n\\n```bash\\naccelerate launch train_instruct_pix2pix_sdxl.py \\\\\\n    --...\"],[\"```\\n\\nAdditionally, we support performing validation inference to monitor training progress\\nwith Weig...\"],[\"```\\n\\n We recommend this type of validation as it can be useful for model debugging. Note that you ne...\"],[\"```bash \\naccelerate launch --mixed_precision=\\\"fp16\\\" --multi_gpu train_instruct_pix2pix_sdxl.py \\\\\\n   ...\"],[\"```\\n\\n ## Inference\\n\\n Once training is complete, we can perform inference:\\n\\n ```python\\nimport PIL\\nimp...\"],[\"```\\n\\nWe encourage you to play with the following three parameters to control\\nspeed and quality durin...\"],[\"accelerate launch train_instruct_pix2pix.py \\\\\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n    ...\"],[\"```\\n\\nWe discovered that compared to training with SD-1.5 as the pretrained model, SDXL-0.9 results i...\"],[\"* SDXL: https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002fsdxl...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's the overview from the [project page](https:\\u002f\\u002fvislearn.github.io\\u002fControlNet-XS\\u002f):\\n\\n*With incre...\"],[\"ControlNet training example\\n\\n[Adding Conditional Control to Text-to-Image Diffusion Models](https:\\u002f\\u002f...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n\\n```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path to save mode...\"],[\"```\\n\\n## Training with multiple GPUs\\n\\n`accelerate` allows for seamless multi-GPU training. Follow the...\"],[\"```\\n\\n## Example results\\n\\n#### After 300 steps with batch size 8\\n\\n| |  | \\n|-------------------|:-----...\"],[\"#### After 6000 steps with batch size 8:\\n\\n| |  | \\n|-------------------|:-------------------------:|\\n...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path to save model\\\"\\n\\nac...\"],[\"```\\n\\n## Training on a 12 GB GPU\\n\\nOptimizations:\\n- Gradient checkpointing\\n- bitsandbyte's 8-bit optim...\"],[\"```\\n\\nWhen using `enable_xformers_memory_efficient_attention`, please make sure to install `xformers`...\"],[\"```\\n\\nSee [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002fusage_guides\\u002fdeepspeed) for more Dee...\"],[\"```\\n\\n## Performing inference with the trained ControlNet\\n\\nThe trained model can be run the same as t...\"],[\"```\\n\\n## Training with Flax\\u002fJAX\\n\\nFor faster training on TPUs and GPUs you can leverage the flax train...\"],[\"```\\n\\n\\nNow let's downloading two conditioning images that we will use to run validation during the tr...\"],[\"```\\n\\nAnd finally start the training\\n\\n```bash\\npython3 train_controlnet_flax.py \\\\\\n --pretrained_model_...\"],[\"```\\n\\nSince we passed the `--push_to_hub` flag, it will automatically create a model repo under your ...\"],[\"```\\n\\nNote, however, that the performance of the TPUs might get bottlenecked as streaming with `datas...\"],[\"```\\n\\nWe support training with the Min-SNR weighting strategy proposed in [Efficient Diffusion Traini...\"],[\"```\\n\\nThe profile can then be inspected at http:\\u002f\\u002flocalhost:6006\\u002f#profile\\n\\nSometimes you'll get versi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n2. Pass a prompt to the pipeline to generate an image:\\n\\n```py\\nimage = pipeline(\\n\\t\\\"stained glass...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\t\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocume...\"],[\"```\\n\\n### Stable Diffusion XL\\n\\nSDXL is a much larger version of the previous Stable Diffusion models,...\"],[\"```\\n\\n### ControlNet\\n\\nControlNet models are auxiliary models or adapters that are finetuned on top of...\"],[\"```\\n\\nPass the `controlnet` to the [`AutoPipelineForText2Image`], and provide the prompt and pose est...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"### Height and width\\n\\nThe `height` and `width` parameters control the height and width (in pixels) o...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\t\\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2I...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2I...\"],[\"```\\n\\n## Control image generation\\n\\nThere are several ways to exert more control over how an image is ...\"],[\"```\\n\\n### ControlNet\\n\\nAs you saw in the [ControlNet](#controlnet) section, these models offer a more ...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2I...\"],[\"```\\n\\nFor more tips on how to optimize your code to save memory and speed up inference, read the [Mem...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For more details about how Stable Diffusion 2 works and how it differs from the original Stable Diff...\"],[\"Stable Diffusion 2 is available for tasks like text-to-image, inpainting, super-resolution, and dept...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to exp...\"],[\"```\\n\\n## Inpainting\\n\\n```py\\nimport torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepSc...\"],[\"```\\n\\n## Super-resolution\\n\\n```py\\nfrom diffusers import StableDiffusionUpscalePipeline\\nfrom diffusers....\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"```\\n\\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. Y...\"],[\"```\\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node. Add t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install diffusers[training]...\"],[\"```\\n\\nWe encourage you to share your model with the community, and in order to do that, you'll need t...\"],[\"```\\n\\n## Training configuration\\n\\nFor convenience, create a `TrainingConfig` class containing the trai...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ You can find additional datasets from the [HugGan Community Event](https:\\u002f\\u002fhuggingface...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Create a UNet2DModel\\n\\nPretrained models in ğŸ§¨ Diffusers are easily created from their model c...\"],[\"```\\n\\nIt is often a good idea to quickly check the sample image shape matches the model output shape:...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nThen, you'll need a way to evaluate the model. For evaluation, you can use the [`DDPMPipeline`]...\"],[\"```\\n\\nNow you can wrap all these components together in a training loop with ğŸ¤— Accelerate for easy Te...\"],[\"...     # Prepare everything\\n...     # There is no specific order to remember, you just need to unpa...\"],[\"...                 accelerator.clip_grad_norm_(model.parameters(), 1.0)\\n...                 optimiz...\"],[\"```\\n\\nPhew, that was quite a bit of code! But you're finally ready to launch the training with ğŸ¤— Acce...\"],[\"ConsistencyDecoderScheduler\\n\\nThis scheduler is a part of the [`ConsistencyDecoderPipeline`] and was ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The callback function should have the following arguments:\\n\\n* `pipe` (or the pipeline instance) prov...\"],[\"```\\n\\nNow, you can pass the callback function to the `callback_on_step_end` parameter and the `prompt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are com...\"],[\"- [**Safety Checker**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fst...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow pass your prompt to the pipeline. You can also pass a `negative_prompt` to prevent certain ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion text-to-image fine-tuning\\n\\nThe `train_text_to_image.py` script shows how to fine-tu...\"],[\"```\\n\\nIf you have already cloned the repo, then you won't need to go through these steps.\\n\\n\\u003cbr\\u003e\\n\\n####...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo run on your own training files prepare the dataset accordin...\"],[\"```\\n\\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\\n\\n```p...\"],[\"```\\n\\n\\n#### Training with Min-SNR weighting\\n\\nWe support training with the Min-SNR weighting strategy ...\"],[\"In a nutshell, LoRA allows adapting pretrained models by adding pairs of rank-decomposition matrices...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-bl...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so\\nwe need t...\"],[\"```\\n\\nThe above command will also run inference as fine-tuning progresses and log the results to Weig...\"],[\"prompt = \\\"A pokemon with green eyes and red legs.\\\"\\nimage = pipe(prompt, num_inference_steps=30, guid...\"],[\"```\\n\\nIf you are loading the LoRA parameters from the Hub and if the Hub repository has\\na `base_model...\"],[\"```\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax\\\"\\nexport DATASET_NAME=\\\"lambdalabs\\u002f...\"],[\"```\\n\\n### Training with xFormers:\\n\\nYou can enable memory efficient attention by [installing xFormers]...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Diffusers examples with ONNXRuntime optimizations\\n\\n**This research project is not actively maintai...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The Intel Labs Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicensed u...\"],[\"The abstract from the paper is:\\n\\n*This research paper proposes a Latent Diffusion Model for 3D (LDM3...\"],[\"# Upscaler\\n\\n[LDM3D-VR](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2311.03226.pdf) is an extended version of LDM3D. \\n\\nThe ...\"],[\"# Training examples\\n\\nCreating a training image set is [described in a different document](https:\\u002f\\u002fhu...\"],[\"Models\\n\\nFor more detail on the models, please refer to the [docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffus...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [openai\\u002fconsistency_models](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fconsiste...\"],[\"!--Copyright 2023 Custom Diffusion authors The HuggingFace Team. All rights reserved.\\n\\nLicensed unde...\"],[\"```\\n\\nNavigate to the example folder with the training script and install the required dependencies:\\n...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic parameters are described in the [DreamBooth](dreambooth#script-parameters) tr...\"],[\"```\\n\\nTo enable regularization, add the following parameters:\\n\\n- `--with_prior_preservation`: whether...\"],[\"```\\n\\n## Training script\\n\\n\\u003cTip\\u003e\\n\\nA lot of the code in the Custom Diffusion training script is similar...\"],[\"```py\\nparams_to_freeze = itertools.chain(\\n    text_encoder.text_model.encoder.parameters(),\\n    text...\"],[\"```\\n\\nNow you'll need to add the [Custom Diffusion weights](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002f...\"],[\"```py\\nst = unet.state_dict()\\nfor name, _ in unet.attn_processors.items():\\n    cross_attention_dim = ...\"],[\"cross_attention_dim=cross_attention_dim,\\n        ).to(unet.device)\\n        custom_diffusion_attn_pro...\"],[\"```\\n\\nThe [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f84cd9e8d01adb47f046b1ee449fc76a0c...\"],[\"```\\n\\nIn the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f84cd9e8d01adb47f046b1ee449...\"],[\"```\\n\\n## Launch the script\\n\\nOnce youâ€™ve made all your changes or youâ€™re okay with the default configu...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport OUTPUT_DIR=\\\"path-to-save-model\\\"\\nexp...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"multiple concepts\\\"\\u003e\\n\\nCustom Diffusion can also learn multiple concept...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nOnce training is finished, you can use your new Custom Diffusion mode...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training a model with Custom Diffus...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to le...\"],[\"Stable Diffusion text-to-image fine-tuning\\n\\nThe `train_text_to_image.py` script shows how to fine-tu...\"],[\"```\\n\\nIf you have already cloned the repo, then you won't need to go through these steps.\\n\\n\\u003cbr\\u003e\\n\\n## U...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Prompt weighting works by increasing or decreasing the scale of the text embedding vector that corre...\"],[\"```\\n\\nFor this guide, let's generate an image with the prompt `\\\"a red cat playing with a ball\\\"` using...\"],[\"```\\n\\ncompel uses `+` or `-` to increase or decrease the weight of a word in the prompt. To increase ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nThis time, let's upweight \\\"ball\\\" by a factor of 1.5 for the first prompt, and downweight \\\"ball\\\"...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow you can load a pipeline, and pass the pre-learned concept to it:\\n\\n```py\\npipeline = StableDi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nThere are two tensors, `\\\"clip_g\\\"` and `\\\"clip_l\\\"`.\\n`\\\"clip_g\\\"` corresponds to the bigger text enc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find the original codebase for Stable Diffusion v1.0 at [CompVis\\u002fstable-diffusion](https:\\u002f\\u002fg...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cdiv class=\\\"rounded-xl border border-gray-200\\\"\\u003e\\n    \\u003ctable cla...\"],[\"\\u003c\\u002ftd\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003eimage-to-image\\u003c\\u002ftd\\u003e\\n            \\u003ctd class=\\\"px-...\"],[\"\\u003c\\u002ftd\\u003e\\n        \\u003c\\u002ftr\\u003e\\n        \\u003ctr\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003e\\n            \\u003ca hre...\"],[\"\\u003c\\u002ftd\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003etext-to-image, inpainting, depth-to-image, sup...\"],[\"\\u003c\\u002ftd\\u003e\\n        \\u003c\\u002ftr\\u003e\\n        \\u003ctr\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003e\\n            \\u003ca hre...\"],[\"## Tips\\n\\nTo help you get the most out of the Stable Diffusion pipelines, here are a few tips for imp...\"],[\"```\\n\\n### Reuse pipeline components to save memory\\n\\nTo save memory and use the same components across...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf a community pipeline doesn't work as expected, please open a GitHub issue and mention the au...\"],[\"diffuser_pipeline.enable_attention_slicing()\\ndiffuser_pipeline = diffuser_pipeline.to(device)\\n\\npromp...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f43138...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fuser-images.githubuse...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe example prompt you'll use is a portrait of an old warrior chief, but feel free to use your ...\"],[\"```\\n\\nNow you can generate an image:\\n\\n```python\\nimage = pipeline(prompt, generator=generator).images[...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```python\\npipeline.scheduler.compatibles\\n[\\n    diffusers.schedulers.scheduling_lms_discrete.LMSDiscr...\"],[\"```\\n\\nThe Stable Diffusion model uses the [`PNDMScheduler`] by default which usually requires ~50 inf...\"],[\"```\\n\\nStart with `batch_size=4` and see how much memory you've consumed:\\n\\n```python\\nfrom diffusers.ut...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```python\\nfrom diffusers import AutoencoderKL\\n\\nvae = AutoencoderKL.from_pretrained(\\\"stabilityai\\u002fsd-v...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional smaller Stable Diffusion XL (SDXL) ControlNet checkpoints from the ğŸ¤— [Diffus...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Inspired by [Stable Diffusion](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fpipelines\\u002fstable_diffusion\\u002f...\"],[\"The abstract of the paper is the following:\\n\\n*Although audio generation shares commonalities across ...\"],[\"All checkpoints share the same model size for the text encoders and VAE. They differ in the size and...\"],[\"### Controlling inference\\n\\n* The _quality_ of the predicted audio sample can be controlled by the `n...\"],[\"!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License,...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fdiffusers\\u002fma...\"],[\"ğŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating ima...\"],[\"```\\n\\nWith `conda` (maintained by the community):\\n\\n```sh\\nconda install -c conda-forge diffusers\\n```\\n\\n...\"],[\"```\\n\\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\\n\\n``...\"],[\"```\\n\\nCheck out the [Quickstart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fquicktour) to launch your diff...\"],[\"| **Documentation**                                                   | **What can I learn?**       ...\"],[\"| [Optimization](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002foptimization\\u002fopt_overview)                   ...\"],[\"We â¤ï¸  contributions from the open-source community!\\nIf you want to contribute to this library, plea...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003cth\\u003eTask\\u003c\\u002fth\\u003e\\n    \\u003cth\\u003ePipeline\\u003c\\u002fth\\u003e\\n    \\u003cth\\u003eğŸ¤— Hub\\u003c\\u002fth\\u003e\\n  \\u003c\\u002ftr\\u003e\\n  \\u003ctr style=\\\"borde...\"],[\"\\u003ctd\\u003e\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fpipelines\\u002fdeepfloyd_if\\\"\\u003eDeepFloyd IF\\u003c\\u002fa\\u003e\\u003c\\u002ftd\\u003e...\"],[\"\\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003eText-guided Image-to-Image\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdi...\"],[\"\\u003ctd\\u003e\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fpipelines\\u002fstable_diffusion\\u002fupscale\\\"\\u003eStable Di...\"],[\"## Popular libraries using ğŸ§¨ Diffusers\\n\\n- https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fTaskMatrix\\n- https:\\u002f\\u002fgithub.c...\"],[\"## Citation\\n\\n```bibtex\\n@misc{von-platen-etal-2022-diffusers,\\n  author = {Patrick von Platen and Sura...\"],[\"Stable Diffusion text-to-image fine-tuning\\nThis extended LoRA training script was authored by [haofa...\"],[\"With LoRA, it's possible to fine-tune Stable Diffusion on a custom image-caption pair dataset\\non con...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so \\nwe need ...\"],[\"```\\n\\nThe above command will also run inference as fine-tuning progresses and log the results to Weig...\"],[\"prompt = \\\"A pokemon with green eyes and red legs.\\\"\\nimage = pipe(prompt, num_inference_steps=30, guid...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"We provide a high level explanation of how the generation can be controlled as well as a snippet of ...\"],[\"|                     **Method**                      | **Inference only** | **Requires training \\u002f\\u003cb...\"],[\"|              [ControlNet](#controlnet)              |         âœ…         |                   âŒ     ...\"],[\"[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.09800)\\n\\n[InstructPix2Pix](..\\u002fapi\\u002fpipelines\\u002fpix2pix) is fine-tuned...\"],[\"Pix2Pix Zero can be used both to edit synthetic images as well as real images.\\n\\n- To edit synthetic ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Attend and Excite\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2301.13826)\\n\\n[Attend and Excite](..\\u002fapi\\u002f...\"],[\"SAG provides guidance from predictions not conditioned on high-frequency details to fully conditione...\"],[\"## ControlNet\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2302.05543)\\n\\n[ControlNet](..\\u002fapi\\u002fpipelines\\u002fcontrolnet) ...\"],[\"## T2I-Adapter\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2302.08453)\\n\\n[T2I-Adapter](..\\u002fapi\\u002fpipelines\\u002fstable_dif...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Sliced VAE\\n\\nSliced VAE enables decoding large batches of images with limited VRAM or batches with...\"],[\"```\\n\\nYou may see a small performance boost in VAE decoding on multi-image batches, and there should ...\"],[\"```\\n\\nThe output image has some tile-to-tile tone variation because the tiles are decoded separately,...\"],[\"```\\n\\nCPU offloading works on submodules rather than whole models. This is the best way to minimize m...\"],[\"Enable model offloading by calling [`~StableDiffusionPipeline.enable_model_cpu_offload`] on the pipe...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIn order to properly offload models after they're called, it is required ...\"],[\"```\\n\\n## Tracing\\n\\nTracing runs an example input tensor through the model and captures the operations ...\"],[\"# warmup and optimize graph\\nfor _ in range(5):\\n    with torch.inference_mode():\\n        inputs = gen...\"],[\"```\\n\\nReplace the `unet` attribute of the pipeline with the traced model:\\n\\n```python\\nfrom diffusers i...\"],[\"```\\n\\n## Memory-efficient attention\\n\\nRecent work on optimizing bandwidth in the attention block has g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\n- SDXL Turbo uses the exact same architecture as [SDXL](.\\u002fstable_diffusion_xl), which means...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nfrom diffusers import AutoencoderKL\\n\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fsd-vae-ft-mse-o...\"],[\"```\\n\\n## AutoencoderKL\\n\\n[[autodoc]] AutoencoderKL\\n\\n## AutoencoderKLOutput\\n\\n[[autodoc]] models.autoenc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"We enormously value feedback from the community, so please do not be afraid to speak up if you belie...\"],[\"* 1. Asking and answering questions on [the Diffusers discussion forum](https:\\u002f\\u002fdiscuss.huggingface....\"],[\"* 9. Add a new pipeline, model, or scheduler, see [\\\"New Pipeline\\u002fModel\\\"](https:\\u002f\\u002fgithub.com\\u002fhuggingf...\"],[\"As said before, **all contributions are valuable to the community**.\\nIn the following, we will expla...\"],[\"**NOTE about channels**:\\n[*The forum*](https:\\u002f\\u002fdiscuss.huggingface.co\\u002fc\\u002fdiscussion-related-to-httpsg...\"],[\"**Please consider the following guidelines when opening a new issue**:\\n- Make sure you have searched...\"],[\"New issues usually include the following.\\n\\n#### 2.1. Reproducible, minimal bug reports\\n\\nA bug report...\"],[\"#### 2.2. Feature requests\\n\\nA world-class feature request addresses the following points:\\n\\n1. Motiva...\"],[\"You can open an issue about a technical question [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fiss...\"],[\"Also, many issues tend to be simply off-topic, duplicates of other issues, or irrelevant. It is of g...\"],[\"### 4. Fixing a \\\"Good first issue\\\"\\n\\n*Good first issues* are marked by the [Good first issue](https:\\u002f...\"],[\"Contributing to the library can have many forms:\\n\\n- Correcting spelling or grammatical errors.\\n- Cor...\"],[\"- Official Pipelines\\n- Community Pipelines\\n\\nBoth official and community pipelines follow the same de...\"],[\"Community pipeline PRs are only checked at a superficial level and ideally they should be maintained...\"],[\"```\\n\\nas well as to install all additional dependencies required for training:\\n\\n```bash\\npip install -...\"],[\"```\\n\\nTherefore when adding an example, the `requirements.txt` file shall define all pip dependencies...\"],[\"To contribute an example, it is highly recommended to look at already existing examples such as [dre...\"],[\"### 8. Fixing a \\\"Good second issue\\\"\\n\\n*Good second issues* are marked by the [Good second issue](http...\"],[\"By adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of th...\"],[\"If you are unsure or stuck in the PR, don't hesitate to leave a message to ask for a first review or...\"],[\"```\\n\\nTo learn more, read this section of the [~Don't~ Repeat Yourself*](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002f...\"],[\"1. Make sure that you've used the correct template for your issue. You can pick between *Bug Report*...\"],[\"4. **Minimalistic**: Try to help the reader as much as you can to understand the issue as quickly as...\"],[\"## How to write a good PR...\"],[\"1. Be a chameleon. Understand existing design patterns and syntax and make sure your code additions ...\"],[\"[`hf-internal-testing`](https:\\u002f\\u002fhuggingface.co\\u002fhf-internal-testing) or [huggingface\\u002fdocumentation-im...\"],[\"## How to open a PR\\n\\nBefore writing code, we strongly advise you to search through the existing PRs ...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n ```bash\\n $ git checkout -b a-descrip...\"],[\"```\\n\\nIt is a good idea to sync your copy of the code with the original\\nrepository regularly. This wa...\"],[\"```\\n\\n### Syncing forked main with upstream (HuggingFace) main\\n\\nTo avoid pinging the upstream reposit...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Training | SDXL-support | LoRA-support | Flax-support |\\n|---|---|---|---|\\n| [unconditional image g...\"],[\"| [ControlNet](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002ftree\\u002fmain\\u002fexamples\\u002fcontrolnet) | ğŸ‘ |  | ğŸ‘ |\\n...\"],[\"These examples are **actively** maintained, so please feel free to open an issue if they aren't work...\"],[\"```\\n\\nThen navigate to the folder of the training script (for example, [DreamBooth](https:\\u002f\\u002fgithub.co...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Recent text-to-video generation approaches rely on computationally...\"],[\"prompt = \\\"A panda is playing guitar on times square\\\"\\nresult = pipe(prompt=prompt).images\\nresult = [(...\"],[\"```\\nYou can change these parameters in the pipeline call:\\n* Motion field strength (see the [paper](h...\"],[\"# Generate the video chunk-by-chunk\\nresult = []\\nchunk_ids = np.arange(0, video_length, chunk_size - ...\"],[\"```\\n\\n\\n- #### SDXL Support\\nIn order to use the SDXL model when generating a video from prompt, use th...\"],[\"```\\n    To extract pose from actual video, read [ControlNet documentation](controlnet).\\n\\n3. Run `Sta...\"],[\"```\\n- #### SDXL Support\\n\\t\\n\\tSince our attention processor also works with SDXL, it can be utilized to...\"],[\"```\\n\\n### Text-To-Video with Edge Control\\n\\nTo generate a video from prompt with additional Canny edge...\"],[\"```\\n\\n\\n### DreamBooth specialization\\n\\nMethods **Text-To-Video**, **Text-To-Video with Pose Control** ...\"],[\"```\\n\\n3. Run `StableDiffusionControlNetPipeline` with custom trained DreamBooth model\\n    ```python\\n ...\"],[\"```\\n\\nYou can filter out some available DreamBooth-trained models with [this link](https:\\u002f\\u002fhuggingfac...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- a *locked copy* keeps everything a large pretrained diffusion model has learned\\n- a *trainable cop...\"],[\"```\\n\\n## Text-to-image\\n\\nFor text-to-image, you normally pass a text prompt to the model. But with Con...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nNow pass your prompt and canny image to the pipeline:\\n\\n```py\\noutput = pipe(\\n    \\\"the mona lisa\\\"...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"```\\n\\nNext, load a ControlNet model conditioned on depth maps and pass it to the [`StableDiffusionCon...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nCreate a function to prepare the control image from the initial and mask images. This'll create...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nNow pass your prompt, initial image, mask image, and control image to the pipeline:\\n\\n```py\\noutp...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"original_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002ftakuma104\\u002fcontrolnet_dev\\u002fresolve\\u002fmain\\u002fbird_512x5...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002ftakuma...\"],[\"image = np.array(original_image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image,...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nNow pass your prompt (and optionally a negative prompt if you're using one) and canny image to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdiffu...\"],[\"image = pipe(\\n    prompt, negative_prompt=negative_prompt, controlnet_conditioning_scale=0.5, image=...\"],[\"```\\n\\n### MultiControlNet\\n\\n\\u003cTip\\u003e\\n\\nReplace the SDXL model with a model like [runwayml\\u002fstable-diffusion...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"vae = AutoencoderKL.from_pretrained(\\\"madebyollin\\u002fsdxl-vae-fp16-fix\\\", torch_dtype=torch.float16, use_...\"],[\"```\\n\\nNow you can pass your prompt (an optional negative prompt if you're using one), canny image, an...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"As said before, **all contributions are valuable to the community**.\\nIn the following, we will expla...\"],[\"#### 2.2. Feature requests\\n\\nA world-class feature request addresses the following points:\\n\\n1. Motiva...\"],[\"You can open an issue about a technical question [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fiss...\"],[\"Contributing to the library can have many forms:\\n\\n- Correcting spelling or grammatical errors.\\n- Cor...\"],[\"To contribute an example, it is highly recommended to look at already existing examples such as [dre...\"],[\"By adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of th...\"],[\"## How to open a PR\\n\\nBefore writing code, we strongly advise you to search through the existing PRs ...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n ```bash\\n $ git checkout -b a-descrip...\"],[\"```\\n\\nIt is a good idea to sync your copy of the code with the original\\nrepository regularly. This wa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## DPMSolverSinglestepScheduler\\n[[autodoc]] DPMSolverSinglestepScheduler\\n\\n## SchedulerOutput\\n[[autod...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"To load any community pipeline on the Hub, pass the repository id of the community pipeline to the `...\"],[\"```\\n\\nLoading an official community pipeline is similar, but you can mix loading weights from an offi...\"],[\"```\\n\\nFor more information about community pipelines, take a look at the [Community pipelines](custom...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIn steps 4 and 5, the custom [UNet](https:\\u002f\\u002fgithub.com\\u002fshowlab\\u002fShow-1\\u002fblo...\"],[\"```\\n\\n5. Finally, you'll load the custom pipeline code. For this example, it has already been created...\"],[\"```\\n\\nPush the pipeline to the Hub to share with the community!\\n\\n```python\\npipeline.push_to_hub(\\\"cust...\"],[\"```\\n\\nAs an additional reference example, you can refer to the repository structure of [stabilityai\\u002fj...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present the vector quantized diffusion (VQ-Diffusion) model for...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo generate images with Stable Diffusion 1 and 2 on Gaudi, you need to instantiate two instance...\"],[\"```\\n\\nFor more information, check out ğŸ¤— Optimum Habana's [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002f...\"],[\"For [Stable Diffusion v2.1](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable-diffusion-2-1) on 768x768 imag...\"],[\"DreamBooth training example for Stable Diffusion XL (SDXL)\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208....\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom ...\"],[\"```\\n\\nThis will also allow us to push the trained LoRA parameters to the Hugging Face Hub platform. \\n...\"],[\"```\\n\\nTo better track our training experiments, we're using the following flags in the command above:...\"],[\"```\\n\\nand making sure that you have the following libraries installed:\\n\\n```\\nbitsandbytes\\u003e=0.40.0\\nxfor...\"],[\"```\\n\\nWe can further refine the outputs with the [Refiner](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable-...\"],[\"```\\n\\nHere's a side-by-side comparison of the with and without Refiner pipeline outputs:\\n\\n| Without R...\"],[\"## Results\\n\\nYou can explore the results from a couple of our internal experiments by checking out th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you look at the [`runwayml\\u002fstable-diffusion-v1-5`](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-di...\"],[\"```\\n\\n## Convert to safetensors\\n\\nNot all weights on the Hub are available in the `.safetensors` forma...\"],[\"```\\n\\n## Why use safetensors?\\n\\nThere are several reasons for using safetensors:\\n\\n- Safety is the numb...\"],[\"Deprecated Pipelines\\n\\nThis folder contains pipelines that have very low usage as measured by model d...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"A demo for the [SimianLuo\\u002fLCM_Dreamshaper_v7](https:\\u002f\\u002fhuggingface.co\\u002fSimianLuo\\u002fLCM_Dreamshaper_v7) c...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https:\\u002f...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```\\n\\n## Training script\\n\\nThe training script is also similar to the [Text-to-image](text2image#train...\"],[\"```py\\ntokenizer_one = AutoTokenizer.from_pretrained(\\n    args.pretrained_model_name_or_path, subfold...\"],[\"```\\n\\nThe [prompt and image embeddings](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7bc...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"accelerate launch train_text_to_image_sdxl.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n  --...\"],[\"```\\n\\nAfter you've finished training, you can use your newly trained SDXL model for inference!\\n\\n\\u003chfop...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training a SDXL model! To learn mor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Schedulers\\n\\nFor more information on the schedulers, please refer to the [docs](https:\\u002f\\u002fhuggingface.c...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage\\n\\nBefore you can use IF, you need to accept its usage conditions. To do so:\\n1. Make sure to ...\"],[\"```\\n\\nrun the login function in a Python shell:\\n\\n```py\\nfrom huggingface_hub import login\\n\\nlogin()\\n```...\"],[\"```\\n\\nThe following sections give more in-detail examples of how to use IF. Specifically:\\n\\n- [Text-to...\"],[\"### Text-to-Image Generation\\n\\nBy default diffusers makes use of [model cpu offloading](..\\u002f..\\u002foptimiz...\"],[\"# stage 2\\nstage_2_output = stage_2(\\n    image=stage_1_output,\\n    prompt_embeds=prompt_embeds,\\n    n...\"],[\"```\\n\\n### Text Guided Image-to-Image Generation\\n\\nThe same IF model weights can be used for text-guide...\"],[\"# stage 3\\nsafety_modules = {\\n    \\\"feature_extractor\\\": stage_1.feature_extractor,\\n    \\\"safety_checker...\"],[\"```\\n\\n### Text Guided Inpainting Generation\\n\\nThe same IF model weights can be used for text-guided im...\"],[\"# stage 3\\nsafety_modules = {\\n    \\\"feature_extractor\\\": stage_1.feature_extractor,\\n    \\\"safety_checker...\"],[\"```\\n\\n### Converting between different pipelines\\n\\nIn addition to being loaded with `from_pretrained`,...\"],[\"```\\n\\nWhen doing image variation or inpainting, you can also decrease the number of timesteps\\nwith th...\"],[\"```\\n\\nor the more aggressive layer based CPU offloading.\\n\\n```py\\npipe = DiffusionPipeline.from_pretrai...\"],[\"```\\n\\nFor CPU RAM constrained machines like Google Colab free tier where we can't load all model comp...\"],[\"#pt_to_pil(stage_1_output)[0].save(\\\".\\u002fif_stage_I.png\\\")\\n\\n# Remove the pipeline so we can load the sup...\"],[\"```\\n\\n## Available Pipelines:\\n\\n| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_if.py](https:\\u002f\\u002f...\"],[\"## IFPipeline\\n[[autodoc]] IFPipeline\\n\\t- all\\n\\t- __call__\\n\\n## IFSuperResolutionPipeline\\n[[autodoc]] IF...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen enable the FreeU mechanism with the FreeU-specific hyperparameters. These values are scali...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002ffre...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002ffre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nTo setup a default ğŸ¤— Accelerate environment without choosing any configurations:\\n\\n```bash\\naccel...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```\\n\\n## Training script\\n\\nAs with the script parameters, a general walkthrough of the training script...\"],[\"```\\n\\nWithin the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da01082055a901f2883...\"],[\"```\\n\\nFinally, in the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da0108205...\"],[\"```\\n\\nOne more thing before you launch the script! Depending on the GPU you have, you may need to ena...\"],[\"```\\n\\nDuring configuration, confirm that you want to use DeepSpeed stage 2. Now it should be possible...\"],[\"```\\n\\nYou should also change the default Adam optimizer to DeepSpeedâ€™s optimized version of Adam [`de...\"],[\"```\\n\\nThen you can inspect the profile at [http:\\u002f\\u002flocalhost:6006\\u002f#profile](http:\\u002f\\u002flocalhost:6006\\u002f#pro...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nOnce training is complete, you can use your newly trained model for i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usability over Performance\\n\\n- While Diffusers has many built-in performance-enhancing features (s...\"],[\"## Simple over easy\\n\\nAs PyTorch states, **explicit is better than implicit** and **simple is better ...\"],[\"## Tweakable, contributor-friendly over abstraction\\n\\nFor large parts of the library, Diffusers adopt...\"],[\"In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffu...\"],[\"### Pipelines\\n\\nPipelines are designed to be easy to use (therefore do not follow [*Simple over easy*...\"],[\"The following design principles are followed:\\n- Pipelines follow the single-file policy. All pipelin...\"],[\"- Pipelines should be very readable, self-explanatory, and easy to tweak.\\n- Pipelines should be desi...\"],[\"### Models\\n\\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch's ...\"],[\"The following design principles are followed:\\n- Models correspond to **a type of model architecture*...\"],[\"- Models all inherit from `ModelMixin` and `ConfigMixin`.\\n- Models can be optimized for performance ...\"],[\"### Schedulers\\n\\nSchedulers are responsible to guide the denoising process for inference as well as t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before you convert a model, though, take a moment to explore the Hugging Face Hub â€“ chances are the ...\"],[\"- The supported inference framework.\\n    * `packages` are suitable for Python inference. This can be...\"],[\"```\\ncoreml-stable-diffusion-v1-4\\nâ”œâ”€â”€ README.md\\nâ”œâ”€â”€ original\\nâ”‚   â”œâ”€â”€ compiled\\nâ”‚   â””â”€â”€ packages\\nâ””â”€â”€ sp...\"],[\"```\\n\\nPass the path of the downloaded checkpoint with `-i` flag to the script. `--compute-unit` indic...\"],[\"```\\n\\n## Core ML inference in Swift\\n\\nRunning inference in Swift is slightly faster than in Python bec...\"],[\"```\\n\\nYou have to specify in `--resource-path` one of the checkpoints downloaded in the previous step...\"],[\"If you feel strongly about any missing features, please feel free to open a feature request or, bett...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ¤— Accelerate is a library for helping you train on multiple GPUs\\u002fTPUs or with mixed-prec...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```\\n\\nYou'll also load the [`WuerstchenPrior`] model for optimization.\\n\\n```py\\nprior = WuerstchenPrior...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f65ef7a0c5c594b4f8409...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```\\n\\nOnce training is complete, you can use your newly trained model for inference!\\n\\n```py\\nimport to...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## UNet2DConditionModel\\n[[autodoc]] UNet2DConditionModel\\n\\n## UNet2DConditionOutput\\n[[autodoc]] model...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## UNet2DModel\\n[[autodoc]] UNet2DModel\\n\\n## UNet2DOutput\\n[[autodoc]] models.unet_2d.UNet2DOutput...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide explores the [train_text_to_image_prior.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```\\n\\n## Training script\\n\\nThe training script is also similar to the [Text-to-image](text2image#train...\"],[\"with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\\n    image_encoder = CLIPVision...\"],[\"```\\n\\nKandinsky uses a [`PriorTransformer`] to generate the image embeddings, so you'll want to setup...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f6e68c71503682c8693cb...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```\\n\\nNext, the script includes several image transforms and a [preprocessing](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Launch the script\\n\\nOnce youâ€™ve made all your changes or youâ€™re oka...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\u003chfoptions id=\\\"training-inference\\\"\\u003e\\n\\u003chfoption id=\\\"prior model\\\"\\u003e\\n\\n```bash\\nexport DATASET_NAME...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"decoder model\\\"\\u003e\\n\\n```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFeel free to replace `kandinsky-community\\u002fkandinsky-2-2-decoder` with your own trained d...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training a Kandinsky 2.2 model! To ...\"],[\"InstructPix2Pix training example\\n\\n[InstructPix2Pix](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.09800) is a method to...\"],[\"## Running locally with PyTorch\\n\\n### Installing the dependencies\\n\\nBefore running the scripts, make s...\"],[\"```\\n\\nThen cd in the example folder and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initiali...\"],[\"```\\n\\nNow, we can launch training:\\n\\n```bash\\naccelerate launch --mixed_precision=\\\"fp16\\\" train_instruct...\"],[\"```\\n\\nAdditionally, we support performing validation inference to monitor training progress\\nwith Weig...\"],[\"```\\n\\n We recommend this type of validation as it can be useful for model debugging. Note that you ne...\"],[\"```\\n\\n ## Inference\\n\\n Once training is complete, we can perform inference:\\n\\n ```python\\nimport PIL\\nimp...\"],[\"```\\n\\nAn example model repo obtained using this training script can be found\\nhere - [sayakpaul\\u002finstru...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nFor example, let's use the [`ptx0\\u002fpseudo-journey-v2`](https:\\u002f\\u002fhuggingface.co\\u002fptx0\\u002fpseudo-journe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The SDE variant of DPMSolver and DPM-Solver++ is also supported, but only for the first and second-o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\n- Using SDXL with a DPM++ scheduler for less than 50 steps is known to produce [visual arti...\"],[\"Check out the [Stability AI](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai) Hub organization for the official b...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\nAll methods of the logging module are documented below. The main methods are\\n[`logging.get_ver...\"],[\"[[autodoc]] utils.logging.get_verbosity\\n\\n[[autodoc]] utils.logging.set_verbosity\\n\\n[[autodoc]] utils....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Training an unconditional diffusion model\\n\\nCreating a training image set is [described in a differ...\"],[\"```\\nAn example trained model: https:\\u002f\\u002fhuggingface.co\\u002fanton-l\\u002fddpm-ema-flowers-64\\n\\nA full training ru...\"],[\"```\\nAn example trained model: https:\\u002f\\u002fhuggingface.co\\u002fanton-l\\u002fddpm-ema-pokemon-64\\n\\nA full training ru...\"],[\"```\\n\\nTo be able to use Weights and Biases (`wandb`) as a logger you need to install the library: `pi...\"],[\"```\\n\\nInternally, the script will use the [`ImageFolder`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.0.0...\"],[\"```\\n\\n`ImageFolder` will create an `image` column containing the PIL-encoded images.\\n\\nNext, push it t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*The most advanced text-to-image (T2I) models require significant t...\"],[\"You can find the original codebase at [PixArt-alpha\\u002fPixArt-alpha](https:\\u002f\\u002fgithub.com\\u002fPixArt-alpha\\u002fPi...\"],[\"```\\n\\nThen load the text encoder in 8-bit:\\n\\n```python\\nfrom transformers import T5EncoderModel\\nfrom di...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNotice that while initializing `pipe`, you're setting `text_encoder` to `None` so that i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Scenarios\\n\\nWe cover Diffusion models with the following pipelines:\\n\\n- Text-guided image generatio...\"],[\"PartiPrompts has the following columns:\\n\\n- Prompt\\n- Category of the prompt (such as â€œAbstractâ€, â€œWor...\"],[\"# Fixing these sample prompts in the interest of reproducibility.\\nsample_prompts = [\\n    \\\"a corgi\\\",\\n...\"],[\"```\\n\\nNow we can use these prompts to generate some images using Stable Diffusion ([v1-4 checkpoint](...\"],[\"```\\n\\n![parti-prompts-14](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002fevaluati...\"],[\"Let's first load a [`StableDiffusionPipeline`]:\\n\\n```python\\nfrom diffusers import StableDiffusionPipe...\"],[\"```\\n\\nGenerate some images with multiple prompts:\\n\\n```python\\nprompts = [\\n    \\\"a photo of an astronaut...\"],[\"```\\n\\nIn the above example, we generated one image per prompt. If we generated multiple images per pr...\"],[\"```\\n\\nIt seems like the [v1-5](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-v1-5) checkpoint perf...\"],[\"We have prepared a mini dataset to implement this metric. Let's first load the dataset.\\n\\n```python\\nf...\"],[\"```\\n\\n```bash\\n{'input': Value(dtype='string', id=None),\\n 'edit': Value(dtype='string', id=None),\\n 'ou...\"],[\"```\\n\\nAnd here is the image:\\n\\n```python\\ndataset[idx][\\\"image\\\"]\\n```\\n\\n![edit-dataset](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\nTo measure the directional similarity, we first load CLIP's image and text encoders:\\n\\n```python...\"],[\"```\\n\\nNotice that we are using a particular CLIP checkpoint, i.e.,Â `openai\\u002fclip-vit-large-patch14`. T...\"],[\"def encode_text(self, text):\\n        tokenized_text = self.tokenize_text(text)\\n        text_features...\"],[\"```\\n\\nLet's putÂ `DirectionalSimilarity`Â to use now.\\n\\n```python\\ndir_similarity = DirectionalSimilarity...\"],[\"```\\n\\nLike the CLIP Score, the higher the CLIP directional similarity, the better it is.\\n\\nIt should b...\"],[\"### Class-conditioned image generation\\n\\nClass-conditioned generative models are usually pre-trained ...\"],[\"def download(url, local_filepath):\\n    r = requests.get(url)\\n    with open(local_filepath, \\\"wb\\\") as ...\"],[\"```\\n\\n```python\\nfrom PIL import Image\\nimport os\\n\\ndataset_path = \\\"sample-imagenet-images\\\"\\nimage_paths ...\"],[\"```\\n\\nWe now load theÂ [`DiTPipeline`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fpipelines\\u002fdit) to gen...\"],[\"```\\n\\nThe lower the FID, the better it is. Several things can influence FID here:\\n\\n- Number of images...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*This paper proposes a unified diffusion framework (dubbed UniDiffu...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis pipeline was contributed by [dg845](https:\\u002f\\u002fgithub.com\\u002fdg845). â¤ï¸\\n\\n## Usage Examples\\n\\nB...\"],[\"```\\n\\nThis is also called \\\"joint\\\" generation in the UniDiffuser paper, since we are sampling from the...\"],[\"```\\n\\n### Text-to-Image Generation\\n\\nUniDiffuser is also capable of sampling from conditional distribu...\"],[\"```\\n\\nThe `text2img` mode requires that either an input `prompt` or `prompt_embeds` be supplied. You ...\"],[\"```\\n\\nThe `img2text` mode requires that an input `image` be supplied. You can set the `img2text` mode...\"],[\"```\\n\\n### Text Variation\\n\\nSimilarly, text variation can be performed on an input prompt with a text-t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo setup a default ğŸ¤— Accelerate environment without choosing any configurations:\\n\\n```bash\\naccel...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```py\\nin_channels = 8\\nout_channels = unet.conv_in.out_channels\\nunet.register_to_config(in_channels=i...\"],[\"```\\n\\nThese UNet parameters are [updated](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da010...\"],[\"```\\n\\nNext, the edited images and and edit instructions are [preprocessed](https:\\u002f\\u002fgithub.com\\u002fhugging...\"],[\"```\\n\\nFinally, in the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da0108205...\"],[\"```\\n\\nThen, the script applies dropout to the original image and edit instruction embeddings to suppo...\"],[\"```\\n\\nThat's pretty much it! Aside from the differences described here, the rest of the script is ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIf youâ€™re training on more than one GPU, add the `--multi_gpu` parameter to the `accelerate ...\"],[\"```\\n\\nAfter training is finished, you can use your new InstructPix2Pix for inference:\\n\\n```py\\nimport P...\"],[\"```\\n\\nYou should experiment with different `num_inference_steps`, `image_guidance_scale`, and `guidan...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can also try experimenting with the `num_inference_steps` parameter, which controls the num...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model che...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [Xiang-cd\\u002fDiffEdit-stable-diffusion](https:\\u002f\\u002fgithub.com\\u002fXiang-...\"],[\"* The pipeline can generate masks that can be fed into other inpainting pipelines.\\n* In order to gen...\"],[\"* Swap the `prompt` and `negative_prompt` in the arguments to call the pipeline to generate the fina...\"],[\"## StableDiffusionDiffEditPipeline\\n[[autodoc]] StableDiffusionDiffEditPipeline\\n    - all\\n    - gener...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"with distributed_state.split_between_processes([\\\"a dog\\\", \\\"a cat\\\"]) as prompt:\\n    result = pipeline(...\"],[\"```\\n\\nUse the `--num_processes` argument to specify the number of GPUs to use, and call `accelerate l...\"],[\"```\\n\\nYou'll want to create a function to run inference; [`init_process_group`](https:\\u002f\\u002fpytorch.org\\u002fd...\"],[\"T2I-Adapter training example for Stable Diffusion XL (SDXL)\\n\\nThe `train_t2i_adapter_sdxl.py` script ...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom ...\"],[\"```\\n\\nThen run `huggingface-cli login` to log into your Hugging Face account. This is needed to be ab...\"],[\"```\\n\\nTo better track our training experiments, we're using the following flags in the command above:...\"],[\"```\\n\\n## Notes\\n\\n### Specifying a better VAE\\n\\nSDXL's VAE is known to suffer from numerical instability...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Scaled dot product attention\\n\\n[`torch.nn.functional.scaled_dot_product_attention`](https:\\u002f\\u002fp...\"],[\"```\\n\\nSDPA should be as fast and memory efficient as `xFormers`; check the [benchmark](#benchmark) fo...\"],[\"```\\n\\nDepending on GPU type, `torch.compile` can provide an *additional speed-up* of **5-300x** on to...\"],[\"prompt = \\\"ghibli style, a fantasy landscape with castles\\\"\\n\\nfor _ in range(3):\\n    images = pipe(prom...\"],[\"```\\n\\n### Stable Diffusion image-to-image\\n\\n```python\\nfrom diffusers import StableDiffusionImg2ImgPipe...\"],[\"```\\n\\n### Stable Diffusion inpainting\\n\\n```python\\nfrom diffusers import StableDiffusionInpaintPipeline...\"],[\"```\\n\\n### ControlNet\\n\\n```python\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetMo...\"],[\"```\\n\\n### DeepFloyd IF text-to-image + upscaling\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\ni...\"],[\"for _ in range(3):\\n    image_1 = pipe_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prom...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\nThe graph below highlights the relative speed-ups for the [`StableDiffusionPipeline`...\"],[\"In the following tables, we report our findings in terms of the *number of iterations\\u002fsecond*.\\n\\n### ...\"],[\"### A100 (batch size: 4)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno...\"],[\"### V100 (batch size: 1)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno...\"],[\"### V100 (batch size: 16)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003en...\"],[\"### T4 (batch size: 4)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno c...\"],[\"### RTX 3090 (batch size: 1)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cb...\"],[\"### RTX 3090 (batch size: 16)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003c...\"],[\"### RTX 4090 (batch size: 4)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cb...\"],[\"## Notes\\n\\n* Follow this [PR](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fpull\\u002f3313) for more details on...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n**ğŸ“‹ Copy-paste the English version with a new language code**\\n\\nThe documentation files are in o...\"],[\"```\\n\\nHere, `\\u003cLANG-ID\\u003e` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](http...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## StableDiffusionPipelineOutput\\n\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom diffusers import DiffusionPipeline\\n\\nrepo_id = \\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\npipe =...\"],[\"```\\n\\nYou can also load a checkpoint with its specific pipeline class. The example above loaded a Sta...\"],[\"```\\n\\nThen pass the local path to [`~DiffusionPipeline.from_pretrained`]:\\n\\n```python\\nfrom diffusers i...\"],[\"```\\n\\nLet's use the [`SchedulerMixin.from_pretrained`] method to replace the default [`PNDMScheduler`...\"],[\"```\\n\\n### Safety checker\\n\\nDiffusion models like Stable Diffusion can generate harmful content, which ...\"],[\"```\\n\\nThen you can pass the `components` to another pipeline without reloading the weights into RAM:\\n...\"],[\"```\\n\\n## Checkpoint variants\\n\\nA checkpoint variant is usually a checkpoint whose weights are:\\n\\n- Stor...\"],[\"There are two important arguments to know for loading variants:\\n\\n- `torch_dtype` defines the floatin...\"],[\"```\\n\\nTo save a checkpoint stored in a different floating-point type or as a non-EMA variant, use the...\"],[\"```\\n\\nHowever, this behavior is now deprecated since the \\\"revision\\\" argument should (just as it's don...\"],[\"```\\n\\nYou can also load and save model variants by specifying the `variant` argument in [`ModelMixin....\"],[\"```\\n\\n## Schedulers\\n\\nSchedulers are loaded from the [`SchedulerMixin.from_pretrained`] method, and un...\"],[\"# replace `dpm` with any of `ddpm`, `ddim`, `pndm`, `lms`, `euler_anc`, `euler`\\npipeline = StableDif...\"],[\"```\\n\\n## DiffusionPipeline explained\\n\\nAs a class method, [`DiffusionPipeline.from_pretrained`] is res...\"],[\"```\\n\\nYou'll see pipeline is an instance of [`StableDiffusionPipeline`], which consists of seven comp...\"],[\"```\\n.\\nâ”œâ”€â”€ feature_extractor\\nâ”‚Â Â  â””â”€â”€ preprocessor_config.json\\nâ”œâ”€â”€ model_index.json\\nâ”œâ”€â”€ safety_checker...\"],[\"```\\n\\nYou can access each of the components of the pipeline as an attribute to view its configuration...\"],[\"```\\n\\nEvery pipeline expects a [`model_index.json`](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is gene...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## DiTPipeline\\n[[autodoc]] DiTPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ImagePipelineOutput\\n[[autodoc]...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n## Load model checkpoints\\n\\nModel weights may be stored in separate subfolders on the Hu...\"],[\"```\\n\\nYou can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model che...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"refiner = DiffusionPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion-xl-refiner-1.0\\\",\\n    ...\"],[\"```\\n\\nTo use this approach, you need to define the number of timesteps for each model to run through ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"img_url = \\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fCompVis\\u002flatent-diffusion\\u002fmain\\u002fdata\\u002finpainting_examples\\u002f...\"],[\"```\\n\\nThis ensemble of expert denoisers method works well for all available schedulers!\\n\\n### Base to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Size conditioning\\n\\nThere are two types of size conditioning:\\n\\n- [`original_size`](https:...\"],[\"prompt = \\\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\\\"\\nimage = pipe(\\n    p...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-col justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffuser...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Use a different prompt for each text-encoder\\n\\nSDXL uses two text-encoders, so it is possible...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Other resources\\n\\nIf you're interested in experimenting with a minimal version of the [`UNet2...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Use the `safety_concept` property of [`StableDiffusionPipelineSafe`] to check and edit the current s...\"],[\"```\\nFor each image generation the active concept is also contained in [`StableDiffusionSafePipelineO...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nGreat, now you can import the rest of the dependencies you'll need:\\n\\n```python\\nimport jax.numpy...\"],[\"```\\n\\nModel parameters and inputs have to be replicated across the 8 parallel devices. The parameters...\"],[\"```\\n\\nTo take advantage of JAX's optimized speed on a TPU, pass `jit=True` to the pipeline to compile...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve\\u002fmain\\u002fstable_diffusion_jax...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve\\u002fmain\\u002fstable_diffusion_jax...\"],[\"```\\n\\nAfter calling `pmap`, the prepared function `p_generate` will:\\n\\n1. Make a copy of the underlyin...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"All community leaders are obligated to respect the privacy and security of the\\nreporter of any incid...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## UNet3DConditionModel\\n[[autodoc]] UNet3DConditionModel\\n\\n## UNet3DConditionOutput\\n[[autodoc]] model...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```py\\nconditioning_image_transforms = transforms.Compose(\\n    [\\n        transforms.Resize(args.resol...\"],[\"```\\n\\nWithin the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7bc81c0807...\"],[\"```\\n\\nLastly, in the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo monitor training progress with Weights & Biases, add the `--report_to=wandb` paramete...\"],[\"```\\n\\nOnce training is complete, you can use your T2I-Adapter for inference:\\n\\n```py\\nfrom diffusers im...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion XL for JAX + TPUv5e\\n\\n[TPU v5e](https:\\u002f\\u002fcloud.google.com\\u002fblog\\u002fproducts\\u002fcompute\\u002fhow-c...\"],[\"ğŸ‘‰ Try it out for yourself:\\n\\n[![Hugging Face Spaces](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002f%F0%9F%A4%97%20Hugg...\"],[\"```\\npip install jax[tpu] -f https:\\u002f\\u002fstorage.googleapis.com\\u002fjax-releases\\u002flibtpu_releases.html\\n```\\n\\nNe...\"],[\"```\\nHere, a pre-trained model `stable-diffusion-xl-base-1.0` from the namespace `stabilityai` is loa...\"],[\"```\\nTo utilize JAX's parallel capabilities, the parameters and input tensors are duplicated across d...\"],[\"```\\nNow that the function is compiled, this section shows how to use it for fast inference. It measu...\"],[\"return pmap(\\n        pipeline._generate,static_broadcasted_argnums=[3, 4, 5, 9]\\n        ).lower(\\n   ...\"],[\"```\\n\\nNext we can compile the generate function by executing `aot_compile`.\\n\\n```python\\nstart = time.t...\"],[\"```\\n\\nFrom this point forward, any calls to generate should result in a faster inference\\ntime and it ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide shows how to perform inference with LCMs for \\n- text-to-image\\n- image-to-image\\n- combined...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# prepare image\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\ngenerator = torch....\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"unet = UNet2DConditionModel.from_pretrained(\\n    \\\"latent-consistency\\u002flcm-sdxl\\\",\\n    torch_dtype=torc...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Diffusion probabilistic models (DPMs) have demonstrated a very pro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"model_id = \\\"google\\u002fddpm-cifar10-32\\\"\\n\\n# load model and scheduler\\nddim = DDIMPipeline.from_pretrained(...\"],[\"```\\n\\nRunning the code above prints one value, but if you run it again you get a different value. Wha...\"],[\"```\\n\\nNow when you run the code above, it always prints a value of `1491.1711` no matter what because...\"],[\"```\\n\\nThe result is not the same even though you're using an identical seed because the GPU uses a di...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ If reproducibility is important, we recommend always passing a CPU generator.\\nThe perf...\"],[\"```\\n\\nNow when you run the same pipeline twice, you'll get identical results.\\n\\n```py\\nimport torch\\nfro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Guided image synthesis enables everyday users to create and edit p...\"],[\"## FlaxStableDiffusionImg2ImgPipeline\\n\\n[[autodoc]] FlaxStableDiffusionImg2ImgPipeline\\n\\t- all\\n\\t- __ca...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNext, we move it to GPU:\\n\\n```python\\npipeline.to(\\\"cuda\\\")\\n```\\n\\n## Access the scheduler\\n\\nThe sched...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"```\\n\\nCool, lots of schedulers to look at. Feel free to have a look at their respective class definit...\"],[\"```\\n\\nreturns a dictionary of the configuration of the scheduler:\\n\\n**Output**:\\n```py\\nFrozenDict([('nu...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"prng_seed = jax.random.PRNGKey(0)\\nnum_inference_steps = 25\\n\\n# shard inputs and rng\\nparams = replicat...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe following Flax schedulers are _not yet compatible_ with the Flax Stab...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"```\\n\\n### Cat toy example\\n\\nFirst, let's login so that we can upload the checkpoint to the Hub during ...\"],[\"```\\n\\nA full training run takes ~1 hour on one V100 GPU.\\n\\n**Note**: As described in [the official pap...\"],[\"```\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax\\\"\\nexport DATA_DIR=\\\"path-to-dir-con...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\n```py\\n# uncomme...\"],[\"```\\n\\n- [ğŸ¤— Accelerate](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002findex) speeds up model loading for infe...\"],[\"| **Task**                     | **Description**                                                    ...\"],[\"Start by creating an instance of a [`DiffusionPipeline`] and specify which pipeline checkpoint you w...\"],[\"```\\n\\nThe [`DiffusionPipeline`] downloads and caches all modeling, tokenization, and scheduling compo...\"],[\"```\\n\\n### Local pipeline\\n\\nYou can also use the pipeline locally. The only difference is you need to d...\"],[\"```\\n\\nTry generating an image with the new scheduler and see if you notice a difference!\\n\\nIn the next...\"],[\"```\\n\\nTo access the model parameters, call `model.config`:\\n\\n```py\\n\\u003e\\u003e\\u003e model.config\\n```\\n\\nThe model con...\"],[\"```\\n\\nTo generate actual examples though, you'll need a scheduler to guide the denoising process. In ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ Unlike a model, a scheduler does not have trainable weights and is parameter-free!\\n\\n\\u003c\\u002f...\"],[\"```\\n\\nTo speed up the denoising process, move the input and model to a GPU:\\n\\n```py\\n\\u003e\\u003e\\u003e model.to(\\\"cuda...\"],[\"```\\n\\nSit back and watch as a cat is generated from nothing but noise! ğŸ˜»\\n\\n\\u003cdiv class=\\\"flex justify-ce...\"],[\"ğŸ§¨ Diffusers Pipelines\\n\\nPipelines provide a simple way to run state-of-the-art diffusion models in in...\"],[\"To that end, we strive to offer all open-sourced, state-of-the-art diffusion system under a unified ...\"],[\"| Pipeline                                                                                          ...\"],[\"| [ddim](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fddim)           ...\"],[\"| [score_sde_ve](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fscore_sd...\"],[\"| [stable_diffusion](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fstab...\"],[\"**Note**: Pipelines are simple examples of how to play around with the diffusion systems as describe...\"],[\"- [`from_pretrained` method](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f5cbed8e0d157f65d3ddc2420d...\"],[\"In addition, a `model_index.json` file is created at the root of the local path, *e.g.* `.\\u002fstable_di...\"],[\"**Note**: All pipelines have PyTorch's autograd disabled by decorating the `__call__` method with a ...\"],[\"- **Self-contained**: A pipeline shall be as self-contained as possible. More specifically, this mea...\"],[\"- **One-purpose-only**: Pipelines should be used for one task and one task only. Even if two tasks a...\"],[\"## Examples\\n\\n### Text-to-Image generation with Stable Diffusion\\n\\n```python\\n# make sure you're logged...\"],[\"```\\n\\n### Image-to-Image text-guided generation with Stable Diffusion\\n\\nThe `StableDiffusionImg2ImgPip...\"],[\"```\\nYou can also run this example on colab [![Open In Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fasset...\"],[\"init_image = download_image(img_url).resize((512, 512))\\nmask_image = download_image(mask_url).resize...\"],[\"```\\n\\nYou can also run this example on colab [![Open In Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fasse...\"],[\"Distillation for quantization on Textual Inversion models to personalize text2image\\n\\n[Textual invers...\"],[\"```\\n\\n## Prepare Datasets\\n\\nOne picture which is from the huggingface datasets [sd-concepts-library\\u002fdi...\"],[\"```\\n\\n## Do distillation for quantization\\n\\nDistillation for quantization is a method that combines [i...\"],[\"```\\n\\nAfter the distillation for quantization process, the quantized UNet would be 4 times smaller (3...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe table below lists all the pipelines currently available in ğŸ¤— Diffusers and the tasks the...\"],[\"| Pipeline | Tasks |\\n|---|---|\\n| [AltDiffusion](alt_diffusion) | image2image |\\n| [AnimateDiff](anima...\"],[\"| [Kandinsky 2.2](kandinsky_v22) | text2image, image2image, inpainting |\\n| [Kandinsky 3](kandinsky3)...\"],[\"| [Stable Diffusion XL Turbo](stable_diffusion\\u002fsdxl_turbo) | text2image, image2image, inpainting |\\n|...\"],[\"## DiffusionPipeline\\n\\n[[autodoc]] DiffusionPipeline\\n\\t- all\\n\\t- __call__\\n\\t- device\\n\\t- to\\n\\t- components...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2...\"],[\"```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramati...\"],[\"```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsaved\\u002fmodel\\\"\\n\\n...\"],[\"```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\\n\\nurl = \\\"https:\\u002f\\u002fhuggi...\"],[\"```\\n\\n## ControlNetModel\\n\\n[[autodoc]] ControlNetModel\\n\\n## ControlNetOutput\\n\\n[[autodoc]] models.contro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[Kandinsky 3](..\\u002fapi\\u002fpipelines\\u002fkandinsky3) simplifies the architecture and shifts away from the two-...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nKandinsky 2.1 and 2.2 usage is very similar! The only difference is Kandi...\"],[\"```\\n\\nNow pass all the prompts and embeddings to the [`KandinskyPipeline`] to generate an image:\\n\\n```...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nğŸ¤— Diffusers also provides an end-to-end API with the [`KandinskyCombi...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Imag...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom diffusers import KandinskyV2...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fraw.githubuserconten...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nğŸ¤— Diffusers also provides an end-to-end API with the [`KandinskyImg2I...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers import AutoPipelineForImage2Ima...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\nFor inpainting, you'll need the original image, a mask of the area to replace in the or...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nLoad an initial image and create a mask:\\n\\n```py\\ninit_image = load_ima...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nimport torch\\nimport numpy as np\\nfrom PIL impor...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Interpolation\\n\\nInterpolation allows you to explore the latent spac...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers import KandinskyV22PriorPipelin...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"htt...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\nGenerate the image embeddings from a prompt and negative prompt:\\n\\n```py\\nprompt = \\\"A robot, 4k p...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\nLoad the prior pipeline and the [`KandinskyV22ControlnetImg2ImgPipeline`]:\\n\\n```py\\nprior_pipelin...\"],[\"```\\n\\nNow you can run the [`KandinskyV22ControlnetImg2ImgPipeline`] to generate an image from the ini...\"],[\"```\\n\\nThis is the same as explicitly setting the attention processor to use [`~models.attention_proce...\"],[\"# Diffusers examples with ONNXRuntime optimizations\\n\\n**This research project is not actively maintai...\"],[\"Multi Subject DreamBooth training\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.12242) is a method to per...\"],[\"```\\n\\n### Multi Subject Training Example\\nIn order to have your model learn multiple concepts at once,...\"],[\"accelerate launch train_multi_subject_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME ...\"],[\"```\\n\\nThis example shows training for 2 subjects, but please note that the model can be trained on an...\"],[\"An example of how to generate the file:\\n```python\\nimport json\\n\\n# here we are using parameters for pr...\"],[\"```\\nAnd then just point to the file when executing the script:\\n\\n```bash\\n# exports...\\naccelerate laun...\"],[\"```\\n\\n### Inference from a training checkpoint\\n\\nYou can also perform inference from one of the checkp...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport INSTANCE_DIR=\\\"path-to-instance-imag...\"],[\"```\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and ...\"],[\"```\\n\\n\\n### Training on a 16GB GPU:\\n\\nWith the help of gradient checkpointing and the 8-bit optimizer f...\"],[\"```\\n\\n### Training on a 8 GB GPU:\\n\\nBy using [DeepSpeed](https:\\u002f\\u002fwww.deepspeed.ai\\u002f) it's possible to o...\"],[\"accelerate launch --mixed_precision=\\\"fp16\\\" train_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$...\"],[\"```\\n\\n### Fine-tune text encoder with the UNet.\\n\\nThe script also allows to fine-tune the `text_encode...\"],[\"```\\n\\n### Using DreamBooth for other pipelines than Stable Diffusion\\n\\nAltdiffusion also support dream...\"],[\"Inference Examples\\n\\n**The inference examples folder is deprecated and will be removed in a future ve...\"],[\"ControlNet training example for Stable Diffusion XL (SDXL)\\n\\nThe `train_controlnet_sdxl.py` script sh...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom ...\"],[\"```\\n\\nThen run `huggingface-cli login` to log into your Hugging Face account. This is needed to be ab...\"],[\"```\\n\\nTo better track our training experiments, we're using the following flags in the command above:...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"# prepare image\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"prompt = \\\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\\\"\\n\\n# pass prompt and ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\\nimport torch\\nfrom ...\"],[\"```\\n\\nNow you can pass this generated image to the image-to-image pipeline:\\n\\n```py\\npipeline = AutoPip...\"],[\"```\\n\\n### Image-to-image-to-image\\n\\nYou can also chain multiple image-to-image pipelines together to c...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is important to specify `output_type=\\\"latent\\\"` in the pipeline to keep all the output...\"],[\"```\\n\\n### Image-to-upscaler-to-super-resolution\\n\\nAnother way you can chain your image-to-image pipeli...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is important to specify `output_type=\\\"latent\\\"` in the pipeline to keep all the output...\"],[\"```\\n\\n## Control image generation\\n\\nTrying to generate an image that looks exactly the way you want ca...\"],[\"```\\n\\n### ControlNet\\n\\nControlNets provide a more flexible and accurate way to control image generatio...\"],[\"```\\n\\nNow generate a new image conditioned on the depth map, initial image, and prompt:\\n\\n```py\\nprompt...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"prompt = \\\"elden ring style astronaut in a jungle\\\" # include the token \\\"elden ring style\\\" in the prom...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nGenerating multiple prompts in a batch seems to take too much memory. Whi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptimum\\u002fdocumen...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion XL text-to-image fine-tuning\\n\\nThe `train_text_to_image_sdxl.py` script shows how to...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom ...\"],[\"```\\n\\n**Notes**:\\n\\n*  The `train_text_to_image_sdxl.py` script pre-computes text embeddings and the VA...\"],[\"```\\n\\n### Inference in Pytorch XLA\\n```python\\nfrom diffusers import DiffusionPipeline\\nimport torch\\nimp...\"],[\"```\\n\\nNote: There is a warmup step in PyTorch XLA. This takes longer because of\\ncompilation and optim...\"],[\"With LoRA, it's possible to fine-tune Stable Diffusion on a custom image-caption pair dataset\\non con...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so\\nwe need t...\"],[\"```\\n\\nThe above command will also run inference as fine-tuning progresses and log the results to Weig...\"],[\"```\\n\\n### Inference\\n\\nOnce you have trained a model using above command, the inference can be done sim...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nGenerating multiple prompts in a batch can [crash](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"```\\n\\n## Troubleshoot\\n\\nM1\\u002fM2 performance is very sensitive to memory pressure. When this occurs, the ...\"],[\"\\u003c!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ve...\"],[\"```\\n\\nNavigate to the example folder with the training script and install the required dependencies f...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nSome other basic and important parameters to specify include:\\n\\n- `--pretrained_model_name_or_pa...\"],[\"Next, you'll find the dataset preprocessing code and training loop in the [`main()`](https:\\u002f\\u002fgithub....\"],[\"# Load scheduler and models\\nnoise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_na...\"],[\"```\\n\\nThe special [placeholder token](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fb81c69e489aad3a0b...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fb81c69e489aad3a0ba73...\"],[\"```\\n\\nSet the environment variable `MODEL_NAME` to a model id on the Hub or a path to a local model, ...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\nFlax doesn't support the [`~loaders.TextualInversionLoaderMix...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training your own Textual Inversion...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The pipeline was contributed by [dg845](https:\\u002f\\u002fgithub.com\\u002fdg845) and [ayushtues](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\n\\n## ConsistencyModelPipeline\\n[[autodoc]] ConsistencyModelPipeline\\n    - all\\n    - __call__\\n\\n## ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [AutoPipeline](..\\u002f..\\u002ftutorials\\u002fautopipeline) tutorial to learn how to use ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo use with Stable Diffusion XL 1.0\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```py\\ndef transform(example):\\n    image = example[\\\"image\\\"]\\n    image = TF.resize(image, resolution, ...\"],[\"```\\n\\nFor improved performance on reading and writing large datasets stored in the cloud, this script...\"],[\"```\\n\\nNow you can create the [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f3b37488fa3280a...\"],[\"```\\n\\nNext, you're ready to setup the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f3...\"],[\"```\\n\\nIt gets the [teacher model predictions](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f3b37488fa...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsaved\\u002fmodel\\\"\\n\\na...\"],[\"```\\n\\nOnce training is complete, you can use your new LCM for inference.\\n\\n```py\\nfrom diffusers import...\"],[\"```\\n\\n## LoRA\\n\\nLoRA is a training technique for significantly reducing the number of trainable parame...\"],[\"## Next steps\\n\\nCongratulations on distilling a LCM model! To learn more about LCM, the following may...\"],[\"Custom Diffusion training example \\n\\n[Custom Diffusion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2212.04488) is a method...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n**___Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https:\\u002f\\u002fhug...\"],[\"```\\n\\n**Use `--enable_xformers_memory_efficient_attention` for faster training with lower VRAM requir...\"],[\"```\\n\\nHere is an example [Weights and Biases page](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002fcustom-diffusion\\u002fruns\\u002f2...\"],[\"```\\n\\nHere is an example [Weights and Biases page](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002fcustom-diffusion\\u002fruns\\u002f3...\"],[\"```\\n\\n## Inference\\n\\nOnce you have trained a model using the above command, you can run inference usin...\"],[\"```\\n\\nHere is an example of performing inference with multiple concepts:\\n\\n```python\\nimport torch\\nfrom...\"],[\"```\\n\\nHere, `cat` and `wooden pot` refer to the multiple concepts.\\n\\n### Inference from a training che...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## UNetMotionModel\\n[[autodoc]] UNetMotionModel\\n\\n## UNet3DConditionOutput\\n[[autodoc]] models.unet_3d_...\"],[\"!---\\nCopyright 2023- The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, ...\"],[\"```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview diffusers docs\\u002fsource\\u002fen\\n```\\n\\nThe docs will be viewab...\"],[\"```\\nand of course, if you moved it to another file, then:\\n\\n```md\\nSections that were moved:\\n\\n[ \\u003ca hre...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"```\\n[[autodoc]] XXXPipeline\\n    - all\\n\\t- __call__\\n```\\n\\nThis will include every public method of the ...\"],[\"```\\n\\nYou can follow the same process to create a new scheduler under the `docs\\u002fsource\\u002f\\u003clanguageCode\\u003e...\"],[\"```\\n\\nIf the description is too long to fit in one line, another indentation is necessary before writ...\"],[\"```\\n    Returns:\\n        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special tok...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\nThe paper [Common Diffusion Noise Schedules and Sample Steps are Flawed](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\n2. train a model with `v_prediction` (add the following argument to the [train_text_to_image.py...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nSome basic and important parameters include:\\n\\n- `--pretrained_model_name_or_path`: the name of ...\"],[\"```\\n\\nYou can compare the loss surfaces for different `snr_gamma` values in this [Weights and Biases]...\"],[\"```\\n\\nThen the script [loads the UNet](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f8959c5b9dec1c94d...\"],[\"```\\n\\nLastly, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f8959c5b9dec1c94d6ba48...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport dataset_name=\\\"lambdalabs\\u002fp...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\nTraining with Flax can be faster on TPUs and GPUs thanks to [...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```py\\nimport jax\\nimport numpy as np\\nfrom flax.jax_utils impor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*StableDiffusion is a revolutionary text-to-image generator that is...\"],[\"## Example Usage\\n\\n```python\\nfrom diffusers import AsymmetricAutoencoderKL, StableDiffusionInpaintPip...\"],[\"```\\n\\n## AsymmetricAutoencoderKL\\n\\n[[autodoc]] models.autoencoders.autoencoder_asym_kl.AsymmetricAutoe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We introduce WÃ¼rstchen, a novel architecture for text-to-image syn...\"],[\"## WÃ¼rstchen Overview\\nWÃ¼rstchen is a diffusion model, whose text-conditional model works in a highly...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fgithub.com\\u002fdome272\\u002fWuerstchen\\u002fassets\\u002f61938694\\u002f2914830f-cbd3-461c-be64-d50734f4b49d...\"],[\"```\\n\\nFor explanation purposes, we can also initialize the two main pipelines of WÃ¼rstchen individual...\"],[\"caption = \\\"Anthropomorphic cat dressed as a fire fighter\\\"\\nnegative_prompt = \\\"\\\"\\n\\nprior_output = prior...\"],[\"```\\n\\n## Speed-Up Inference\\nYou can make use of `torch.compile` function and gain a speed-up of about...\"],[\"```\\n\\n## Limitations\\n\\n- Due to the high compression employed by WÃ¼rstchen, generations can lack a goo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*By decomposing the image formation process into a sequential appli...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## StableDiffusionPipeline\\n\\n[[autodoc]] StableDiffusionPipeline\\n\\t- all\\n\\t- __call__\\n\\t- enable...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Simple over easy\\n\\nAs PyTorch states, **explicit is better than implicit** and **simple is better ...\"],[\"In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffu...\"],[\"The following design principles are followed:\\n- Models correspond to **a type of model architecture*...\"],[\"- Models all inherit from `ModelMixin` and `ConfigMixin`.\\n- Models can be optimized for performance ...\"],[\"### Schedulers\\n\\nSchedulers are responsible to guide the denoising process for inference as well as t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Install from source\\n\\nBefore installing ğŸ¤— Diffusers from source, make sure you have PyTorch a...\"],[\"```\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n```bash\\npip install -e \\\".[torch]\\\"\\n```\\n\\u003c\\u002fpt\\u003e\\n\\u003cjax\\u003e\\n```bash\\npip install -...\"],[\"```\\n\\nFor more details about managing and cleaning the cache, take a look at the [caching](https:\\u002f\\u002fhu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| A1111\\u002fk-diffusion    | ğŸ¤— Diffusers                         | Usage                                ...\"],[\"| DPM++ SDE           | [`DPMSolverSinglestepScheduler`]    |                                       ...\"],[\"All schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*The past few years have witnessed the great success of Diffusion m...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"```\\n\\nAnd initialize an [ğŸ¤—Accelerate](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\u002f) environment with:\\n\\n...\"],[\"```\\n\\nThis will be our training data.\\nNow we can launch the training using\\n\\n## Use ONNXRuntime to acc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n# DDPMPipeline\\n[[autodoc]] DDPMPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ImagePipelineOutput\\n[[autodoc...\"],[\"Kandinsky2.2 text-to-image fine-tuning\\n\\nKandinsky 2.2 includes a prior pipeline that generates image...\"],[\"```\\n\\nAnd initialize an [ğŸ¤—Accelerate](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\u002f) environment with:\\n\\n...\"],[\"```\\n\\nTo disable wandb logging, remove the `--report_to==\\\"wandb\\\"` and `--validation_prompts=\\\"A robot ...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo train on your own training files, prepare the dataset accor...\"],[\"```\\n\\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\\n```py...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo perform inference with the fine-tuned prior model, you will...\"],[\"```\\n\\nIf you want to use a fine-tuned decoder checkpoint along with your fine-tuned prior checkpoint,...\"],[\"```\\n\\n\\n#### Training with Min-SNR weighting\\n\\nWe support training with the Min-SNR weighting strategy ...\"],[\"With LoRA, it's possible to fine-tune Kandinsky 2.2 on a custom image-caption pair dataset\\non consum...\"],[\"```\\n\\n#### Train prior\\n\\n```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip-captions\\\"\\n\\naccelerate la...\"],[\"```\\n\\n**___Note: When using LoRA we can use a much higher learning rate compared to non-LoRA fine-tun...\"],[\"```\\n\\n### Training with xFormers:\\n\\nYou can enable memory efficient attention by [installing xFormers]...\"],[\"e don't yet support training T2I-Adapters on Stable Diffusion yet. For training T2I-Adapters on Stab...\"],[\"Research projects\\n\\nThis folder contains various research projects using ğŸ§¨ Diffusers.\\nThey are not re...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's the overview from the [project page](https:\\u002f\\u002fvislearn.github.io\\u002fControlNet-XS\\u002f):\\n\\n*With incre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Use TensorFloat-32\\n\\nOn Ampere and later CUDA devices, matrix multiplications and convolutions can...\"],[\"```\\n\\nYou can learn more about TF32 in the [Mixed precision training](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftra...\"],[\"[DreamBooth](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002ftree\\u002fmain\\u002fexamples\\u002fdreambooth) by [colossalai]...\"],[\"```\\n\\n## Dataset for Teyvat BLIP captions\\nDataset used to train [Teyvat characters text to image mode...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport INSTANCE_DIR=\\\"path-to-instance-imag...\"],[\"```\\n\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and...\"],[\"```\\n\\n## Inference\\n\\nOnce you have trained a model using above command, the inference can be done simp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe partially inverted latents are generated from the [`~StableDiffusionDiffEditPipeline.invert...\"],[\"```\\n\\nUse the [`~StableDiffusionDiffEditPipeline.generate_mask`] function to generate the image mask....\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fgithub.com\\u002fXiang-cd\\u002fD...\"],[\"```\\n\\nNext, create a utility function to generate the prompts:\\n\\n```py\\n@torch.no_grad()\\ndef generate_p...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [generation strategy](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fgen...\"],[\"```\\n\\nFinally, pass the embeddings to the [`~StableDiffusionDiffEditPipeline.generate_mask`] and [`~S...\"],[\"```\\n\\n## Generate a caption for inversion\\n\\nWhile you can use the `source_prompt` as a caption to help...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cfigure\\u003e\\n        \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fgit...\"],[\"Community Examples\\n\\n\\u003e **For more information about community pipelines, please have a look at [this ...\"],[\"| Example                                                                                           ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"----------------------------------------------------------------------------------------------------...\"],[\"----------|--------------------------------------------------------------:|...\"],[\"| LLM-grounded Diffusion (LMD+)                                                                     ...\"],[\"| Stable Diffusion Interpolation                                                                    ...\"],[\"| Wild Card Stable Diffusion                                                                        ...\"],[\"| Text Based Inpainting Stable Diffusion                                                            ...\"],[\"| Stable UnCLIP                                                                                     ...\"],[\"| TensorRT Stable Diffusion Text to Image Pipeline                                                  ...\"],[\"| TensorRT Stable Diffusion Inpainting Pipeline                                                     ...\"],[\"sketch inpaint - Inpainting with non-inpaint Stable Diffusion | sketch inpaint much like in automati...\"],[\"| SDE Drag Pipeline                                                                                 ...\"],[\"To load a custom pipeline you just need to pass the `custom_pipeline` argument to `DiffusionPipeline...\"],[\"```\\n\\n## Example usages\\n\\n### LLM-grounded Diffusion\\n\\nLMD and LMD+ greatly improves the prompt underst...\"],[\"# Generate directly from a text prompt and an LLM response\\nprompt = \\\"a waterfall and a modern high s...\"],[\"```\\n\\n#### Use this pipeline on its own for layout generation\\n```python\\nimport torch\\nfrom diffusers i...\"],[\"```\\n\\n### CLIP Guided Stable Diffusion\\n\\nCLIP guided stable diffusion can help to generate more realis...\"],[\"```\\n\\nThe `images` list contains a list of PIL images that can be saved locally or displayed directly...\"],[\"```\\n\\nThe output of the `walk(...)` function returns a list of images saved under the folder as defin...\"],[\"images = pipe.img2img(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\\n\\n##...\"],[\"```\\n\\nAs shown above this one pipeline can run all both \\\"text-to-image\\\", \\\"image-to-image\\\", and \\\"inpai...\"],[\"torch_dtype=torch.float16\\n)\\npipe=pipe.to(\\\"cuda\\\")\\n\\nprompt = \\\"best_quality (1girl:1.3) bow bride brown...\"],[\"```\\n\\n#### onnxruntime\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\nimport torch\\n\\npipe = Diffus...\"],[\"```\\n\\nif you see `Token indices sequence length is longer than the specified maximum sequence length ...\"],[\"```\\nThis example produces the following image:\\n\\n![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f4...\"],[\"```\\ndog\\ncat\\nmouse\\n```\\n\\ncreate `object.txt`, with contents like:\\n\\n```\\nchair\\nsofa\\nbench\\n```\\n\\n```python...\"],[\"```\\n\\n### Composable Stable diffusion\\n\\n[Composable Stable Diffusion](https:\\u002f\\u002fenergy-based-model.githu...\"],[\"pipe = DiffusionPipeline.from_pretrained(\\n    args.model_path,\\n    custom_pipeline=\\\"composable_stabl...\"],[\"```\\n\\n### Imagic Stable Diffusion\\nAllows you to edit an image using stable diffusion....\"],[\"```python\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nimport torch\\nimport os\\nfrom d...\"],[\"res = pipe(alpha=2, guidance_scale=7.5, num_inference_steps=50)\\nimage = res.images[0]\\nimage.save('.\\u002f...\"],[\"```\\n\\n### Seed Resizing\\nTest seed resizing. Originally generate an image in 512 by 512, then generate...\"],[\"pipe_compare = DiffusionPipeline.from_pretrained(\\n    \\\"CompVis\\u002fstable-diffusion-v1-4\\\",\\n    custom_pi...\"],[\"```\\n\\n### Multilingual Stable Diffusion Pipeline\\n\\nThe following code can generate an images from text...\"],[\"diffuser_pipeline = DiffusionPipeline.from_pretrained(\\n    \\\"CompVis\\u002fstable-diffusion-v1-4\\\",\\n    cust...\"],[\"```\\n\\nThis example produces the following images:\\n![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f...\"],[\"```\\n\\n![2 by 2 grid demonstrating image to image inpainting.](https:\\u002f\\u002fuser-images.githubusercontent.c...\"],[\"```\\n\\n### Bit Diffusion\\nBased https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.04202, this is used for diffusion on discret...\"],[\"```\\n\\n![diffusers_euler](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimages\\u002fresolve\\u002fmain\\u002fk_diffu...\"],[\"```\\n\\n![diffusers_euler](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimages\\u002fresolve\\u002fmain\\u002fk_diffu...\"],[\"#Three checkpoint merging. Only \\\"add_difference\\\" method actually works on all three checkpoints. Usi...\"],[\"```\\nSome examples along with the merge details:\\n\\n1. \\\"CompVis\\u002fstable-diffusion-v1-4\\\" + \\\"hakurei\\u002fwaifu...\"],[\"```python\\nfrom diffusers import DiffusionPipeline\\nimport matplotlib.pyplot as plt\\n\\npipe = DiffusionP...\"],[\"```\\n\\nAs a result, you can look at a grid of all 4 generated images being shown together, that captur...\"],[\"```\\nThe `mix_img` is a PIL image that can be saved locally or displayed directly in a google colab. ...\"],[\"```python\\nimport torch\\nfrom diffusers import DiffusionPipeline\\n\\ndevice = torch.device(\\\"cpu\\\" if not t...\"],[\"# this pipeline only use prior module in \\\"kakaobrain\\u002fkarlo-v1-alpha\\\"\\n# It is used to convert clip te...\"],[\"```\\n\\n\\n`shiba-inu.jpg`\\n\\n\\n![shiba-inu](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f16448529\\u002f209185639-6e...\"],[\"```\\n\\nThe resulting images in order:-\\n\\n![result_0](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fNagaSaiAbhinay\\u002fUnC...\"],[\"pipe = DiffusionPipeline.from_pretrained(\\n    \\\"kakaobrain\\u002fkarlo-v1-alpha-image-variations\\\",\\n    torc...\"],[\"```\\nThe original images:-\\n\\n![starry](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fNagaSaiAbhinay\\u002fUnCLIPImageInter...\"],[\"### DDIM Noise Comparative Analysis Pipeline\\n#### **ResearchÂ question: What visual concepts do the d...\"],[\"for strength in np.linspace(0.1, 1, 25):\\n    denoised_image, latent_timestep = pipe(\\n        image_p...\"],[\"```\\n\\nHere is the result of this pipeline (which is DDIM) on CelebA-HQ dataset.\\n\\n![noise-comparative-...\"],[\"The following code requires roughly 12GB of GPU RAM.\\n\\n```python\\nfrom io import BytesIO\\nimport reques...\"],[\"```\\n\\nInit Image\\n\\n![img2img_init_clip_guidance](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fnjindal\\u002fimages\\u002fresolv...\"],[\"```\\n\\n### EDICT Image Editing Pipeline\\n\\nThis pipeline implements the text-guided image editing approa...\"],[\"# initialize pipeline\\npipeline = DiffusionPipeline.from_pretrained(\\n    pretrained_model_name_or_pat...\"],[\"```\\n\\nInit Image\\n\\n![img2img_init_edict_text_editing](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fJoqsan\\u002fimages\\u002fre...\"],[\"Disclaimer: The mask gets transferred into latent space, this may lead to unexpected changes on the ...\"],[\"```\\n\\n### TensorRT Image2Image Stable Diffusion Pipeline\\n\\nThe TensorRT Pipeline can be used to accele...\"],[\"```\\n\\n### Stable Diffusion Reference\\n\\nThis pipeline uses the Reference Control. Refer to the [sd-webu...\"],[\"```\\n\\nReference Image\\n\\n![reference_image](https:\\u002f\\u002fhf.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"```py\\nimport cv2\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nfrom diffusers import UniPCMu...\"],[\"```\\n\\nReference Image\\n\\n![reference_image](https:\\u002f\\u002fhf.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"```\\n**Note:** To install a specific version, run with the following command:\\n```\\npython -m pip insta...\"],[\"```\\n\\nThe following code compares the performance of the original stable diffusion pipeline with the ...\"],[\"##############     fp32 inference performance    ###############\\n\\n# 1. IPEX Pipeline initialization\\n...\"],[\"```\\n\\n### CLIP Guided Images Mixing With Stable Diffusion\\n\\n![clip_guided_images_mixing_examples](http...\"],[\"# inpaint\\ninput_mask = load_image(\\\"\\u002fpath\\u002fto\\u002flocal\\u002fmask.png\\\") # or URL to your input inpainting mask\\n...\"],[\"```\\n\\nIn the above code, the `prompt2` is appended to the `prompt`, which is more than 77 tokens. \\\"bi...\"],[\"# Pipeline creating\\nmixing_pipeline = DiffusionPipeline.from_pretrained(\\n    \\\"CompVis\\u002fstable-diffusi...\"],[\"```\\n\\n![image_mixing_result](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fTheDenk\\u002fimages_mixing\\u002fresolve\\u002fmain\\u002fborom...\"],[\"```\\n![mixture_tiling_results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkadirnar\\u002fdiffusers_readme_images\\u002fresol...\"],[\"mask_url = \\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fCompVis\\u002flatent-diffusion\\u002fmain\\u002fdata\\u002finpainting_examples...\"],[\"```\\n\\n### Stable Diffusion Mixture Canvas\\n\\nThis pipeline uses the Mixture. Refer to the [Mixture](htt...\"],[\"```\\n![Input_Image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkadirnar\\u002fdiffusers_readme_images\\u002fresolve\\u002fmain\\u002finp...\"],[\"```\\n\\nThe training loop is also straightforward:\\n\\n```python\\n\\n# Training loop\\nwhile True:\\n    x0 = sam...\"],[\"```\\n\\n### Zero1to3 pipeline\\n\\nThis pipeline is the implementation of the [Zero-1-to-3: Zero-shot One I...\"],[\"# load image\\n# H, W = (256, 256) # H, W = (512, 512)   # zero123 training is 256,256\\n\\n# for batch in...\"],[\"# better do preprocessing\\nfrom gradio_new import preprocess_image, create_carvekit_interface\\nimport ...\"],[\"```\\n\\n### Stable Diffusion XL Reference\\n\\nThis pipeline uses the Reference . Refer to the [stable_diff...\"],[\"```\\n\\nReference Image\\n\\n![reference_image](https:\\u002f\\u002fhf.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"```python\\nimport requests\\nimport torch\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\nfrom diffusers ...\"],[\"```\\n\\n*With enough feedbacks you can create very similar high quality images.*\\n\\nThe original codebase...\"],[\"pipeline = MaskedStableDiffusionImg2ImgPipeline.from_pretrained(\\\"frankjoshua\\u002ficbinpICantBelieveIts_v...\"],[\"```\\n\\noriginal image mech.png\\n\\n\\u003cimg src=https:\\u002f\\u002fgithub.com\\u002fnoskill\\u002fdiffusers\\u002fassets\\u002f733626\\u002f10ad972d-d...\"],[\"```\\n\\nAnd abbreviated examples for the other edits:\\n\\n`ReplaceEdit with local blend`\\n```python\\nprompts...\"],[\"```\\n\\nSide note: See [this GitHub gist](https:\\u002f\\u002fgist.github.com\\u002fUmerHA\\u002fb65bb5fb9626c9c73f3ade2869e361...\"],[\"The model can be used with `diffusers` as follows:\\n\\n - *1. Load the model from the community pipelin...\"],[\"```\\n\\n- 2. Run inference with as little as 4 steps:\\n\\n```py\\nprompt = \\\"Self-portrait oil painting, a be...\"],[\"```\\n\\n\\n\\n### Latent Consistency Interpolation Pipeline\\n\\nThis pipeline extends the Latent Consistency P...\"],[\"assert len(images) == (len(prompts) - 1) * num_interpolation_steps...\"],[\"```\\n\\n###  StableDiffusionUpscaleLDM3D Pipeline\\n[LDM3D-VR](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2311.03226.pdf) is a...\"],[\"prompt =f\\\"A picture of some lemons on a table\\\"\\noutput = pipe_ldm3d(prompt)\\nrgb_image, depth_image = ...\"],[\"```py\\nimport cv2\\nimport numpy as np\\nimport torch\\nfrom controlnet_aux.midas import MidasDetector\\nfrom...\"],[\"pipe = StableDiffusionXLControlNetAdapterPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion...\"],[\"```\\n\\n### ControlNet + T2I Adapter + Inpainting Pipeline\\n```py\\nimport cv2\\nimport numpy as np\\nimport t...\"],[\"pipe = StableDiffusionXLControlNetAdapterInpaintPipeline.from_pretrained(\\n    \\\"diffusers\\u002fstable-diff...\"],[\"depth_image = midas_depth(\\n  image, detect_resolution=512, image_resolution=1024\\n)\\n\\nstrength = 0.4\\n\\n...\"],[\"```\\n\\n### Regional Prompting Pipeline\\nThis pipeline is a port of the [Regional Prompter extension](ht...\"],[\"```\\n### Cols, Rows mode\\nIn the Cols, Rows mode, you can split the screen vertically and horizontally...\"],[\"```\\n![sample](https:\\u002f\\u002fgithub.com\\u002fhako-mikan\\u002fsd-webui-regional-prompter\\u002fblob\\u002fimgs\\u002frp_pipeline4.png)\\n\\n...\"],[\"```\\n![sample](https:\\u002f\\u002fgithub.com\\u002fhako-mikan\\u002fsd-webui-regional-prompter\\u002fblob\\u002fimgs\\u002frp_pipeline3.png)\\n#...\"],[\"```\\nbest quality, 3persons in garden, ADDCOMM\\na girl white dress BREAK\\na boy blue shirt BREAK\\nan old...\"],[\"```\\n    @article{chung2022diffusion,\\n    title={Diffusion posterior sampling for general noisy inver...\"],[\"```\\n* This pipeline allows zero-shot conditional sampling from the posterior distribution $p(x|y)$, ...\"],[\"def forward(self, x):\\n                    return self.seq(x)\\n\\n                def weights_init(self)...\"],[\"```\\n* Next, you should obtain the corrupted image $y$ by the operator. In this example, we generate ...\"],[\"```\\n* And next, as any other diffusion models, we need the score estimator and scheduler. As we are ...\"],[\"```\\n* The zeta is a hyperparameter that is in range of $[0,1]$. It need to be tuned for best effect....\"],[\"motion_id = \\\"guoyww\\u002fanimatediff-motion-adapter-v1-5-2\\\"\\nadapter = MotionAdapter.from_pretrained(motio...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\u003ctd colspan=\\\"2\\\" align=center\\u003e\\u003cb\\u003eConditioning Frames\\u003c\\u002fb\\u003e\\u003c\\u002ftd\\u003e\\u003c\\u002ftr\\u003e\\n  \\u003ctr align=center\\u003e\\n...\"],[\"\\u003ctd align=center\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fassets\\u002f72266394\\u002feb7d2952-72e4-44...\"],[\"- `stride` (`int`, defaults to 64):\\n  The stride of moving local patches. A smaller stride is better...\"],[\"images = pipe(\\n    prompt,\\n    negative_prompt=negative_prompt,\\n    height=3072,\\n    width=3072,\\n   ...\"],[\"```\\nYou can display and save the generated images as:\\n```py\\ndef image_grid(imgs, save_path=None):\\n\\n ...\"],[\"```\\n ![output_example](https:\\u002f\\u002fgithub.com\\u002fPRIS-CV\\u002fDemoFusion\\u002fblob\\u002fmain\\u002foutput_example.png)\\n\\n### SDE ...\"],[\"# train_lora is optional, and in most cases, using train_lora can better preserve consistency with t...\"],[\"DreamBooth training example\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.12242) is a method to personali...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and ...\"],[\"```\\n\\n\\n### Training on a 16GB GPU:\\n\\nWith the help of gradient checkpointing and the 8-bit optimizer f...\"],[\"```\\n\\n\\n### Training on a 12GB GPU:\\n\\nIt is possible to run dreambooth on a 12GB GPU by using the follo...\"],[\"```\\n\\n\\n### Training on a 8 GB GPU:\\n\\nBy using [DeepSpeed](https:\\u002f\\u002fwww.deepspeed.ai\\u002f) it's possible to ...\"],[\"accelerate launch --mixed_precision=\\\"fp16\\\" train_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$...\"],[\"```\\n\\n### Fine-tune text encoder with the UNet.\\n\\nThe script also allows to fine-tune the `text_encode...\"],[\"```\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\" --\\u003e export MODEL_NAME=\\\"BAAI\\u002fAltDiffusion-m9\\\"\\no...\"],[\"```\\n\\n### Inference from a training checkpoint\\n\\nYou can also perform inference from one of the checkp...\"],[\"### Training\\n\\nLet's get started with a simple example. We will re-use the dog example of the [previo...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so \\nwe need ...\"],[\"```\\n\\n**___Note: When using LoRA we can use a much higher learning rate compared to vanilla dreamboot...\"],[\"After training, LoRA weights can be loaded very easily into the original pipeline. First, you need t...\"],[\"```\\n\\nNext, we can load the adapter layers into the UNet with the [`load_attn_procs` function](https:...\"],[\"```\\n\\nIf you used `--train_text_encoder` during training, then use `pipe.load_lora_weights()` to load...\"],[\"```\\n\\n* LoRA parameters that have separate identifiers for the UNet and the text encoder such as: [`\\\"...\"],[\"```\\n\\n\\n### Training with prior preservation loss\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion...\"],[\"```\\n\\n### Training with xformers:\\nYou can enable memory efficient attention by [installing xFormers](...\"],[\"```py\\nfrom diffusers import DiffusionPipeline\\n\\npipe = DiffusionPipeline.from_pretrained(\\\"DeepFloyd\\u002fI...\"],[\"```\\n\\nAdditionally, a few alternative cli flags are needed for IF.\\n\\n`--resolution=64`: IF is a pixel ...\"],[\"### Stage II additional validation images\\n\\nThe stage II validation requires images to upscale, we ca...\"],[\"```\\n\\n### IF stage I LoRA Dreambooth\\nThis training configuration requires ~28 GB VRAM.\\n\\n```sh\\nexport ...\"],[\"```\\n\\n### IF stage II LoRA Dreambooth\\n\\n`--validation_images`: These images are upscaled during valida...\"],[\"```\\n\\n### IF Stage I Full Dreambooth\\n`--skip_save_text_encoder`: When training the full model, this w...\"],[\"```\\n\\n### IF Stage II Full Dreambooth\\n\\n`--learning_rate=5e-6`: With a smaller effective batch size of...\"],[\"```\\n\\n## Stable Diffusion XL\\n\\nWe support fine-tuning of the UNet shipped in [Stable Diffusion XL](htt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Let's load the [herge_style](https:\\u002f\\u002fhuggingface.co\\u002fsd-dreambooth-library\\u002fherge-style) checkpoint, w...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n### Load multiple LoRAs\\n\\nIt can be fun to use multiple LoRAs together to create something entir...\"],[\"```\\n\\nThen fuse this pipeline with the next set of LoRA weights:\\n\\n```py\\npipeline.load_lora_weights(\\\"o...\"],[\"```\\n\\nNow use the [`~loaders.UNet2DConditionLoadersMixin.set_adapters`] to activate both LoRAs, and y...\"],[\"```\\n\\nLoad the LoRA checkpoint with the [`~loaders.LoraLoaderMixin.load_lora_weights`] method, and sp...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nSome limitations of using Kohya LoRAs with ğŸ¤— Diffusers include:\\n\\n- Images...\"],[\"```\\n\\n## IP-Adapter \\n\\n[IP-Adapter](https:\\u002f\\u002fip-adapter.github.io\\u002f) is an effective and lightweight ada...\"],[\"```\\n\\n\\u003cTip\\u003e\\nIP-Adapter relies on an image encoder to generate the image features, if your IP-Adapter ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\nÂ  Â  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftesting-...\"],[\"pipeline.load_ip_adapter(\\\"h94\\u002fIP-Adapter\\\", subfolder=\\\"models\\\", weight_name=\\\"ip-adapter_sd15.bin\\\")\\nge...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"inpaint\\\"\\u003e\\n\\n```py\\nfrom diffusers import AutoPipelineForInpaint\\nimport ...\"],[\"```\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\nIP-Adapters can also be used with [SDXL](..\\u002fapi\\u002fpipelines\\u002fstable_diff...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is recommended to use `DDIMScheduler` and `EulerDiscreteScheduler` for face model. \\n\\n...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"prompt = \\\"best quality, high quality\\\"\\nimage = load_image(\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f...\"],[\"```\\n\\n### Other pipelines\\n\\nIP-Adapter is compatible with any pipeline that (1) uses a text prompt and...\"],[\"```\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\\nimport torch\\nfrom diffu...\"],[\"```\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:...\"],[\"# enable memory savings\\npipe.enable_vae_slicing()\\npipe.enable_model_cpu_offload()\\n\\n# load ip_adapter...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo ensure your pipeline and its components (`unet` and `scheduler`) can be saved with [`~Diffus...\"],[\"```\\n\\nThat's it! ğŸš€ You can now run this pipeline by passing a `unet` and `scheduler` to it:\\n\\n```pytho...\"],[\"```\\n\\nAnother way to share your community pipeline is to upload the `one_step_unet.py` file directly ...\"],[\"```\\n\\nTake a look at the following table to compare the two sharing workflows to help you decide the ...\"],[\"Sometimes you can't load all the pipeline components weights from an official repository. In this ca...\"],[\"```\\n\\nThe magic behind community pipelines is contained in the following code. It allows the communit...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Image to Video Generation\\n\\nThe are two variants of SVD. [SVD](https:\\u002f\\u002fhuggingface.co\\u002fstabili...\"],[\"```\\n\\n\\u003cvideo controls width=\\\"1024\\\" height=\\\"576\\\"\\u003e\\n  \\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggi...\"],[\"```\\n\\n### Low-memory\\n\\nVideo generation is very memory intensive as we have to essentially generate `n...\"],[\"```\\n\\n\\nIncluding all these tricks should lower the memory requirement to less than 8GB VRAM.\\n\\n### Mic...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"RealFill\\n\\n[RealFill](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2309.16668) is a method to personalize text2image inpaint...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n### Training on a low-memory GPU:\\n\\nIt is possible to run realfill on a low-memory GPU by using ...\"],[\"```\\n\\n### Training with gradient checkpointing and 8-bit optimizers:\\n\\nWith the help of gradient check...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell like a notebook, you can use:\\n\\n```b...\"],[\"```\\n\\nSome basic and important parameters to specify include:\\n\\n- `--dataset_name`: the name of the da...\"],[\"```py\\nmodel = UNet2DModel(\\n    sample_size=args.resolution,\\n    in_channels=3,\\n    out_channels=3,\\n ...\"],[\"```\\n\\nNext, the script initializes a [scheduler](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f096f84...\"],[\"```\\n\\nThen it [loads a dataset](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f096f84b05f9514fae9f185c...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f096f84b05f9514fae9f1...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nThe training script creates and saves a checkpoint file in your repos...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Text-to-Image Generation\\nStable unCLIP can be leveraged for text-to-image generation by pipelini...\"],[\"image = pipe(prompt=wave_prompt).images[0]\\nimage...\"],[\"```\\n\\u003cTip warning={true}\\u003e\\n\\nFor text-to-image we use `stabilityai\\u002fstable-diffusion-2-1-unclip-small` a...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Diffusion models have shown promising results in cross...\"],[\"During inference:\\n\\n* The _quality_ of the generated audio sample can be controlled by the `num_infer...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo check a specific pipeline or model output, refer to its corresponding API documentati...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [salesforce\\u002fLAVIS](https:\\u002f\\u002fgithub.com\\u002fsalesforce\\u002fLAVIS\\u002ftree\\u002fma...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [takuma104](https:\\u002f\\u002fhuggingface.co\\u002ftakuma104). â¤ï¸\\n\\nThe original codeba...\"],[\"## StableDiffusionPipelineOutput\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutpu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"## Popular models\\n\\n[Stable Diffusion Inpainting](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-in...\"],[\"generator = torch.Generator(\\\"cuda\\\").manual_seed(92)\\nprompt = \\\"concept art digital painting of an elv...\"],[\"```\\n\\n### Stable Diffusion XL (SDXL) Inpainting\\n\\nSDXL is a larger and more powerful version of Stable...\"],[\"```\\n\\n### Kandinsky 2.2 Inpainting\\n\\nThe Kandinsky model family is similar to SDXL because it uses two...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"## Non-inpaint specific checkpoints\\n\\nSo far, this guide has used inpaint specific checkpoints such a...\"],[\"generator = torch.Generator(\\\"cuda\\\").manual_seed(92)\\nprompt = \\\"concept art digital painting of an elv...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"runwayml\\u002fstable-diffusion-inpainting\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom diffu...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"htt...\"],[\"# load base and mask image\\ninit_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"runwayml\\u002fstable-diffusion-inpaint\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom diffuser...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"htt...\"],[\"img_url = \\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fCompVis\\u002flatent-diffusion\\u002fmain\\u002fdata\\u002finpainting_examples\\u002f...\"],[\"# Take the masked pixels from the repainted image and the unmasked pixels from the initial image\\nunm...\"],[\"```\\n\\n## Configure pipeline parameters\\n\\nImage features - like quality and \\\"creativity\\\" - are dependen...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load_...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load_...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cfigure\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingfa...\"],[\"```\\n\\nLoad the mask image of the output from above:\\n\\n```py\\nmask_image = load_image(\\\"https:\\u002f\\u002fhuggingfa...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"# load base and mask image\\ninit_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"```\\n\\nNow let's pass the image to another inpainting pipeline with SDXL's refiner model to enhance th...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is important to specify `output_type=\\\"latent\\\"` in the pipeline to keep all the output...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"### Prompt weighting\\n\\nPrompt weighting provides a quantifiable way to scale the representation of co...\"],[\"```\\n\\n### ControlNet\\n\\nControlNet models are used with other diffusion models like Stable Diffusion, a...\"],[\"assert init_image.shape[0:1] == mask_image.shape[0:1], \\\"image and image_mask must have the same imag...\"],[\"```\\n\\nNow generate an image from the base, mask and control images. You'll notice features of the bas...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"You can also offload the model to the CPU to save even more memory:\\n\\n```diff\\n+ pipeline.enable_xform...\"],[\"```\\n\\nTo speed-up your inference code even more, use [`torch_compile`](..\\u002foptimization\\u002ftorch2.0#torch...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Text-to-Video on the [project page](https:\\u002f\\u002fmodelscope.cn\\u002f...\"],[\"```\\n\\nDiffusers supports different optimization techniques to improve the latency\\nand memory footprin...\"],[\"```\\n\\nHere are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        An astronaut ridin...\"],[\"pipe = DiffusionPipeline.from_pretrained(\\\"cerspense\\u002fzeroscope_v2_576w\\\", torch_dtype=torch.float16)\\np...\"],[\"```\\n\\nNow the video can be upscaled:\\n\\n```py\\npipe = DiffusionPipeline.from_pretrained(\\\"cerspense\\u002fzeros...\"],[\"```\\n\\nHere are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd \\u003e\\u003ccenter\\u003e\\n        Darth vader surfi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## LoRAAttnAddedKVProcessor\\n[[autodoc]] models.attention_processor.LoRAAttnAddedKVProcessor\\n\\n## XFor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## DPMSolverMultistepInverseScheduler\\n[[autodoc]] DPMSolverMultistepInverseScheduler\\n\\n## SchedulerOu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Models\\n\\nTo push a model to the Hub, call [`~diffusers.utils.PushToHubMixin.push_to_hub`] and...\"],[\"```\\n\\nThe [`~diffusers.utils.PushToHubMixin.push_to_hub`] function saves the scheduler's `scheduler_c...\"],[\"```\\n\\n## Pipeline\\n\\nYou can also push an entire pipeline with all it's components to the Hub. For exam...\"],[\"```\\n\\nPass all of the components to the [`StableDiffusionPipeline`] and call [`~diffusers.utils.PushT...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\nPaint by Example is supported by the official [Fantasy-Studio\\u002fPaint-by-Example](https:\\u002f\\u002fhug...\"],[\"Dreambooth for the inpainting model\\n\\nThis script was added by @thedarkzeno .\\n\\nPlease note that this ...\"],[\"```\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and ...\"],[\"```\\n\\n\\n### Training with gradient checkpointing and 8-bit optimizer:\\n\\nWith the help of gradient check...\"],[\"```\\n\\n### Fine-tune text encoder with the UNet.\\n\\nThe script also allows to fine-tune the `text_encode...\"],[\"Create a dataset for training\\n\\nThere are many datasets on the [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets?...\"],[\"```\\n\\n## Upload your data to the Hub\\n\\n\\u003cTip\\u003e\\n\\nğŸ’¡ For more details and context about creating and upload...\"],[\"```\\n\\nThen use the [`~datasets.Dataset.push_to_hub`] method to upload the dataset to the Hub:\\n\\n```pyt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow, define four different `Generator`s and assign each `Generator` a seed (`0` to `3`) so you ...\"],[\"```\\n\\nCreate four generators with seed `0`, and generate another batch of images, all of which should...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nGiven a prompt, get the inference time for the original model:\\n\\n```py\\nimport time\\n\\nseed = 2023\\n...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nTime the distilled model and distilled VAE inference:\\n\\n```py\\nstart = time.time_ns()\\nfor _ in ra...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about the model on the [project page](https:\\u002f\\u002fdiffusion-planning...\"]],\"hovertemplate\":\"source=diffusers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"diffusers, circle\",\"marker\":{\"color\":\"#FECB52\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"diffusers, circle\",\"showlegend\":true,\"x\":[-4.7028136,-3.1561136,-4.0386815,-4.1646533,-3.029652,6.1942496,6.204471,-3.2501655,-0.97729415,-2.3751194,-1.3273668,-1.1129684,-2.0142334,-7.5563693,-7.614785,-3.280297,-3.9823263,-3.5967114,-2.6874995,-2.9885378,-1.2250077,-2.7575629,-2.9114418,-1.2532412,-2.6823008,-1.5335579,-2.84124,-1.5416126,-2.2555332,-3.1810832,-1.1829267,-3.9608257,-3.4520042,-1.5578953,-1.0685171,-3.6995714,-1.0998926,-4.692477,-2.4268055,-4.580509,-3.2767231,-2.7382941,-2.4374027,-1.973421,-2.3780284,-0.99741393,-4.732447,-3.159993,-2.5829575,-5.048436,-4.1659184,-2.8712292,-5.033785,-2.6277587,-3.9840147,-3.6756546,-4.0751996,-3.4017022,-3.943334,1.6807656,-4.0961556,-4.983198,-2.8512561,-3.120086,-3.6143093,-4.084489,-3.8747072,8.392068,-3.1411657,-4.5956903,-3.23512,-3.2061422,-2.0300105,-2.437823,-0.6191523,-2.1970532,-1.9149762,-2.3835714,-2.2323732,-3.6899505,-4.4941416,-3.2730534,-2.8910747,-3.7448976,-4.0556316,-5.0803194,-3.2178226,-4.802176,-4.923494,-3.19551,-5.160441,-2.7469668,-1.5940598,-2.6022785,-1.9698788,-3.5153255,-3.6702526,-3.6837137,-3.6772301,-3.7156234,-3.6767192,-3.7649572,-10.352936,7.6506104,-2.4193892,-1.8882983,-3.6891139,-2.7568462,-2.593455,-3.4040167,-4.4439926,-4.226887,7.097257,-1.3541223,-1.9833317,-0.89290637,-1.2848678,-1.5703207,6.944739,-5.1176686,-2.9492435,-1.3758364,-2.7919934,-3.9775121,-1.4325542,-3.428362,-4.684772,-4.892313,-2.8549352,-2.9883177,-1.9504654,-4.1990128,-1.0310695,-3.4583178,-4.2531376,-4.837686,-3.3515472,-3.707744,-3.4853294,2.943868,-1.4207628,-2.0407383,-2.3987172,-4.8927197,-3.2168484,-2.949609,-1.6828284,-1.7498513,-2.1185603,-1.7816944,-2.04026,-1.5737913,-7.3958006,-3.6079721,-1.168647,-3.2345178,-1.0942028,-3.7294736,-3.5064456,-0.8905013,-3.385584,-3.42464,-3.7244072,-3.2398534,-3.4100437,-3.5582743,-3.9161415,-3.3824275,6.192573,6.2046733,-1.38439,-2.2470434,-1.8672724,-2.3452506,-1.4983386,-2.126679,-2.1111202,-2.1532836,-2.2194793,-1.5839535,-1.9991578,-3.5531847,-3.9560926,-3.8547451,-3.0616379,-2.313587,-2.423181,-4.025443,-1.662736,-2.3906624,-2.6267755,-2.265502,-2.0602267,-2.5735834,-2.8967214,-3.7278757,1.0175147,2.998083,-3.9273705,-2.9444804,-2.587978,-1.814783,-3.8755426,-3.710047,-1.057474,-1.4875317,-2.1126459,-1.1194175,-1.9526498,-4.3088074,-1.1306021,-4.072709,-3.8660982,-3.9038866,-4.2958174,-4.000909,-2.829927,-1.5091964,-1.2580357,-1.851893,-3.8163214,-3.4143744,-1.2128812,-1.9933759,-2.321565,-2.2839994,-2.1763997,-2.526906,-1.526489,-0.6865761,1.9056346,-1.7848616,-2.1327999,0.56289387,-4.096866,-3.7436247,-1.0096778,-3.4602005,-3.2047896,-2.802062,-1.492181,-2.266937,-2.0377502,-2.1677294,-2.2363524,-1.5935475,-2.2475178,-1.6623256,-1.5645096,-2.6010845,-1.7957193,-2.8468606,-4.334325,-4.1560245,-3.9909701,-2.9025092,-1.5203176,-1.7069076,-3.6853952,-2.1708124,-2.8682747,-3.9264672,-3.4124563,3.0980072,-0.56616837,1.9434263,-0.12327299,-2.692242,-2.1577435,-2.437167,-2.0038426,0.47826844,-2.356505,-0.79912686,-2.9933329,-3.1435995,-4.314296,-1.8446583,-1.1551242,-1.846263,-4.107615,-4.2015133,4.3709884,-4.243169,-3.3219728,-1.1004665,-4.5532813,-3.4651332,-1.9631999,-1.3391266,-1.87421,-4.3106894,-3.6731136,-1.1645525,2.608321,-2.2289443,-0.8462051,-1.3983567,-1.2405484,-3.0927076,-3.4581623,-3.6636133,-4.28797,-4.8853283,-5.0492215,-5.0583043,-2.7615452,-4.080666,-4.938167,-3.0073795,-3.880297,6.191095,6.2036295,-3.1365373,-2.2545178,-3.0988774,-1.2578075,-3.3106318,-1.1447905,-1.4263726,-2.9846063,-2.6345642,-2.0279634,-1.1415362,-3.3119197,-2.1889663,-4.433654,-4.6810913,-3.2061346,-3.5270073,-1.2910041,-3.3018441,-2.1438425,-1.7687652,-1.4385417,-2.3679075,-2.3125632,-2.5267286,-2.6092482,-2.8900256,-2.259388,-4.1014752,-3.7062972,-1.6307602,-3.7086883,-1.5885918,-4.380231,-3.7465835,-3.944729,-4.0282564,-4.138518,-4.1156173,-3.9804046,-3.0604155,-2.6069195,-3.7491894,-2.795522,-1.4699965,-2.7142713,9.041345,-2.9705403,-1.2260774,-2.7337801,-3.5247552,-2.792979,-2.8575017,-2.1443303,-3.3731854,-1.6595789,-2.4379675,-2.784614,-3.1830275,-5.0402513,-3.2191546,-9.140795,-4.7232966,-3.7040603,-7.400869,-7.4951973,-7.4923887,-7.093912,-6.8767915,6.884718,-4.0638914,-3.9937787,-2.8994634,-2.6831627,-4.0053215,-3.8385348,-3.9087336,-4.113749,-3.451704,-3.5949314,-3.8333647,-4.1223693,-4.252647,-4.2781096,-3.8531697,-3.6235113,2.556767,-2.2502825,-0.858468,-4.5764866,-4.317313,-8.668437,-3.6611786,-4.5707116,-4.228466,-4.287565,-4.691381,-4.676937,-3.9641151,-3.1743178,-1.9761268,-2.5863862,-1.865656,-3.045447,-2.0621536,-1.7367249,-1.3982639,-2.779124,-1.9894068,-3.0112379,-4.75065,-3.6202087,-9.013989,-2.598448,-2.4115784,-4.092548,5.1680036,-4.112214,-4.0454564,-4.174599,-4.1976185,-4.141325,4.9784307,5.8338513,-4.0749536,5.221886,-4.1536684,-4.0297704,-3.4550648,-3.67536,1.4249413,-3.592099,-3.79134,-4.1630063,-3.9188828,-2.7536411,6.8077197,-4.0592456,3.974817,7.3183823,4.446356,4.79877,-4.1747117,3.5671556,3.3130732,4.039682,-3.8671665,-3.7623289,-3.8385692,-4.0542183,-2.771916,-5.357841,-5.442018,-0.93614835,-1.5793828,-1.6032661,-2.025095,-1.7486422,-1.93451,-2.1935692,-2.5496159,-1.7695731,-3.1542034,-4.2351456,-3.1449058,-2.196792,-2.5386713,-0.9521968,-1.9267218,-2.3835523,-2.2875192,-0.9863437,-2.5563116,-1.262581,-2.5656884,-1.3195205,-2.6889734,-0.7296379,-2.5860457,-1.7129582,-2.3832788,-0.92802,-3.0609207,-2.6264148,-2.6070638,-2.0091188,-1.313032,-3.9962764,-4.1657734,5.682394,-4.0372043,-4.00297,-3.7668514,-3.9733543,-4.128294,3.4357793,3.3231025,-4.7063794,-2.8236535,-4.924216,-3.8071272,-3.531587,-2.7696364,-2.9220269,-2.7740967,-2.5516257,-1.8305879,-2.6469092,-2.381322,-4.772927,-4.994902,-3.9860423,-2.8442163,-4.1316433,-4.1903715,-3.3212132,-1.3352365,-1.4563023,-1.6910015,-2.0776045,-2.178296,-3.2514472,-3.7588701,0.55547565,-2.7447605,-2.7596707,0.37372407,-2.8960912,-4.755385,-3.4570692,-4.0403695,-3.9626007,-3.6257794,6.1936426,6.204558,-2.2348902,-2.2691102,-2.6438649,-2.196224,-2.2665799,-1.8460315,-1.2508905,-2.1605568,-3.674202,-3.1402771,-3.8085027,-5.003962,3.393111,3.606427,-3.6834614,-2.2452288,-0.9000255,-2.527024,-1.2235235,-2.3936806,-1.3191226,-2.5584743,-2.5702348,-2.0209887,-1.7178627,-1.2510608,-3.1790974,-2.6624382,-3.6258707,-2.7196424,-2.618332,-2.543894,-3.4168012,-1.3068706,6.203634,-1.9539585,-1.9998773,-2.5824826,-2.3707538,-2.116826,-2.088713,-2.00014,-0.71022123,-2.369805,-4.0496564,-3.8566382,-3.8512216,-4.0108166,-3.6894786,-2.9279957,-3.6559565,-3.0188882,-0.54516363,-3.95589,-4.1088934,-3.487791,-4.1656084,-4.3171787,-3.8669734,-3.8580794,-2.1433394,-3.9916737,-4.2442484,-4.378454,-4.088227,-3.8282716,6.190363,6.2061305,-2.1688209,-2.1795762,-2.351405,-1.7904168,-1.8427964,-4.796386,-2.4122994,-4.8123975,-2.4314775,-4.269042,-2.4510489,6.193599,6.2044163,-2.1098106,-2.1309717,-1.1667006,-2.483156,-2.2335808,-2.2061641,-2.130582,-0.7716889,-1.1245089,-1.4271439,-1.7508674,-3.4889483,-4.0632124,-3.4273114,6.1966314,-1.1289004,-1.4656186,-1.9223605,-2.0072227,-4.3012,-4.00297,-2.8937156,-4.770076,-4.006718,-4.553663,-3.9336061,-3.4817624,-3.2728052,1.2217317,1.6393317,-2.3448026,-2.9650676,-1.6360188,-2.1548011,0.2950541,2.6916795,2.779952,-4.561725,-5.004382,-3.6130052,-1.4676756,-1.8596587,-4.3714952,-4.223914,-4.8191757,7.650023,-2.244409,-3.9046173,-2.1184037,-1.3180101,-2.0803084,-4.350896,0.9613281,0.3446461,-1.821687,-9.7973385,-9.873921,-4.2299776,-0.009707909,-9.651459,-9.896435,2.5459855,-0.34207514,-1.8060223,-4.3784227,-4.5859504,-4.884942,-1.8600398,-1.7753001,-1.907466,-1.8158231,-2.1178854,-1.8878295,-3.6641498,6.2008414,-2.3224046,-0.9354796,-2.3674438,-1.6290575,-0.930175,-0.82774836,-2.1891682,-1.7453408,-2.0233128,-4.270605,-3.5701492,-1.867297,-3.3703778,-2.1607409,-1.6286857,-2.4332848,-3.6475542,-4.976588,-3.6575398,-2.0126941,-1.3689843,-2.9143212,-4.335811,-2.1279087,-1.2495911,-1.73555,-1.6836008,-3.4356246,-1.4230094,1.9318053,-1.4496962,-1.4826434,-3.1835856,-2.8288558,-2.4062665,-2.9731972,-0.7273042,-1.75779,-1.5838156,-1.7214925,-1.8188004,-0.94235545,-2.894991,-3.77776,-3.6904094,-3.6851833,-3.7656014,-3.7646945,-3.7242074,-3.675549,-3.680335,-3.4245372,-3.9730258,5.150637,5.7309327,-3.859163,-2.806278,-3.8192136,-2.4741576,-2.4448504,-2.6950655,-2.8401327,-2.8379598,-2.2038872,-1.534343,-1.9638807,-2.3988652,-2.792851,-2.5680487,-2.7104063,-2.6734195,-2.7774675,-2.957309,-2.7364068,-0.49978963,-2.9156907,-4.0881844,-4.936153,-4.820315,-2.586248,-3.8474138,-1.1495878,-2.0426354,-2.146392,-2.2970107,-3.1031861,-1.9892328,-2.9413621,-2.6141036,-1.9903595,-2.9589427,-3.2219415,-3.0968819,-0.78417224,-2.620246,-2.2054234,-1.9213357,-3.6976051,-3.1522186,-5.1545677,-2.175235,-2.7748322,-3.7343776,-2.8569047,-2.2933502,-2.3430362,-2.29667,-1.9953809,-2.5967436,-2.1756144,6.9187064,6.8596115,-3.744285,-3.4546878,-4.1599884,-4.9468217,-2.4514327,-3.6314049,6.1907325,6.2044687,-2.7096624,-0.38772726,-2.5057285,-2.3898325,-2.0683205,-1.4869239,-2.4537942,-4.1906095,-3.021489,-3.1934736,-2.6577604,-2.609621,-2.305534,-2.3403761,-1.5074089,-1.8628827,-1.923446,-4.6335497,-2.9615636,-2.7698255,-1.5295478,-2.9462714,-2.5557196,-1.7461023,-2.7333224,-1.6286644,-3.9471593,-4.6625757,-4.840331,-2.854358,-1.0578097,-1.9753268,-2.285681,-2.3880286,-2.5262058,-2.2131245,-4.3577385,-5.056333,-2.9350934,-3.5270586,-2.2272725,-3.1229088,-2.956407,-2.5518892,-3.4380617,-2.4591494,-2.6607897,-2.177405,-2.946859,-3.5593545,-1.4418004,-2.2037988,-1.7703881,-4.003813,1.05015,-3.7248964,-3.1774402,-2.6981063,-2.7161663,-2.9410014,-2.7730944,-1.655807,-3.0093422,-2.4955106,-2.233681,-4.0353594,-3.569038,-3.7947927,-3.7228377,-4.071477,-4.16786,-4.028568,-3.697613,-2.883848,-2.306679,-2.4769537,-3.1413238,-2.9957538,-2.5115135,-2.2127924,-2.2955973,-1.6010381,-3.6005855,-3.596638,-1.2792423,-4.837942,-5.145826,-3.68816,-3.2998514,-4.093053,-4.3854585,-4.0400643,-2.8016453,-4.2040443,-3.7288725,8.289566,-3.567613,-1.5786978,-1.1487459,-3.2313197,-4.9474306,-2.5137188,-2.423122,-4.7058597,-3.7305536,-1.8026153,-1.5261028,-1.9785767,-2.0009477,-1.5091372,-1.7446684,-1.5899671,-2.3701613,-1.7679108,-1.7490836,-1.487233,-1.5329525,-2.164308,-2.2577214,-1.6349167,-2.0547311,-1.7444263,-1.9909089,-2.6077912,-2.098025,-1.2000835,-2.1824408,-1.5322192,-1.8162818,-2.5801744,-4.210626,-3.2071905,-1.4193524,-1.1540883,-3.1266406,1.2803195,-1.6829164,-1.9530871,-1.2157084,-2.3901346,-2.2555044,-2.3396883,-1.2114991,-2.3590386,-2.7676,-3.5055015,-2.931783,-1.4741071,2.0326462,-1.5377381,-3.4877453,-2.5947409,-3.226031,-1.4941058,-2.9455566,-1.6018616,-2.7494242,-0.90905774,-2.2865078,-1.513651,-2.341972,-1.5227947,-2.2582831,-1.5210563,-2.0995486,-1.667342,-1.3206513,-1.6079712,-1.7338428,-1.8417809,-1.76337,-1.4335457,-2.5544636,-1.2326331,-2.5139365,-0.74597025,-3.5611062,-4.221485,-2.8683987,-1.853285,-2.9941924,-4.0181212,-3.480304,-1.5029575,-2.3643482,-1.9950262,-3.8809962,-3.594684,2.5460777,-2.4221404,-2.058441,-3.3323226,-2.159194,-2.6352363,-3.642685,6.193989,6.2040324,-1.0311706,-2.3869793,-2.745704,-1.9074602,-2.3633835,-1.1060491,-1.5841666,-2.235904,-3.954925,-4.9527683,-2.2693162,-2.5203521,-2.5993083,-3.115893,-4.2378664,-3.5086894,-2.081205,-3.8195524,6.1927714,6.204603,-3.028388,0.03683103,-1.5834882,-1.6783555,-2.5816135,-1.0688665,-2.0180924,-1.1415173,-2.1845138,-3.628745,-3.602331,-3.4217944,-1.3925084,-1.348936,-1.7462475,-2.2671928,-2.6294653,-2.0986028,-1.94519,-1.8996451,-5.0580993,-2.443739,5.1810803,5.288467,5.747635,4.918967,-2.5877,-2.7214088,-0.60287094,-1.73815,-1.3006105,-4.8454156,-3.6143084,-2.4144416,-3.5006816,6.1940727,6.202837,-1.3785907,-2.0974166,-1.8739253,-2.1474016,-1.1313885,-2.199517,-2.2764065,-9.546423,-4.970511,-1.568099,-2.4245822,-4.7182884,-4.963622,-4.5967126,-1.6581866,-1.678585,-0.8310743,-2.6971953,-4.82496,-4.531994,-4.8829303,-2.7784204,-3.904953,-3.6848226,-3.7909453,-3.5796773,-4.1153607,-3.5439625,-3.4214559,-3.647415,-3.2798657,4.0976324,-3.9879084,-4.2093253,-3.4857051,-3.5882587,-4.565893,-4.882611,-3.611824,1.806902,-1.3636833,-4.8592815,-2.596619,-2.9147599,2.5525947,0.6588363,-1.1816922,-1.8290852,-1.7973933,-1.8387066,-4.2046137,-3.107599,-0.97706306,-1.8296968,-3.2949483,-3.8530557,-3.563222,-4.095043,-3.8806393,-3.4138677,-2.6990561,-2.4781363,-3.6461964,-3.3295116,-1.2615107,-2.4355392,-2.013839,-3.600676,-2.5615747,-1.3695463,-2.2230158,-2.6025224,-3.5641546,-1.788772,-1.8829169,-3.4659796,-3.980791,7.0468435,9.015998,9.068198,8.783394,-4.4453096,-3.8229597,-3.8486917,-3.6258194,-3.8875477,-3.7606013,-3.6377711,-3.700685,-3.514388,-2.7693162,-3.2308223,-2.8463614,-1.7180674,-2.7594283,-2.6366184,-2.2875686,-1.1869432,-2.2689278,-0.5834455,-1.8997045,-2.1893525,-2.0839784,-1.5710547,-2.5583382,-1.688555,-4.1019535,-1.7912173,-0.8449299,-1.9990754,-1.7650664,-3.2756424,-1.9157634,-2.0705888,-2.4587681,-2.5003793,-2.0685766,-3.0783303,-1.7201222,-3.8450696,-2.0485575,-3.2980056,-2.2603228,-1.8511202,-2.342991,-2.0961738,-1.9521294,-1.3868105,8.190577,-3.9978092,-1.1856028,-3.7901983,-2.4373045,-3.152359,-1.9776304,-1.6000944,-3.3426523,-1.7518109,-2.991147,-2.3464274,-2.9125426,-2.0179114,-3.4130113,-2.066448,-2.636566,-2.755613,-2.4913266,-1.3212417,-1.1105378,-2.1767368,-2.608233,-3.2940736,-1.2820188,-2.2717762,-2.913838,-0.9616111,-1.9909577,-0.67257476,12.938862,-2.2743258,-3.7035904,-1.6299942,-3.2840948,-1.5349284,-1.6584051,-0.5488019,-4.9141245,-2.53998,-2.4954307,-2.6614923,-0.7361566,-4.764758,-1.5519909,-1.6766518,-2.0893598,-1.8153546,-2.1641557,-0.9597427,-2.603823,9.35379,-1.8185165,-2.6961236,-2.039251,-4.5418234,-1.5231307,-1.2372801,-1.3683587,-2.927298,-1.7322315,-1.4069879,-3.6173258,-3.3436816,-2.6824477,-0.7845749,-0.90882826,-2.1406283,-1.4721367,-3.157883,-1.4427527,-2.4104755,-2.228464,-2.2312136,-2.2862105,-1.1882306,-2.711664,-2.0728202,-3.7997553,-2.289578,2.6152346,-3.0711157,-2.0830014,-1.9325166,-1.9973458,-2.000966,-1.8223792,-2.6603966,-2.5846632,-3.5154278,-2.0762186,-2.184937,-2.2892587,-2.4774067,-2.9686294,-3.8201144,-4.045741,-3.832142,-1.7269641,-3.020253,-2.3226216,-3.273754,-1.8673153,-1.8403424,-1.7674135,-1.7672199,-1.7244349,-1.7342927,-1.8005595,-2.5861237,-2.1215687,-2.336808,-1.185859,-1.5338374,-2.3689313,-1.608938,-2.1672273,-2.316211,-0.8170472,-2.9342978,-1.684148,-2.231759,-1.4808769,7.33952,-3.0115068,-2.5881274,-2.9057875,-3.0278695,-3.08602,-2.5215476,-2.8750606,-4.8556285,-9.916975,-3.0451417,-2.078093,-2.9683053,-2.3965855,-2.459146,-4.247269,-2.7501917,-1.3906695,-2.2572446,-2.4383552,-2.8518066,6.2047215,-1.4034045,-1.0382402,-2.4929078,-0.960247,-2.0893123,-1.9746056,-8.43038,-5.053076,-3.1002421,-0.8210647,-2.3346536,-3.2708168,-7.4853597,-7.551353,-6.556337,-4.1134315,-4.2254796,-4.281679,0.4850601,-2.622432,-5.0379786,-3.3266141,-4.8371305,-3.2765908,-2.8753552,-3.963435,-2.3870785,-2.6235285,-2.600217,-3.4183152,-1.2015371,-2.872464,-2.463409,-3.4810154,-2.9960053,-1.0623547,-1.490549,-2.7445974,-1.6563612,-1.5201268,-2.5289881,-1.3732693,-1.0647825,-1.7440144,-2.1833959,-1.5483876,-2.2563972,-1.4159493,-2.398073,-1.502174,-2.431929,-1.8318753,-2.010344,-2.316755,-2.3941517,-1.8880455,-2.8693416,-0.79341614,-1.3358916,-3.3775377,-2.9663737,-2.704535,-5.6336794,-2.1755068,-2.6303337,-2.031626,-1.5974882,-1.7339615,-3.3593,-9.049549,-2.4886568,-4.742343,-2.8428936,-3.4318886,1.509907,3.1047096,-2.6972308,-2.9130988,-4.6396213,-3.2551696,-1.4991938,-2.4005702,-2.281077,-2.3410077,2.182544,2.7769632,3.0563273,-2.8545442,-1.6180469,-1.780142,-3.9350774,-2.3999984,-3.5525973,-2.911211,8.61708,-3.6817224],\"xaxis\":\"x\",\"y\":[-6.2541237,-8.131053,-6.5024514,-3.5458117,-5.782914,20.744324,20.777077,-3.4973965,1.6365564,-3.8212376,-3.3454854,-3.1092036,-5.543593,7.0846915,6.9681892,-8.382248,-5.9486628,-5.5519176,-6.504625,-6.006866,-6.71915,-6.2383766,-6.006731,-6.5268307,-6.235704,-6.5498214,-6.104425,-7.017238,-7.1987047,-6.186001,-6.679432,-7.2378864,-5.6273284,-2.811633,-3.9170833,-5.092966,-3.6921644,-6.826799,-11.936452,-6.654584,-7.373243,-7.802093,-7.718101,-7.3146996,-7.4467916,-7.170767,-6.0908422,-7.8512373,-10.592505,-6.627598,-6.8457413,-8.798513,-6.500142,-9.865898,-7.674528,-7.539909,-7.221348,-6.0563345,-8.338762,0.9669002,-8.126054,-7.177961,-6.6551986,-5.5657606,-7.853313,-2.412033,-2.3070521,4.1060596,-4.104646,-6.0185905,-8.372575,-7.293033,-6.7939167,-7.121328,-6.996686,-7.51811,-6.453285,-7.2917438,-7.494444,-7.634339,-6.961741,-5.838432,-6.098002,-6.0412154,-6.8589735,-6.3203573,-8.30377,-7.424533,-6.261785,-8.457954,-6.4780154,-6.8424172,-6.7973447,-6.7761335,-7.102519,-6.9220653,-6.535413,-6.615734,-6.7236075,-6.78779,-6.5430064,-6.588434,-1.0068177,1.6426655,-6.24895,-6.9607215,-6.577309,-6.7780185,-6.457288,-4.0717144,-7.8940654,-7.005556,-9.426279,-6.948765,-7.200849,-7.0950165,-7.2866592,-7.226512,-9.42419,-6.388942,-7.111313,-6.643313,-6.6302977,-6.6696467,-6.628468,-7.847846,-6.371959,-6.5843487,-8.462933,-5.9588013,-4.084746,-3.2569149,-3.1495965,-7.862392,-7.8288536,-6.8468566,-8.265398,-5.7927613,-8.265531,-0.3962726,-3.791571,-3.8472097,-3.6819584,-6.8668504,-8.347062,-6.427771,-5.629307,-5.926372,-5.5127964,-5.723352,-5.833742,-5.142428,4.9917994,-3.436451,-3.2711198,-2.3516533,-3.1737802,-3.592543,-2.4981291,-2.99074,-2.560381,-3.1083035,-5.814857,-4.584578,-4.230387,-5.5263805,-7.3321342,-5.5400596,20.740633,20.777786,-2.9285681,-4.5886264,-6.0271616,-6.2629833,1.4179275,-5.528194,-4.8839364,-2.9288409,-2.7835956,-4.254604,-5.2465234,-4.7724104,-7.4502134,-7.632764,-7.197229,-7.173247,-7.03959,-6.9949174,-7.2644157,-7.150928,3.8825185,-7.0256805,-7.0018635,-6.9658847,-6.8492713,-6.8675756,0.33050317,1.0057348,-6.5440416,-5.8295603,-6.696416,-5.956453,-5.8146806,-6.0047135,-2.559233,-3.3840172,-2.7324812,-3.0820448,-5.472323,-5.318541,-3.1200378,-5.5118484,-6.5592093,-8.176246,-7.099291,-6.927284,-5.826058,-3.6988285,-3.7816906,-2.6677814,-6.3982325,-5.839767,-3.7982311,-3.1039999,-2.6496,-2.8402457,-5.92825,-3.3769622,-4.2878213,-2.4357862,-0.24133688,-0.43350706,-0.10264093,1.0620253,-7.1756,-6.9160857,-7.0618095,-6.2987638,-6.635004,-6.5958962,-6.89917,-7.42412,-7.0209785,-7.15928,-7.387079,-7.757381,-7.4045753,-7.7943764,-6.9185348,-6.6038857,-7.6469383,-3.776695,-6.819035,-6.9916587,-7.1085496,-7.5453753,-7.7183986,-7.505296,-5.7222986,-2.6468801,-4.6979465,-6.5590577,-8.506912,0.15982993,-1.8502735,3.5955148,-6.22018,-6.504861,-6.8860993,-5.4321313,-6.6543856,-0.15935488,-6.663074,-0.24102263,-5.233787,-9.108547,-8.115867,0.59830713,0.70952344,-6.7173004,-7.5877285,-7.5227013,0.7865268,-8.096524,-7.0190854,-7.166987,-6.4223447,-5.57082,-3.0020547,-3.3940747,-3.8921728,-2.9505396,-4.5721345,-3.7192886,-0.20877191,-5.476064,-7.0053525,-2.7998533,-3.562645,-4.857384,-4.812788,-6.450994,-8.128751,-6.6015716,-6.46733,-6.60803,-5.601543,-7.731105,-7.132756,-9.584538,-6.411628,20.736475,20.774145,-5.288142,-4.505907,-5.8022356,1.9276923,-5.7967815,1.929547,1.6440749,-5.5940933,-5.8373394,-4.6557283,-3.8360622,-5.8329053,-5.447928,-6.236367,-6.272384,-8.446013,-5.759874,-3.0022135,-6.5896025,-6.8084645,-7.1459985,-6.7969866,-6.773351,-7.032556,-6.7289515,-6.786022,-7.1690936,-7.088907,-7.116896,-6.3904834,-6.713092,-6.1528134,-6.2628713,-6.8986335,-6.74818,-7.31587,-7.298534,-7.169903,-7.228149,-7.2045145,-8.163731,-7.5087657,-8.129623,-6.6454835,-7.342251,-7.1963596,2.8106863,-6.85946,-6.862485,-6.9066224,-6.72995,-8.131244,-6.7906013,-6.1713376,-6.7696533,-7.6542063,-6.9479227,-6.78429,-5.4837184,-6.5881124,-8.57855,0.73298645,-6.884522,-7.0301914,7.0796027,6.9542108,7.106262,6.3622723,7.366422,-1.152375,-7.704734,-7.3999515,-6.596055,-7.204395,-7.3201413,-7.33134,-6.4622583,-7.900586,-7.291558,-6.981354,-7.2893624,-7.292203,-6.6930127,-7.38963,-4.751751,-4.9892707,-0.27400735,-5.48473,-7.011126,-6.6124597,-6.1100264,-0.28020895,-6.6176314,-6.035546,-6.179045,-6.0377765,-6.577409,-6.463935,-6.467207,-4.246136,-5.7678113,-5.5203004,-6.541866,-2.9870033,-6.607607,-0.50231576,1.0971824,-3.9966326,-6.9188967,-4.668683,-6.874679,-6.3369823,0.5233101,-7.96627,-12.07574,-7.7777452,1.7258837,-8.506794,-8.502322,-8.285235,-8.078172,-8.5328455,1.3960493,1.0303154,-8.462575,1.1882813,-8.580065,-8.644655,-8.251485,-8.380563,0.9782898,-8.177706,-8.001245,-8.528073,-8.054541,-8.029954,2.3802142,-8.626077,1.7453535,2.840094,1.0234822,1.3182206,-8.648696,1.1303558,1.1130179,0.92294866,-7.17384,-6.558094,-7.072151,-8.308964,-5.064273,-6.402287,-6.463636,-6.8350167,-6.297894,-6.410033,-6.5032177,-6.613931,-6.5594273,-6.5670347,-6.206193,-6.553504,-7.6926622,-6.8821054,-6.314279,-6.6728196,-7.166312,-7.125832,-7.042462,-6.3602004,-7.5721726,-6.920786,-7.2119513,-7.101422,-6.9100323,-6.8141823,-6.9736805,-6.625952,-7.131562,-6.924918,-7.131115,-7.090915,-6.668551,-6.961672,-7.235581,-7.0721197,-7.0284805,-7.819106,-8.259439,1.1726234,-8.516177,-8.522211,-8.039603,-8.022355,-8.553496,1.089686,1.1393816,-7.5857754,-10.036206,-6.32729,-7.7742987,-7.6380577,-8.309442,-8.289177,-8.336384,-8.06462,-6.7647724,-8.059979,-7.8917346,-6.9337,-6.7024517,-7.792044,-6.899552,-6.2357235,-6.333684,-5.4417515,-2.7134511,-3.7119272,-2.6333983,-5.663137,-6.430992,-5.3884864,-6.286217,1.0746357,-8.096937,-8.211493,1.0120028,-8.332657,-6.7832475,-6.43498,-6.9819016,-6.0170207,-8.358103,20.74546,20.778084,-4.829276,-5.601393,3.8772216,-5.463459,-6.0407867,-4.5768843,-3.1465616,-5.7285676,-5.746991,-8.021794,-8.25521,-6.42697,0.20122543,0.2402974,-6.41535,-6.3453827,-6.6650186,-6.7561707,-6.484607,-6.8096256,-6.6324744,-7.441565,-5.811135,-6.0849023,-6.1306667,-6.833905,-7.457936,-9.75281,-6.851538,-6.2475247,-6.817932,-6.3584504,-5.4709225,-2.3142836,20.774662,-2.7155585,-5.6018524,-7.4994264,-5.4115033,-2.6467204,-2.6383374,-2.979969,0.9299519,-6.120772,-7.4030523,-7.160984,-7.0007653,-7.868684,-7.6808434,-8.441186,-7.8814855,-8.500577,0.8528631,-7.591653,-7.3615937,-7.623628,-5.9809446,-6.0253196,-5.4971337,-5.5878572,-4.7520995,-5.3822927,-6.0341625,-6.399829,-7.418349,-6.080831,20.73443,20.781227,-5.341249,-5.7688155,-5.532609,-4.356953,-5.913194,-6.8094525,-11.994846,-6.722514,-11.848513,-5.9488554,-5.7064548,20.743744,20.775827,-4.7430606,-5.5493126,-5.097784,-6.0358076,-5.5934386,-5.2799454,-5.717991,-2.879305,-3.5586483,-4.424228,-4.9050527,-5.785421,-5.629616,-8.289187,20.754585,-2.408073,-3.2205794,-3.0530586,-5.4699273,-5.506821,-6.7495823,-7.8599634,-7.440243,-7.507313,-6.787756,-6.2703953,-6.588364,-8.053081,2.276044,2.6466584,0.13422693,-5.7227874,-2.8031723,-2.3083076,0.5280694,3.1776495,2.7275305,-6.571718,-6.462002,-6.261662,-6.137102,-6.3383055,-6.9482956,-6.673708,-5.6477575,3.501606,-6.9507837,-6.5946846,-7.3438306,-6.633179,-5.8012056,-6.6078963,4.1902866,3.5412042,-6.8054695,-0.1415276,-0.30057597,3.3524144,6.169115,-0.31351277,0.05379536,2.9222114,-6.3199925,-6.9729314,-6.809586,-7.0739875,-6.8871255,-6.6948404,-6.7093606,-6.8185043,-6.6552334,-6.7517643,-6.923668,-5.803518,20.766258,-5.798595,1.5004742,-1.3798645,-6.0974345,-6.116038,-6.606622,-4.7830606,-2.272339,-5.724592,-5.567511,-6.685717,-6.3342957,-6.57539,-6.235327,-7.077594,-6.927043,-4.176179,-6.481956,-6.8360505,-6.916841,-7.068075,-8.552762,-6.306937,-2.2912087,-7.0094266,-1.9620444,-1.6985388,-6.014918,-2.7450824,-0.39899188,-3.510756,0.35626328,-5.387391,-5.121797,-5.900423,-4.559037,-7.1500216,-7.629275,-7.8005905,-7.810507,-7.329004,-6.903776,-4.4582176,-3.8476775,-3.7938733,-3.8845801,-3.7329803,-3.8569717,-3.606355,-3.8948917,-3.9321053,-4.326792,-7.9011774,2.2893374,3.0829866,-7.2092257,-8.876067,-7.45582,-7.9725285,-7.339765,-7.983939,-8.049254,-8.050855,-7.3430214,-0.11085255,-6.441601,-7.1936502,-8.088799,-7.575388,-8.006061,-8.11661,-8.197134,-8.140058,-8.322058,1.8715918,-8.404341,-8.063523,-7.237431,-6.943886,-9.696747,-6.7677903,-0.038203876,-5.8960896,-7.2136827,-7.410683,-6.8029838,-7.180883,-6.8816514,-7.158082,-7.1459394,-7.063325,-6.2486205,-6.5369635,-7.155212,-6.992909,-7.2065954,-6.9801416,-5.5658946,-7.628253,-6.4571915,-7.5172505,-7.4965305,-7.480636,-4.450914,-4.533641,-4.647145,-4.2932863,-6.603576,-4.516009,-4.6915026,1.6097714,1.435987,-7.607618,-4.2697268,-8.173026,-6.844589,-11.782126,-6.67824,20.7425,20.775682,-5.6399813,-6.46241,-6.0668774,-5.644212,-4.8347197,-3.7058723,-6.0644574,-7.489344,-4.4365177,-4.7712765,-4.513809,-4.929697,-4.6636333,-4.4375052,0.60872376,-4.8625317,-4.839732,-6.5752983,-6.1260138,-6.044767,-7.5725927,-6.130506,-6.3744216,-7.100933,-6.346282,-6.8148565,-7.112847,-7.4958653,-7.3295865,-7.092819,1.0110705,-4.8714437,-4.1560955,-5.1566,-3.2687414,-5.8731914,-6.9518704,-6.641658,-8.928669,-7.480482,-6.560841,-8.072741,-8.594552,-7.6091704,-7.5648565,-7.3699574,-4.743864,-4.747584,-8.257437,-5.803824,-4.1085153,-2.974708,-3.7723792,-7.212011,0.5596231,-7.044407,-6.8251834,-7.1348557,-7.025659,-7.9879684,-7.0957823,-6.1963825,-7.412953,-7.0685525,-7.0193663,-6.8133926,-7.691632,-7.39096,-7.3916383,-6.839125,-6.890938,-6.9904594,-7.6886106,-8.446732,-6.663591,-5.66684,-8.145874,-8.401381,-7.0261936,-7.046959,-7.1139116,-7.429812,-7.0508237,-5.508862,-3.7719493,-1.3004335,-1.6324776,-7.478542,-8.410244,-7.089165,-7.0350733,-6.954408,-9.397827,-7.511944,-7.240406,2.5216956,-5.6591125,-2.7505138,-3.7963285,-4.5712776,-6.6540966,-7.897553,-12.009797,-6.1258187,-6.163217,-6.94505,-7.101407,-7.295232,-7.073854,-7.1071563,-7.4764786,-7.088991,-7.268428,-7.0271635,-7.198785,-7.298307,-7.199538,-7.27402,-7.5032353,-7.4890676,-7.022465,-7.441618,-7.0237803,-7.228018,-7.249423,-7.040875,-7.122992,-7.034364,-6.9725065,-5.5425744,-6.6960545,-5.592775,-3.827559,-3.034274,-5.6941195,3.1351817,-4.7449985,-4.45626,-3.869857,-4.722572,-2.952395,-2.6860836,-3.1923661,-4.672674,-5.7031045,-7.0625267,-5.532403,-2.701236,-0.43589306,-3.3392644,-6.8868494,-5.398015,-6.8960743,-7.991671,-7.1406765,-7.692882,-7.3023744,-7.2047358,-7.3248205,-7.8789144,-7.3714733,-7.98153,-7.4718313,-7.8555865,-7.3165593,-7.7435155,-7.0522285,-6.891634,-6.7814717,-6.724249,-6.61421,-6.9209723,-6.761052,-7.0199428,-7.4217505,-7.162442,-5.8450923,-8.205911,-6.417452,0.2372321,-6.396943,-7.101569,-5.7154436,-2.5731456,-5.1762486,-5.847297,-4.4819884,-5.2881203,-0.18558952,-4.685332,-5.6502285,-6.2279205,-5.1312466,-5.3691325,-5.5372047,20.745226,20.777586,-2.8261914,-5.6161776,-8.66867,3.875792,-5.421963,-3.306884,-4.1297736,-4.3810477,-5.663248,-7.0385475,-5.944097,-9.280823,-7.2337837,-7.071512,-8.176446,-6.8363657,-7.2534523,-5.6703124,20.739853,20.777967,-5.1485753,-6.324007,-5.949033,-1.2899355,-5.728477,2.4780002,-4.816441,-3.8104496,-5.6373467,-4.6769495,-5.0809345,-5.6429567,-3.5225601,-3.9547467,-3.517458,-4.972919,-4.749822,-5.39649,-5.4244466,-4.4425488,-6.8422437,-11.757086,0.15175304,2.3174484,2.4681098,1.9885485,-9.906722,-8.966884,3.5492854,3.991315,-6.697743,-7.2161617,-7.042877,-6.208786,-5.533285,20.744783,20.779043,-3.1190562,-5.1590004,-5.869659,-4.942998,-3.6999743,-3.5695026,-4.5879974,0.15790181,-6.522899,-7.28128,-11.97935,-6.4382744,-6.560744,-6.2631087,-6.8457336,-6.6074696,-6.876352,-3.9798048,-6.2362933,-6.862277,-6.652577,-8.568552,-5.488157,-6.85407,-7.8286843,-7.6177554,-7.651276,-7.6321898,-7.848374,-8.3407,-8.275644,1.6304171,-7.8772297,-7.5711827,-8.558812,-7.9603906,-7.4352355,-7.382782,-5.6702814,-0.43355858,-3.2847543,-7.2695217,-9.711329,-5.6059194,-0.45297256,0.71411926,-3.019519,-4.5466876,-5.241725,-4.160917,-3.866338,-4.8946185,-3.23563,-5.1790175,-4.626815,-6.168333,-8.371506,-6.9427805,-6.99118,-4.2551064,-1.6602494,-4.288713,-5.672571,-5.5857606,-4.05043,-4.6742086,-5.441872,-6.74047,-7.0717263,-7.003073,-6.784655,3.832078,3.6412256,-7.110468,-6.641317,-6.843628,-8.230776,2.7925043,1.6848794,1.7193388,1.857576,-6.590005,-7.030656,-6.954208,-7.21286,-6.9503736,-7.0411024,-7.189992,-7.1261606,-7.0328164,-8.174526,-6.446907,3.8107414,-6.94355,-6.7680044,-7.0873733,-7.135561,-7.2108073,-7.0379586,-7.0838156,-7.433972,4.5392284,-7.306737,-7.200986,-6.9375725,-7.3009906,-7.020974,-7.470908,-6.852281,-7.320751,-7.4803514,-6.5567603,-7.3860803,-7.367792,-7.3030243,-7.110888,-7.9083433,-7.1771684,-6.2295866,-6.751093,-7.5260477,-6.52916,-6.946099,-7.4302897,-7.7207513,-7.149386,-7.0042157,-7.0773964,2.5341082,-6.693926,-6.428779,-6.575808,-5.613999,-6.7280927,-6.9356055,-7.3447347,-7.013829,-7.3360057,-6.3929605,-7.4417987,-6.9290123,-7.1902905,-6.133551,-6.7472014,-5.55936,-4.7772818,-7.1381664,-7.0138693,-6.3505325,-7.1175323,-7.3225074,-6.897524,-7.1241446,-7.1868844,-6.9801035,0.91532964,-6.7427797,-6.352446,4.319235,-7.1349525,-6.782073,-7.6146526,-6.930396,-7.0400977,-6.9232726,4.234,-6.773373,-6.453883,-6.1293716,-7.0004883,-6.572402,-6.695206,-6.8762627,-7.650266,-7.2218113,-7.443895,-7.190035,-6.743043,-7.16798,2.782874,-6.5048304,-6.348177,-6.7127924,-7.265565,-6.5665183,1.6793236,-6.603145,-7.327109,-6.43144,-6.7457614,-6.235866,-6.449469,-6.8719015,-6.982272,-6.7903633,-6.8472896,-6.0779643,-5.6369557,-2.8806472,-4.7079997,-2.8942056,-3.1273859,-2.6989605,-3.1579647,-4.909514,-5.411831,-3.8605795,-4.90375,-0.1378065,-4.4955378,-6.66998,-5.7723827,-5.447262,-3.8080142,-4.1933217,-4.603613,-7.7753353,-5.3729243,-5.093613,-2.9226534,-4.4093933,-3.3563228,-3.253236,-5.7537537,-7.7899055,-6.5888305,-6.2219367,-6.7433453,-6.6455083,-4.4677577,-5.7553368,-5.3988056,-5.645298,-5.740164,-5.8404512,-5.919844,-5.9765882,-6.670452,-6.859561,-7.1842628,-6.6780305,-7.7178483,-6.947131,-6.723241,-7.068432,-7.059563,-7.032048,-7.003667,-7.6228733,-7.026632,-6.581141,3.0062659,-8.153137,-7.684763,-8.098691,-8.260215,-8.38068,-8.041795,-8.349425,-7.0991044,0.05629144,-6.6482573,-6.1819677,-5.4410577,-5.4261117,-5.676278,-7.5974503,-6.2167006,-2.6263342,-2.8399909,-2.6272764,-5.074669,20.77832,-3.8102725,1.8196973,-6.8191977,-6.3096404,-4.3286796,-4.8911805,1.0270431,-6.366828,-6.610322,-7.077481,-7.022925,-8.333517,7.1217694,7.218658,7.4041414,-7.4415374,-8.119221,-8.082486,2.4997835,-9.604336,-6.412607,-8.231731,-6.750599,-8.163449,-9.007558,-7.9410424,-7.3387384,-5.392323,-7.4440584,-7.052375,-7.005995,-7.044836,-6.950674,-7.3562326,-7.165325,-7.102372,-7.8724384,-7.2903876,-7.3927917,-7.835809,-7.5064454,-7.2037168,-6.9123354,-7.022344,-7.498183,-7.7753224,-7.4795623,-7.8697624,-7.4793377,-7.5646567,-7.318723,-7.404801,-6.9329357,-6.943991,-7.5089,-6.8485017,-6.752451,-6.690566,-6.953632,-5.6942983,-3.426984,-3.9883745,-6.401954,-6.5090714,-6.0471673,-6.6522093,-6.5560155,-6.429638,-7.7848053,1.7639809,-11.569021,-7.4992228,-10.149965,-8.065282,0.059624244,1.243598,-8.195873,-8.277098,-6.4140525,-7.147472,-4.149048,-4.443234,-3.025809,-4.6333203,3.0671813,2.7497177,2.32775,-6.9214535,-6.6118603,-6.8291388,-6.370519,-3.8310177,-6.842467,-6.2162166,-11.700858,-7.900615],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Datasets server - worker\\n\\n\\u003e Workers that pre-compute and cache the response to \\u002fsplits, \\u002ffirst-rows,...\"],[\"- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker...\"],[\"- `WORKER_MAX_LOAD_PCT`: maximum load of the machine (in percentage: the max between the 1m load and...\"],[\"Also, it's possible to force the parent directory in which the temporary files (as the current job s...\"],[\"- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\\n\\nNo...\"],[\"Also, set the assets-related configuration for the first-rows worker. See [..\\u002f..\\u002flibs\\u002flibcommon\\u002fREAD...\"],[\"- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet file...\"],[\"- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet ...\"],[\"### Duckdb Index worker\\n\\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_I...\"],[\"### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistic...\"],[\"`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n\\u003cdet...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e \\n\\n##### float\\n\\nBin size for histogram is counted as `(max_value - min_value) \\u002f D...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e \\n\\n##### int\\n\\nAs bin edges for integer values also must be integers, bin size is ...\"],[\"```python\\n{\\n    \\\"column_name\\\": \\\"direction\\\",\\n    \\\"column_type\\\": \\\"int\\\",\\n    \\\"column_statistics\\\": {\\n   ...\"],[\"84,\\n                89,\\n                94,\\n                99,\\n                99\\n            ]\\n   ...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n##### string_label\\n\\nIf the number of unique values in a column (within request...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n##### bool\\n\\n\\u003cdetails\\u003e\\u003csummary\\u003eexample: \\u003c\\u002fsummary\\u003e\\n\\u003cp\\u003e\\n\\n```python\\n{\\n    'column_...\"],[\"--\\ntitle: Datasets Server Admin UI\\nemoji: ğŸ“Š\\ncolorFrom: gray\\ncolorTo: purple\\nsdk: gradio\\nsdk_version:...\"],[\"Filter rows in a dataset\\n\\nDatasets Server provides a `\\u002ffilter` endpoint for filtering rows in a data...\"],[\"```\\nwhere=age\\u003e30 AND (name='Simone' OR children=0)\\n```\\nwill filter the data to select only those row...\"],[\"List Parquet files\\n\\nDatasets can be published in any format (CSV, JSONL, directories of images, etc....\"],[\"The `\\u002fparquet` endpoint accepts the dataset name as its query parameter:\\n\\n\\u003cinferencesnippet\\u003e\\n\\u003cpython...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing a list of the dataset's ...\"],[\"```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"config\\\": \\\"ParaphraseRC\\\",\\n     ...\"],[\"\\\"size\\\": 3035735\\n    },\\n    {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"config\\\": \\\"SelfRC\\\",\\n      \\\"split\\\": \\\"tra...\"],[\"```\\n\\n## Sharded Parquet files\\n\\nBig datasets are partitioned into Parquet files (shards) of about 500...\"],[\"```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n      \\\"dataset\\\": \\\"amazon_polarity\\\",\\n      \\\"config\\\": \\\"amazon_pol...\"],[\"\\\"size\\\": 320587882\\n    },\\n    {\\n      \\\"dataset\\\": \\\"amazon_polarity\\\",\\n      \\\"config\\\": \\\"amazon_polarity\\\"...\"],[\"```\\n\\nTo read and query the Parquet files, take a look at the [Query datasets from Datasets Server](p...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003ccurl\\u003e\\n```curl\\ncurl https:\\u002f\\u002fhuggingface.co\\u002fapi\\u002fdatasets\\u002fduorc\\u002fparquet \\\\\\n        -X GET \\\\\\n ...\"],[\"```\\n\\nOptionally you can specify which configuration name to return, as well as which split:\\n\\n\\u003cinfere...\"],[\"datasets-server Helm chart\\n\\nThe `datasets-server` Helm [chart](https:\\u002f\\u002fhelm.sh\\u002fdocs\\u002ftopics\\u002fcharts\\u002f) ...\"],[\"How to contribute to the Datasets Server?\\n\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fCon...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   git checkout -b a-descr...\"],[\"DuckDB\\n\\n[DuckDB](https:\\u002f\\u002fduckdb.org\\u002fdocs\\u002f) is a database that supports reading and querying Parquet ...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nNow you can write and execute your SQL query on the Parquet file:\\n\\n\\u003ci...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nTo query multiple files - for example, if the dataset is sharded:\\n\\n\\u003ci...\"],[\"Overview\\n\\nDatasets Server automatically converts and publishes public datasets less than 5GB on the ...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fpandas.pydata.org\\u002fdocs\\u002findex.html) is a popular DataFrame library for data ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nYou can adapt the `BUILD_DIR` environment variable to set any temporary folder that you prefer....\"],[\"Check dataset validity\\n\\nBefore you download a dataset from the Hub, it is helpful to know if a speci...\"],[\"## Check if a dataset is valid\\n\\n`\\u002fis-valid` checks whether a specific dataset loads without any erro...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\n\\u003c!--\\nUse this section to tell people about which versions of...\"],[\"Datasets server admin machine\\n\\n\\u003e Admin endpoints\\n\\n## Configuration\\n\\nThe worker can be configured usi...\"],[\"### Prometheus\\n\\n- `PROMETHEUS_MULTIPROC_DIR`: the directory where the uvicorn workers share their pr...\"],[\"Get dataset information\\n\\nDatasets Server provides an `\\u002finfo` endpoint for exploring the general info...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON with the `dataset_info` key. Its st...\"],[\"```json\\n{\\n    \\\"dataset_info\\\": {\\n        \\\"description\\\": \\\"DuoRC contains 186,089 unique question-answe...\"],[\"\\\"major\\\": 1,\\n            \\\"minor\\\": 0,\\n            \\\"patch\\\": 0\\n        },\\n        \\\"splits\\\": {\\n          ...\"],[\"Datasets server\\n\\n\\u003e Integrate into your apps over 10,000 datasets via simple HTTP requests, with pre-...\"],[\"Download slices of rows\\n\\nDatasets Server provides a `\\u002frows` endpoint for visualizing any slice of ro...\"],[\"The `\\u002frows` endpoint accepts five query parameters:\\n\\n- `dataset`: the dataset name, for example `glu...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing two keys:\\n\\n- The [`featu...\"],[\"```json\\n\\u002f\\u002f https:\\u002f\\u002fdatasets-server.huggingface.co\\u002frows?dataset=duorc&config=SelfRC&split=train&offse...\"],[\"\\\"plot\\\": \\\"The film is centered on Mortal Kombat, a fighting tournament between the representatives of...\"],[\"any opponent of his choosing, anytime and anywhere he chooses. Raiden tries to intervene, but the co...\"],[\"\\\"title\\\": \\\"Mortal Kombat\\\",\\n        \\\"question_id\\\": \\\"40c1866a-b214-11ba-be57-8979d2cefa90\\\",\\n        \\\"qu...\"],[\"\\\"title\\\": \\\"Mortal Kombat\\\",\\n        \\\"question_id\\\": \\\"f1fdefcf-1191-b5f9-4cae-4ce4d0a59da7\\\",\\n        \\\"qu...\"],[\"```\\n\\n## Image and audio samples\\n\\nImage and audio are represented by a URL that points to the file.\\n\\n...\"],[\"Quickstart\\n\\n[[open-in-colab]]\\n\\nIn this quickstart, you'll learn how to use the Datasets Server's RES...\"],[\"| Endpoint                    | Method | Description                                             | Q...\"],[\"| [\\u002frows](.\\u002frows)             | GET    | Get a slice of rows of a dataset split.                 | -...\"],[\"There is no installation or setup required to use Datasets Server.\\n\\n\\u003cTip\\u003e\\n  Sign up for a \\u003ca href=\\\"h...\"],[\"```\\nhttps:\\u002f\\u002fdatasets-server.huggingface.co\\n```\\n\\n## Gated datasets\\n\\nFor gated datasets, you'll need t...\"],[\"```\\n\\n## Check dataset validity\\n\\nTo check whether a specific dataset is valid, for example, [Rotten T...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThis returns the first 100 rows of the dataset:\\n\\n```json\\n{\\n  \\\"datas...\"],[\"```\\n\\n## Download slices of a dataset\\n\\nThe `\\u002frows` endpoint returns a JSON list of a slice of rows of...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nYou can download slices of 100 rows maximum at a time.\\n\\nThe respons...\"],[\"```\\n\\n## Search text in a dataset\\n\\nThe `\\u002fsearch` endpoint returns a JSON list of a slice of rows of a...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nYou can get slices of 100 rows maximum at a time, and you can ask f...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003ccurl\\u003e\\n```curl\\ncurl https:\\u002f\\u002fdatasets-server.huggingface.co\\u002fparquet?dataset=rotten_tomatoes...\"],[\"```\\n\\n## Get the size of the dataset\\n\\nThe `\\u002fsize` endpoint returns a JSON with the size (number of ro...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThis returns a URL to the Parquet file for each split:\\n\\n```json\\n{\\n ...\"],[\"Datasets server SSE API\\n\\n\\u003e Server-sent events API for the Datasets server. It's used to update the H...\"],[\"libapi\\n\\nA Python library for the API services\\n\\n## Configuration\\n\\nThe APIs can be configured using en...\"],[\"- `API_HF_AUTH_PATH`: the path of the external authentication service, on the hub (see `HF_ENDPOINT`...\"],[\"- `API_MAX_AGE_LONG`: number of seconds to set in the `max-age` header on data endpoints. Defaults t...\"],[\"### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`API_UVIC...\"],[\"ClickHouse\\n\\n[ClickHouse](https:\\u002f\\u002fclickhouse.com\\u002fdocs\\u002fen\\u002fintro) is a fast and efficient column-orient...\"],[\"```\\n\\n## Aggregate functions\\n\\nNow you can begin to analyze the dataset. Use the `-q` argument to spec...\"],[\"```\\n\\nClickHouse also provides functions for visualizing your queries. For example, you can use the [...\"],[\"```\\n\\nRemember to set `enable_url_encoding` to 0 and `max_https_get_redirects` to 1 to redirect to th...\"],[\"```\\n\\nYou can make this even easier by creating another function that calls `hugging_paths` and outpu...\"],[\"```\\n\\nNow use the `hf` function to query any dataset by passing the dataset name:\\n\\n```bash\\nSELECT hor...\"],[\"Analyze a dataset on the Hub\\n\\n[[open-in-colab]]\\n\\nIn the Quickstart, you were introduced to various e...\"],[\"Use the `\\u002fparquet` endpoint to convert the dataset to a Parquet file and return the URL to it:\\n\\n```p...\"],[\"```\\n\\n## Read dataset with Pandas\\n\\nWith the URL, you can read the Parquet file into a Pandas DataFram...\"],[\"```\\n\\n|                                               src | complexity |                         prob...\"],[\"Polars \\n\\n[Polars](https:\\u002f\\u002fpola-rs.github.io\\u002fpolars-book\\u002fuser-guide\\u002f) is a fast DataFrame library wri...\"],[\"```\\n\\nTo read from a single Parquet file, use the [`read_parquet`](https:\\u002f\\u002fpola-rs.github.io\\u002fpolars\\u002fp...\"],[\"```\\n\\nTo read multiple Parquet files - for example, if the dataset is sharded - you'll need to use th...\"],[\"```\\n\\n## Lazy API\\n\\nPolars offers a [lazy API](https:\\u002f\\u002fpola-rs.github.io\\u002fpolars-book\\u002fuser-guide\\u002flazy\\u002fu...\"],[\"Explore statistics over split data\\n\\nDatasets Server provides a `\\u002fstatistics` endpoint for fetching s...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe response JSON contains two keys:\\n* `num_examples` - number of s...\"],[\"```json\\n{\\n  \\\"num_examples\\\": 8551,\\n  \\\"statistics\\\": [\\n    {\\n      \\\"column_name\\\": \\\"idx\\\",\\n      \\\"column_...\"],[\"```\\n\\n## Response structure by data type\\n\\nCurrently, statistics are supported for strings, float and ...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n### float\\n\\nThe following measures are returned for float data types:\\n\\n* minimu...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n### bool\\n\\nThe following measures are returned for bool data type:\\n\\n* number an...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n### string_text\\n\\nIf string column has more than 30 unique values within the re...\"],[\"libcommon\\n\\nA Python library with common code (cache, queue, workers logic, processing steps, configu...\"],[\"## Cached Assets configuration\\n\\nSet the cached-assets (images and audio files) environment variables...\"],[\"## Common configuration\\n\\nSet the common environment variables to configure the following aspects:\\n\\n-...\"],[\"## Queue configuration\\n\\nSet environment variables to configure the job queues to precompute API resp...\"],[\"Splits and configurations\\n\\nMachine learning datasets are commonly organized in *splits* and they may...\"],[\"Configurations are flexible, and can be used to organize a dataset along whatever objective you'd li...\"],[\"e2e\\n\\nEnd to end tests, written in Python...\"],[\"Preview a dataset\\n\\nDatasets Server provides a `\\u002ffirst-rows` endpoint for visualizing the first 100 r...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing two keys:\\n\\n- The [`featu...\"],[\"```json\\n{\\n  \\\"dataset\\\": \\\"duorc\\\",\\n  \\\"config\\\": \\\"SelfRC\\\",\\n  \\\"split\\\": \\\"train\\\",\\n  \\\"features\\\": [\\n    {\\n    ...\"],[\"}\\n  ],\\n  \\\"rows\\\": [\\n    {\\n      \\\"row_idx\\\": 0,\\n      \\\"row\\\": {\\n        \\\"plot_id\\\": \\\"\\u002fm\\u002f03vyhn\\\",\\n        ...\"],[\"\\\"no_answer\\\": false\\n      },\\n      \\\"truncated_cells\\\": []\\n    },\\n    {\\n      \\\"row_idx\\\": 1,\\n      \\\"row\\\"...\"],[\"```\\n\\n## Truncated responses\\n\\nFor some datasets, the response size from `\\u002ffirst-rows` may exceed 1MB,...\"],[\"```json\\n  ...\\n  \\\"rows\\\": [\\n    {\\n      \\\"row_idx\\\": 0,\\n      \\\"row\\\": {\\n        \\\"start\\\": \\\"2016-07-01T00:0...\"],[\"Search text in a dataset\\n\\nDatasets Server provides a `\\u002fsearch` endpoint for searching words in a dat...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing two keys (same format as...\"],[\"```json\\n{\\n  \\\"features\\\": [\\n    {\\n      \\\"feature_idx\\\": 0,\\n      \\\"name\\\": \\\"plot_id\\\",\\n      \\\"type\\\": { \\\"dt...\"],[\"\\\"plot\\\": \\\"The film begins with clips that track a telephone call between London and Geneva, where a u...\"],[\"they love each other. Kern disagrees. That evening Valentine is alone at home and hopes that her boy...\"],[\"leaves. Later, Auguste sees Karin and her new boyfriend in a restaurant. He gets her attention by ta...\"],[\"and Valentine and Auguste, who meet for the first time, as well as an English bartender named Stephe...\"],[\"\\\"title\\\": \\\"Three Colors: Red\\\",\\n        \\\"question_id\\\": \\\"7c583513-0b7f-ddb3-be43-64befc7e90cc\\\",\\n       ...\"],[\"\\\"title\\\": \\\"Three Colors: Red\\\",\\n        \\\"question_id\\\": \\\"80becb22-908d-84bc-3a5f-00b620d551bc\\\",\\n       ...\"],[\"```\\n\\nIf the result has `partial: true` it means that the search couldn't be run on the full dataset ...\"],[\"Developer guide\\n\\nThis document is intended for developers who want to install, test or contribute to...\"],[\"```\\n\\nIt will create a virtual environment in a `.\\u002f.venv\\u002f` subdirectory.\\n\\nIf you use VSCode, it might...\"],[\"The application is distributed in several components.\\n\\n[api](.\\u002fservices\\u002fapi) is a web server that ex...\"],[\"Note also that the workers create local files when the dataset contains images or audios. A shared d...\"],[\"## Quality\\n\\nThe CI checks the quality of the code through a [GitHub action](.\\u002f.github\\u002fworkflows\\u002f_qua...\"],[\"```\\n\\nTo check the quality (which includes checking the style, but also security vulnerabilities):\\n\\n`...\"],[\"```\\n\\nInstall Python 3.9.18:\\n\\n```bash\\n$ pyenv install 3.9.18\\n```\\n\\nCheck that the expected local versi...\"],[\"```\\n\\n#### Then: as a normal user\\n\\nAdd ICU to the path:\\n\\n```bash\\n$ echo 'export PATH=\\\"\\u002fopt\\u002fhomebrew\\u002fo...\"],[\"Server infrastructure\\n\\nThe [Datasets Server](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets-server) has two...\"],[\"Take a look at the [workers configuration](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets-server\\u002ftree\\u002fmain\\u002f...\"],[\"Datasets server maintenance job\\n\\n\\u003e Job to run maintenance actions on the datasets-server\\n\\nAvailable ...\"],[\"Datasets server - storage admin\\n\\n\\u003e A Ubuntu machine to log into and manage the storage manually...\"],[\"ğŸ¤— Datasets Server\\n\\nDatasets Server is a lightweight web API for visualizing and exploring all types ...\"],[\"\\u003cp style=\\\"text-align: center; font-style: italic; margin-top: 0;\\\"\\u003e\\n  Dataset viewer of the\\n  \\u003ca href...\"],[\"Get the number of rows and the size in bytes\\n\\nThis guide shows you how to use Datasets Server's `\\u002fsi...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing the size of the dataset,...\"],[\"```json\\n{\\n  \\\"size\\\": {\\n    \\\"dataset\\\": {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"num_bytes_original_files\\\": 9...\"],[\"},\\n      {\\n        \\\"dataset\\\": \\\"duorc\\\",\\n        \\\"config\\\": \\\"ParaphraseRC\\\",\\n        \\\"split\\\": \\\"test\\\",\\n  ...\"],[\"```\\n\\nIf the size has `partial: true` it means that the actual size of the dataset couldn't been dete...\"],[\"Datasets server API - rows endpoint\\n\\n\\u003e \\u002frows endpoint\\n\\n## Configuration\\n\\nThe service can be configur...\"],[\"Datasets server - reverse proxy\\n\\n\\u003e Reverse-proxy in front of the API\\n\\nSee [docker-compose-datasets-s...\"],[\"It takes various environment variables, all of them are mandatory:\\n\\n- `ASSETS_DIRECTORY`: the direct...\"],[\"Data types\\n\\nDatasets supported by Datasets Server have a tabular format, meaning a data point is rep...\"],[\"```\\n\\nThis dataset has two columns, `text` and `label`:\\n\\n- The `text` column has a type of `Value`. T...\"],[\"List splits and configurations\\n\\nDatasets typically have splits and may also have configurations. A _...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"Datasets server API\\n\\n\\u003e API on ğŸ¤— datasets\\n\\n## Configuration\\n\\nThe service can be configured using envi...\"],[\"Datasets server API - search service\\n\\n\\u003e \\u002fsearch endpoint\\n\\u003e \\u002ffilter endpoint\\n\\n## Configuration\\n\\nThe s...\"],[\"Datasets server databases migrations\\n\\n\\u003e Scripts to migrate the datasets server databases\\n\\n## Configu...\"]],\"hovertemplate\":\"source=datasets-server\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets-server, circle\",\"marker\":{\"color\":\"#636efa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets-server, circle\",\"showlegend\":true,\"x\":[2.9805372,2.337225,1.5212051,3.0902348,3.76512,3.1155467,3.4739547,3.4035916,3.221232,0.65094185,0.4906694,0.45077536,0.44757113,0.23620883,8.482835,0.56558585,0.94909656,14.841476,2.556013,0.9442035,3.2043257,3.1409242,3.8221385,3.0315397,3.4891157,8.40753,2.4403555,3.5034122,7.088848,3.115958,3.4749274,3.3540347,3.315422,3.7157362,3.6988854,2.9941416,2.4621675,3.0909894,3.0231786,2.676276,5.209687,4.8235846,3.2509701,2.998799,3.6661427,5.262407,3.2306476,2.4792366,3.2000942,3.1095154,3.0583918,1.6688124,3.2327843,3.0511718,2.97091,3.758793,2.869698,3.8722777,7.709611,7.7520924,7.706011,7.706002,2.391292,3.1916242,2.9663906,2.002889,3.6210756,3.412942,3.3534565,3.3266914,3.6118765,2.5809166,3.2384195,2.817811,2.9893558,3.0610526,3.716577,3.3384004,3.2187977,3.2665079,3.3081691,3.4927597,4.458272,2.2470992,3.0150983,2.9794555,2.9219363,2.719933,3.1318629,2.1998904,2.3900583,3.0619504,3.0727215,2.7596655,9.051233,2.7936976,2.6529627,2.6630485,2.6055808,3.15406,2.056596,3.024348,0.7585628,0.7205932,0.78355455,0.58857507,3.3955255,3.5882018,3.2211494,3.1877995,2.5432034,2.3724484,0.6393179,3.0295017,3.7731066,2.7187085,3.7456632,7.693068,7.6990504,1.3334882,3.3774214,2.842044,3.8248506,2.7203999,3.718562,7.627628,7.701762,7.7094855,7.642824,7.7552505,7.705903,0.8285816,3.0720296,3.3097336,3.5477345,3.4198296,2.4163086,2.7917933,1.1773195,1.3800694,3.2543046,3.29157,3.080743,3.4409728,3.2484043,3.3568435,3.125269,2.6889942,3.5170999,8.548361,0.7860263,3.2516372,4.1985345,3.8710747,1.417917,0.50556344,2.4539187,3.6324756,3.4146626,3.053625,3.1468775],\"xaxis\":\"x\",\"y\":[2.5901632,2.6683352,1.6442578,2.12521,1.869087,2.0534341,1.9405127,1.8213799,2.5059369,4.80617,4.568167,4.9979753,4.9381003,5.086108,3.6682818,5.0481453,4.5572023,3.4752865,4.402977,4.7378883,3.8797698,4.235309,4.3220296,4.471613,4.4793553,3.0387452,4.2346354,4.5544615,2.9567387,4.240761,4.344011,4.202846,3.3670797,2.2045486,1.1169399,4.068228,4.4959865,4.33269,4.2592816,4.2454796,0.22563197,1.793382,3.5514276,3.9659975,4.3029504,0.85527337,2.7358358,2.3544858,4.120821,4.284823,5.05036,4.121886,3.3798099,3.9729335,4.328985,4.431942,4.530009,4.437698,3.3385983,3.2694309,3.5627418,3.5164711,3.5539944,3.7460196,4.018717,4.46522,3.183309,3.736713,4.215593,4.411172,4.4452443,4.7345986,4.370598,4.720816,4.4495006,4.4913497,4.464726,4.4785204,4.3343563,4.468731,3.4496405,2.0990407,1.2598615,3.2504442,2.24276,4.0702853,4.4005294,4.3094125,4.3463097,3.5340507,4.4001346,4.0651145,4.1451154,4.047221,3.2927125,4.3206096,4.39782,4.3394914,4.3487244,4.182238,4.695314,4.652668,4.793862,5.026702,4.854021,4.8228025,2.1253572,2.0533962,2.705774,2.4826477,4.100737,3.452978,2.9014287,3.948057,4.357451,4.465822,4.4575887,3.4398186,3.4402754,4.271911,4.699699,4.249195,4.350782,4.467573,4.514427,3.2683036,3.3078592,3.3551934,3.320358,3.6060982,3.5767703,4.6295304,1.2247386,2.0139112,3.0405347,2.4602911,1.8476093,1.4362214,0.641055,0.6885947,3.259763,2.8369956,2.828612,2.7094188,3.7828648,3.530472,4.3403716,4.5738616,4.5874557,3.0583706,4.547356,3.6567097,2.8433263,1.8671074,4.226157,4.332745,4.2619967,4.410507,3.6583283,3.8279324,2.755848],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Differences between Dataset and IterableDataset\\n\\nThere are two types of dataset objects, a [`Dataset...\"],[\"```\\n\\nStreaming can read online data without writing any file to disk.\\nFor example, you can stream da...\"],[\"```\\n\\n## Loading local files entirely and progressively\\n\\nIt is possible to convert local or remote da...\"],[\"```\\n\\nOn the other hand, due to the \\\"lazy\\\" nature of an `IterableDataset`, calling [`IterableDataset....\"],[\"```\\n\\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use...\"],[\"```\\n\\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learni...\"],[\"```\\n\\n## Speed differences\\n\\nRegular [`Dataset`] objects are based on Arrow which provides fast random...\"],[\"```\\n\\n\\nIn this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approxi...\"],[\"```\\n\\nIf you want to shuffle your dataset or [use it with a PyTorch DataLoader](.\\u002fuse_with_pytorch#st...\"],[\"Metric Card for F1\\n\\n\\n## Metric Description\\n\\nThe F1 score is the harmonic mean of the precision and r...\"],[\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of ...\"],[\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending ...\"],[\"```\\n```python\\n{'f1': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a si...\"],[\"```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e pr...\"],[\"Cache management\\n\\nWhen you download a dataset, the processing scripts and data are stored locally on...\"],[\"```\\n\\n## Enable or disable caching\\n\\nIf you're using a cached file locally, it will automatically relo...\"],[\"Metric Card for MSE\\n\\n\\n## Metric Description\\n\\nMean Squared Error(MSE) represents the average of the s...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mse': array([0.41666667, 1. ])}\\n```\\n\\n#### Values fro...\"],[\"```\\n\\n## Limitations and Bias\\nMSE has the disadvantage of heavily weighting outliers -- given that it...\"],[\"All about metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in ğŸ¤— Datasets. To learn more about ho...\"],[\"A common way to overcome this issue is to fallback on single process evaluation. The metrics are eva...\"],[\"Metric Card for METEOR\\n\\n## Metric description\\n\\nMETEOR (Metric for Evaluation of Translation with Exp...\"],[\"```\\n\\n## Output values\\n\\nThe metric outputs a dictionary containing the METEOR score. Its values range...\"],[\"```\\n\\n## Limitations and bias\\n\\nWhile the correlation between METEOR and human judgments was measured ...\"],[\"Preprocess\\n\\nIn addition to loading datasets, ğŸ¤— Datasets other main goal is to offer a diverse set of...\"],[\"```\\n\\n**2**. Call your tokenizer on the first row of `text` in the dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e tokenizer(data...\"],[\"```\\n\\nThe tokenizer returns a dictionary with three items:\\n\\n- `input_ids`: the numbers representing t...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nUse the [`~Dataset.to_tf_dataset`] function to set the dataset format to be compatibl...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n**5**. The dataset is now ready for training with your machine learni...\"],[\"```\\n\\n**2**. Index into the first row of the dataset. When you call the `audio` column of the dataset...\"],[\"```\\n\\n**3**. Reading a dataset card is incredibly useful and can give you a lot of information about ...\"],[\"```\\n\\n**4**. Use the [`~Dataset.map`] function to resample the entire dataset to 16kHz. This function...\"],[\"```\\n\\n**2**. Index into the first row of the dataset. When you call the `image` column of the dataset...\"],[\"Image classification\\n\\nImage classification datasets are used to train a model to classify an entire ...\"],[\"```\\n\\nThe dataset has three fields:\\n\\n* `image`: a PIL image object.\\n* `image_file_path`: the path to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.c...\"],[\"Beam Datasets\\n\\nSome datasets are too large to be processed on a single machine. Instead, you can pro...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nWhen you run your pipeline, you can adjust the parameters to change the runner (Flink or...\"],[\"Load image data\\n\\nImage datasets have [`Image`] type columns, which contain PIL objects. \\n\\n\\u003cTip\\u003e\\n\\nTo ...\"],[\"```\\n\\nIf you only want to load the underlying path to the image dataset without decoding the image ob...\"],[\"```\\n\\nLoad your dataset by specifying `imagefolder` and the directory of your dataset in `data_dir`:\\n...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor more information about creating your own `ImageFolder` dataset, take a look at the [...\"],[\"Metric Card for Recall\\n\\n\\n## Metric Description\\n\\nRecall is the fraction of the positive examples that...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): The predicted labels.\\n- **references** (`list` of `i...\"],[\"- **zero_division** (): Sets the value to return when there is a zero division. Defaults to .\\n    - ...\"],[\"### Output Values\\n- **recall**(`float`, or `array` of `float`, for multiclass targets): Either the g...\"],[\"```\\n```python\\n{'recall': array([1., 0., 0.])}\\n```\\n\\nThis metric outputs a dictionary with one entry, ...\"],[\"```\\n\\nExample 4-A multiclass example, using different averages.\\n```python\\n\\u003e\\u003e\\u003e recall_metric = dataset...\"],[\"Dataset features\\n\\n[`Features`] defines the internal structure of a dataset. It is used to specify th...\"],[\"```\\n\\nThe [`Value`] feature tells ğŸ¤— Datasets:\\n\\n- The `idx` data type is `int32`.\\n- The `sentence1` an...\"],[\"```\\n\\nThe `answers` field is constructed using the [`Sequence`] feature because it contains two subfi...\"],[\"```\\n\\n## Audio feature\\n\\nAudio datasets have a column with type [`Audio`], which contains three import...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIndex into an audio dataset using the row index first and then the `audio...\"],[\"```\\n\\n## Image feature\\n\\nImage datasets have a column with type [`Image`], which loads `PIL.Image` obj...\"],[\"```\\n\\nDepending on the dataset, you may get the path to the local downloaded image, or the content of...\"],[\"Metric Card for GLUE\\n\\n## Metric description\\nThis metric is used to compute the GLUE evaluation metri...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the GLUE subset chosen, consisting of a di...\"],[\"### Values from popular papers\\nThe [original GLUE paper](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fglue) repor...\"],[\"```\\n\\nMinimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfrom d...\"],[\"```\\n    \\n## Further References \\n\\n- [GLUE benchmark homepage](https:\\u002f\\u002fgluebenchmark.com\\u002f)\\n- [Fine-tun...\"],[\"Metric Card for Matthews Correlation Coefficient\\n\\n## Metric Description\\nThe Matthews correlation coe...\"],[\"```\\n\\nThe same example as above, but also including sample weights:\\n```python\\n\\u003e\\u003e\\u003e matthews_metric = d...\"],[\"```\\n\\n## Further References\\n\\n- This Hugging Face implementation uses [this scikit-learn implementatio...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"All community leaders are obligated to respect the privacy and security of the\\nreporter of any incid...\"],[\"Community Impact Guidelines were inspired by \\n[Mozilla's code of conduct enforcement ladder][Mozilla...\"],[\"Table Classes\\n\\nEach `Dataset` object is backed by a PyArrow Table.\\nA Table can be loaded from either...\"],[\"## ConcatenationTable\\n\\n[[autodoc]] datasets.table.ConcatenationTable\\n    - validate\\n    - equals\\n   ...\"],[\"Metric Card for Precision\\n\\n\\n## Metric Description\\n\\nPrecision is the fraction of correctly labeled po...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels.\\n- **references** (`list` of ...\"],[\"- **zero_division** (): Sets the value to return when there is a zero division. Defaults to .\\n    - ...\"],[\"### Output Values\\n- **precision**(`float` or `array` of `float`): Precision score or list of precisi...\"],[\"```\\n```python\\n{'precision': array([0.66666667, 0.0, 0.0])}\\n```\\n\\n\\n\\n\\n#### Values from Popular Papers\\n\\n...\"],[\"```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e pr...\"],[\"```\\n\\n\\n## Limitations and Bias\\n\\n[Precision](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fprecision) and [recall](ht...\"],[\"Load tabular data\\n\\nA tabular dataset is a generic dataset used to describe any data stored in rows a...\"],[\"```\\n\\nTo load zipped CSV files:\\n\\n```py\\n\\u003e\\u003e\\u003e url = \\\"https:\\u002f\\u002fdomain.org\\u002ftrain_data.zip\\\"\\n\\u003e\\u003e\\u003e data_files =...\"],[\"```\\n\\nIf the dataset doesn't look as expected, you should explicitly [specify your dataset features](...\"],[\"```\\n\\nThis creates a `states` table in the `us_covid_data.db` database which you can now load into a ...\"],[\"```\\n\\nThen you can use all of ğŸ¤— Datasets process features like [`~datasets.Dataset.filter`] for examp...\"],[\"--\\nTODO: Add YAML tags here. Copy-paste the tags obtained with the online tagging app: https:\\u002f\\u002fhuggi...\"],[\"[More Information Needed]\\n\\n### Data Splits\\n\\n[More Information Needed]\\n\\n## Dataset Creation\\n\\n### Cura...\"],[\"Use with Spark\\n\\nThis document is a quick introduction to using ğŸ¤— Datasets with Spark, with a particu...\"],[\"```\\n\\n### Caching\\n\\nWhen using [`Dataset.from_spark`], the resulting [`Dataset`] is cached; if you cal...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import Dataset, Features, Image, Value\\n\\u003e\\u003e\\u003e data = [(0, open(\\\"image.png\\\", \\\"rb...\"],[\"```\\n\\nYou can check the [`Features`] documentation to know about all the feature types available....\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cpicture\\u003e\\n    \\u003csource media=\\\"(prefers-color-scheme: dark)\\\" srcset=\\\"https:\\u002f\\u002fhuggi...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets\\u002factions\\u002fworkflows\\u002fci.yml?que...\"],[\"\\u003cimg alt=\\\"Contributor Covenant\\\" src=\\\"https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20Covenant-2.0-4baaaa....\"],[\"ğŸ¤— Datasets is a lightweight library providing **two** main features:\\n\\n- **one-line dataloaders for m...\"],[\"ğŸ¤— Datasets is designed to let the community easily add and share new datasets.\\n\\nğŸ¤— Datasets has many ...\"],[\"```\\n\\n## With conda\\n\\nğŸ¤— Datasets can be installed using conda as follows:\\n\\n```bash\\nconda install -c hu...\"],[\"```\\n\\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\\n...\"],[\"```\\n\\nIf your dataset is bigger than your disk or if you don't want to wait to download the data, you...\"],[\"```\\n\\nFor more details on using the library, check the quick start page in the documentation: https:\\u002f...\"],[\"# Main differences between ğŸ¤— Datasets and `tfds`\\n\\nIf you are familiar with the great TensorFlow Data...\"],[\"```bibtex\\n@inproceedings{lhoest-etal-2021-datasets,\\n    title = \\\"Datasets: A Community Library for N...\"],[\"publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https:\\u002f\\u002faclanthology.org\\u002f2021.em...\"],[\"```\\n\\nIf you need to cite a specific version of our ğŸ¤— Datasets library for reproducibility, you can u...\"],[\"The cache\\n\\nThe cache is one of the reasons why ğŸ¤— Datasets is so efficient. It stores previously down...\"],[\"```\\n\\nIn order for a transform to be hashable, it needs to be picklable by [dill](https:\\u002f\\u002fdill.readth...\"],[\"```\\n\\nThe hash is computed by dumping the object using a `dill` pickler and hashing the dumped bytes....\"],[\"Metric Card for chrF(++)\\n\\n\\n## Metric Description\\nChrF and ChrF++ are two MT evaluation metrics that ...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `str`): The predicted sentences.\\n- **`references`** (...\"],[\"The output is formatted as below:\\n```python\\n{'score': 61.576379378113785, 'char_order': 6, 'word_ord...\"],[\"```\\n\\nThe chrF(++) score can be any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Pop...\"],[\"```\\n\\nThe same chrF++ example as above, but with `lowercase=True` to normalize all case:\\n```python\\n\\u003e\\u003e...\"],[\"```\\n\\n\\n## Limitations and Bias\\n- According to [PopoviÄ‡ 2017](https:\\u002f\\u002fwww.statmt.org\\u002fwmt17\\u002fpdf\\u002fWMT70.p...\"],[\"## Citation\\n```bibtex\\n@inproceedings{popovic-2015-chrf,\\n    title = \\\"chr{F}: character n-gram {F}-sc...\"],[\"```\\n\\n## Further References\\n- See the [sacreBLEU README.md](https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#chrf-...\"],[\"Metric Card for BERT Score\\n\\n## Metric description\\n\\nBERTScore is an automatic evaluation metric for t...\"],[\"```\\n\\nBERTScore also accepts multiple optional arguments: \\n\\n\\n`num_layers` (int): The layer of represe...\"],[\"### Values from popular papers\\nThe [original BERTScore paper](https:\\u002f\\u002fopenreview.net\\u002fpdf?id=SkeHuCVF...\"],[\"```\\n\\nPartial match with the `bert-base-uncased` model:\\n\\n```python\\nfrom datasets import load_metric\\nb...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe [original BERTScore paper](https:\\u002f\\u002fopenreview.net\\u002fpdf?id=SkeHuCVFD...\"],[\"Metric Card for ROUGE\\n\\n## Metric Description\\nROUGE, or Recall-Oriented Understudy for Gisting Evalua...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n        sh...\"],[\"```python\\n{'rouge1': [Score(precision=1.0, recall=0.5, fmeasure=0.6666666666666666), Score(precision...\"],[\"```\\n\\nIf `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=True`, the output is of the following...\"],[\"```\\n\\nThe same example, but with aggregation:\\n```python\\n\\u003e\\u003e\\u003e rouge = datasets.load_metric('rouge')\\n\\u003e\\u003e\\u003e...\"],[\"Metric Card for Exact Match\\n\\n\\n## Metric Description\\nA given predicted string's exact match score is ...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `str`): List of predicted texts.\\n- **`references`** (...\"],[\"```\\n\\nThis metric's range is 0-100, inclusive. Here, 0.0 means no prediction\\u002freference pairs were mat...\"],[\"```\\nNote that in the example above, because the regexes are ignored before the case is normalized, \\\"...\"],[\"```\\n\\nAn example that includes sentences:\\n```python\\n\\u003e\\u003e\\u003e exact_match = datasets.load_metric(\\\"exact_mat...\"],[\"Metric Card for COMET\\n\\n## Metric description\\n\\nCrosslingual Optimized Metric for Evaluation of Transl...\"],[\"```\\n\\nIt has several configurations, named after the COMET model to be used. It will default to `wmt2...\"],[\"## Examples\\n\\nFull match:\\n\\n```python\\nfrom datasets import load_metric\\ncomet_metric = load_metric('com...\"],[\"```\\n\\nPartial match:\\n\\n```python\\nfrom datasets import load_metric\\ncomet_metric = load_metric('comet') ...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe models provided for calculating the COMET metric are built on top ...\"],[\"## Citation\\n\\n```bibtex\\n@inproceedings{rei-EtAl:2020:WMT,\\n   author    = {Rei, Ricardo  and  Stewart,...\"],[\"```\\n\\n```bibtex\\n@inproceedings{rei-etal-2020-comet,\\n   title = \\\"{COMET}: A Neural Framework for {MT} ...\"],[\"Metric Card for seqeval\\n\\n## Metric description\\n\\nseqeval is a Python framework for sequence labeling ...\"],[\"```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqeval = load_metric('seqeval')\\n\\u003e\\u003e\\u003e predictions ...\"],[\"```\\n\\n## Output values\\n\\nThis metric returns a dictionary with a summary of scores for overall and per...\"],[\"## Examples \\n\\nMaximal values (full match) :\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqe...\"],[\"```\\n\\nMinimal values (no match):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqeval = load_m...\"],[\"```\\n\\nPartial match:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqeval = load_metric('seqev...\"],[\"```\\n\\n## Limitations and bias\\n\\nseqeval supports following IOB formats (short for inside, outside, beg...\"],[\"Utilities\\n\\n## Configure logging\\n\\nğŸ¤— Datasets strives to be transparent and explicit about how it work...\"],[\"```\\n\\nAll the methods of this logging module are documented below. The main ones are:\\n\\n- [`logging.ge...\"],[\"[[autodoc]] datasets.utils.enable_progress_bars\\n\\n[[autodoc]] datasets.utils.disable_progress_bars\\n\\n[...\"],[\"Use with PyTorch\\n\\nThis document is a quick introduction to using `datasets` with PyTorch, with a par...\"],[\"```\\n\\n## N-dimensional arrays\\n\\nIf your dataset consists of N-dimensional arrays, you will see that by...\"],[\"```\\n\\n\\n## Other feature types\\n\\n[`ClassLabel`] data are properly converted to tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n\\nString and binary objects are unchanged, since PyTorch only supports numbers.\\n\\nThe [`Image`] an...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\\n`pip inst...\"],[\"```\\n\\n## Data loading\\n\\nLike `torch.utils.data.Dataset` objects, a [`Dataset`] can be passed directly ...\"],[\"```\\n\\n### Optimize data loading\\n\\nThere are several ways you can increase the speed your data is loade...\"],[\"```\\n\\n### Stream data\\n\\nStream a dataset by loading it as an [`IterableDataset`]. This allows you to p...\"],[\"```\\n\\nIn this case each worker is given a subset of the list of shards to stream from.\\n\\n### Distribut...\"],[\"Semantic segmentation\\n\\nSemantic segmentation datasets are used to train a model to classify every pi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"\\u003e\\u003e\\u003e visualize_seg_mask(\\n...     np.array(dataset[index][\\\"image\\\"]),\\n...     np.array(dataset[index][\\\"...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nYou can verify the transformation worked by indexing into the `pixel_values` and `label` of an ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"Metric Card for *Current Metric*\\n\\n***Metric Card Instructions:*** *Copy this file into the relevant ...\"],[\"Metric Card for WikiSplit\\n\\n## Metric description\\n\\nWikiSplit is the combination of three metrics: [SA...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary containing three scores:\\n\\n`sari`: the [SARI](...\"],[\"```\\n\\n### Values from popular papers\\n\\nThis metric was initially used by [Rothe et al.(2020)](https:\\u002f\\u002f...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e ...\"],[\"Metric Card for FrugalScore\\n\\n\\n## Metric Description\\nFrugalScore is a reference-based metric for Natu...\"],[\"```\\n\\n### Values from popular papers\\nThe [original FrugalScore paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.0855...\"],[\"How to add one new datasets\\n\\nAdd datasets directly to the ğŸ¤— Hugging Face Hub!\\n\\nYou can share your da...\"],[\"Metric Card for Google BLEU (GLEU)\\n\\n\\n## Metric Description\\nThe BLEU score has some undesirable prope...\"],[\"```\\n\\n### Inputs\\n- **predictions** (list of list of str): list of translations to score. Each transla...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nExample with one reference per sample:\\n```python...\"],[\"```\\n\\nExample with multiple references for the first sample:\\n```python\\n\\u003e\\u003e\\u003e hyp1 = ['It', 'is', 'a', '...\"],[\"\\u003e\\u003e\\u003e list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n\\u003e\\u003e\\u003e hypotheses = [hyp1, hyp2]\\n\\u003e\\u003e\\u003e google_b...\"],[\"```\\n\\nExample with multiple references for the first sample, and with `min_len` adjusted to `2`, inst...\"],[\"\\u003e\\u003e\\u003e list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n\\u003e\\u003e\\u003e hypotheses = [hyp1, hyp2]\\n\\u003e\\u003e\\u003e google_b...\"],[\"```\\n\\nExample with multiple references for the first sample, with `min_len` adjusted to `2`, instead ...\"],[\"\\u003e\\u003e\\u003e hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\\n...         'interested', 'in', 'wo...\"],[\"```\\n\\n## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@misc{wu2016googles,\\ntitle={Google's Neural Mac...\"],[\"# Add Dummy data test\\n\\n**Important** In order to pass the `load_dataset_\\u003cdataset_name\\u003e` test, dummy ...\"],[\"Here we have to pay close attention to the ``_split_generators(self, dl_manager)`` function of the d...\"],[\"**Note** if ``\\u003cvalue_of_dict\\u003e`` is a zipped file then the dummy data folder structure should contain...\"],[\"Main classes\\n\\n\\n## DatasetInfo\\n\\n[[autodoc]] datasets.DatasetInfo\\n\\n## Dataset\\n\\nThe base class [`Datase...\"],[\"[[autodoc]] datasets.concatenate_datasets\\n\\n[[autodoc]] datasets.interleave_datasets\\n\\n[[autodoc]] dat...\"],[\"## IterableDatasetDict\\n\\nDictionary with split names as keys ('train', 'test' for example), and `Iter...\"],[\"Metric Card for COVAL\\n\\n## Metric description\\n\\nCoVal is a coreference evaluation tool for the [CoNLL]...\"],[\"```python\\nfrom datasets import load_metric\\ncoval = load_metric('coval')\\nwords = ['bc\\u002fcctv\\u002f00\\u002fcctv_00...\"],[\"```\\nIt also has several optional arguments:\\n\\n`keep_singletons`: After extracting all mentions of key...\"],[\"### Values from popular papers\\n\\nGiven that many of the metrics returned by COVAL come from different...\"],[\"## Examples \\n\\nMaximal values\\n\\n```python\\nfrom datasets import load_metric\\ncoval = load_metric('coval'...\"],[\"```\\n\\n## Limitations and bias\\n\\nThis wrapper of CoVal currently only works with [CoNLL line format](ht...\"],[\"| Column | Type                  | Description                                                      ...\"],[\"| 5      | Part-of-Speech        |                                                                  ...\"],[\"## Citations\\n\\n```bibtex\\n@InProceedings{moosavi2019minimum,\\n  author = { Nafise Sadat Moosavi, Leo Bo...\"],[\"```\\n```bibtex\\n@inproceedings{10.3115\\u002f1072399.1072405,\\n  author = {Vilain, Marc and Burger, John and ...\"],[\"```\\n\\n```bibtex\\n@inproceedings{moosavi-strube-2016-coreference,\\n    title = \\\"Which Coreference Evalua...\"],[\"Overview\\n\\nThe how-to guides offer a more comprehensive overview of all the tools ğŸ¤— Datasets offers a...\"],[\"Metric Card for SacreBLEU\\n\\n\\n## Metric Description\\nSacreBLEU provides hassle-free computation of shar...\"],[\"### Inputs\\n- **`predictions`** (`list` of `str`): list of translations to score. Each translation sh...\"],[\"- `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https:\\u002f\\u002fpypi.org\\u002fproject\\u002fmecab-pyt...\"],[\"### Output Values\\n- `score`: BLEU score\\n- `counts`: Counts\\n- `totals`: Totals\\n- `precisions`: Precis...\"],[\"```\\nThe score can take any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Popular Pap...\"],[\"Cloud storage\\n\\nğŸ¤— Datasets supports access to cloud storage providers through a `fsspec` FileSystem i...\"],[\"```\\n\\u003e\\u003e\\u003e pip install s3fs\\n```\\n\\n2. Define your credentials\\n\\nTo use an anonymous connection, use `anon=...\"],[\"```\\n\\n### Azure Blob Storage\\n\\n1. Install the Azure Blob Storage implementation:\\n\\n```\\n\\u003e\\u003e\\u003e conda instal...\"],[\"```\\n\\n3. Create your FileSystem instance\\n\\n```py\\n\\u003e\\u003e\\u003e import ocifs\\n\\u003e\\u003e\\u003e fs = ocifs.OCIFileSystem(**stora...\"],[\"```\\n\\nUse your own data files (see [how to load local and remote files](.\\u002floading#local-and-remote-fi...\"],[\"```\\n\\n#### Dask\\n\\nDask is a parallel computing library and it has a pandas-like API for working with l...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nRemember to define your credentials in your [FileSystem instance](#set-up-your-cloud-sto...\"],[\"Metric Card for BLEU\\n\\n\\n## Metric Description\\nBLEU (Bilingual Evaluation Understudy) is an algorithm ...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list`): Translations to score. Each translation should be tokeni...\"],[\"```\\n\\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidat...\"],[\"### Examples\\n\\nExample where each sample has 1 reference:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\n...     [\\\"hel...\"],[\"```\\n\\nExample where the first sample has 2 references:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\n...     [\\\"hello\\\"...\"],[\"```\\n\\n## Limitations and Bias\\nThis metric hase multiple known limitations and biases:\\n- BLEU compares...\"],[\"Create an image dataset\\n\\nThere are two methods for creating and sharing an image dataset. This guide...\"],[\"```\\n\\nYou can also use `imagefolder` to load datasets involving multiple splits. To do so, your datas...\"],[\"```\\n\\nor using `metadata.jsonl`:\\n\\n```jsonl\\n{\\\"file_name\\\": \\\"0001.png\\\", \\\"additional_feature\\\": \\\"This is a...\"],[\"```\\n\\n### Object detection\\n\\nObject detection datasets have bounding boxes and categories identifying ...\"],[\"```\\n\\n### Upload dataset to the Hub\\n\\nOnce you've created a dataset, you can share it to the Hub with ...\"],[\"```\\n\\nYou can put your images labels\\u002fcaptions\\u002fbounding boxes using JSON or text files for example.\\n\\nF...\"],[\"```\\n\\nThis guide will show you how to create a dataset loading script for image datasets, which is a ...\"],[\"```\\n\\n#### Multiple configurations\\n\\nIn some cases, a dataset may have more than one configuration. Fo...\"],[\"```\\n\\nNow you can define your subsets at the top of [`GeneratorBasedBuilder`]. Imagine you want to cr...\"],[\"```\\n\\n### Add dataset metadata\\n\\nAdding information about your dataset is useful for users to learn mo...\"],[\"```\\n\\n### Download and define the dataset splits\\n\\nNow that you've added some information about your d...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```py\\ndef _split_generators(self, dl_manager):\\n    archive_path = dl_manager.download(_BASE_...\"],[\"```\\n\\n### Generate the dataset\\n\\nThe last method in the [`GeneratorBasedBuilder`] class actually gener...\"],[\"```\\n\\nIf your loading script passed the test, you should now have the `dataset_info` YAML fields in t...\"],[\"Create a dataset card\\n\\nEach dataset should have a dataset card to promote responsible usage and info...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n3. Click on the **Import dataset card template** link to automatically create a template wit...\"],[\"Process\\n\\nğŸ¤— Datasets provides many tools for modifying the structure and content of a dataset. These ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nAll processing methods in this guide return a new [`Dataset`] object. Mod...\"],[\"```\\n\\nShuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to create an indices ...\"],[\"```\\n\\n- [`~Dataset.filter`] returns rows that match a specified condition:\\n\\n```py\\n\\u003e\\u003e\\u003e start_with_ar =...\"],[\"```\\n\\nUnless the list of indices to keep is contiguous, those methods also create an indices mapping ...\"],[\"```\\n\\nAfter sharding the dataset into four chunks, the first shard will only have 6250 examples:\\n\\n```...\"],[\"```\\n\\n### Remove\\n\\nWhen you need to remove one or more columns, provide the column name to remove to t...\"],[\"```\\n\\n### Cast\\n\\nThe [`~Dataset.cast`] function transforms the feature type of one or more columns. Th...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCasting only works if the original feature type and new feature type are compatible. For...\"],[\"```\\n\\nThe `answers` field contains two subfields: `text` and `answer_start`. Use the [`~Dataset.flatt...\"],[\"```\\n\\nNow use [`~Dataset.map`] to apply the `add_prefix` function to the entire dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e u...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nğŸ¤— Datasets also has a [`~Dataset.remove_columns`] function which is faster because it do...\"],[\"```\\n\\nThe [`~Dataset.map`] also works with the rank of the process if you set `with_rank=True`. This ...\"],[\"```\\n\\n### Batch processing\\n\\nThe [`~Dataset.map`] function supports working with batches of examples. ...\"],[\"```\\n\\nNotice how the sentences are split into shorter chunks now, and there are more rows in the data...\"],[\"```\\n\\nUse [`~Dataset.map`] to apply the function over the whole dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e augmented_dataset...\"],[\"```\\n\\nFor each original sentence, RoBERTA augmented a random word with three alternatives. The origin...\"],[\"```\\n\\n### Distributed usage\\n\\nWhen you use [`~Dataset.map`] in a distributed setting, you should also ...\"],[\"```\\n\\nYou can also concatenate two datasets horizontally by setting `axis=1` as long as the datasets ...\"],[\"```\\n\\nYou can also specify the `stopping_strategy`. The default strategy, `first_exhausted`, is a sub...\"],[\"```\\n\\nThe [`~Dataset.with_format`] function also changes the format of a column, except it returns a ...\"],[\"```\\n\\n### Format transform\\n\\nThe [`~Dataset.set_transform`] function applies a custom formatting trans...\"],[\"```\\n\\nYou can also use the [`~Dataset.set_transform`] function to decode formats not supported by [`F...\"],[\"```\\n\\n## Save\\n\\nOnce you are done processing your dataset, you can save and reuse it later with [`~Dat...\"],[\"Metric Card for SQuAD v2\\n\\n## Metric description\\nThis metric wraps the official scoring script for ve...\"],[\"For more recent model performance, see the [dataset leaderboard](https:\\u002f\\u002fpaperswithcode.com\\u002fdataset\\u002f...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom datasets import load_met...\"],[\"```\\n\\nPartial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom datasets import load_metric\\nsquad...\"],[\"Metric Card for Perplexity\\n\\n## Metric Description\\nGiven a model and an input text sequence, perplexi...\"],[\"```\\n\\n### Inputs\\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on...\"],[\"```\\n\\nThis metric's range is 0 and up. A lower score is better.\\n\\n#### Values from Popular Papers\\n\\n\\n##...\"],[\"```\\n\\n## Limitations and Bias\\nNote that the output value is based heavily on what text the model was ...\"],[\"Overview\\n\\nWelcome to the ğŸ¤— Datasets tutorials! These beginner-friendly tutorials will guide you thro...\"],[\"Metric Card for Pearson Correlation Coefficient (pearsonr)\\n\\n\\n## Metric Description\\n\\nPearson correlat...\"],[\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels, as returned by a model...\"],[\"```\\n\\nExample 2-The same as Example 1, but that also returns the `p-value`.\\n```python\\n\\u003e\\u003e\\u003e pearsonr_me...\"],[\"Load audio data\\n\\nYou can load an audio dataset using the [`Audio`] feature that automatically decode...\"],[\"```\\nfolder\\u002ftrain\\u002fmetadata.csv\\nfolder\\u002ftrain\\u002ffirst_audio_file.mp3\\nfolder\\u002ftrain\\u002fsecond_audio_file.mp3\\nf...\"],[\"```\\n\\nYou can load remote datasets from their URLs with the data_files parameter:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor more information about creating your own `AudioFolder` dataset, take a look at the [...\"],[\"Search index\\n\\n[FAISS](https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002ffaiss) and [Elasticsearch](https:\\u002f\\u002fwww.ela...\"],[\"```\\n\\n3. Create the index with [`Dataset.add_faiss_index`]:\\n\\n```py\\n\\u003e\\u003e\\u003e ds_with_embeddings.add_faiss_i...\"],[\"```\\n\\n6. When you are done querying, save the index on disk with [`Dataset.save_faiss_index`]:\\n\\n```py...\"],[\"```\\n\\n4. If you want to reuse the index, define the `es_index_name` parameter when you build the inde...\"],[\"```\\n\\nFor more advanced Elasticsearch usage, you can specify your own configuration with custom setti...\"],[\"How to contribute to Datasets?\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20C...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n    ```bash\\n    git checkout -b a-des...\"],[\"```\\n\\n   Go the webpage of your fork on GitHub. Click on \\\"Pull request\\\" to send your to the project m...\"],[\"If you are a **user of a dataset**, the main source of information should be the dataset paper if it...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-4\\\"\\u003e\\n   \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eTokenize a dataset and get it ready for a model to determine whether a pair...\"],[\"\\u003cTip\\u003e\\n\\nCheck out [Chapter 5](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fchapter5\\u002f1?fw=pt) of the Hugging Face cou...\"],[\"```\\n\\nğŸ¤— Datasets also support audio and image data formats:\\n\\n* To work with audio datasets, install t...\"],[\"```\\n\\n**2**. Next, load a pretrained [Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fwav2vec2-base) model ...\"],[\"```\\n\\n**3**. The [MInDS-14](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fPolyAI\\u002fminds14) dataset card indicates th...\"],[\"```\\n\\n**4**. Create a function to preprocess the audio `array` with the feature extractor, and trunca...\"],[\"```\\n\\n**6**. Set the dataset format according to the machine learning framework you're using.\\n\\n\\u003cframe...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n**7**. Start training with your machine learning framework! Check out...\"],[\"```\\n\\n**2**. Now you can add some data augmentations with any library ([Albumentations](https:\\u002f\\u002falbum...\"],[\"```\\n\\n**5**. Set the dataset format according to the machine learning framework you're using.\\n\\n\\u003cframe...\"],[\"```\\n\\n```py\\n\\u003e\\u003e\\u003e import albumentations\\n\\u003e\\u003e\\u003e import numpy as np\\n\\n\\u003e\\u003e\\u003e transform = albumentations.Compose(...\"],[\"```\\n\\n**2**. Next, load a pretrained [BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased) model and its c...\"],[\"```\\n\\n**3**. Create a function to tokenize the dataset, and you should also truncate and pad the text...\"],[\"\\u003e\\u003e\\u003e dataset = dataset.map(encode, batched=True)\\n\\u003e\\u003e\\u003e dataset[0]\\n{'sentence1': 'Amrozi accused his bro...\"],[\"```\\n\\n**4**. Rename the `label` column to `labels`, which is the expected input name in [BertForSeque...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\nUse the [`~transformers.TFPreTrainedModel.prepare_tf_dataset`] method from ğŸ¤— Transfo...\"],[\"Process text data\\n\\nThis guide shows specific methods for processing text datasets. Learn how to:\\n\\n- ...\"],[\"```\\n\\nSet the `batched` parameter to `True` in the [`~Dataset.map`] function to apply the tokenizer t...\"],[\"```\\n\\nThe [`~Dataset.map`] function converts the returned values to a PyArrow-supported format. But e...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\u003c!--\\nUse this section to tell people about which versions of ...\"],[\"Process audio data\\n\\nThis guide shows specific methods for processing audio datasets. Learn how to:\\n\\n...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.c...\"],[\"```\\n\\n- For fine-tuned speech recognition models, you only need to load a `processor`:\\n\\n    ```py\\n   ...\"],[\"Task templates\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe Task API is deprecated in favor of [`train-eval-index`](ht...\"],[\"Object detection\\n\\nObject detection models identify something in an image, and object detection datas...\"],[\"```\\n\\nThe dataset has the following fields:\\n\\n- `image`: PIL.Image.Image object containing the image.\\n...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nNow when you visualize the result, the image should be flipped, but the `bboxes` should still b...\"],[\"```\\n\\nUse the [`~Dataset.set_transform`] function to apply the transform on-the-fly which consumes le...\"],[\"Load\\n\\nYour data can be stored in various places; they can be on your local machine's disk, in a Gith...\"],[\"```\\n\\nSome datasets may have more than one version based on Git tags, branches, or commits. Use the `...\"],[\"```\\n\\nThe `split` parameter can also map a data file to a specific split:\\n\\n```py\\n\\u003e\\u003e\\u003e data_files = {\\\"v...\"],[\"```\\n\\n## Local and remote files\\n\\nDatasets can be loaded from local files stored on your computer and ...\"],[\"```\\n\\nAnother JSON format you may encounter is a nested field, in which case you'll need to specify t...\"],[\"```\\n\\nTo load remote Parquet files via HTTP, pass the URLs instead:\\n\\n```py\\n\\u003e\\u003e\\u003e base_url = \\\"https:\\u002f\\u002fst...\"],[\"```\\n\\nUnlike [`load_dataset`], [`Dataset.from_file`] memory maps the Arrow file without preparing the...\"],[\"```\\n\\nTo load remote WebDatasets via HTTP, pass the URLs instead:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import...\"],[\"```\\n\\n### Python list of dictionaries\\n\\nLoad a list of Python dictionaries with [`~Dataset.from_list`]...\"],[\"```\\n\\n### Pandas DataFrame\\n\\nLoad Pandas DataFrames with [`~Dataset.from_pandas`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from dat...\"],[\"```\\n\\nSelect specific rows of the `train` split:\\n\\n```py\\n\\u003e\\u003e\\u003e train_10_20_ds = datasets.load_dataset(\\\"b...\"],[\"```\\n\\nFinally, you can even create cross-validated splits. The example below creates 10-fold cross-va...\"],[\"```\\n\\n### Percent slicing and rounding\\n\\nThe default behavior is to round the boundaries to the neares...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n`pct1_dropremainder` rounding may truncate the last examples in a dataset...\"],[\"For example, if you try to download a configuration from the [MATINF](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nIf you've already downloaded a dataset from the *Hub with a loading script* to your computer, t...\"],[\"```\\n\\n## Metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in ğŸ¤— Datasets. To learn more about how ...\"],[\"```\\n\\n### Distributed setup\\n\\nWhen working in a distributed or parallel processing environment, loadin...\"],[\"Metric Card for SuperGLUE\\n\\n## Metric description\\nThis metric is used to compute the SuperGLUE evalua...\"],[\"```python\\nfrom datasets import load_metric\\nsuper_glue_metric = load_metric('super_glue', 'copa') \\npr...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the SuperGLUE subset chosen, consisting of...\"],[\"```\\n\\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfro...\"],[\"Stream\\n\\nDataset streaming lets you work with a dataset without downloading it.\\nThe data is streamed ...\"],[\"```\\n\\nDataset streaming also lets you work with a dataset made of local files without doing any conve...\"],[\"```\\n\\nLoading a dataset in streaming mode creates a new dataset type instance (instead of the classic...\"],[\"```\\n\\nThe [`~Dataset.to_iterable_dataset`] function supports sharding when the [`IterableDataset`] is...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n[`IterableDataset.shuffle`] will also shuffle the order of the shards if the dataset is ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n`take` and `skip` prevent future calls to `shuffle` because they lock in ...\"],[\"```\\n\\nAround 80% of the final dataset is made of the `en_dataset`, and 20% of the `fr_dataset`.\\n\\nYou ...\"],[\"```\\n\\n### Remove\\n\\nWhen you need to remove one or more columns, give [`IterableDataset.remove_columns`...\"],[\"```\\n\\n### Cast\\n\\n[`IterableDataset.cast`] changes the feature type of one or more columns. This method...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCasting only works if the original feature type and new feature type are compatible. For...\"],[\"```\\n\\nNext, apply this function to the dataset with [`IterableDataset.map`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets...\"],[\"```\\n\\n### Batch processing\\n\\n[`IterableDataset.map`] also supports working with batches of examples. O...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nSee other examples of batch processing in the [batched map processing](.\\u002fprocess#batch-p...\"],[\"```\\n\\nLastly, create a simple training loop and start training:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from torc...\"],[\"Depth estimation\\n\\nDepth estimation datasets are used to train a model to approximate the relative di...\"],[\"```\\n\\nThe dataset has two fields:\\n\\n* `image`: a PIL PNG image object with `uint8` data type.\\n* `depth...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"...    plt.imshow(depth_map.astype(\\\"uint8\\\"))\\n...    plt.axis(\\\"off\\\")\\n...    plt.show()\\n\\n\\u003e\\u003e\\u003e show_dept...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nWith `additional_targets` defined, you can pass the target depth maps to the `depth` argument o...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"Using Datasets with TensorFlow\\n\\nThis document is a quick introduction to using `datasets` with Tenso...\"],[\"```\\n\\n## N-dimensional arrays\\n\\nIf your dataset consists of N-dimensional arrays, you will see that by...\"],[\"```\\n\\n\\n## Other feature types\\n\\n[`ClassLabel`] data are properly converted to tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n\\nString and binary objects are unchanged, since PyTorch only supports numbers.\\n\\nThe [`Image`] an...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\\n`pip inst...\"],[\"```\\n\\n## Data loading\\n\\nAlthough you can load individual samples and batches just by indexing into you...\"],[\"Since the entire data preprocessing pipeline can be compiled in a `tf.data.Dataset`, this approach a...\"],[\"```\\n\\nThe returned `tf_ds` object here is now fully ready to train on, and can be passed directly to ...\"],[\"```\\n\\nFor a full description of the arguments, please see the [`~Dataset.to_tf_dataset`] documentatio...\"],[\"- Your dataset is too large to fit in RAM. `to_tf_dataset()` streams only one batch at a time, so ev...\"],[\"Metric Card for Mahalanobis Distance\\n\\n## Metric Description\\nMahalonobis distance is the distance bet...\"],[\"```\\n\\n```bibtex\\n@article{de2000mahalanobis,\\n  title={The Mahalanobis distance},\\n  author={De Maesscha...\"],[\"Troubleshooting\\n\\nThis guide aims to provide you the tools and knowledge required to navigate some co...\"],[\"```\\n\\nIf you encounter this issue, you need to upgrade the `datasets` library to the latest version (...\"],[\"```\\n\\nThis error can also occur when using a generator function that uses a global object that is not...\"],[\"```text\\n# Dataset rewiew request for \\u003cDataset name\\u003e\\n\\n## Description\\n\\n\\u003cbrief description of the datas...\"],[\"```\\n\\n### GitHub Issues\\n\\nFinally, if you suspect to have found a bug related to the library itself, c...\"],[\"Process image data\\n\\nThis guide shows specific methods for processing image datasets. Learn how to:\\n\\n...\"],[\"```\\n\\nThe cache file saves time because you don't have to execute the same transform twice. The [`~Da...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nFor example, if you'd like to change the color properties of an image randomly:\\n\\n```py\\n\\u003e\\u003e\\u003e f...\"],[\"```\\n\\nCreate a function to apply the `ColorJitter` transform:\\n\\n```py\\n\\u003e\\u003e\\u003e def transforms(examples):\\n.....\"],[\"Builder classes\\n\\n## Builders\\n\\nğŸ¤— Datasets relies on two main classes during the dataset building proc...\"],[\"Metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in ğŸ¤— Datasets. To learn more about how to use m...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nMetrics accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc....\"],[\"```\\n\\n\\u003ca id='metric_script'\\u003e\\u003c\\u002fa\\u003e\\n\\n## Custom metric loading script\\n\\nWrite a metric loading script to u...\"],[\"4. [`MetricInfo.features`] defines the name and type of the predictions and references.\\n\\nAfter you'v...\"],[\"```\\n\\n### Download metric files\\n\\nIf your metric needs to download, or retrieve local files, you will ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf the files are stored locally, provide a dictionary of path(s) instead of URLs.\\n\\n\\u003c\\u002fTip...\"],[\"```\\n\\n### Compute score\\n\\n[`DatasetBuilder._compute`] provides the actual instructions for how to comp...\"],[\"```\\n\\n2. Create [`DatasetBuilder._compute`] with instructions for what metric to calculate for each c...\"],[\"Metric Card for MAUVE\\n\\n## Metric description\\n\\nMAUVE is a library built on PyTorch and HuggingFace Tr...\"],[\"```\\n\\nIt also has several optional arguments:\\n\\n`num_buckets`: the size of the histogram to quantize P...\"],[\"`frontier_integral`: Frontier Integral, which ranges between 0 and 1. **Smaller** values indicate th...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\nmau...\"],[\"```\\n\\n## Further References \\n- [Official MAUVE implementation](https:\\u002f\\u002fgithub.com\\u002fkrishnap25\\u002fmauve)\\n-...\"],[\"Create a dataset loading script\\n\\n\\n\\u003cTip\\u003e\\n\\nThe dataset loading script is likely not needed if your dat...\"],[\"```\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e load_dataset(\\\"path\\u002fto\\u002fmy_dataset\\\")\\n```\\n\\nThe fol...\"],[\"```\\n\\n3. `DatasetInfo.homepage` contains the URL to the dataset homepage so users can find more detai...\"],[\"```\\n\\n### Multiple configurations\\n\\nIn some cases, your dataset may have multiple configurations. For ...\"],[\"Args:\\n        features: *list[string]*, list of the features that will appear in the\\n            fea...\"],[\"```\\n\\n2. Create instances of your config to specify the values of the attributes of each configuratio...\"],[\"```\\n\\n### Default configurations\\n\\nUsers must specify a configuration name when they load a dataset wi...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf the data files live in the same folder or repository of the dataset script, you can j...\"],[\"```\\n\\n## Generate samples\\n\\nAt this point, you have:\\n\\n- Added the dataset attributes.\\n- Provided instr...\"],[\"```\\n\\n## (Optional) Generate dataset metadata\\n\\nAdding dataset metadata is a great way to include info...\"],[\"```\\n\\n## Advanced features\\n\\n### Sharding\\n\\nIf your dataset is made of many big files, ğŸ¤— Datasets autom...\"],[\"```\\n\\nTo yield Arrow tables instead of single examples, make your dataset builder inherit from [`Arro...\"],[\"Metric Card for Competition MATH\\n\\n## Metric description\\n\\nThis metric is used to assess performance o...\"],[\"```\\n\\nN.B. To be able to use Competition MATH, you need to install the `math_equivalence` dependency ...\"],[\"```\\n\\nPartial match:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e math = load_metric(\\\"competit...\"],[\"Metric Card for SARI\\n\\n\\n## Metric description\\nSARI (***s**ystem output **a**gainst **r**eferences and...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary with the SARI score:\\n\\n```\\nprint(sari_score)\\n{...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\nsar...\"],[\"Metric Card for Mean IoU \\n\\n\\n## Metric Description\\n\\nIoU (Intersection over Union) is the area of over...\"],[\"### Examples\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e mean_iou = l...\"],[\"Metric Card for ROC AUC\\n\\n\\n## Metric Description\\nThis metric computes the area under the curve (AUC) ...\"],[\"```\\n\\nThe default implementation of this metric is the **binary** implementation. If employing the **...\"],[\"```\\n\\nSee the [Examples Section Below](#examples_section) for more extensive examples....\"],[\"### Inputs\\n- **`references`** (array-like of shape (n_samples,) or (n_samples, n_classes)): Ground t...\"],[\"- **`sample_weight`** (array-like of shape (n_samples,)): Sample weights. Defaults to None.\\n- **`max...\"],[\"### Output Values\\nThis metric returns a dict containing the `roc_auc` score. The score is a `float`,...\"],[\"```\\n\\nIn contrast, though, the output takes the following format in the multilabel case when `average...\"],[\"```\\n\\nExample 3, the **multilabel** use case:\\n```python\\n\\u003e\\u003e\\u003e roc_auc_score = datasets.load_metric(\\\"roc...\"],[\"```\\n\\n```bibtex\\n@article{10.1023\\u002fA:1010920819831,\\nauthor = {Hand, David J. and Till, Robert J.},\\ntitl...\"],[\"```\\n\\n## Further References\\nThis implementation is a wrapper around the [Scikit-learn implementation]...\"],[\"Use with JAX\\n\\nThis document is a quick introduction to using `datasets` with JAX, with a particular ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nA [`Dataset`] object is a wrapper of an Arrow table, which allows fast reads from arrays...\"],[\"```\\n\\nAnother thing you'll need to take into consideration is that the formatting is not applied\\nunti...\"],[\"```\\n\\nNote that if the `device` argument is not provided to `with_format` then it will use the defaul...\"],[\"```\\n\\nString and binary objects are unchanged, since JAX only supports numbers.\\n\\nThe [`Image`] and [`...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\\n`pip inst...\"],[\"```\\n\\n## Data loading\\n\\nJAX doesn't have any built-in data loading capabilities, so you'll need to use...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e ds = load_dataset(\\\"mnist\\\")\\n\\u003e\\u003e\\u003e ds = ds.with_format(\\\"...\"],[\"```\\n\\nOnce the format is set we can feed the dataset to the JAX model in batches using the `Dataset.i...\"],[\"Build and load\\n\\nNearly every deep learning workflow begins with loading a dataset, which makes it on...\"],[\"* [`datasets.packaged_modules.text.Text`] for text\\n* [`datasets.packaged_modules.csv.Csv`] for CSV a...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n   \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"### DatasetBuilder[[datasets-datasetbuilder]]\\n\\n[`DatasetBuilder`] accesses all the attributes inside...\"],[\"3. [`DatasetBuilder._generate_examples`] reads and parses the data files for a split. Then it yields...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIn this case, an error is raised to alert that the dataset has changed.\\nTo ignore the error,...\"],[\"Metric Card for Accuracy\\n\\n\\n## Metric Description\\n\\nAccuracy is the proportion of correct predictions ...\"],[\"```\\n\\nThis metric outputs a dictionary, containing the accuracy score.\\n\\n\\n#### Values from Popular Pap...\"],[\"```\\n\\n\\n## Limitations and Bias\\nThis metric can be easily misleading, especially in the case of unbala...\"],[\"Metric Card for CER\\n\\n## Metric description\\n\\nCharacter error rate (CER) is a common metric of the per...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the character error rate.\\n\\n```\\nprint(...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\ncer = lo...\"],[\"Metric Card for XTREME-S\\n\\n\\n## Metric Description\\n\\nThe XTREME-S metric aims to evaluate model perform...\"],[\"- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. T...\"],[\"```\\n\\nFor the `covost2` subset (which outputs `bleu`):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metri...\"],[\"```\\n \\nFor the `minds14` subset (which outputs `f1` and `accuracy`):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets imp...\"],[\"```\\n    \\n## Further References \\n\\n- [XTREME-S dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgoogle\\u002fxtreme_...\"],[\"Metric Card for WER\\n\\n## Metric description\\nWord error rate (WER) is a common metric of the performan...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the word error rate.\\n\\n```\\nprint(wer_s...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\nwer = lo...\"],[\"Share a dataset to the Hub\\n\\nThe [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets) is home to an extensive colle...\"],[\"Text file extensions are not tracked by Git LFS by default, and if they're greater than 10MB, they w...\"],[\"1. Click on **Create Dataset Card** to create a Dataset card. This button creates a `README.md` file...\"],[\"```\\n\\n## Upload with Python\\n\\nUsers who prefer to upload a dataset programmatically can use the [huggi...\"],[\"```\\n\\nTo add a new configuration (or subset) to a dataset or to add a new split (train\\u002fvalidation\\u002ftes...\"],[\"Metric Card for SQuAD\\n\\n## Metric description\\nThis metric wraps the official scoring script for versi...\"],[\"```\\n{'exact_match': 100.0, 'f1': 100.0}\\n```\\n\\nThe range of `exact_match` is 0-100, where 0.0 means no...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom datasets import load_met...\"],[\"```\\n\\nPartial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom datasets import load_metric\\nsquad...\"],[\"Loading methods\\n\\nMethods for listing and loading datasets and metrics:\\n\\n## Datasets\\n\\n[[autodoc]] dat...\"],[\"```\\n\\n### Text\\n\\n[[autodoc]] datasets.packaged_modules.text.TextConfig\\n\\n[[autodoc]] datasets.packaged_...\"],[\"Metric Card for CUAD\\n\\n## Metric description\\n\\nThis metric wraps the official scoring script for versi...\"],[\"Note that `answer_start` values are not taken into account to compute the metric.\\n\\n```python\\nfrom da...\"],[\"```\\n## Output values\\n\\nThe output of the CUAD metric consists of a dictionary that contains one or se...\"],[\"For more recent model performance, see the [dataset leaderboard](https:\\u002f\\u002fpaperswithcode.com\\u002fdataset\\u002f...\"],[\"```\\n\\nMinimal values:\\n\\n```python\\nfrom datasets import load_metric\\ncuad_metric = load_metric(\\\"cuad\\\")\\np...\"],[\"```\\n\\nPartial match: \\n\\n```python\\nfrom datasets import load_metric\\ncuad_metric = load_metric(\\\"cuad\\\")\\np...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as the [...\"],[\"Datasets ğŸ¤ Arrow\\n\\n## What is Arrow?\\n\\n[Arrow](https:\\u002f\\u002farrow.apache.org\\u002f) enables large amounts of dat...\"],[\"```\\n\\nThis is possible because the Arrow data is actually memory-mapped from disk, and not loaded in ...\"],[\"Metric Card for IndicGLUE\\n\\n## Metric description\\nThis metric is used to compute the evaluation metri...\"],[\"```\\n    \\n## Output values\\n\\nThe output of the metric depends on the IndicGLUE subset chosen, consisti...\"],[\"```\\n\\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\\n\\n```python\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n    \\n## Further References \\n- [IndicNLP website](https:\\u002f\\u002findicnlp.ai4bharat.org\\u002fhome\\u002f)\\n-...\"],[\"Structure your repository\\n\\nTo host and share your dataset, create a dataset repository on the Huggin...\"],[\"```\\nmy_dataset_repository\\u002f\\nâ”œâ”€â”€ README.md\\nâ”œâ”€â”€ data\\u002f\\nâ”‚   â”œâ”€â”€ abc.csv\\nâ”‚   â””â”€â”€ def.csv\\nâ””â”€â”€ holdout\\u002f\\n    ...\"],[\"```\\n\\n## Builder parameters\\n\\nNot only `data_files`, but other builder-specific parameters can be pass...\"],[\"```\\nmy_dataset_repository\\u002f\\nâ”œâ”€â”€ README.md\\nâ””â”€â”€ data\\u002f\\n    â”œâ”€â”€ train\\u002f\\n    â”‚   â””â”€â”€ bees.csv\\n    â”œâ”€â”€ test\\u002f...\"],[\"```\\n\\n### Single split\\n\\nWhen ğŸ¤— Datasets can't find any of the above patterns, then it'll treat all th...\"],[\"Load a dataset from the Hub\\n\\nFinding high-quality datasets that are reproducible and accessible can ...\"],[\"```\\n\\nIf you're happy with the dataset, then load it with [`load_dataset`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets ...\"],[\"```\\n\\n## Configurations\\n\\nSome datasets contain several sub-datasets. For example, the [MInDS-14](http...\"],[\"```\\n\\n## Remote code\\n\\nCertain datasets repositories contain a loading script with the Python code use...\"],[\"Metric Card for Code Eval\\n\\n## Metric description\\n\\nThe CodeEval metric estimates the pass@k metric fo...\"],[\"```\\n\\n## Output values\\n\\nThe Code Eval metric outputs two things:\\n\\n`pass_at_k`: a dictionary with the ...\"],[\"```\\n\\nPartial match at k=1, full match at k=2:\\n\\n```python\\nfrom datasets import load_metric\\ncode_eval ...\"],[\"Create a dataset\\n\\nSometimes, you may need to create a dataset if you're working with your own data. ...\"],[\"* [`ImageFolder`] uses the [`~datasets.Image`] feature to decode an image file. Many image extension...\"],[\"```\\npokemon\\u002ftrain\\u002fgrass\\u002fbulbasaur.png\\npokemon\\u002ftrain\\u002ffire\\u002fcharmander.png\\npokemon\\u002ftrain\\u002fwater\\u002fsquirtle...\"],[\"```\\n\\nTo learn more about each of these folder-based builders, check out the and \\u003ca href=\\\"https:\\u002f\\u002fhug...\"],[\"```\\n\\n    A generator-based [`IterableDataset`] needs to be iterated over with a `for` loop for examp...\"],[\"```\\n\\n## Next steps\\n\\nWe didn't mention this in the tutorial, but you can also create a dataset with a...\"],[\"Create an audio dataset\\n\\nYou can share a dataset with your team or with anyone in the community by c...\"],[\"```\\n\\nThen upload the dataset to the Hugging Face Hub using [`Dataset.push_to_hub`]:\\n\\n```py\\naudio_dat...\"],[\"```\\nmy_dataset\\u002f\\nâ”œâ”€â”€ README.md\\nâ”œâ”€â”€ metadata.csv\\nâ””â”€â”€ data\\u002f\\n```\\n\\nThe `data` folder can be any name you ...\"],[\"```\\n\\nThen you can store your dataset in a directory structure like this:\\n\\n```\\nmetadata.csv\\ndata\\u002ffirs...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n    Note that if audio files are located not right next to a metadata fil...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIf all audio files are contained in a single directory or if they are not...\"],[\"```\\n\\nThis guide will show you how to create a dataset loading script for audio datasets, which is a ...\"],[\"```\\n\\nIn addition to learning how to create a streamable dataset, you'll also learn how to:\\n\\n* Create...\"],[\"def _info(self):\\n\\n    def _split_generators(self, dl_manager):\\n\\n    def _generate_examples(self, pro...\"],[\"```\\n\\n#### Multiple configurations\\n\\nIn some cases, a dataset may have more than one configuration. Fo...\"],[\"```\\n\\nDefine your configurations in the `BUILDER_CONFIGS` class variable inside [`GeneratorBasedBuild...\"],[\"```\\n\\n### Add dataset metadata\\n\\nAdding information about your dataset helps users to learn more about...\"],[\"```\\n\\n### Download and define the dataset splits\\n\\nNow that you've added some information about your d...\"],[\"return [\\n        datasets.SplitGenerator(\\n            name=datasets.Split.TRAIN,\\n            gen_kwa...\"],[\"```\\n\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThis implementation does not extract downloaded archives. If you want to...\"],[\"```\\n\\nFinally, iterate over files in `audio_files` and yield them along with their corresponding meta...\"],[\"```\\n\\n### Upload the dataset to the Hub\\n\\nOnce your script is ready, [create a dataset card](.\\u002fdataset...\"],[\"```\\n\\n3. Use the [`~DownloadManager.iter_archive`] method to iterate over the archive at `audio_path`...\"],[\"```py\\ndef _split_generators(self, dl_manager):\\n    \\\"\\\"\\\"Returns SplitGenerators.\\\"\\\"\\\"\\n    dl_manager.dow...\"],[\"```\\n\\n#### Generate the dataset\\n\\nHere `_generate_examples` accepts `local_extracted_archive`, `audio_...\"],[\"```\\n\\nPut both of these steps together, and the whole `_generate_examples` method should look like:\\n\\n...\"],[\"Share a dataset using the CLI\\n\\nAt Hugging Face, we are on a mission to democratize good Machine Lear...\"],[\"For more information on how to load a dataset from the Hub, take a look at the [load a dataset from ...\"],[\"```\\nhuggingface-cli login\\n```\\n\\n2. Login using your Hugging Face Hub credentials, and create a new da...\"],[\"```\\ncp \\u002fsomewhere\\u002fdata\\u002f*.json .\\ngit lfs track *.json\\ngit add .gitattributes\\ngit add *.json\\ngit commi...\"],[\"Metric Card for Spearman Correlation Coefficient Metric (spearmanr)\\n\\n\\n## Metric Description\\nThe Spea...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `float`): Predicted labels, as returned by a model.\\n-...\"],[\"```\\n\\nThe same example, but that also returns the pvalue:\\n```python\\n\\u003e\\u003e\\u003e spearmanr_metric = datasets.l...\"],[\"Datasets\\n\\n\\u003cimg class=\\\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\\\" src...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eHigh-level explanations for building a better understanding about important...\"],[\"Load text data\\n\\nThis guide shows you how to load text datasets. To learn how to load any type of dat...\"],[\"```\\n\\nTo load remote text files via HTTP, pass the URLs instead:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"t...\"],[\"Metric Card for TER\\n\\n## Metric Description\\nTER (Translation Edit Rate, also called Translation Error...\"],[\"```\\n\\nThe metric can take on any value `0` and above. `0` is a perfect score, meaning the predictions...\"],[\"```\\n\\nExample ignoring punctuation and capitalization, and everything matches:\\n```python\\n\\u003e\\u003e\\u003e predicti...\"],[\"Metric Card for MAE\\n\\n\\n## Metric Description\\n\\nMean Absolute Error (MAE) is the mean of the magnitude ...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mae': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popul...\"],[\"--\\nYAML tags (full spec here: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblob\\u002fmain\\u002fdatasetcard.md?plain...\"],[\"## Dataset Description\\n\\n- **Homepage:** [Add homepage URL here if available (unless it's a GitHub re...\"],[\"- `task-category-tag`: The dataset can be used to train a model for [TASK NAME], which consists in [...\"],[\"```\\n{\\n  'example_field': ...,\\n  ...\\n}...\"],[\"```\\n\\nProvide any additional information that is not covered in the other sections about the data her...\"],[\"### Source Data\\n\\nThis section describes the source data (e.g. news text and headlines, social media ...\"],[\"Describe the people or systems who originally created the annotations and their selection criteria i...\"],[\"Also describe in this section if the proposed dataset contains a low-resource or under-represented l...\"],[\"```\\n@article{article_id,\\n  author    = {Author List},\\n  title     = {Dataset Paper Title},\\n  journal...\"],[\"Know your dataset\\n\\nThere are two types of dataset objects, a regular [`Dataset`] and then an âœ¨ [`Ite...\"],[\"```\\n\\nUse the `-` operator to start from the end of the dataset:\\n\\n```py\\n# Get the last row in the dat...\"],[\"```\\n\\nBut it is important to remember that indexing order matters, especially when working with large...\"],[\"```\\n\\n### Slicing\\n\\nSlicing returns a slice - or subset - of the dataset, which is useful for viewing ...\"],[\"```\\n\\nYou can also create an [`IterableDataset`] from an *existing* [`Dataset`], but it is faster tha...\"],[\"```\\n\\nYou can return a subset of the dataset with a specific number of examples in it with [`Iterable...\"],[\"Metric Card for XNLI\\n\\n## Metric description\\n\\nThe XNLI metric allows to evaluate a model's score on t...\"],[\"```\\n\\n## Output values\\n\\nThe output of the XNLI metric is simply the `accuracy`, i.e. the proportion o...\"],[\"Evaluate predictions\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in ğŸ¤— Datasets. To learn more about...\"],[\"```\\n\\nThis will load the metric associated with the MRPC dataset from the GLUE benchmark.\\n\\n## Select ...\"],[\"```\\n\\n## Metrics object\\n\\nBefore you begin using a [`Metric`] object, you should get to know it a litt...\"],[\"```\\n\\nNotice for the MRPC configuration, the metric expects the input format to be zero or one. For a...\"],[\"Batch mapping\\n\\nCombining the utility of [`Dataset.map`] with batch mode is very powerful. It allows ...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import Dataset\\n\\u003e\\u003e\\u003e dataset = Dataset.from_dict({\\\"a\\\": [0, 1, 2]})\\n\\u003e\\u003e\\u003e dataset...\"],[\"```\\n\\nTo make it valid, you have to drop one of the columns:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import Dataset\\n...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Hugging Face's notebooks ğŸ¤—\\n\\n### Documentation notebooks\\n\\nYou can open any page of the documentati...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThen run the following command:\\n\\n```bash\\ndoc-builder preview datasets docs\\u002fsource\\u002f\\n```\\n\\nThe doc...\"],[\"```\\nand of course if you moved it to another file, then:\\n\\n```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n    Returns:\\n        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special tok...\"],[\"```\\n    Example:\\n\\n    ```py\\n    \\u003e\\u003e\\u003e from datasets import load_dataset\\n    \\u003e\\u003e\\u003e ds = load_dataset(\\\"rot...\"],[\"Installation\\n\\nBefore you start, you'll need to setup your environment and install the appropriate pa...\"],[\"```\\n\\nThis command downloads version 1 of the [Stanford Question Answering Dataset (SQuAD)](https:\\u002f\\u002fr...\"],[\"```\\n\\n## Audio\\n\\nTo work with audio datasets, you need to install the [`Audio`] feature as an extra de...\"]],\"hovertemplate\":\"source=datasets\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets, circle\",\"showlegend\":true,\"x\":[1.7465464,1.684101,1.8343079,0.9880347,1.2629778,1.1973469,1.3554833,1.3009297,1.2669535,0.31223014,0.5510878,0.25485352,0.6244116,0.61937505,2.5213232,1.3633877,0.22548312,0.8485361,0.74835175,-0.23392886,0.98875856,0.7381183,-0.42497927,0.407855,-0.8677335,-1.5815805,-2.0636053,-2.201481,-0.70825076,-6.5970645,-6.327059,-6.565072,-6.4752593,0.08620979,-9.837804,0.17428048,-9.427026,1.8761237,2.2573395,1.7480737,1.9494804,2.1223962,2.1738846,0.15422231,0.51488036,-0.008057177,0.2948936,0.6292375,0.4923236,0.6245674,0.2833966,-0.7289749,-6.3108397,2.7731013,1.7452353,1.5558547,0.040385228,0.16963026,0.18646167,-0.1317852,-2.905958,0.29493606,0.37364542,7.068716,6.7006235,6.9398456,6.902999,1.8055007,1.7911304,0.19967493,0.46985555,-0.23385383,0.27793962,0.6935842,0.5636557,0.06304298,2.12402,2.3829093,1.0520476,1.8504299,2.1169372,2.9931896,2.8977606,1.8099729,1.727168,1.5857915,1.7717049,0.33753887,6.937641,6.9305882,2.9488733,2.1743395,2.7006054,1.7020394,1.9291565,2.8019266,-0.94994825,-1.6184303,3.0688756,5.031456,1.3429724,0.7298123,0.37108877,-0.07499857,0.22751254,-0.26588354,0.08224222,0.102870554,-0.5247883,-1.2499508,7.5085874,-0.85286534,-3.5622048,-4.626623,-0.13293797,-4.8095036,-0.29174,0.115143396,0.019623429,0.36916062,0.4402942,0.1572681,0.09291044,0.08047909,0.018360307,0.121164955,-0.38704017,-0.21976224,0.49832928,0.5408935,-0.77525705,-1.5532014,-1.2344524,-0.060066946,0.6055483,0.16241078,0.45305648,0.5639123,0.5420008,-1.4573376,1.6456711,1.4322467,1.8282658,-0.10705021,-0.19138259,-0.2619082,0.5135858,-6.265628,-0.045004677,1.0166979,1.7752901,1.4152068,-10.694602,-0.1437948,0.20298545,0.18924706,0.23304121,0.22612016,-10.200238,0.18070409,-0.13151503,0.29393503,-0.15922305,-0.11993562,-0.2656917,-0.0969781,3.705411,-0.37796372,0.1735044,0.14296049,-0.4149508,0.44325203,-0.5376539,0.46703553,-0.42566088,0.24550103,-7.126852,2.1778905,2.3723366,2.3822155,1.6662945,1.5292835,1.5657996,-0.1728454,0.59940827,-0.37953448,-0.2545053,0.5225408,-0.8333476,-0.4193337,-2.4308352,-1.2129794,-1.402863,-0.9327756,3.1716888,-0.27254876,0.117584154,-3.4757934,0.32934186,0.008469315,2.5970845,3.4822984,3.618258,2.7105558,2.4492347,2.2642946,2.8650773,-0.45871094,0.25493744,-0.61604106,0.38450065,0.2119866,-0.74870926,2.3806808,2.025634,1.9109706,1.9280037,3.3752413,2.1916025,2.2337213,2.3499727,2.02638,2.042172,2.4260237,2.364122,2.1315036,2.604899,3.7573204,3.611771,0.16305959,1.000792,1.3269532,0.7038512,0.9647634,0.6236575,0.6837033,0.46507907,0.826451,-0.19743855,0.52821857,0.6194115,0.03188675,0.19480914,-1.804035,0.59353715,-2.5138545,0.9119463,1.000523,0.41336045,0.006035083,-1.567878,-6.3209047,2.4961135,-0.35653973,0.3364729,0.44018474,0.48223382,-0.49192357,-1.090136,0.17104323,-0.7985447,2.7672226,0.2669131,0.549614,0.54766,2.6700466,2.6946297,2.6833272,2.7207823,-0.89094913,-0.40936676,0.18719572,1.2971147,2.3651528,3.5385864,3.6946058,3.6509826,3.347825,3.117431,-6.8043523,-3.79684,3.095217,-6.725868,-1.8453006,-6.5988183,-6.488327,-0.20889135,-6.812079,0.20443822,-0.38769785,-0.14040905,-2.5827904,-1.7075562,0.21009491,-0.841544,-0.93078136,-1.0683619,-0.69305146,-1.0616384,5.1794724,-6.38292,-0.21308352,-6.4624925,2.621515,-10.226375,0.9542768,0.19525272,0.13490435,0.23521164,2.5775943,2.388367,2.3698995,2.507644,2.683601,2.567983,1.9488415,2.1841013,1.2655128,1.9597942,0.6979136,0.9668573,0.49003667,1.631501,2.4356484,2.4415262,0.8119776,1.112773,-0.10375439,0.74516726,0.3015818,0.55795485,2.1921272,1.8040675,1.636139,1.4924878,1.3052063,1.1000472,0.6824679,0.67015326,0.44056275,0.88339037,0.6628487,-0.905442,0.5180679,-1.1135807,0.1673826,1.8728536,0.21933492,10.882431,0.08004296,0.060387738,-0.06305537,-0.015755454,-0.6521548,-0.4746599,-0.2289789,0.86200553,-6.29858,-0.53886604,-0.70699793,-0.9740749,-0.8353595,-1.0138782,0.02490043,-0.49764884,3.8976188,2.0721302,1.7079326,1.4772122,4.831423,0.30066487,0.96856093,-0.1094865,0.24344902,2.0122077,0.8452775,0.5668205,1.0234456,0.6471278,2.4505212,2.1455114,0.6781596,0.7650855,-0.17385879,-0.13786134,-0.520738,-0.19651087,7.367379,2.5238323,2.4588096,2.195144,2.205551,1.1366361,2.2315848,2.2362742,2.469848,1.3846822,2.5226138,1.6896948,1.5595695,0.2507083,0.4621681,0.31591177,-0.24269137,0.32022366,-0.1960608,0.2911728,0.46176055,0.22948417,0.35514605,7.2656035,0.5845674,0.41057515,0.3359057,0.32238954,0.41736135,0.1042686,0.29130277,1.0989225,1.0477637,1.5295073,0.93189204,1.2816329,-6.3692703,0.62924165,1.2760894,0.6880743,2.4052637,2.5782654,2.470889,2.8936198,2.0518389,3.5570273,0.31049785,0.3502997,0.19229731,-0.19310123,-0.085054785,-0.08875292,-0.4132309,-0.09856995,0.5565954,-0.14904182,7.087183,-0.18079686,-0.05717014,-0.10516786,3.640693,3.471269,3.72442,3.27408,2.713223,-0.05137769,0.22499588,0.47547224,0.4837874,2.0793955,1.85148,-0.11435984,0.45313394,0.011713137,0.44533086,0.52825296,0.501654,-0.30713537,1.7489955,1.3827144,-0.15080497,0.27143347,-0.20415474,7.182626,2.891259,2.736167,2.4720778,2.1334004,2.0681772,2.6038098,1.4251503,2.2782948,2.833414,0.5036075,0.52138716,0.47143146,2.5355604,2.3060825,2.3794842,2.5024424,1.6013787,2.7612166,2.819043,2.9131236,2.6968954,2.680229,2.659333,2.687275,2.5505767,2.6182344,2.811309,2.2101207,2.299769,2.5302997,2.6225817,2.3569632,2.7244797,2.7210836,2.7474108,2.6003008,2.605474,2.6724517,2.3483655,3.4788098,3.5667167,3.7087727,3.690452,0.2126829,0.43785337,0.52911603,3.053761,8.5409975,8.525621,2.2677507,2.70885,-0.28708565,0.04690149,0.033142436,0.24061811,0.9026026,0.7558001,3.4047377,2.9671135,2.3615696,5.9761367,3.0300214,3.0616326,2.6499705,2.433981,5.891451,1.5301412,0.7297621,0.9478208,1.2237182,1.774793,1.6656592,-0.045497764,0.41514695,0.8503642,0.91931504,0.5534981,0.674728,-0.46041498,0.45855218,0.6237688,5.3197904,4.567169,5.0071316,5.3077903,5.618386,4.765485,-0.10182219,-0.82124037,-1.7957126,-0.18908744,1.1528933,-1.997899,2.575729],\"xaxis\":\"x\",\"y\":[4.2822366,4.286631,4.2717514,4.6579423,4.6373897,4.4981413,4.4229865,4.696694,4.562899,7.504981,7.18921,7.3222017,7.9663806,7.87289,3.4125638,3.9476612,7.587958,7.5732822,7.9138455,7.2022443,6.5328608,6.572939,8.265679,7.770374,8.056362,4.431854,5.0522532,4.9087305,2.9152446,7.585267,7.87568,7.853786,7.777075,-6.290755,-0.7236294,-6.2877398,-0.8056819,3.842543,1.4473112,3.632125,3.6153316,3.7515328,3.809304,7.805993,7.1284466,3.880691,7.4099436,8.019623,7.862098,4.4031568,4.2700768,4.7693915,7.837316,4.309899,3.562954,3.5525563,8.1121645,8.05213,8.054214,8.256358,-0.13081722,7.7500544,7.6626854,1.3054448,1.4530634,1.4615502,1.5808853,4.149319,4.0274673,7.6421866,7.1341696,4.190172,7.6818027,8.009582,7.8659344,6.9043927,4.148837,4.1809855,4.355899,3.9600105,4.142669,5.6614175,5.614741,4.306381,3.9887981,3.7028427,2.282187,-5.939278,0.20277373,2.2404776,3.6503446,3.9568841,2.8620188,2.731368,4.3119917,3.307303,2.5532858,7.377401,5.46598,2.9521437,4.1123133,3.8652115,3.1938472,8.504691,8.287957,6.090074,8.441493,8.468776,7.4897738,7.6674085,2.9732096,7.803765,1.3714718,1.67723,8.143128,2.098345,8.002513,7.7994576,6.907344,7.730917,7.9602,8.172438,8.436733,8.415034,8.441752,8.53868,8.183904,7.563509,8.438025,8.5206785,7.9014816,7.5358686,7.584445,7.726816,8.088977,7.9175115,8.230244,8.306307,8.450012,5.991617,3.2169785,2.480176,2.9388103,2.043683,2.9632092,3.3359199,2.8500302,7.7280116,2.6722958,3.884601,4.283234,4.4362674,-1.1012986,-6.2295604,-6.221546,-6.2224364,-6.3123064,-6.28808,-0.91918856,7.5414686,8.319004,8.105783,8.509059,8.462517,8.170104,7.9822497,2.8542411,8.401187,8.119558,7.6320148,7.039371,8.046163,6.987141,8.009687,6.774813,7.871323,2.7502105,3.9141564,3.7824266,3.3418355,4.0792794,4.1834016,3.71728,8.196896,8.207641,7.8625927,7.6878877,8.089474,5.1072416,4.9779425,6.087374,7.5352902,7.442286,7.6663923,3.8360395,8.381498,8.403726,6.0333123,7.705025,8.453909,3.6572866,0.8586878,1.9542862,3.361069,3.7364867,4.015255,3.1549273,8.287683,8.263854,8.2601385,7.926927,7.836148,8.1633835,3.4927707,3.8568265,3.823301,3.6591609,2.7579489,3.6859703,3.7215295,4.0142937,3.906178,3.8456092,3.8831804,3.9983902,4.1475716,3.6920993,3.801612,3.4419825,4.3437786,4.4854956,4.6214194,4.828315,4.4121265,4.534794,4.6638236,4.2014303,4.248483,4.7685914,4.778507,4.6737285,3.3074756,4.7629423,4.6920457,4.5824537,5.0485816,4.0145597,4.4054093,4.1292543,3.8639536,4.545323,7.784369,3.9039147,7.9114184,8.233352,8.483937,8.45245,7.3410177,6.9983234,8.09513,7.2244015,3.4201715,8.125213,7.910836,7.910817,4.329344,4.4200935,4.422229,4.2107463,4.5310116,4.7882833,4.638569,4.133256,3.2208428,2.562953,1.1124458,2.8723428,4.8042836,2.5721295,7.030111,5.133449,3.026266,7.124336,0.6752149,7.8823457,7.599286,3.0516632,7.23209,-6.324525,3.1249456,4.0372033,1.6048332,4.775979,4.6794662,3.557062,2.4978287,4.4211564,4.6662927,4.1998663,0.88890004,7.8317895,-6.031329,7.574117,2.9730668,-0.63194996,3.0911083,-6.233378,-6.2798476,-6.3124247,3.9089189,3.8181899,3.834118,3.9657805,4.0087695,4.0265555,4.1595945,4.085237,4.421603,4.2359242,4.488458,4.433641,4.6183877,4.176293,3.592976,3.6773822,6.8382516,5.809178,8.160098,7.949731,8.202452,8.170611,4.1069865,4.288738,4.392569,4.567719,4.7306066,4.6239443,4.342909,4.570471,4.3195887,4.3572283,4.613387,4.8396187,4.424442,-0.26604715,-6.2353926,3.6873734,-6.1571803,4.104933,-6.181471,-6.255217,-6.3972397,-6.076918,2.8788958,3.149842,3.799796,3.354916,7.746255,2.5967166,2.718504,1.9518445,2.739582,3.243508,7.5857496,7.4535446,1.5069362,4.120478,4.0302763,4.29483,1.6996105,-6.3198347,4.0483575,-6.398863,-6.32454,4.0272126,6.7289624,7.531052,6.5724883,7.1685457,3.1243937,2.3566003,7.33592,7.4726453,8.248165,5.172115,7.889012,8.433769,1.7143049,3.951987,3.8794398,4.000093,3.976576,4.2240477,3.9183588,3.8969169,3.676411,4.526271,3.9174833,4.3523617,4.2688265,7.945813,8.09734,8.383849,8.349415,8.058043,8.481622,7.4689803,7.122373,7.63244,7.302061,3.1251853,7.235701,6.9355197,7.612049,7.4747844,7.215365,7.2407327,7.5000153,3.9793923,4.11058,3.913477,3.973361,3.4942758,7.943157,3.830509,3.864476,4.0017586,3.846801,3.781144,3.7559538,3.4519813,4.1344004,2.8557775,7.602373,8.12886,6.7163363,8.629239,8.703547,8.566438,8.405728,8.700186,8.043188,8.534609,3.1865132,8.714696,8.550097,8.591257,2.9096344,2.1743126,3.2979808,2.1746657,3.6205277,7.7243385,8.230631,8.498721,8.490709,3.9576566,4.0832295,8.250933,8.272401,7.7634172,8.250159,8.385913,8.466548,7.496768,4.2319646,4.3462405,8.308602,8.133052,8.4025545,3.0969872,3.669516,3.7613907,3.709393,3.730405,3.9019122,4.0413327,4.47481,4.107715,3.4897077,6.44344,7.363708,8.417853,3.9219007,3.8264391,3.8718047,3.9497116,4.1741705,3.8245342,3.8961878,3.751775,4.3295007,4.4087315,4.4014297,4.2241063,4.290748,4.3792963,4.679018,4.0806527,3.7838168,4.258219,4.412589,4.395819,4.4936857,4.6719074,4.2983522,4.494451,4.362243,4.508869,4.512711,2.9922516,2.8611639,2.0953593,2.387214,8.073907,7.9397273,7.9884877,3.4913104,2.8675947,2.9958804,4.073603,3.875214,8.538669,8.465876,8.445538,7.755689,7.522247,7.955413,4.723806,5.3908906,5.925199,3.4101088,5.0422273,5.307141,5.7426033,6.005585,2.853052,4.3828287,4.714039,4.2306232,4.338187,4.294257,3.9958725,8.111534,8.087455,6.613252,6.749268,7.8533063,7.1487126,4.7811155,4.383877,4.5326653,0.627141,1.5390692,0.390881,2.3087318,2.605941,1.8303293,2.7261143,3.506798,4.0156217,4.787792,1.7875848,4.3893023,3.888898],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Q-Learning Recap [[q-learning-recap]]\\n\\n\\n*Q-Learning* **is the RL algorithm that** :\\n\\n- Trains a *Q-f...\"],[\"This is the Q-Learning pseudocode:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-co...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q3: Which of the following statements are true about Monte Carlo method?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n...\"],[\"### Q6: What is `Advantage` in the A2C method?\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\n\\nInstead of us...\"],[\"Additional Readings\\n\\nThese are **optional readings** if you want to go deeper.\\n\\n\\n## Introduction to ...\"],[\"Hands-on\\n\\nNow that you learned the basics of multi-agents, you're ready to train your first agents i...\"],[\"More precisely, AI vs. AI is three tools:\\n\\n- A *matchmaking process* defining the matches (which mod...\"],[\"In order for your model to get correctly evaluated against others you need to follow these rules:\\n\\n1...\"],[\"```\\n\\nTo be able to train our agents correctly and push to the Hub, we need to install ML-Agents\\n\\n```...\"],[\"```\\n\\nFinally, you need to install git-lfs: https:\\u002f\\u002fgit-lfs.com\\u002f\\n\\nNow that itâ€™s installed, we need to...\"],[\"\\u003cfigcaption\\u003eThis environment was made by the \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fUnity-Technologies\\u002fml-agent...\"],[\"The Unity MLAgents team developed the solution in a new multi-agent trainer called *MA-POCA (Multi-A...\"],[\"The config file weâ€™re going to use here is in  `.\\u002fconfig\\u002fpoca\\u002fSoccerTwos.yaml`. It looks like this:\\n...\"],[\"```\\n\\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How ...\"],[\"```\\n\\nThe executable contains 8 copies of SoccerTwos.\\n\\nâš ï¸ Itâ€™s normal if you donâ€™t see a big increase...\"],[\"```\\n\\nThen, we need to run `mlagents-push-to-hf`.\\n\\nAnd we define four parameters:\\n\\n1. `-run-id`: the ...\"],[\"```\\n\\nIf everything worked you should see this at the end of the process (but with a different url ğŸ˜†)...\"],[\"We strongly suggest that you create a new model when you push to the Hub if you want to train it aga...\"],[\"Two types of value-based methods [[two-types-value-based-methods]]\\n\\nIn value-based methods,Â **we lea...\"],[\"- *Value-based methods:*Â **Indirectly, by training a value function**Â that outputs the value of a st...\"],[\"As we mentioned above, we have two types of value-based functions:\\n\\n## The state-value function [[st...\"],[\"We see that the difference is:\\n\\n- For the state-value function, we calculateÂ **the value of a state ...\"],[\"The advantages and disadvantages of policy-gradient methods\\n\\nAt this point, you might ask, \\\"but Deep...\"],[\"Under a deterministic policy, the policy will either always move right when in a red state or always...\"],[\"For instance, if during the training, the best action was left (with a Q-value of 0.22) and the trai...\"],[\"Glossary \\n\\nThis is a community-created glossary. Contributions are welcome!\\n\\n- **Deep Q-Learning:** ...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this unit! **That was the biggest one**, and there ...\"],[\"Introduction\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fres...\"],[\"Introduction to Q-Learning [[introduction-q-learning]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"Conclusion\\n\\nCongrats on finishing this unit! Youâ€™ve just trained your first ML-Agents and shared it ...\"],[\"Language models in RL\\n## LMs encode useful knowledge for agents\\n\\n**Language models** (LMs) can exhib...\"],[\"\\u003cvideo src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen...\"],[\"## Author\\n\\nThis section was written by \\u003ca href=\\\"https:\\u002f\\u002ftwitter.com\\u002fClementRomac\\\"\\u003e ClÃ©ment Romac \\u003c\\u002fa...\"],[\"The Deep Q-Network (DQN)  [[deep-q-network]]\\nThis is the architecture of our Deep Q-Learning network...\"],[\"**Why do we stack four frames together?**\\nWe stack frames together because it helps us **handle the ...\"],[\"The certification process\\n\\n\\nThe certification process is **completely free**:\\n\\n- To get a *certifica...\"],[\"Summary [[summary]]\\n\\nThat was a lot of information! Let's summarize:\\n\\n- Reinforcement Learning is a ...\"],[\"Glossary [[glossary]]\\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n\\n### Strat...\"],[\"### Off-policy vs on-policy algorithms\\n\\n- **Off-policy algorithms:** A different policy is used at t...\"],[\"The Reinforcement Learning Framework [[the-reinforcement-learning-framework]]\\n\\n## The RL Process [[t...\"],[\"This RL loop outputs a sequence ofÂ **state, action, reward and next state.**\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhugg...\"],[\"There is a differentiation to make between *observation* and *state*, however:\\n\\n- *State s*: is **a ...\"],[\"## Action Space [[action-space]]\\n\\nThe Action space is the set ofÂ **all possible actions in an enviro...\"],[\"The cumulative reward at each time step **t** can be written as:\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhugging...\"],[\"ToÂ discount the rewards, we proceed like this:\\n\\n1. We define a discount rate called gamma. **It must...\"],[\"Introduction [[introduction]]\\n\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"From Q-Learning to Deep Q-Learning [[from-q-to-dqn]]\\n\\nWe learned thatÂ **Q-Learning is an algorithm w...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Hands-on\\n\\n\\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\nnotebooks={[\\n  {label: \\\"Goo...\"],[\"We strongly **recommend students use Google Colab for the hands-on exercises** instead of running th...\"],[\"### ğŸ“š RL-Library:\\n\\n- [ML-Agents](https:\\u002f\\u002fgithub.com\\u002fUnity-Technologies\\u002fml-agents)\\n\\nWe're constantly ...\"],[\"```\\n\\n```bash\\n# Go inside the repository and install the package\\ncd ml-agents\\npip install -e .\\u002fml-age...\"],[\"```\\n\\nDownload the file SnowballTarget.zip from https:\\u002f\\u002fdrive.google.com\\u002ffile\\u002fd\\u002f1YHHLjyj6gaZ3Gemx1hQg...\"],[\"```\\n\\nMake sure your file is accessible\\n\\n```bash\\nchmod -R 755 .\\u002ftraining-envs-executables\\u002flinux\\u002fSnowb...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain...\"],[\"The training will take 10 to 35min depending on your config. Go take a â˜•ï¸ you deserve it ğŸ¤—.\\n\\n```bash...\"],[\"```\\n\\n### Push the agent to the Hugging Face Hub\\n\\n- Now that we've trained our agent, weâ€™re **ready t...\"],[\"```\\n\\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste...\"],[\"```\\n\\nIt's the link to your model. It contains a model card that explains how to use it, your Tensorb...\"],[\"Download the file Pyramids.zip from https:\\u002f\\u002fdrive.google.com\\u002fuc?export=download&id=1UiFNdKlsH0NTu32x...\"],[\"```\\n\\nUnzip it\\n\\n```python\\n%%capture\\n!unzip -d .\\u002ftraining-envs-executables\\u002flinux\\u002f .\\u002ftraining-envs-exec...\"],[\"```\\n\\n###  Modify the PyramidsRND config file\\n  \\n- Contrary to the first environment, which was a cus...\"],[\"```\\n\\n### Push the agent to the Hugging Face Hub\\n\\n- Now that we trained our agent, weâ€™re **ready to p...\"],[\"Decision Transformers\\n\\nThe Decision Transformer model was introduced by [\\\"Decision Transformer: Rein...\"],[\"## Author\\n\\nThis section was written by \\u003ca href=\\\"https:\\u002f\\u002ftwitter.com\\u002fedwardbeeching\\\"\\u003eEdward Beeching\\u003c...\"],[\"Additional Readings [[additional-readings]]\\n\\n##  An introduction to multi-agents\\n\\n- [Multi-agent rei...\"],[\"Introducing Q-Learning [[q-learning]]\\n## What is Q-Learning? [[what-is-q-learning]]\\n\\nQ-Learning is a...\"],[\"Let's go through an example of a maze.\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-r...\"],[\"If we recap,Â *Q-Learning*Â **is the RL algorithm that:**\\n\\n- TrainsÂ a *Q-function* (an **action-value ...\"],[\"## The Q-Learning algorithm [[q-learning-algo]]\\n\\nThis is the Q-Learning pseudocode; let's study each...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"How do we form the TD target?\\n1. We obtain the reward \\\\\\\\(R_{t+1}\\\\\\\\) after taking the action \\\\\\\\(A_t\\\\\\\\...\"],[\"For instance, with Sarsa, another value-based algorithm,Â **the epsilon-greedy policy selects the nex...\"],[\"Play with Huggy [[play]]\\n\\nNow that you've trained Huggy and pushed it to the Hub. **You will be able...\"],[\"Discord 101 [[discord-101]]\\n\\nHey there! My name is Huggy, the dog ğŸ•, and I'm looking forward to trai...\"],[\"They are in the reinforcement learning category. **Don't forget to sign up to these channels** by cl...\"],[\"Introduction [[introduction]]\\n\\nOne of the most critical tasks in Deep Reinforcement Learning is to *...\"],[\"Designing Multi-Agents systems\\n\\nFor this section, you're going to watch this excellent introduction ...\"],[\"## Centralized approach\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-cour...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Hands-on\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      notebooks={[\\n ...\"],[\"Then, to test its robustness, we're going to train it in:\\n- [LunarLander-v2](https:\\u002f\\u002fwww.gymlibrary....\"],[\"In this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL impl...\"],[\"- `Hardware Accelerator \\u003e GPU`\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n- Add new argument in `parse_args()` function to define the repo-id where we want to push the m...\"],[\"```\\n\\n- Next, we add the methods needed to push the model to the Hub\\n\\n- These methods will:\\n  - `_eva...\"],[\"# Step 3: Evaluate the model and build JSON\\n        mean_reward, std_reward = _evaluate_agent(eval_e...\"],[\"msg.info(f\\\"Your model is pushed to the Hub. You can view your model here: {repo_url}\\\")\\n    return re...\"],[\"def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\\n    \\\"\\\"\\\"\\n    ...\"],[\"# Add metrics\\n    eval = metadata_eval_result(\\n        model_pretty_name=model_name,\\n        task_pr...\"],[\"```\\n\\n- Finally, we call this function at the end of the PPO training\\n\\n```python\\n# Create the evaluat...\"],[\"```\\n\\n- Here's what the final ppo.py file looks like:\\n\\n```python\\n# docs and experiment results can be...\"],[\"from pathlib import Path\\nimport datetime\\nimport tempfile\\nimport json\\nimport shutil\\nimport imageio\\n\\nf...\"],[\"# Algorithm specific arguments\\n    parser.add_argument(\\\"--env-id\\\", type=str, default=\\\"CartPole-v1\\\",\\n...\"],[\"help=\\\"the number of mini-batches\\\")\\n    parser.add_argument(\\\"--update-epochs\\\", type=int, default=4,\\n ...\"],[\"# Adding HuggingFace argument\\n    parser.add_argument(\\\"--repo-id\\\", type=str, default=\\\"ThomasSimonini...\"],[\"with tempfile.TemporaryDirectory() as tmpdirname:\\n        tmpdirname = Path(tmpdirname)\\n\\n        # S...\"],[\"# Copy logdir into repo logdir\\n        shutil.copytree(logdir, repo_logdir)\\n\\n\\ndef make_env(env_id, s...\"],[\"def get_value(self, x):\\n        return self.critic(x)\\n\\n    def get_action_and_value(self, x, action=...\"],[\"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() and args.cuda else \\\"cpu\\\")\\n\\n    # env setup...\"],[\"for update in range(1, num_updates + 1):\\n        # Annealing the rate if instructed to do so.\\n      ...\"],[\"# bootstrap value if not done\\n        with torch.no_grad():\\n            next_value = agent.get_value...\"],[\"# Optimizing the policy and value network\\n        b_inds = np.arange(args.batch_size)\\n        clipfr...\"],[\"# Value loss\\n                newvalue = newvalue.view(-1)\\n                if args.clip_vloss:\\n      ...\"],[\"# TRY NOT TO MODIFY: record rewards for plotting purposes\\n        writer.add_scalar(\\\"charts\\u002flearning...\"],[\"```\\n\\nTo be able to share your model with the community there are three more steps to follow:\\n\\n1ï¸âƒ£ (I...\"],[\"```\\n\\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste...\"],[\"The SnowballTarget Environment\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"We could have a more complex reward function (with a penalty to push the agent to go faster, for exa...\"],[\"## The action space\\n\\nThe action space is discrete:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggin...\"],[\"What are the policy-based methods?\\n\\nThe main goal of Reinforcement learning is to **find the optimal...\"],[\"- On the other hand, in *policy-based methods*, we directly learn to approximate \\\\\\\\(\\\\pi^{*}\\\\\\\\) witho...\"],[\"The difference between these two methods **lies on how we optimize the parameter** \\\\\\\\(\\\\theta\\\\\\\\):\\n\\n- ...\"],[\"Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym ğŸ¤– [[hands-on]]\\n\\n\\n      \\u003cCours...\"],[\"# Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym ğŸ¤–\\n\\n### ğŸ® Environmen...\"],[\"## Create a virtual display ğŸ”½\\n\\nDuring the notebook, we'll need to generate a replay video. To do so,...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n## PandaReachDense-v3 ğŸ¦¾\\n\\nThe agent we're going to train is a robotic arm that needs to do contr...\"],[\"```\\n\\nThe observation space **is a dictionary with 3 different elements**:\\n\\n- `achieved_goal`: (x,y,z...\"],[\"```\\n\\n### Create the A2C Model ğŸ¤–\\n\\nFor more information about A2C implementation with StableBaselines3...\"],[\"```\\n\\n### Evaluate the agent ğŸ“ˆ\\n\\n- Now that's our  agent is trained, we need to **check its performanc...\"],[\"```\\n### Publish your trained model on the Hub ğŸ”¥\\n\\nNow that we saw we got good results after the train...\"],[\"```\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command inst...\"],[\"```\\n\\n## Some additional challenges ğŸ†\\n\\nThe best way to learn **is to try things by your own**! Why no...\"],[\"```\\n\\n```python\\n# 6\\nmodel_name = \\\"a2c-PandaPickAndPlace-v3\\\";\\nmodel.save(model_name)\\nenv.save(\\\"vec_nor...\"],[\"(Optional) What is Curiosity in Deep Reinforcement Learning?\\n\\nThis is an (optional) introduction to ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Using Curiosity will push our agent to favor transitions with high prediction error (which will be h...\"],[\"The â€œDeepâ€ in Reinforcement Learning [[deep-rl]]\\n\\n\\u003cTip\\u003e\\nWhat we've talked about so far is Reinforcem...\"],[\"What is RL? A short recap [[what-is-rl]]\\n\\nIn RL, we build an agent that canÂ **make smart decisions**...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Conclusion [[Conclusion]]\\n\\nThatâ€™s all for today. Congrats on finishing this unit and the tutorial!\\n\\n...\"],[\"Diving deeper into policy-gradient methods\\n\\n## Getting the big picture\\n\\nWe just learned that policy-...\"],[\"Now that we got the big picture, let's dive deeper into policy-gradient methods.\\n\\n## Diving deeper i...\"],[\"- \\\\\\\\(R(\\\\tau)\\\\\\\\) :  Return from an arbitrary trajectory. To take this quantity and use it to calculat...\"],[\"Our update step for gradient-ascent is:\\n\\n\\\\\\\\( \\\\theta \\\\leftarrow \\\\theta + \\\\alpha *  \\\\nabla_\\\\theta J(\\\\t...\"],[\"The Reinforce algorithm, also called Monte-Carlo policy-gradient, is a policy-gradient algorithm tha...\"],[\"Additional Readings [[additional-readings]]\\n\\n## Bias-variance tradeoff in Reinforcement Learning\\n\\nIf...\"],[\"Let's train and play with Huggy ğŸ¶ [[train]]\\n\\n\\n\\n\\n          \\u003cCourseFloatingBanner classNames=\\\"absolute...\"],[\"### The environment ğŸ®\\n\\n- Huggy the Dog, an environment created by [Thomas Simonini](https:\\u002f\\u002ftwitter....\"],[\"## Clone the repository and install the dependencies ğŸ”½\\n\\n- We need to clone the repository, that cont...\"],[\"```\\n\\n```bash\\n# Go inside the repository and install the package (can take 3min)\\n%cd ml-agents\\npip3 i...\"],[\"```\\n\\nDownload the file Huggy.zip from https:\\u002f\\u002fdrive.google.com\\u002fuc?export=download&id=1zv3M95ZJTWHUVO...\"],[\"```\\n\\n## Let's recap how this environment works\\n\\n### The State Space: what Huggy perceives.\\n\\nHuggy do...\"],[\"- *Orientation bonus*: we **reward him for getting close to the target**.\\n- *Time penalty*: a fixed-...\"],[\"```\\nbehaviors:\\n  Huggy:\\n    trainer_type: ppo\\n    hyperparameters:\\n      batch_size: 2048\\n      buff...\"],[\"```\\n\\n- Don't forget to save the file!\\n\\n- **In the case you want to modify the hyperparameters**, in ...\"],[\"```\\n\\n## Push the agent to the ğŸ¤— Hub\\n\\n- Now that we trained our agent, weâ€™re **ready to push it to th...\"],[\"```\\n\\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste...\"],[\"```\\n\\nItâ€™s the link to your model repository. The repository contains a model card that explains how ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fn...\"],[\"Mid-way Quiz [[mid-way-quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n### Q4: What is the difference between Monte Carlo and Temporal Difference learning meth...\"],[\"Bonus: Learn to create your own environments with Unity and MLAgents\\n\\n**You can create your own rein...\"],[\"Train your first Deep Reinforcement Learning Agent ğŸ¤– [[hands-on]]\\n\\n\\n\\n\\n      \\u003cCourseFloatingBanner cl...\"],[\"**If you don't find your model, go to the bottom of the page and click on the refresh button.**\\n\\nFor...\"],[\"### The library used ğŸ“š\\n\\n- [Stable-Baselines3](https:\\u002f\\u002fstable-baselines3.readthedocs.io\\u002fen\\u002fmaster\\u002f)\\n\\n...\"],[\"## Prerequisites ğŸ—ï¸\\n\\nBefore diving into the notebook, you need to:\\n\\nğŸ”² ğŸ“ **[Read Unit 0](https:\\u002f\\u002fhugg...\"],[\"There are **two** ways to find your optimal policy:\\n\\n- By **training your policy directly**: policy-...\"],[\"## Install dependencies and create a virtual screen ğŸ”½\\n\\nThe first step is to install the dependencies...\"],[\"```\\n\\n```bash\\npip install -r https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fdeep-rl-class\\u002fmain\\u002fnoteboo...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n## Understand Gymnasium and how it works ğŸ¤–\\n\\nğŸ‹ The library containing our environment is called ...\"],[\"For more explanations check this ğŸ‘‰ https:\\u002f\\u002fgymnasium.farama.org\\u002fapi\\u002fenv\\u002f#gymnasium.Env.step\\n\\nIf the ...\"],[\"```\\n\\n## Create the LunarLander environment ğŸŒ› and understand how it works\\n\\n### The environment ğŸ®\\n\\nIn ...\"],[\"```\\n\\nThe action space (the set of possible actions the agent can take) is discrete with 4 actions av...\"],[\"```\\n\\n## Create the Model ğŸ¤–\\n\\n- We have studied our environment and we understood the problem: **being...\"],[\"```\\n# Create environment\\nenv = gym.make('LunarLander-v2')\\n\\n# Instantiate the agent\\nmodel = PPO('MlpP...\"],[\"```\\n\\n## Evaluate the agent ğŸ“ˆ\\n\\n- Remember to wrap the environment in a [Monitor](https:\\u002f\\u002fstable-basel...\"],[\"```\\n\\n- In my case, I got a mean reward is `200.20 +\\u002f- 20.80` after training for 1 million steps, whi...\"],[\"```\\n\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins...\"],[\"# TODO: Define the model architecture we used\\nmodel_architecture = \\\"\\\"\\n\\n## TODO: Define the commit me...\"],[\"```\\n\\n#### Solution\\n\\n\\n```python\\nimport gymnasium as gym\\n\\nfrom stable_baselines3 import PPO\\nfrom stabl...\"],[\"```\\n\\nCongrats ğŸ¥³ you've just trained and uploaded your first Deep Reinforcement Learning agent. The s...\"],[\"```\\n\\n```python\\nfrom huggingface_sb3 import load_from_hub\\n\\nrepo_id = \\\"Classroom-workshop\\u002fassignment2-...\"],[\"```\\n\\n## Some additional challenges ğŸ†\\nThe best way to learn **is to try things by your own**! As you ...\"],[\"If youâ€™re still feel confused with all these elements...it's totally normal! **This was the same for...\"],[\"Glossary \\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n- **Tabular Method:** ...\"],[\"- **Fixed Q-Target:** In order to calculate the **Q-Target** we need to estimate the discounted opti...\"],[\"Live 1: How the course work, Q&A, and playing with Huggy\\n\\nIn this first live stream, we explained ho...\"],[\"Quiz [[quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera....\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"an action a0, action a0, state s0, state s1, reward r1\\\",\\n\\t\\t\\texpl...\"],[\"### Q3: What's the difference between a state and an observation?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext...\"],[\"### Q5: What is the exploration\\u002fexploitation tradeoff?\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\n\\nIn Re...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\nCongrats on finishing this Quiz ğŸ¥³, if you missed some elements, take time to read again...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"(Optional) the Policy Gradient Theorem\\n\\nIn this optional section where we're **going to study how we...\"],[\"\\\\\\\\(= \\\\sum_{\\\\tau} P(\\\\tau;\\\\theta) \\\\frac{\\\\nabla_\\\\theta P(\\\\tau;\\\\theta)}{P(\\\\tau;\\\\theta)}R(\\\\tau) \\\\\\\\)\\n\\nWe c...\"],[\"But we still have some mathematics work to do there: we need to simplify \\\\\\\\(  \\\\nabla_\\\\theta log P(\\\\t...\"],[\"We also know that the gradient of the sum is equal to the sum of gradient:\\n\\n\\\\\\\\( \\\\nabla_\\\\theta log P(...\"],[\"So, the final formula for estimating the policy gradient is:\\n\\n\\\\\\\\( \\\\nabla_{\\\\theta} J(\\\\theta) = \\\\hat{g...\"],[\"Introduction to Deep Reinforcement Learning [[introduction-to-deep-reinforcement-learning]]\\n\\n\\u003cimg sr...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Advantage Actor-Critic (A2C) [[advantage-actor-critic]]\\n\\n## Reducing variance with Actor-Critic meth...\"],[\"Let's see the training process to understand how the Actor and Critic are optimized:\\n- At each times...\"],[\"- The Critic then updates its value parameters.\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingfa...\"],[\"(Automatic) Curriculum Learning for RL\\n\\nWhile most of the RL methods seen in this course work well i...\"],[\"\\u003e â€¦ a family of mechanisms that automatically adapt the distribution of training data by learning to...\"],[\"### Recent methods\\n\\n- [Evolving Curricula with Regret-Based Environment Design](https:\\u002f\\u002farxiv.org\\u002fab...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this chapter!Â There was a lot of information. And c...\"],[\"Hands-on [[hands-on]]\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      no...\"],[\"To find your result, go to the [leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhuggingface-projects\\u002fDeep...\"],[\"### ğŸ® Environments:\\n\\n- [FrozenLake-v1](https:\\u002f\\u002fgymnasium.farama.org\\u002fenvironments\\u002ftoy_text\\u002ffrozen_lak...\"],[\"The best way to keep in touch is to join our discord server to exchange with the community and with ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fn...\"],[\"The Hugging Face Hub ğŸ¤— works as a central place where anyone can share and explore models and datase...\"],[\"```\\n\\n```bash\\nsudo apt-get update\\nsudo apt-get install -y python3-opengl\\napt install ffmpeg xvfb\\npip3...\"],[\"```\\n\\nWe're now ready to code our Q-Learning algorithm ğŸ”¥\\n\\n# Part 1: Frozen Lake â›„ (non slippery versi...\"],[\"```\\n\\n### Solution\\n\\n```python\\nenv = gym.make(\\\"FrozenLake-v1\\\", map_name=\\\"4x4\\\", is_slippery=False, rend...\"],[\"```\\n\\nWe see with `Observation Space Shape Discrete(16)` that the observation is an integer represent...\"],[\"```\\n\\nThe action space (the set of possible actions the agent can take) is discrete with 4 actions av...\"],[\"```\\n\\n```python\\n# Let's create our Qtable of size (state_space, action_space) and initialized each va...\"],[\"```\\n\\n## Define the epsilon-greedy policy ğŸ¤–\\n\\nEpsilon-greedy is the training policy that handles the e...\"],[\"```\\n\\n## Define the hyperparameters âš™ï¸\\n\\nThe exploration related hyperparamters are some of the most i...\"],[\"```\\n\\n```python\\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, ...\"],[\"```\\n\\n#### Solution\\n\\n```python\\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, e...\"],[\"```\\n\\n## Let's see what our Q-Learning table looks like now ğŸ‘€\\n\\n```python\\nQtable_frozenlake\\n```\\n\\n## Th...\"],[\"```\\n\\n## Evaluate our Q-Learning agent ğŸ“ˆ\\n\\n- Usually, you should have a mean reward of 1.0\\n- The **env...\"],[\"```\\n\\n```python\\ndef record_video(env, Qtable, out_directory, fps=1):\\n    \\\"\\\"\\\"\\n    Generate a replay vi...\"],[\"```\\n\\n```python\\ndef push_to_hub(repo_id, model, env, video_fps=1, local_repo_path=\\\"hub\\\"):\\n    \\\"\\\"\\\"\\n   ...\"],[\"evaluate_data = {\\n        \\\"env_id\\\": model[\\\"env_id\\\"],\\n        \\\"mean_reward\\\": mean_reward,\\n        \\\"n_...\"],[\"## Usage\\n\\n  model = load_from_hub(repo_id=\\\"{repo_id}\\\", filename=\\\"q-learning.pkl\\\")\\n\\n  # Don't forget ...\"],[\"```\\n\\n### .\\n\\nBy using `push_to_hub` **you evaluate, record a replay, generate a model card of your ag...\"],[\"```\\n\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins...\"],[\"```\\n\\nCongrats ğŸ¥³ you've just implemented from scratch, trained, and uploaded your first Reinforcement...\"],[\"```\\n\\n```python\\naction_space = env.action_space.n\\nprint(\\\"There are \\\", action_space, \\\" possible action...\"],[\"```\\n\\n## Define the hyperparameters âš™ï¸\\n\\nâš  DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to...\"],[\"```\\n\\n## Create a model dictionary ğŸ’¾ and publish our trained model to the Hub ğŸ”¥\\n\\n- We create a model ...\"],[\"```\\n\\nNow that it's on the Hub, you can compare the results of your Taxi-v3 with your classmates usin...\"],[\"```\\n\\n### .\\n\\n```python\\nmodel = load_from_hub(repo_id=\\\"ThomasSimonini\\u002fq-Taxi-v3\\\", filename=\\\"q-learning...\"],[\"```\\n\\n## Some additional challenges ğŸ†\\n\\nThe best way to learn **is to try things on your own**! As you...\"],[\"That's why we'll study Deep Q-Learning in the next unit, an algorithm **where we use a neural networ...\"],[\"An Introduction to Unreal Learning Agents\\n\\n[Learning Agents](https:\\u002f\\u002fdev.epicgames.com\\u002fcommunity\\u002flea...\"],[\"Armed with the basics, **you're now prepared to play with Learning Agents**:\\n\\n3. Get the Big Picture...\"],[\"Conclusion\\n\\nThatâ€™s all for today. Congrats on finishing this unit and the tutorial!\\n\\nThe best way to...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this bonus unit!\\n\\nYou can now sit and enjoy playing...\"],[\"Type of tasks [[tasks]]\\n\\nA task is an **instance** of a Reinforcement Learning problem. We can have ...\"],[\"The intuition behind PPO [[the-intuition-behind-ppo]]\\n\\n\\nThe idea with Proximal Policy Optimization (...\"],[\"The Bellman Equation: simplify our value estimation [[bellman-equation]]\\n\\nThe Bellman equationÂ **sim...\"],[\"So you may have noticed, we're repeating the computation of the value of different states, which can...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolv...\"],[\"Conclusion\\n\\nThat's all for today. Congrats on finishing this Unit and the tutorial! â­ï¸\\n\\nNow that you...\"],[\"Brief introduction to RL documentation\\n\\nIn this advanced topic, we address the question: **how shoul...\"],[\"Building on the documentation frameworks for [model cards](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1810.03993) and [da...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this unit and the tutorial. You've just trained you...\"],[\"Monte Carlo vs Temporal Difference Learning [[mc-vs-td]]\\n\\nThe last thing we need to discuss before d...\"],[\"- At the end of the episode,Â **we have a list of State, Actions, Rewards, and Next States tuples**\\nF...\"],[\"- We have a list of state, action, rewards, next_state,Â **we need to calculate the return \\\\\\\\(G{t=0}\\\\...\"],[\"The idea withÂ **TD is to update the \\\\\\\\(V(S_t)\\\\\\\\) at each step.**\\n\\nBut because we didn't experience a...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this chapter!Â There was a lot of information. And c...\"],[\"Glossary [[glossary]]\\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n### Agent\\n...\"],[\"### Policy\\n\\n- **Policy**: It is called the agent's brain. It tells us what action to take, given the...\"],[\"Offline vs. Online Reinforcement Learning\\n\\nDeep Reinforcement Learning (RL) is a framework **to buil...\"],[\"The process is as follows:\\n- **Create a dataset** using one or more policies and\\u002for human interactio...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\n### Q3: What's the difference between policy-based methods and policy-gradient methods?...\"],[\"Hands-on: advanced Deep Reinforcement Learning. Using Sample Factory to play Doom from pixels\\n\\n\\u003cCour...\"],[\"Please note the following points:\\n\\n*   [Sample Factory](https:\\u002f\\u002fwww.samplefactory.dev\\u002f) is an advanc...\"],[\"```\\n\\nTo validate this hands-on for the [certification process](https:\\u002f\\u002fhuggingface.co\\u002fdeep-rl-course...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"- Highly optimized algorithmÂ [architecture](https:\\u002f\\u002fwww.samplefactory.dev\\u002f06-architecture\\u002foverview\\u002f)...\"],[\"- [HuggingFace ğŸ¤— integration](https:\\u002f\\u002fwww.samplefactory.dev\\u002f10-huggingface\\u002fhuggingface\\u002f)Â (upload tra...\"],[\"All of the above policies are available on the ğŸ¤— hub. Search for the tag [sample-factory](https:\\u002f\\u002fhu...\"],[\"## ViZDoom\\n\\n[ViZDoom](https:\\u002f\\u002fvizdoom.cs.put.edu.pl\\u002f) is an **open-source python interface for the D...\"],[\"```python\\n# Install ViZDoom deps from\\n# https:\\u002f\\u002fgithub.com\\u002fmwydmuch\\u002fViZDoom\\u002fblob\\u002fmaster\\u002fdoc\\u002fBuilding...\"],[\"```\\n\\n## Then we can install Sample Factory and ViZDoom\\n\\n- This can take 7min\\n\\n```bash\\npip install sa...\"],[\"```\\n\\n## Setting up the Doom Environment in sample-factory\\n\\n```python\\nimport functools\\n\\nfrom sample_f...\"],[\"def register_vizdoom_components():\\n    register_vizdoom_envs()\\n    register_vizdoom_models()\\n\\n\\n# par...\"],[\"```\\n\\nNow that the setup if complete, we can train the agent. We have chosen here to learn a ViZDoom ...\"],[\"## Training the agent\\n\\n- We're going to train the agent for 4000000 steps. It will take approximatel...\"],[\"```\\n\\n## Let's take a look at the performance of the trained policy and output a video of the agent.\\n...\"],[\"```\\n\\nThe agent has learned something, but its performance could be better. We would clearly need to ...\"],[\"```\\n\\n## Let's load another model\\n\\n\\n\\n\\nThis agent's performance was good, but we can do better! Let's ...\"],[\"```\\n\\nGiven the agent plays for a long time the video generation can take **10 minutes**.\\n\\n```python\\n...\"],[\"Introducing the Clipped Surrogate Objective Function\\n## Recap: The Policy Objective Function\\n\\nLetâ€™s ...\"],[\"As we can see, \\\\\\\\( r_t(\\\\theta) \\\\\\\\) denotes the probability ratio between the current and old policy:...\"],[\"Consequently, we need to constrain this objective function by penalizing changes that lead to a rati...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q4: Explain in your own words what is the `Self-Play` approach\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsum...\"],[\"### Q6: What are the main motivations to use a ELO rating Score?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n   \\t\\t {\\n\\t\\t\\tt...\"],[\"The Problem of Variance in Reinforce [[the-problem-of-variance-in-reinforce]]\\n\\nIn Reinforce, we want...\"],[\"The solution is to mitigate the variance by **using a large number of trajectories, hoping that the ...\"],[\"An introduction to Multi-Agents Reinforcement Learning (MARL)\\n\\n## From single agent to multiple agen...\"],[\"Or a road with **several autonomous vehicles**.\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002f...\"],[\"- *Mixed of both adversarial and cooperative*: like in our SoccerTwos environment, two agents are pa...\"],[\"Student Works\\n\\nSince the launch of the Deep Reinforcement Learning Course, **many students have crea...\"],[\"In this project, Eric Dong recreates Bill Seiler's 1985 version of Space War in Pygame and uses rein...\"],[\"Introduction [[introduction]]\\n\\nIn this bonus unit, we'll reinforce what we learned in the first unit...\"],[\"Conclusion\\n\\n\\n**Congrats on finishing this unit**!Â There was a lot of information.\\nAnd congrats on fi...\"],[\"Visualize the Clipped Surrogate Objective Function\\n\\nDon't worry. **It's normal if this seems complex...\"],[\"Since the ratio is between intervals, **we can decrease the probability that our policy takes that a...\"],[\"If the probability ratio is higher than \\\\\\\\( [1 + \\\\epsilon] \\\\\\\\), the probability of taking that actio...\"],[\"The final Clipped Surrogate Objective Loss for PPO Actor-Critic style looks like this, it's a combin...\"],[\"Welcome to the ğŸ¤— Deep Reinforcement Learning Course [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface....\"],[\"Letâ€™s get started!\\n\\n## What to expect? [[expect]]\\n\\nIn this course, you will:\\n\\n- ğŸ“– Study Deep Reinfor...\"],[\"Sign up  ğŸ‘‰ \\u003ca href=\\\"http:\\u002f\\u002feepurl.com\\u002fic5ZUD\\\"\\u003ehere\\u003c\\u002fa\\u003e\\n\\n\\n## What does the course look like? [[course...\"],[\"There's **no deadlines, the course is self-paced**.\\nBoth paths **are completely free**.\\nWhatever pat...\"],[\"## What tools do I need? [[tools]]\\n\\nYou need only 3 things:\\n\\n- *A computer* with an internet connect...\"],[\"## What are the challenges in this course? [[challenges]]\\n\\nIn this new version of the course, you ha...\"],[\"Hands on\\n\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      notebooks={[\\n...\"],[\"**To start the hands-on click on Open In Colab button** ğŸ‘‡ :\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.researc...\"],[\"### ğŸ® Environments:\\n\\n- [CartPole-v1](https:\\u002f\\u002fwww.gymlibrary.dev\\u002fenvironments\\u002fclassic_control\\u002fcart_po...\"],[\"- `Hardware Accelerator \\u003e GPU`\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n## Import the packages ğŸ“¦\\n\\nIn addition to importing the installed libraries, we also import:\\n\\n- ...\"],[\"```\\n\\n```python\\nprint(device)\\n```\\n\\nWe're now ready to implement our Reinforce algorithm ğŸ”¥\\n\\n# First ag...\"],[\"```\\n\\n```python\\nprint(\\\"_____OBSERVATION SPACE_____ \\\\n\\\")\\nprint(\\\"The State Space is: \\\", s_size)\\nprint(\\\"...\"],[\"```\\n\\n## Let's build the Reinforce Architecture\\n\\nThis implementation is based on three implementation...\"],[\"```\\n\\n### Solution\\n\\n```python\\nclass Policy(nn.Module):\\n    def __init__(self, s_size, a_size, h_size)...\"],[\"```\\n\\n- Here we see that the error says `ValueError: The value argument to log_prob must be a Tensor`...\"],[\"```\\n\\nBy using CartPole, it was easier to debug since **we know that the bug comes from our integrati...\"],[\"The second question you may ask is **why do we minimize the loss**? Didn't we talk about Gradient As...\"],[\"# Line 6 of pseudocode: calculate the return\\n        returns = deque(maxlen=max_t)\\n        n_steps =...\"],[\"## standardization of the returns is employed to make training more stable\\n        eps = np.finfo(np...\"],[\"```\\n\\n#### Solution\\n\\n```python\\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, pr...\"],[\"## Given the above, we calculate the returns at timestep t as:\\n        #               gamma[t] * re...\"],[\"```\\n\\n##  Train it\\n- We're now ready to train our agent.\\n- But first, we define a variable containing...\"],[\"```\\n\\n## Define evaluation method ğŸ“\\n- Here we define the evaluation method that we're going to use to...\"],[\"```\\n\\n```python\\ndef record_video(env, policy, out_directory, fps=30):\\n    \\\"\\\"\\\"\\n    Generate a replay v...\"],[\"```\\n\\n```python\\ndef push_to_hub(repo_id,\\n                model,\\n                hyperparameters,\\n    ...\"],[\"evaluate_data = {\\n          \\\"env_id\\\": hyperparameters[\\\"env_id\\\"],\\n          \\\"mean_reward\\\": mean_rewar...\"],[\"with readme_path.open(\\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n      f.write(readme)\\n\\n    # Save our metrics to ...\"],[\"```\\n\\nBy using `push_to_hub`, **you evaluate, record a replay, generate a model card of your agent, a...\"],[\"```\\n\\nNow that we tested the robustness of our implementation, let's try a more complex environment: ...\"],[\"```\\n\\nThe observation space (7) ğŸ‘€:\\n- player y position\\n- player velocity\\n- player distance to floor\\n-...\"],[\"```\\n\\n#### Solution\\n\\n```python\\nclass Policy(nn.Module):\\n    def __init__(self, s_size, a_size, h_size...\"],[\"```\\n\\n###  Train it\\n- We're now ready to train our agent ğŸ”¥.\\n\\n```python\\n# Create policy and place it t...\"],[\"```\\n\\n## Some additional challenges ğŸ†\\n\\nThe best way to learn **is to try things on your own**! As you...\"],[\"See you in Unit 5! ğŸ”¥\\n\\n### Keep Learning, stay awesome ğŸ¤—...\"],[\"Setup [[setup]]\\n\\nAfter all this information, it's time to get started. We're going to do two things:...\"],[\"Second Quiz [[quiz2]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.c...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n### Q4: Can you explain what is Epsilon-Greedy Strategy?\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fs...\"],[\"Two main approaches for solving RL problems [[two-methods]]\\n\\n\\u003cTip\\u003e\\nNow that we learned the RL framew...\"],[\"This function will define a mapping from each state to the best corresponding action. Alternatively,...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"Here we see that our value functionÂ **defined values for each possible state.**\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"...\"],[\"The Exploration\\u002fExploitation trade-off [[exp-exp-tradeoff]]\\n\\nFinally, before looking at the differen...\"],[\"If itâ€™s still confusing, **think of a real problem: the choice of picking a restaurant:**\\n\\n\\n\\u003cfigure\\u003e...\"],[\"Generalization in Reinforcement Learning\\n\\nGeneralization plays a pivotal role in the realm of Reinfo...\"],[\"Godot RL Agents\\n\\n[Godot RL Agents](https:\\u002f\\u002fgithub.com\\u002fedbeeching\\u002fgodot_rl_agents) is an Open Source ...\"],[\"In this section, you will **learn how to create a custom environment in the Godot Game Engine** and ...\"],[\"In order to create games in Godot, **you must first download the editor**. Godot RL Agents supports ...\"],[\"The Godot RL Agents plugin can be installed from the Github repo or with the Godot Asset Lib in the ...\"],[\"The AI Controller Node should have been added to the scene tree, next to it is a scroll. Click on it...\"],[\"```\\n\\nIn order to implement these methods, we will need to create a class that inherits from AIContro...\"],[\"```\\n\\nWe have now defined the agentâ€™s observation, which is the position and velocity of the ball in ...\"],[\"```\\n\\nWe now need to synchronize between the game running in Godot and the neural network being train...\"],[\"An Introduction to Unity ML-Agents [[introduction-to-ml-agents]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"In this Unit, we'll learn to use ML-Agents, but **don't worry if you don't know how to use the Unity...\"],[\"Introduction [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002f...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q2: What of the following statements are true about Unity ML-Agents?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n...\"],[\"### Q5: Which are the differences between capturing the environment using `frames` or `raycasts`?\\n\\n\\u003c...\"],[\"Model Based Reinforcement Learning (MBRL)\\n\\nModel-based reinforcement learning only differs from its ...\"],[\"We employ sample-based model-predictive control (MPC) using the learned dynamics model, which optimi...\"],[\"Hands-on [[hands-on]]\\n\\nNow that you've learned to use Optuna, here are some ideas to apply what you'...\"],[\"[The Hugging Face Deep Reinforcement Learning Course ğŸ¤— (v2.0)](https:\\u002f\\u002fhuggingface.co\\u002fdeep-rl-course...\"],[\"How do Unity ML-Agents work? [[how-mlagents-works]]\\n\\nBefore training our agent, we need to understan...\"],[\"- The first is the *Learning Environment*, which contains **the Unity scene (the environment) and th...\"],[\"Now, letâ€™s imagine an agent learning to play a platform game. The RL process looks like this:\\n\\n\\u003cimg ...\"],[\"The Pyramid environment\\n\\nThe goal in this environment is to train our agent to **get the gold brick ...\"],[\"We also use a **boolean variable indicating the switch state** (did we turn on or off the switch to ...\"],[\"How Huggy works [[how-huggy-works]]\\n\\nHuggy is a Deep Reinforcement Learning environment made by Hugg...\"],[\"## The Reward Function [[reward-function]]\\n\\nThe reward function is designed so that **Huggy will ful...\"],[\"The training environment looks like this:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-dee...\"],[\"Interesting Environments to try\\n\\nHere we provide a list of interesting environments you can try to t...\"],[\"To start using this environment, check these resources:\\n- [DonkeyCar Simulator documentation](https:...\"],[\"Self-Play: a classic technique to train competitive agents in adversarial games\\n\\nNow that we've stud...\"],[\"Itâ€™s the same way humans learn in competition:\\n\\n- We start to train against an opponent of similar l...\"],[\"Training against a set of slowly changing or unchanging adversaries with low diversity **results in ...\"],[\"This ELO (starting at a specific score: frequently 1200) can decrease initially but should increase ...\"],[\"- K=16 for master.\\n- K=32 for weaker players.\\n\\nIf Player A has Ea points but scored Sa points, then ...\"],[\"### The Disadvantages\\n\\n- ELO **does not take into account the individual contribution** of each peop...\"],[\"Quiz [[quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera....\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\n### Q4: What are the two phases of Deep Q-Learning?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext:...\"],[\"Deep Q-Learning [[deep-q-learning]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-c...\"],[\"Apache License\\n                           Version 2.0, January 2004\\n                        http:\\u002f\\u002fw...\"],[\"\\\"Contribution\\\" shall mean any work of authorship, including\\n      the original version of the Work a...\"],[\"4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works the...\"],[\"6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, serv...\"],[\"END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To ap...\"],[\"Optuna Tutorial [[optuna]]\\n\\nThe content below comes from [Antonin's Raffin ICRA 2022 presentations](...\"],[\"Hands-on [[hands-on]]\\n\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      ...\"],[\"For more information about the certification process, check this section ğŸ‘‰ https:\\u002f\\u002fhuggingface.co\\u002fde...\"],[\"### ğŸ® Environments:\\n\\n- [SpacesInvadersNoFrameskip-v4](https:\\u002f\\u002fgymnasium.farama.org\\u002fenvironments\\u002fatar...\"],[\"For more information about the certification process, check this section ğŸ‘‰ https:\\u002f\\u002fhuggingface.co\\u002fde...\"],[\"```\\n\\nIF AND ONLY IF THE VERSION ABOVE DOES NOT EXIST ANYMORE. UNCOMMENT AND INSTALL THE ONE BELOW\\n\\n`...\"],[\"```\\nSpaceInvadersNoFrameskip-v4:\\n  env_wrapper:\\n    - stable_baselines3.common.atari_wrappers.AtariW...\"],[\"```\\n\\nHere we see that:\\n- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,gray...\"],[\"```\\n\\n## Let's evaluate our agent ğŸ‘€\\n\\n- RL-Baselines3-Zoo provides `enjoy.py`, a python script to eval...\"],[\"```\\n\\n## Publish our trained model on the Hub ğŸš€\\nNow that we saw we got good results after the trainin...\"],[\"```\\n\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins...\"],[\"```\\n\\n###.\\n\\nCongrats ğŸ¥³ you've just trained and uploaded your first Deep Q-Learning agent using RL-Bas...\"],[\"1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can cal...\"],[\"```\\n\\n2. Let's evaluate if for 5000 timesteps\\n\\n```bash\\npython -m rl_zoo3.enjoy --algo dqn --env BeamR...\"],[\"```\\n\\nWhy not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? ğŸ†.**\\n...\"],[\"If youâ€™re still feel confused with all these elements...it's totally normal! **This was the same for...\"],[\"Introduction to PPO with Sample-Factory\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-...\"],[\"The Deep Q-Learning Algorithm [[deep-q-algorithm]]\\n\\nWe learned that Deep Q-Learning **uses a deep ne...\"],[\"To help us stabilize the training, we implement three different solutions:\\n1. *Experience Replay* to...\"],[\"Experience replay also has other benefits. By randomly sampling the experiences, we remove correlati...\"],[\"Itâ€™s like if you were a cowboy (the Q estimation) and you wanted to catch a cow (the Q-target). Your...\"],[\"## Double DQN [[double-dqn]]\\n\\nDouble DQNs, or Double Deep Q-Learning neural networks, were introduce...\"],[\"A Q-Learning example [[q-learning-example]]\\n\\nTo better understand Q-Learning, let's take a simple ex...\"],[\"## Step 1: Initialize the Q-table [[step1]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-d...\"],[\"## Step 4: Update Q(St, At) [[step4]]\\n\\nWe can now update \\\\\\\\(Q(S_t, A_t)\\\\\\\\) using our formula.\\n\\n\\u003cimg ...\"],[\"## Step 4: Update Q(St, At) [[step4-4]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-...\"],[\"Congratulations\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002f...\"],[\"What is Reinforcement Learning? [[what-is-reinforcement-learning]]\\n\\nTo understand Reinforcement Lear...\"],[\"Thatâ€™s how humans and animals learn,Â **through interaction.**Â Reinforcement Learning is just aÂ **com...\"],[\"Introduction [[introduction]]\\n\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-cours...\"],[\"Mid-way Recap [[mid-way-recap]]\\n\\nBefore diving into Q-Learning, let's summarize what we've just lear...\"],[\"Introduction [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002f...\"],[\"\\u003cfigcaption\\u003eThis environment was made by the \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fUnity-Technologies\\u002fml-agent...\"],[\"RLHF\\n\\nReinforcement learning from human feedback (RLHF) is a **methodology for integrating human dat...\"],[\"## Additional readings\\n\\n*Note, this is copied from the Illustrating RLHF blog post above*.\\nHere is a...\"],[\"And here is a snapshot of the growing set of papers that show RLHF's performance for LMs:\\n- [Fine-Tu...\"],[\"- [ChatGPT: Optimizing Language Models for Dialogue](https:\\u002f\\u002fopenai.com\\u002fblog\\u002fchatgpt\\u002f) (OpenAI 2022)...\"],[\"## Author\\n\\nThis section was written by \\u003ca href=\\\"https:\\u002f\\u002ftwitter.com\\u002fnatolambert\\\"\\u003e Nathan Lambert \\u003c\\u002fa...\"]],\"hovertemplate\":\"source=deep-rl-class\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"deep-rl-class, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"deep-rl-class, circle\",\"showlegend\":true,\"x\":[9.082232,8.999114,8.642559,8.577522,8.676041,8.638391,8.401682,8.375944,8.1775055,2.020638,8.123057,8.414981,8.427068,-0.81178075,8.132774,8.338744,3.1972353,8.16499,8.343395,8.821538,8.822193,8.72224,8.616065,8.748145,8.791688,8.718999,8.891427,8.646789,8.691977,8.962857,8.416289,-6.6304393,-6.299041,7.6195035,9.115114,9.192769,8.57818,8.794606,8.786401,8.662167,8.550231,8.650272,8.278041,8.151134,8.453836,8.315736,8.742804,9.10681,9.15272,8.602476,8.489215,8.342662,8.29891,2.1993868,2.3554757,8.27485,8.120452,-1.2440627,3.1547048,2.7744064,8.137736,2.706188,2.1021929,8.290221,8.25878,8.486786,7.73067,8.483252,9.0281315,8.922104,9.043943,9.047673,8.875346,8.803884,8.794601,8.211672,8.431579,8.610087,8.768017,8.44974,8.432088,8.519263,8.439908,8.331088,8.272685,1.9276117,2.044846,3.2488132,2.2572484,2.1248274,1.6340247,2.2046542,1.1134902,0.68818784,0.6951071,0.8698857,-0.81135696,-0.5689781,2.6645827,1.895599,1.1367757,-0.93302405,-0.9706309,-1.0850939,-0.91968143,-1.24042,-0.758206,0.13779083,3.6613426,2.2370298,8.196243,8.288314,8.00194,8.831772,8.828469,8.693312,8.400035,8.389606,2.0000732,2.1027803,8.164759,8.0138035,7.948877,7.915245,2.9610286,3.1007478,8.215169,8.132303,8.571606,8.523002,8.55728,8.877097,8.778144,8.726915,8.632843,8.776603,8.839568,8.751232,8.734461,8.757188,8.680123,8.383336,8.344689,3.0107388,2.0906699,2.886678,8.2569275,8.239781,-3.3404005,8.158526,3.229689,2.705446,2.6616783,8.600835,8.654151,8.593495,8.09599,8.4278965,8.495787,8.717987,8.657905,8.717322,7.7693486,1.9290419,2.5789027,8.239241,7.742728,7.898656,8.038462,8.346417,8.075457,8.089702,8.327946,2.5939186,2.3622909,1.8253064,8.124799,1.5334085,8.643532,8.628873,8.957867,9.135451,8.726754,8.62008,8.32896,8.488026,8.606496,7.834009,8.681048,8.776783,8.737244,8.669976,8.714568,8.71718,8.547196,8.9021225,8.70089,8.753157,8.746846,8.48682,8.354637,8.5997505,8.882463,8.646354,8.50234,8.6504965,8.906685,8.563431,3.4396095,1.9272323,8.448267,7.633091,7.9889755,8.045785,8.379992,8.687294,8.515057,8.409438,8.57172,8.129609,8.347002,10.269739,2.2844758,1.1871305,1.7337176,3.0083098,2.680479,8.189047,7.921729,7.933166,0.7820188,2.647764,0.9915296,8.8948965,9.144382,8.262278,8.380074,8.689887,8.549029,8.352057,8.671303,8.582362,8.638687,8.65454,8.664137,8.490782,-5.93732,8.585136,8.826104,8.476687,8.75051,8.746545,8.817956,8.735486,8.573028,8.880741,8.576795,8.576173,8.584611,8.570264,8.320436,-3.1269126,8.37849,6.8939123,-3.325588,3.7895854,8.301346,7.7081084,1.5014875,1.8488082,-0.647912,1.6535597,8.2130165,7.907288,10.127296,2.6944463,1.9288372,8.138784,8.684806,8.679673,8.687953,8.568922,8.503099,7.7350607,8.693755,8.741102,8.513485,8.4232645,8.42744,8.277926,8.161587,8.447956,8.474505,8.692953,8.691723,8.684793,8.671024,8.804343,8.593083,8.615129,8.639133,8.986182,8.470638,8.4587755,8.477673,8.199504,1.7750643,1.9739652,1.5278149,7.8511715,0.20827338,8.434444,-0.9925285,-0.7691424,8.687982,-1.9790366,-0.87163085,-1.7859818,-1.4218367,-1.3563942,8.02491,8.079379,10.24824,2.2473814,1.0723543,1.9492925,2.8402176,7.9687877,8.098903,-1.2931373,8.201355,8.6047735,8.670275,8.720342,8.7772255,8.737441,8.797924,8.761155,8.829822,8.744303,8.487331,8.537685,8.648601,8.129311,7.9968,7.8001943,7.937824,7.7318683,7.6969714,7.828338,7.939207,8.244325,8.35843,8.690242,8.282819,8.350029,8.420992,8.373494,8.587723,8.587222,8.587123,8.771027,8.21314,8.255376,8.531927,8.286782,8.018243,8.312536,8.296321,8.370205,8.030981,8.2240095,8.411908,8.490978,8.395534,-3.847412,-4.572332,-0.6616797,8.829788,9.009596,9.0528755,6.6008782,6.6109104,6.8039417,6.8257675,6.560846,-3.95078,8.61527,8.503494,8.481922,8.063228,1.8233094,-3.1447961,-2.697414,7.972219,2.9367418,2.622645,8.28487,1.0397906,0.40436593,8.763264,8.732917,8.25415,9.149808,9.084589,9.10531,9.096444,9.150548,8.945236,8.953996,8.841853,8.958067,8.690667,8.683035,8.677141,8.943985,8.90954,8.406175,8.1792755,-5.943043,8.552911,-6.0627556,-6.0237265,7.5354776],\"xaxis\":\"x\",\"y\":[-11.472403,-11.174465,-11.1912155,-11.001291,-11.446587,-11.280019,-10.199799,-10.164031,-10.01395,0.63414353,-9.821891,-10.392504,-10.402736,-0.5789134,-9.943647,-9.963054,0.7262455,-9.905213,-10.05165,-11.728169,-11.658994,-11.410547,-11.361828,-11.65361,-11.69143,-11.698207,-11.478808,-10.3261,-10.342405,-11.139804,-10.233415,0.60893345,0.25769794,2.71437,-11.393459,-11.408186,-10.067792,-11.501299,-11.566722,-11.470225,-11.094958,-11.377182,-10.696449,-10.374725,-11.115271,-11.530745,-11.793262,-11.353968,-11.37717,-11.110999,-10.234813,-10.164047,-10.050055,1.1559435,2.0680234,-9.9829445,-9.911651,-2.4456701,-0.44520152,0.03699744,-9.9128275,2.5101237,1.929756,-10.080036,-10.032623,-11.749749,3.0583642,-10.577182,-11.366183,-11.391156,-11.390092,-11.3520565,-11.488247,-11.548677,-11.521033,-9.8716755,-9.908572,-10.809223,-10.93559,-10.513131,-10.659608,-11.594514,-10.429038,-10.358491,-10.195367,0.37262934,0.32829168,0.8908806,-0.18235017,0.16737537,0.43776712,0.67196697,1.884301,-0.34720775,1.4923693,1.1393967,1.4122034,1.4094142,-0.14201221,0.3695598,1.1031946,1.3880947,0.04537896,1.51807,1.8927983,1.2200667,1.8310263,1.3966548,0.1778958,-0.25466627,-10.288278,-10.745262,-10.1830635,-11.711018,-11.780824,-11.723026,-10.636149,-10.462124,0.41554824,0.40747842,-10.451025,-10.420318,-9.978585,-10.215093,-0.573587,0.2341923,-10.291489,-10.138378,-11.136772,-11.049223,-11.160748,-11.238683,-11.556668,-11.467108,-10.238617,-11.800613,-11.85616,-11.856557,-11.900127,-11.865432,-11.481926,-10.047878,-10.110617,1.0919214,1.2004974,2.0760355,-10.412341,-10.239975,-0.42653942,-9.877797,-0.535585,0.024089808,0.79123074,-10.140753,-11.079676,-10.97646,-9.971324,-10.2556505,-10.319182,-10.460613,-10.67461,-11.11039,-9.602515,0.33492127,0.38445085,-10.560316,-10.287791,-10.304345,-10.421047,-10.3312025,-10.167944,-10.313607,-10.029456,-0.1469237,-0.0152085805,0.050386015,-9.8706,0.5751288,-10.53777,-10.336204,-11.3628235,-11.392421,-10.247207,-10.994419,-10.620565,-10.854355,-11.12801,3.2375026,-11.306908,-11.845905,-11.992383,-12.012331,-12.02316,-12.092017,-10.393377,-11.339894,-11.66135,-11.562132,-11.929795,-10.849316,-10.258038,-11.057779,-10.672799,-10.527141,-10.274689,-10.5120735,-11.145873,-10.245041,0.9998495,0.44558078,-10.695011,-10.175175,-10.4165325,-10.410098,-11.026456,-11.560595,-11.060931,-10.977571,-11.06868,-10.431503,-10.295445,3.4691966,-0.2223838,2.9795566,-0.123828426,-0.54902136,-0.01798361,-10.373845,-10.446473,-10.580269,-0.31967455,0.34959304,0.25948206,-10.8973255,-11.339877,-10.229481,-10.407193,-10.338179,-10.166871,-10.858924,-11.909621,-11.374677,-11.515728,-11.626209,-10.513588,-11.388557,0.95217687,-10.349426,-11.521992,-10.959152,-11.601144,-11.48597,-11.4840975,-10.630956,-11.124046,-11.768319,-11.1453905,-11.216107,-11.265567,-11.210943,-10.205404,-2.0053508,-10.256834,0.15254316,-1.2878476,1.2894585,-10.42849,-9.713386,0.59487927,0.8836881,0.95867443,0.9826278,-10.167229,-9.758104,3.4302669,-0.56199646,0.31924665,-10.123463,-11.945753,-12.066825,-12.071565,-10.9179,-10.894409,3.6602023,-11.746693,-11.700902,-10.485382,-10.430741,-10.339965,-10.20063,-10.279483,-10.133717,-10.399923,-12.054867,-12.124007,-12.038487,-11.940956,-10.441983,-10.41078,-10.367095,-9.96321,-10.204072,-10.308052,-10.25264,-10.315082,-10.14044,0.5456637,0.38429177,0.5447352,-10.336063,3.3146472,-10.966887,1.9360813,2.343457,-12.01509,1.2076185,2.021735,1.5102979,1.9671612,2.1570323,-10.145013,-10.246076,3.4526694,-0.10203106,2.4492085,-0.04634294,-0.4772787,-10.412549,-10.67276,1.5641596,-10.009359,-10.336566,-10.180801,-10.256016,-11.186958,-11.525835,-11.631367,-11.683028,-11.714265,-11.498503,-10.97188,-11.009605,-11.603224,-10.020047,-9.781457,-9.820551,-9.831733,-10.02687,-10.085093,-10.261285,-9.927764,-10.0597515,-10.2354355,-11.750009,-10.142351,-10.683357,-10.651752,-10.738196,-11.508844,-11.533028,-10.5329075,-10.496541,-10.035824,-10.420488,-11.109904,-10.495644,-10.344913,-10.039217,-10.316629,-10.083289,-9.987296,-10.203039,-10.30061,-10.733816,-10.468435,-0.08223127,0.21053553,7.4191566,-11.186923,-11.38089,-11.232345,2.8543024,2.804363,2.9310987,2.918298,2.922949,-0.68317467,-10.412729,-10.283795,-10.333526,-9.831897,0.45762664,-2.3484797,-1.769893,-10.130726,-0.70803946,-0.14159718,-10.027346,0.1899869,2.7363632,-10.69208,-10.591703,-10.132546,-11.458122,-11.406462,-11.432023,-11.410418,-11.465892,-11.343833,-11.34997,-11.315784,-11.254743,-10.272769,-11.2774725,-11.32047,-11.574326,-11.461724,-10.311048,-9.967734,0.58855057,-11.250946,0.71931607,0.5790036,2.7633078],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"__The following example applies the acceleration features powered by ONNX Runtime.__\\n\\n\\n### Onnxrunti...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [t5-large](https:\\u002f\\u002fhuggingface.co\\u002ft5-large) m...\"],[\"Inference pipelines with the ONNX Runtime accelerator\\n\\nThe [`~pipelines.pipeline`] function makes it...\"],[\"```\\n\\n_Note: The default models used in the [`~pipelines.pipeline`] function are not optimized for in...\"],[\"```\\n\\nIt is also possible to load it with the `from_pretrained(model_name_or_path, export=True)`\\nmeth...\"],[\"```\\n\\nIt is also possible to load it with the `from_pretrained(model_name_or_path)`\\nmethod associated...\"],[\"```\\n\\n\\n## Optimizing and quantizing in pipelines\\n\\nThe [`~pipelines.pipeline`] function can not only r...\"],[\"\\u003e\\u003e\\u003e # Load the quantized model from a local repository\\n\\u003e\\u003e\\u003e model = ORTModelForSequenceClassification...\"],[\"```\\n\\n### Optimizing with `ORTOptimizer`\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e fr...\"],[\"# Save and push the model to the hub\\n\\u003e\\u003e\\u003e tokenizer.save_pretrained(\\\"new_path_for_directory\\\")  # doct...\"],[\"Quantization\\n\\n## AutoGPTQ Integration\\n\\nğŸ¤— Optimum collaborated with [AutoGPTQ library](https:\\u002f\\u002fgithub...\"],[\"With ğŸ¤— Transformers integration, you don't need to pass the `block_name_to_quantize` and `model_seql...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\nGPTQ quantization only works for text model for now. Futhermore, the quant...\"],[\"```\\n\\n### Exllama kernels for faster inference\\n\\nWith the release of exllamav2 kernels, you can get fa...\"],[\"```\\n\\nNote that only 4-bit models are supported with exllama\\u002fexllamav2 kernels for now. Furthermore, ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### ORTModelForImageClassification\\n\\n[[autodoc]] onnxruntime.ORTModelForImageClassification\\n\\n### ORTM...\"],[\"[[autodoc]] onnxruntime.ORTStableDiffusionPipeline\\n    - __call__\\n\\n#### ORTStableDiffusionImg2ImgPip...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Notebook                                                                                          ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"----------------------------------------------------------------------------------------------------...\"],[\"| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https:\\u002f\\u002fgithub....\"],[\"## Optimum Intel\\n\\n### OpenVINO...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to run inference with OpenVINO](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel\\u002fblob\\u002fmain\\u002fnotebo...\"],[\"| [How to quantize a question answering model with NNCF](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-inte...\"],[\"| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https:...\"],[\"### Neural Compressor...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## Optimum ONNX Runtime...\"],[\"| Notebook                                                                                          ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"---------------------------------------------------------------|------------------------------------...\"],[\"| [How to quantize a model with ONNX Runtime for text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to fine-tune a model for summarization with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnote...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\\n\\n```python\\n\\u003e\\u003e\\u003e from opti...\"],[\"```\\n\\nStatic quantization relies on feeding batches of data through the model to estimate the activat...\"],[\"# Define the processing function to apply to each example after loading the dataset\\n\\u003e\\u003e\\u003e def preproce...\"],[\"```\\n\\nAs a final example, let's take a look at applying _graph optimizations_ techniques such as oper...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fintel\\u002finf...\"],[\"```\\n\\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\\n\\n```python\\n\\u003e\\u003e\\u003e from opti...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fonnxrunti...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fhabana\\u002fqu...\"],[\"```\\n\\nCheck out the help for more options:\\n\\n```bash\\noptimum-cli export onnx --help\\n```\\n\\nCheck out the...\"],[\"```\\n\\nCheck out the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fbettertransformer\\u002foverview) f...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"```\\n\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Instal...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eOptimize your model to speedup inference with \\u003cspan class=\\\"underline\\\" oncli...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eEnable performance optimizations for \\u003cspan class=\\\"underline\\\" onclick=\\\"event...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eApply quantization and graph optimization to accelerate Transformers models...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"```\\n\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Instal...\"],[\"All community leaders are obligated to respect the privacy and security of the\\nreporter of any incid...\"],[\"Community Impact Guidelines were inspired by\\n[Mozilla's code of conduct enforcement ladder][Mozilla ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Step 2: Set your model on your preferred device\\n\\nIf you did not used `device_map=\\\"auto\\\"` to ...\"],[\"```\\nIf you want to run a pipeline on a GPU device, run:\\n```python\\n\\u003e\\u003e\\u003e from optimum.pipelines import ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nAfter implementing it, your transformation can be used as a regular function:\\n\\n```python\\n\\u003e\\u003e\\u003e fr...\"],[\"```\\n\\n### Composing transformations together\\n\\nAs applying multiple transformations in chain is needed...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Register commands in the Optimum CLI from a subpackage\\n\\nIt is possible to register a command in the ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Quantizing a model to be used with Optimum's CLI\\n\\nThe Optimum ONNX Runtime quantization t...\"],[\"```\\n\\nQuantizing an ONNX model can be done as follows:\\n\\n```bash\\n optimum-cli onnxruntime quantize --o...\"],[\"```\\n\\n\\n## Apply Dynamic Quantization\\n\\nThe [`~optimum.onnxruntime.ORTQuantizer`] class can be used to ...\"],[\"```\\n\\n## Static Quantization example\\n\\nThe [`~optimum.onnxruntime.ORTQuantizer`] class can be used to ...\"],[\"# Create the calibration configuration containing the parameters related to calibration.\\n\\u003e\\u003e\\u003e calibra...\"],[\"```\\n\\n## Quantize Seq2Seq models\\n\\nThe [`~optimum.onnxruntime.ORTQuantizer`] class currently doesn't s...\"],[\"```\\n\\n3. Quantize all models\\n\\n```python\\n# Define the quantization strategy by creating the appropriat...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion Text-to-Image Fine-Tuning\\n\\nThis example shows how to leverage ONNX Runtime Training...\"],[\"```\\n\\nAnd initialize an [ğŸ¤—Accelerate](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\u002f) environment with:\\n\\n...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo run on your own training files prepare the dataset accordin...\"],[\"```\\n\\n#### Training with multiple GPUs\\n\\n`accelerate` allows for seamless multi-GPU training. Follow t...\"],[\"Overview\\n\\nğŸ¤— Optimum provides an integration with Torch FX, a library for PyTorch that allows develop...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eTechnical descriptions of how the Torch FX classes and methods of ğŸ¤— Optimum...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe Optimum TFLite export can be used through Optimum command-line. As only static input shapes...\"],[\"Required arguments:\\n  -m MODEL, --model MODEL\\n                        Model ID on huggingface.co or ...\"],[\"Input shapes:\\n  --batch_size BATCH_SIZE\\n                        Batch size that the TFLite exported ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Accelerated inference on NVIDIA GPUs\\n\\nBy default, ONNX Runtime runs inference on CPU devices. Howeve...\"],[\"```\\n\\nTo avoid conflicts between `onnxruntime` and `onnxruntime-gpu`, make sure the package `onnxrunt...\"],[\"```\\nValueError: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the av...\"],[\"```\\n\\nAdditionally, you can pass the session option `log_severity_level = 0` (verbose), to check whet...\"],[\"```\\n\\nIn this example, we can see that all the costly MatMul operations are placed on the CUDA execut...\"],[\"To avoid the slowdown, ğŸ¤— Optimum adopts the IOBinding to copy inputs onto GPUs and pre-allocate memo...\"],[\"```\\n\\nFor the time being, IOBinding is supported for task-defined ORT models, if you want us to add s...\"],[\"\\u003ctable\\u003e\\u003ctr\\u003e\\n\\u003ctd\\u003e\\n  \\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg alt=\\\"GPT2\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptim...\"],[\"And here is a summary for the saving time with different sequence lengths (32 \\u002f 128) and generation ...\"],[\"```\\n+-----------------------------------------------------------------------------+\\n| NVIDIA-SMI 440...\"],[\"- Platform: Linux-5.4.0-1089-aws-x86_64-with-glibc2.29\\n- Python version: 3.8.10\\n- `transformers` ver...\"],[\"```\\n\\nNote that previous experiments are run with __vanilla ONNX__ models exported directly from the ...\"],[\"```\\n\\n### Checking the TensorRT installation is successful\\n\\nBefore going further, run the following s...\"],[\"```\\n\\nsomething is wrong with the TensorRT or ONNX Runtime installation.\\n\\n### TensorRT engine build a...\"],[\"```\\n\\nTensorRT builds its engine depending on specified input shapes. Unfortunately, in the [current ...\"],[\"Passing the engine cache path in the provider options, the engine can therefore be built once for al...\"],[\"```\\n\\nThe engine is stored as:\\n\\n![TensorRT engine cache folder](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptim...\"],[\"```\\n\\n#### Warmup\\n\\nOnce the engine is built, it is recommended to do before inference **one or a few ...\"],[\"```\\n\\nThe model can then be used with the common ğŸ¤— Transformers API for inference and evaluation, suc...\"],[\"```\\n\\nUsing this `qconfig`, static quantization can be performed as explained in the [static quantiza...\"],[\"\\u003e\\u003e\\u003e res = ort_model(**inp)\\n\\n\\u003e\\u003e\\u003e print(res)\\n\\u003e\\u003e\\u003e print(ort_model.config.id2label[res.logits[0].argmax(...\"],[\"```\\n\\nThe model can then be used with the common ğŸ¤— Transformers API for inference and evaluation, suc...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] exporters.onnx.OnnxConfigWithPast\\n    - add_past_key_values\\n\\n[[autodoc]] exporters.onnx....\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you'd like to use the accelerator-specific features of ğŸ¤— Optimum, you can install the requir...\"],[\"The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the...\"],[\"```\\n\\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator_t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Supported architectures from [ğŸ¤— Transformers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002findex):\\n\\n- AS...\"],[\"Supported architectures from [ğŸ¤— Diffusers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002findex):\\n- Stable Di...\"],[\"Optimum Inference with ONNX Runtime\\n\\nOptimum is a utility package for building and running inference...\"],[\"```\\n\\n### Loading a vanilla Transformers model\\n\\nBecause the model you want to work with might not be ...\"],[\"```\\n\\n## Sequence-to-sequence models\\n\\nSequence-to-sequence (Seq2Seq) models can also be used when run...\"],[\"```\\n\\n### Text-to-Image\\n\\nHere is an example of how you can load an ONNX Stable Diffusion model and ru...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptimum\\u002fdocumen...\"],[\"```\\n\\n### Inpaint\\n\\n```python\\nimport PIL\\nimport requests\\nimport torch\\nfrom io import BytesIO\\nfrom opti...\"],[\"```\\n\\n### Text-to-Image\\n\\nHere is an example of how you can load a SDXL ONNX model from [stabilityai\\u002fs...\"],[\"```\\n\\n\\n### Refining the image output\\n\\nThe image can be refined by making use of a model like [stabili...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\noptimum-cli export onnx --model gpt2 --optimize O3 gpt2_onnx\\u002f\\n```\\n\\nThe optimization levels are:\\n...\"],[\"```\\n\\n\\n### Optimization Configuration\\n\\nThe [`~onnxruntime.configuration.OptimizationConfig`] class al...\"],[\"While [`~onnxruntime.configuration.OptimizationConfig`] gives you full control on how to do optimiza...\"],[\"```\\n\\nYou can also specify custom argument that were not defined in the O2 configuration, for instanc...\"],[\"```\\n\\n\\nBelow you will find an easy end-to-end example on how to optimize a Seq2Seq model [sshleifer\\u002fd...\"],[\"```\\n\\n## Optimizing a model with Optimum CLI\\n\\nThe Optimum ONNX Runtime optimization tools can be used...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## How to convert a model into its `BetterTransformer` format?\\n\\n### Step 1: Identifying the source l...\"],[\"\\u003e\\u003e\\u003e model = AutoModel.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e print(model)  # doctest: +IGNORE_RESU...\"],[\"```\\n\\nYou can clearly see that the layers that need to be replaced are the `BertLayer` modules since ...\"],[\"```\\n\\nNow, make sure to fill all the necessary attributes, the list of attributes are:\\n\\n- `in_proj_we...\"],[\"```\\n\\n\\n### Step 3: Building the forward pass\\n\\nFirst of all, start with the line `super().forward_chec...\"],[\"```\\n\\nOnce the `hidden_states` are nested, call `torch._transformer_encoder_layer_fwd` using the righ...\"],[\"Symbolic tracer\\n\\nIn Torch FX, the symbolic tracer feeds dummy values through the code to record the ...\"],[\"Accelerated inference on AMD GPUs supported by ROCm\\n\\nBy default, ONNX Runtime runs inference on CPU ...\"],[\"```\\n\\n**Local Installation Steps:**\\n\\n##### 2.1 PyTorch with ROCm Support\\nOptimum ONNX Runtime integra...\"],[\"```\\n\\n\\u003cTip\\u003e\\nTo avoid conflicts between `onnxruntime` and `onnxruntime-rocm`, make sure the package `o...\"],[\"```\\nValueError: Asked to use ROCMExecutionProvider as an ONNX Runtime execution provider, but the av...\"],[\"```\\n\\nAdditionally, you can pass the session option `log_severity_level = 0` (verbose), to check whet...\"],[\"BetterTransformer benchmark\\n\\nPlease refer to https:\\u002f\\u002fmedium.com\\u002fpytorch\\u002fbettertransformer-out-of-the...\"],[\"# using bitsandbytes fp4\\u002ffp16 scheme\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-ll...\"],[\"```\\n\\nHere are results obtained on a single NVIDIA A100-SXM4-80GB GPU. We use a prompt length of 512,...\"],[\"Bitsandbytes uses the fp4 scheme, with the compute in fp16.\\n\\n### Batch size = 1\\n\\n|quantization |act_...\"],[\"### Batch size = 2\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ...\"],[\"### Batch size = 4\\n\\n|quantization |act_order|bits|group_size|kernel           |Load time (s)|Per-tok...\"],[\"### Batch size = 8\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ...\"],[\"### Batch size = 16\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency...\"],[\"# GPTQ with exllamav2 kernel (int4\\u002ffp16)\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model The...\"],[\"```\\n\\nThe benchmark below is for a prompt length of 512, measuring only the prefill step on a single ...\"],[\"### Batch size = 2\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 4\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 8\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 16\\n\\n|quantization |act_order|bits|group_size|kernel    |prompt_length|new_tokens|Lo...\"],[\"# GPTQ with exllamav2 kernel (int4\\u002ffp16)\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model The...\"],[\"```\\n\\n| quantization | act_order | bits | group_size | kernel           | perplexity |\\n|-------------...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In the 2.0 version, PyTorch includes a native scaled dot-product attention operator (SDPA) as part o...\"],[\"- [AlBERT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1909.11942)\\n- [Bark](https:\\u002f\\u002fgithub.com\\u002fsuno-ai\\u002fbark)\\n- [BART](http...\"],[\"- [FSMT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1907.06616)\\n- [GPT2](https:\\u002f\\u002fd4mucfpksywv.cloudfront.net\\u002fbetter-langu...\"],[\"- [Marian](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1804.00344)\\n- [MBart](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2001.08210)\\n- [M2M100](...\"],[\"- [XLMRoberta](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.02116)\\n- [YOLOS](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.00666)...\"],[\"Let us know by opening an issue in ğŸ¤— Optimum if you want more models to be supported, or check out t...\"],[\"```\\nYou can leave `keep_original_model=False` in case you want to overwrite the current model with i...\"],[\"ONNX ğŸ¤ ONNX Runtime\\n\\nONNX is an open standard that defines a common set of operators and a common fi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e print(distilbert_tasks)\\n['default', 'fill-mask', 'text-classification', 'multiple-choice', 'toke...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n### PyTorch\\n\\n| Task                                 | Auto Class                       ...\"],[\"### TensorFlow\\n\\n| Task                                 | Auto Class                             |\\n|-...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# How to contribute to Optimum?\\n\\nOptimum is an open source project, so all contributions and suggest...\"],[\"```\\n\\n\\t**do not** work on the `main` branch.\\n\\n4. Set up a development environment by running the foll...\"],[\"Overview\\n\\nğŸ¤— Optimum provides an integration with ONNX Runtime, a cross-platform, high performance en...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eTechnical descriptions of how the ONNX Runtime classes and methods of ğŸ¤— Opt...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\ntorchrun --nproc_per_node=NUM_GPUS_YOU_HAVE run_classification.py \\\\\\n    --model_name_or_path...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [meta-llama\\u002fLlama-2-7b-hf](https:\\u002f\\u002fhuggingfac...\"],[\"## GLUE Tasks\\n\\nBy running the script [`run_glue.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002fblob\\u002fmai...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [roberta-base](https:\\u002f\\u002fhuggingface.co\\u002froberta...\"],[\"\\u003e *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add t...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"* Computation graph optimizations: constant foldings, node eliminations, node fusions\\n* Efficient me...\"],[\"```\\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFa...\"],[\"```\\n\\n* If you want to install the dependencies beyond in a local Python environment. You can pip ins...\"],[\"```\\n\\n* If you want to install the dependencies beyond in a local Python environment. You can pip ins...\"],[\"```\\n\\nOr install from source:\\n\\n```bash\\npip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum.git\\n```...\"],[\"```\\n\\nCheck out more detailed [example scripts](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002ftree\\u002fmain\\u002fexam...\"],[\"```\\n\\nCheck out more detailed [example scripts](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002ftree\\u002fmain\\u002fexam...\"],[\"```\\n\\n\\n\\u003cTip warning={false}\\u003e\\n\\nDeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the ...\"],[\"```\\n\\n\\u003cTip warning={false}\\u003e\\n\\nDeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the m...\"],[\"```\\n\\n## Other Resources\\n\\n* Blog posts\\n    * [Optimum + ONNX Runtime: Easier, Faster training for you...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"__The following example applies the acceleration features powered by ONNX Runtime.__\\n\\n\\n### Onnx Runt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-habana.git\\ncd optimum-habana\\nmake doc BUILD_DIR...\"],[\"```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"#section-b\\\"\\u003eSection A\\u003c\\u002fa\\u003e\\u003ca id=\\\"section-a\\\"\\u003e\\u003c\\u002fa\\u003e ]\\n```\\nand ...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs\\ncontinue to work.\\n\\nFo...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or\\n\\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\nWe follow the [doctest](https:\\u002f\\u002fdocs.pyth...\"],[\"```\\n\\n## Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that no...\"],[\"```\\n\\nThe docstring should give a minimal, clear example of how the respective model \\nis to be used i...\"],[\"# Need node to build doc HTML. Taken from https:\\u002f\\u002fstackoverflow.com\\u002fa\\u002f67491580\\nRUN apt-get update &&...\"],[\"```\\n\\nThe main thing to note here is the need to install Node in the Docker image -\\nthat's because we...\"],[\"```\\n# Add this\\n- uses: actions\\u002fcheckout@v2\\nwith:\\n    repository: 'huggingface\\u002foptimum-habana'\\n    pa...\"],[\"![ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002factions\\u002fworkflows\\u002ftest_onnxruntime.yml\\u002fbadge....\"],[\"```\\n\\nIf you'd like to use the accelerator-specific features of ğŸ¤— Optimum, you can install the requir...\"],[\"The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the...\"],[\"```\\n\\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:...\"],[\"```\\n\\n## Accelerated Inference\\n\\nğŸ¤— Optimum provides multiple tools to export and run optimized models ...\"],[\"### Features summary\\n\\n| Features                           | [ONNX Runtime](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"### OpenVINO\\n\\nBefore you begin, make sure you have all the necessary libraries installed :\\n\\n```bash\\n...\"],[\"```\\n\\nIt is possible to export ğŸ¤— Transformers and Diffusers models to the OpenVINO format easily:\\n\\n``...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fintel\\u002finf...\"],[\"```\\n\\nThe model can then be quantized using `onnxruntime`:\\n\\n```bash\\noptimum-cli onnxruntime quantize ...\"],[\"```\\n\\nThese commands will export `deepset\\u002froberta-base-squad2` and perform [O2 graph optimization](ht...\"],[\"```\\n\\nMore details on how to run ONNX models with `ORTModelForXXX` classes [here](https:\\u002f\\u002fhuggingface...\"],[\"```\\n\\n```diff\\n- from transformers import Trainer, TrainingArguments\\n+ from optimum.habana import Gaud...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fonnxrunti...\"],[\"Quantization\\n\\nQuantization is a technique to reduce the computational and memory costs of running in...\"],[\"```\\nC = A + B\\n```\\n\\nHere the result is much bigger than the biggest representable value in `int8`, wh...\"],[\"```\\nx = S * (x_q - Z)\\n```\\n\\nwhere:\\n\\n- `x_q` is the quantized `int8` value associated to `x`\\n- `S` and...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nUsually `round(a\\u002fS + Z)` corresponds to the smallest representable value in the consider...\"],[\"### Per-tensor and per-channel quantization\\n\\nDepending on the accuracy \\u002f latency trade-off you are t...\"],[\"1. Post training **dynamic quantization**: the range for each activation is computed on the fly at *...\"],[\"For both post training static quantization and quantization aware training, it is necessary to defin...\"],[\"### Pratical steps to follow to quantize a model to `int8`\\n\\nTo effectively quantize a model to `int8...\"],[\"## Supported tools to perform quantization in ğŸ¤— Optimum\\n\\nğŸ¤— Optimum provides APIs to perform quantiza...\"],[\"Example: `19` is represented as an unsigned int8 as `00010011` because :...\"],[\"```\\n19 = 0 x 2^7 + 0 x 2^6 + 0 x 2^5 + 1 x 2^4 + 0 x 2^3 + 0 x 2^2 + 1 x 2^1 + 1 x 2^0\\n```\\n\\n2. Signe...\"],[\"```\\nx = sign x mantissa x (2^exponent)\\n```\\n\\n\\n## References\\n\\n- The\\n[Quantization and Training of Neur...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nCheck out the help for more options:\\n\\n```bash\\noptimum-cli export onnx --help\\n```\\n\\n## Why use ON...\"],[\"```\\n\\nThe Optimum ONNX export can be used through Optimum command-line:\\n\\n```bash\\noptimum-cli export o...\"],[\"Optional arguments:\\n  --task TASK           The task to export the model for. If not specified, the ...\"],[\"This is needed by some models, for some tasks. If not provided, will attempt to use the tokenizer to...\"],[\"```\\n\\nExporting a checkpoint can be done as follows:\\n\\n```bash\\noptimum-cli export onnx --model distilb...\"],[\"```\\n\\nNote that providing the `--task` argument for a model on the Hub will disable the automatic tas...\"],[\"```\\n\\nPrinting the outputs would give that:\\n\\n```bash\\nQuestionAnsweringModelOutput(loss=None, start_lo...\"],[\"```\\n\\nFor more information, check the `optimum.onnxruntime` documentation [page on this topic](\\u002fonnxr...\"],[\"```\\n\\n### Exporting a model to be used with Optimum's ORTModel\\n\\nModels exported through `optimum-cli ...\"],[\"```\\n\\nand\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e from optimum.onnxruntime import O...\"],[\"```\\n\\nYou can then pass one of these tasks to the `--task` argument in the `optimum-cli export onnx` ...\"],[\"if self._behavior is ConfigBehavior.ENCODER:\\n            for i in range(self._config.encoder_layers)...\"],[\"custom_onnx_configs={\\n    \\\"encoder_model\\\": encoder_config,\\n    \\\"decoder_model\\\": decoder_config,\\n    ...\"],[\"```\\n\\nFor tasks that require only a single ONNX file (e.g. encoder-only), an exported model with cust...\"],[\"from optimum.exporters.onnx.config import TextDecoderOnnxConfig\\nfrom optimum.utils import Normalized...\"],[\"def add_past_key_values(self, inputs_or_outputs: Dict[str, Dict[int, str]], direction: str):\\n       ...\"],[\"custom_onnx_configs = {\\n    \\\"decoder_model\\\": onnx_config,\\n    \\\"decoder_with_past_model\\\": onnx_config...\"],[\"```\\n\\nMoreover, the advanced argument `fn_get_submodels` to `main_export` allows to customize how the...\"],[\"Helpful tips for testing & debugging optimum\\n\\n## VSCODE\\n\\nIf you are using vscode you might have hard...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\nWhen inheriting from a middle-end class, look for the one handling the same modality \\u002f cate...\"],[\"```\\n\\nFirst let's explain what `TextEncoderOnnxConfig` is all about. While most of the features are a...\"],[\"Once you have implemented an ONNX configuration, you can instantiate it by providing the base model'...\"],[\"```\\n\\nThe resulting object has several useful properties. For example, you can view the ONNX\\noperator...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out [`BartOnnxConfig`] for an advanced example.\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\n## Registering the ONNX ...\"],[\"```\\n\\n## Exporting the model\\n\\nOnce you have implemented the ONNX configuration, the next step is to e...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf your model is larger than 2GB, you will see that many additional files are created du...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [bert-large-cased](https:\\u002f\\u002fhuggingface.co\\u002fber...\"]],\"hovertemplate\":\"source=optimum\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"optimum, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"optimum, circle\",\"showlegend\":true,\"x\":[6.6049943,6.737832,-3.3780324,-4.4328322,-3.0368402,-3.5434294,-4.207724,-3.0177972,-2.57739,-2.6984391,-3.4326124,-2.4616518,-2.9002736,1.7689213,-5.3978677,-3.913093,-4.475699,-3.9441247,-4.857456,-9.063964,-2.4338944,-2.6557643,6.304944,7.44862,9.067194,9.056847,-4.3194604,-3.0657458,7.514059,-3.9923346,-5.1486397,-4.0433655,-7.3011928,-6.2796025,-3.1528547,7.4694557,9.1304865,8.9811325,-5.9881997,-6.3108134,-4.8552036,-2.164534,-3.4975877,-3.783229,-4.0717254,-1.1264839,-3.160111,-4.7995906,-3.3669374,-3.782355,-3.2993393,-3.1142306,-3.3080394,-4.6084986,-2.8957498,-8.978348,-1.9761701,-3.8451953,-3.8770144,-4.297756,-4.5001845,-3.5024245,-5.2491064,-3.4280057,-4.253614,-4.8850594,-1.86966,6.885448,6.8851705,-3.2041767,-2.805916,-2.7525725,-2.3174758,-2.8593912,-0.9720692,-3.253713,-0.17908545,-4.1138406,-3.7658083,-3.5611672,-3.7977657,-3.8779247,-3.8799496,-3.3894186,-4.034376,-4.094413,-3.8197722,-3.3524196,-0.83085465,-1.1715631,-1.923132,-2.1825116,8.540442,-1.3749814,-3.4858959,-2.5195005,-2.5485895,-10.247123,-4.4925685,-3.4554517,-2.0360012,-2.3098905,-0.28383863,-3.7361808,-3.494054,-4.6706977,-3.326105,-3.4156952,-3.11357,0.6139439,-3.0293236,-2.4092743,-2.5947883,-2.272181,-2.5431137,-2.622628,-3.184177,-3.712968,-3.9099784,-1.4130728,-4.3832974,-3.0784283,-2.440059,6.568868,5.476817,-1.4247147,1.089164,0.15414482,-3.2288947,-7.163186,-4.1947703,-3.4224036,-2.9486232,-3.066633,-2.638547,-2.2815213,-1.8610187,-2.550936,-2.5399923,-4.773456,-3.601049,-3.290165,-3.1614797,-2.6829453,-2.0845516,-3.337882,-3.3022969,-3.3036933,-5.1430035,-3.1746294,-3.7504735,-2.6662972,-1.7776542,-2.5762908,-2.7527752,-1.2218615,-2.7754393,-0.85340226,-1.7579783,-2.4680593,0.016434683,-4.0713296,-3.8215911,-4.0667653,-4.755031,-5.081303,-5.118405,-5.019014,-5.027719,-4.272412,-3.426889,-5.0974793,-5.056572,-5.157416,-5.0166507,-4.100594,-5.089912,-4.187375,-4.0329914,-5.783527,-6.213869,-7.617476,7.798906,-4.0715165,-1.9442081,-2.855574,-2.941131,-1.1346744,-5.5147996,-2.9600596,-3.7610452,3.9908788,3.587828,-3.4075227,8.604894,-3.1099958,-4.648953,-0.9405462,-4.062894,-4.5848393,-3.6812007,-0.7593432,-4.585355,-3.883225,-4.358992,-2.120319,0.41575882,0.5979355,-2.3306112,-3.0257468,-2.9531012,-2.0241134,-2.4236999,-4.2966475,-4.4448237,-4.8747973,-3.5144923,-3.2236855,5.856979,4.528745,5.5656962,4.695138,0.07799468,-0.7074412,-1.7044933,4.6781697,-0.9343756,4.034954,3.8333044,3.4314685,-3.448752,-1.4384423,1.1627533,0.007723027,-3.5401766,-4.7306647,0.49544817,-3.4668493,-3.6256502,-3.9954484,-3.3537939,-3.5438447,-2.7487056,-3.1038015,-5.0716276,-5.1489625,-5.14285,-5.257193,-5.005649,-4.7222366,-4.7066007,-4.9180517,-4.969878,0.0009421762,-5.4100695,-5.2510157,-4.615036,6.3020477,-4.7607884,-3.032419,-3.1444018,-2.6689956,-2.6542625,-2.8399937,-2.3879397,-2.7946804,-1.1600517,-2.6589823,-2.8318756,-2.716359,-2.582037,-2.048748,-1.6975526,-3.0300732,-2.4639115,-1.5768629,-1.6855263,-1.2063112,1.0564654,-4.237763,-2.8254855,-2.5586436,-3.0616229,-2.6575515,-2.416187,-2.6307325,-2.5765772,-2.6920989,6.619251,-4.9386454,-3.815847],\"xaxis\":\"x\",\"y\":[-0.863732,-0.99467987,0.9647838,2.0984492,-0.99881476,-1.0979292,2.408284,1.306568,1.2255487,1.6125511,0.02151935,1.4423982,0.53690535,0.06779625,-1.4674466,-0.6127082,-0.9280613,-0.7843948,-1.2616041,-0.67901146,-11.988454,-10.255316,-0.5816858,2.6471267,1.6943216,1.707642,-2.0308216,-1.6684117,2.7678885,-0.9117579,2.7932508,-6.2829432,-0.017880155,2.1124814,-0.3071376,2.6576962,1.7630167,1.7932886,2.562947,3.7102563,1.5234152,-0.055829886,0.01472534,-0.27267352,-0.3382019,4.265334,-0.038319677,-1.1062293,0.04274669,-0.27792057,0.12347014,-0.09144552,0.23913662,-1.1853086,0.7410928,-0.83643955,0.082201645,-0.37813628,-1.2342192,-1.16568,-1.6340009,-2.1905446,-0.7920623,0.91156125,0.37028447,1.8631397,-0.032221574,1.6116798,1.5434339,0.12250247,-0.014651136,0.24572286,0.9340988,2.3182218,1.7332398,0.4762324,0.6293472,-0.13927375,-0.4503277,-0.1019593,-0.27399716,-0.32448953,-0.4238572,0.056760553,-0.5527437,0.46383357,0.08797591,-5.7293763,-2.6760583,-3.2803054,-2.4815686,-0.2903232,2.688919,0.21996385,1.2623547,0.62465966,0.97631156,0.22086492,0.9827026,-0.58521914,0.24809386,0.1595117,0.6161187,-0.6919934,-0.14928636,-0.8524535,-0.0460053,-0.6333937,-2.1696582,0.741386,-0.016556153,0.7695383,0.27964392,1.8404584,2.2980077,1.3525956,-0.5011383,-0.31909505,-0.60171497,3.1748147,-1.0396974,0.84778595,-11.816569,-0.893498,-0.3734655,-0.32345507,0.54106694,0.24997734,0.3382202,1.1788467,-6.4725165,0.031387012,0.67548865,0.638505,-6.585555,-6.8266764,-7.4842396,-6.441297,-6.3465166,1.2532073,0.086181425,0.07109391,-0.04898859,-0.5975945,0.14155118,0.22493342,-0.023280714,0.34372923,0.022787675,1.7762284,2.003676,1.840688,1.6002178,2.6748853,2.4186444,0.6286841,-0.20109823,0.19001469,0.40023383,0.36678025,1.253728,-0.72024673,-0.29376104,-2.2768652,-2.415383,-2.6673942,-2.744664,-2.6154206,-2.6707683,-0.47463265,-2.000729,-2.7571244,-2.6600444,-2.835834,-2.5679889,-0.5544747,-2.0919323,-1.1954461,-2.1711056,2.0781345,0.20136887,0.35027793,2.696983,1.4434934,1.3989583,0.0065748086,0.58183223,4.4429107,2.0228271,1.7465844,-0.06821211,1.1963074,1.032438,-0.116326734,2.6295247,-0.32549232,1.6002622,-0.9139457,-1.2297025,1.5227005,-1.3224629,-1.2422864,1.2425001,-0.44534522,-0.8252534,-0.27382323,0.30668908,0.43241593,0.017337754,-0.14371365,-0.2582137,-2.1690586,-3.068107,-0.699862,0.9972861,1.7783884,-0.39524168,0.48692992,-0.3845314,1.5210782,2.5510008,2.0573137,2.749421,3.4049294,4.1279535,1.5161301,0.42455056,1.0608909,1.1305506,0.64923847,-0.00048704754,-0.29129604,0.52278614,0.3376141,-0.05558495,-0.43189803,0.3010582,0.33443648,-0.054822296,-0.38100216,0.19068025,0.02749498,0.48447713,-0.14580145,-1.4791197,-1.467402,-1.4705238,-1.6003228,-1.5402163,-1.3563175,-1.1963792,-1.3779262,-1.2096531,3.9377654,-1.3080369,-1.2113361,0.855273,-0.91956985,0.41023442,0.611832,0.33996964,0.5388699,0.7543537,0.79988074,0.81716913,0.5132595,2.75738,0.39410377,0.712177,0.7550254,0.7180546,3.2040288,1.2921182,0.6659315,2.0759432,2.3387043,1.0813161,1.2810335,1.5215843,0.6350847,0.8848173,1.6084188,1.4430845,1.0179423,1.5023705,0.8652116,0.96362394,0.68034905,-0.80354536,1.8498317,-0.72807795],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Fine-tuning for image classification using LoRA and ğŸ¤— PEFT\\n\\n## Vision Transformer model from transfo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThen run the following command:\\n\\n```bash\\ndoc-builder preview {package_name} {path_to_docs}\\n```\\n...\"],[\"```\\nand of course, if you moved it to another file, then:\\n\\n```\\nSections that were moved:\\n\\n[ \\u003ca href=...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\n\\n#...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n```python\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\n#### Writing a return block\\n\\nThe re...\"],[\"```\\n    Example:\\n\\n    ```python\\n    \\u003e\\u003e\\u003e import time\\n    \\u003e\\u003e\\u003e from accelerate import Accelerator\\n    \\u003e...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nStart by defining the model and tokenizer, text and label columns, and some hyperpara...\"],[\"```\\n\\n## Load dataset\\n\\nFor this guide, you'll train on the `sentences_allagree` subset of the [`finan...\"],[\"```\\n\\n## Preprocess dataset\\n\\nInitialize a tokenizer, and create a function to pad and truncate the `m...\"],[\"```\\n\\nCreate a [`DataLoader`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fdata.html#torch.utils.data.DataLoader) ...\"],[\"```\\n\\nSetup the optimizer and learning rate scheduler:\\n\\n```py\\noptimizer = torch.optim.AdamW(model.par...\"],[\"```\\n\\nMove the model to the GPU, and then write a training loop to begin!\\n\\n```py\\nmodel = model.to(dev...\"],[\"```\\n\\nLet's see how well the model performs on the validation set:\\n\\n```py\\ncorrect = 0\\ntotal = 0\\nfor p...\"],[\"```\\n\\nIf you check the model file size in the repository, you'll see that it is only 3.93MB! ğŸ¤\\n\\n## In...\"],[\"Fine-tuning a multilayer perceptron using LoRA and ğŸ¤— PEFT\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.research....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In this guide, you'll see how to quantize a model to 4-bits and train it with LoRA.\\n\\n## Quantize a m...\"],[\"```\\n\\nPass the `config` to the [`~transformers.AutoModelForCausalLM.from_pretrained`] method.\\n\\n```py\\n...\"],[\"```\\n\\nYou're all set for training with whichever training method you prefer!\\n\\n### LoftQ initializatio...\"],[\"```\\n\\n## Next steps\\n\\nIf you're interested in learning more about quantization, the following may be h...\"],[\"Using PEFT with timm\\n\\n`peft` allows us to train any model with LoRA as long as the layer type is sup...\"],[\"```\\n\\nThese are the transformations steps necessary to process the image.\\n\\n\\n```python\\ntransform = cre...\"],[\"```\\n\\n## Training\\n\\nThis is just a function that performs the train loop, nothing fancy happening.\\n\\n\\n`...\"],[\"```\\n\\n### Selecting which layers to fine-tune with LoRA\\n\\nLet's take a look at the layers of our model...\"],[\"```\\n\\nFinally, let's create the `peft` model, the optimizer and criterion, and we can get started. As...\"],[\"```\\n\\n\\n```python\\nx = ds_train[:1][\\\"x\\\"]\\ny_peft = peft_model(x.to(device))\\ny_loaded = loaded(x)\\ntorch.a...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"class DummyModel(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.e...\"],[\"```\\n\\nIf you print the model, you will notice that the adapters have been correctly injected into the...\"],[\"``python\\nimport os\\n\\nimport torch\\nfrom transformers import (\\n    AutoTokenizer,\\n    default_data_coll...\"],[\"```\\n\\n\\n```python\\n# loading dataset\\ndataset = load_dataset(\\\"financial_phrasebank\\\", \\\"sentences_allagree...\"],[\"```\\n\\n\\n```python\\n# training and evaluation\\n\\n\\ndef compute_metrics(eval_preds):\\n    preds, labels = eva...\"],[\"```\\n\\n\\n```python\\nckpt = f\\\"{peft_model_id}\\u002fadapter_model.bin\\\"\\n!du -h $ckpt\\n```\\n\\n\\n```python\\nfrom peft i...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nLet's take care of some of the setup first so you can start training faster later. Se...\"],[\"```\\n\\nYou can also log in to your Hugging Face account to save and share your trained model on the Hu...\"],[\"```\\n\\n## Preprocess dataset\\n\\nLet's prepare the dataset for training. Load a feature extractor, tokeni...\"],[\"```\\n\\nIf you look at the `sampling_rate`, you'll see the audio was sampled at 48kHz. The Whisper mode...\"],[\"```\\n\\nOnce you've cleaned up the dataset, you can write a function to generate the correct model inpu...\"],[\"```\\n\\nFinally, create a `DataCollator` class to pad the labels in each batch to the maximum length, a...\"],[\"```\\n\\n## Train\\n\\nNow that the dataset is ready, you can turn your attention to the model. Start by loa...\"],[\"```\\n\\nLet's also apply LoRA to the training to make it even more efficient. Load a [`~peft.LoraConfig...\"],[\"```\\n\\nNow you're ready to define some training hyperparameters in the [`~transformers.Seq2SeqTraining...\"],[\"```\\n\\nIt is also a good idea to write a custom [`~transformers.TrainerCallback`] to save model checkp...\"],[\"```\\n\\n## Evaluate\\n\\n[Word error rate](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fevaluate-metric\\u002fwer) (WER) is a co...\"],[\"```\\n\\nWrite a loop to evaluate the model performance. Set the model to evaluation mode first, and wri...\"],[\"```\\n\\n## Share model\\n\\nOnce you're happy with your results, you can upload your model to the Hub with ...\"],[\"```\\n\\nLoad an audio sample (you can listen to it in the [Dataset Preview](https:\\u002f\\u002fhuggingface.co\\u002fdata...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Load a dataset\\n\\nTo ensure that this example runs within a reasonable time frame, here we are...\"],[\"```\\n\\n## Prepare datasets for training and evaluation\\n\\nNext, load the SegFormer image processor to pr...\"],[\"```\\n\\nFinally, combine everything in two functions that you'll use to transform training and validati...\"],[\"```\\n\\n## Create evaluation function\\n\\nIncluding a metric during training is helpful for evaluating you...\"],[\"per_category_accuracy = metrics.pop(\\\"per_category_accuracy\\\").tolist()\\n        per_category_iou = met...\"],[\"```\\n\\n## Load a base model \\n\\nBefore loading a base model, let's define a helper function to check the...\"],[\"```\\n\\nAt this point you can check with the `print_trainable_parameters` helper function that all 100%...\"],[\"```\\n\\nLet's review the `LoraConfig`. To enable LoRA technique, we must define the target modules with...\"],[\"When all is configured, and the base model is wrapped, the `print_trainable_parameters` helper funct...\"],[\"```\\n\\nThis confirms that only the LoRA parameters appended to the attention blocks and the `decode_he...\"],[\"```\\n\\n## Save the model and run inference\\n\\nUse the `save_pretrained()` method of the `lora_model` to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nNext, visualize the results.  We need a color palette for this. Here, we use ade_palette(). As ...\"],[\"```\\n\\nAs you can see, the results are far from perfect, however, this example is designed to illustra...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fstevhliu-peft-methods.hf.space\\\"\\n\\tframeborder=\\\"0\\\"\\n\\twidth=\\\"850\\\"\\n\\theight=\\\"620\\\"\\n\\u003e\\u003c...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[Prompt tuning](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2104.08691) was developed for text classification tasks on T5 m...\"],[\"The main difference is that the prefix parameters are inserted in **all** of the model layers, where...\"],[\"The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables G...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Fine-tuning large pre-trained language models on downstream tasks ...\"],[\"Training PEFT models with new tokens being added to the embedding layers and tokenizer\\n\\nIn this exam...\"],[\"```\\n\\n## Prepare Model and Tokenizer\\n\\nNow, we will be adding 27 new tokens as well as replace the exi...\"],[\"```\\n\\nWe will be finetuning Mistral-7B model. Let's load the tokenizer and add the special tokens fol...\"],[\"```\\n\\n## Preapre Dataset\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"smang...\"],[\"def preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    targets = [str(x)...\"],[\"model_inputs[\\\"input_ids\\\"][i] = model_inputs[\\\"input_ids\\\"][i][:max_length]\\n        model_inputs[\\\"atten...\"],[\"processed_datasets = dataset.map(\\n    preprocess_function,\\n    batched=True,\\n    num_proc=1,\\n    rem...\"],[\"```\\n\\n\\n```python\\ntrain_dataset\\n```\\n\\n\\n```python\\ntrain_dataloader = DataLoader(\\n    train_dataset, shuf...\"],[\"```\\n\\n# Check the model output on a sample from evaluation dataset\\n\\n\\n```python\\nimport random\\n\\ni = ran...\"],[\"```\\n\\n# Check the model loading is working as expected and generating plausible outputs.\\n\\n\\n```python\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PromptEncoderConfig\\n\\n[[autodoc]] tuners.p_tuning.config.PromptEncoderConfig\\n\\n## PromptEncoder\\n\\n[[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For example, load a base model and then load the [artificialguybr\\u002f3DRedmond-V1](https:\\u002f\\u002fhuggingface....\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fybelkada\\u002fdocume...\"],[\"```\\n\\nLearn more about how PEFT supports Diffusers in the [Inference with PEFT](https:\\u002f\\u002fhuggingface.c...\"],[\"```\\n\\nIf you're interested in comparing or using more than one adapter, you can also call the [`~Peft...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNavigate to the directory containing the training scripts for fine-tuning Dreambooth with LoRA:...\"],[\"```\\n\\nHere: \\n- `INSTANCE_DIR`: The directory containing the images that you intend to use for trainin...\"],[\"Here's what the full set of script arguments may look like:\\n\\n```bash\\naccelerate launch train_dreambo...\"],[\"```\\n\\nIf you are running this script on Windows, you may need to set the `--num_dataloader_workers` t...\"],[\"```\\n\\nNext, add a function that will create a Stable Diffusion pipeline for image generation. It will...\"],[\"```\\n\\nNow you can use the function above to create a Stable Diffusion pipeline using the LoRA weights...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nTo switch between adapters, write a function that uses `set_adapter()` method of `PeftModel` (s...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_peft_config, get_peft_m...\"],[\"```\\n\\n\\n```python\\n# loading dataset\\ndataset = load_dataset(\\\"financial_phrasebank\\\", \\\"sentences_allagree...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\n\\n...\"],[\"```\\n\\n\\n```python\\n# training and evaluation\\nmodel = model.to(device)\\n\\nfor epoch in range(num_epochs):\\n...\"],[\"```\\n\\n\\n```python\\n# print accuracy\\ncorrect = 0\\ntotal = 0\\nfor pred, true in zip(eval_preds, dataset[\\\"va...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 107\\ninputs = tokenizer(dataset[\\\"validation\\\"][text_column][i], retur...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_peft_config, get_peft_m...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\n\\n...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 13\\ninputs = tokenizer(dataset[\\\"validation\\\"][text_column][i], return...\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import get_peft_config, get_peft_mo...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\ni...\"],[\"def preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f\\\"{text...\"],[\"\\\"attention_mask\\\"\\n        ][i]\\n        labels[\\\"input_ids\\\"][i] = [-100] * (max_length - len(sample_inp...\"],[\"processed_datasets = dataset.map(\\n    preprocess_function,\\n    batched=True,\\n    num_proc=1,\\n    rem...\"],[\"```\\n\\n\\n```python\\ndef test_preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n...\"],[\"```\\n\\n\\n```python\\nlen(test_dataloader)\\n```\\n\\n\\n```python\\nnext(iter(test_dataloader))\\n```\\n\\n\\n```python\\n# c...\"],[\"```\\n\\n\\n```python\\n# training and evaluation\\nmodel = model.to(device)\\n\\nfor epoch in range(num_epochs):\\n...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 16\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet ...\"],[\"```\\n- Or save model locally\\n```python\\npeft_model_id = f\\\"{dataset_name}_{model_name_or_path}_{peft_co...\"],[\"```\\n\\n\\n```python\\nmodel.to(device)\\nmodel.eval()\\ni = 4\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"LoftQ: LoRA-fine-tuning-aware Quantization\\n\\n## Introduction\\n\\nLoftQ finds quantized LoRA initializati...\"],[\"```\\n\\n## LoftQ DIY\\n\\n### Apply LoftQ and save\\nWe provide [quantize_save_load.py](quantize_save_load.py...\"],[\"```\\n\\nThe above commands end up with creating the model directory under `$SAVE_DIR`. \\nSpecifically, t...\"],[\"```\\n\\n## LoftQ Fine-tuning\\n\\nWe also provide an example to fine-tune LoftQ on GSM8K. \\nWe load the quan...\"],[\"Finetuning Whisper-large-V2 on Colab using PEFT-Lora + BNB INT8 training\\n\\nIn this Colab, we present ...\"],[\"```\\n\\n\\n```python\\n# Select CUDA device index\\nimport os\\n\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0\\\"\\nmodel...\"],[\"```\\n\\n### Prepare Data\\n\\n\\n```python\\nprint(common_voice[\\\"train\\\"][0])\\n```\\n\\nSince \\nour input audio is sam...\"],[\"```\\n\\nWe can apply the data preparation function to all of our training examples using dataset's `.ma...\"],[\"```\\n\\n\\n```python\\ncommon_voice[\\\"train\\\"]\\n```\\n\\n## Training and Evaluation\\n\\n### Define a Data Collator\\n\\n\\n...\"],[\"```\\n\\nLet's initialise the data collator we've just defined:\\n\\n\\n```python\\ndata_collator = DataCollator...\"],[\"```\\n\\n###Â Load a Pre-Trained Checkpoint\\n\\nNow let's load the pre-trained Whisper `small` checkpoint. A...\"],[\"```\\n\\n### Apply LoRA\\n\\nHere comes the magic with `peft`! Let's load a `PeftModel` and specify that we ...\"],[\"```\\n\\n**Few Important Notes:**\\n1. `remove_unused_columns=False` and `label_names=[\\\"labels\\\"]` are requ...\"],[\"```\\n\\n\\n```python\\ntrainer.train()\\n```\\n\\n\\n```python\\nmodel_name_or_path = \\\"openai\\u002fwhisper-large-v2\\\"\\npeft_...\"],[\"```\\nwithout normalizer: 'à¤¸à¥à¤µà¤¿à¤šà¥à¤šà¤¾à¤¨ à¤¨à¤°à¥à¤µà¤¿à¤¤à¥à¤¤à¥€à¤šà¥€ à¤ªà¤¦à¥à¤¦à¤¤ à¤®à¥‹à¤ à¥à¤¯à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£à¤¾à¤¤ à¤†à¤®à¤²à¤¾à¤¤ à¤†à¤£à¤²à¥à¤¯à¤¾ à¤¬à¤¸à¥‹à¤¨ à¤¯à¤¾ à¤¦à¥à¤ªà¤¨à¥à¤¯à¤¾à¤¨à¥‡ ...\"],[\"```\\nPost fixing this bug, we report the 2 metrics for the top model of the leaderboard and the PEFT ...\"],[\"```\\n\\n\\n```python\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\nimport numpy as np\\nimp...\"],[\"```\\nThe model 'PeftModel' is not supported for . Supported models are ['SpeechEncoderDecoderModel', ...\"],[\"```\\n\\n\\n```python\\nimport torch\\nimport gradio as gr\\nfrom transformers import (\\n    AutomaticSpeechRecog...\"],[\"iface.launch(share=True)...\"],[\"``python\\nimport argparse\\nimport gc\\nimport hashlib\\nimport itertools\\nimport logging\\nimport math\\nimport...\"],[\"```\\n\\n\\n```python\\ndef get_lora_sd_pipeline(\\n    ckpt_dir, base_model_name_or_path=None, dtype=torch.fl...\"],[\"if dtype in (torch.float16, torch.bfloat16):\\n        pipe.unet.half()\\n        pipe.text_encoder.half...\"],[\"if os.path.exists(text_encoder_sub_dir):\\n        if isinstance(pipe.text_encoder, PeftModel):\\n      ...\"],[\"```\\n\\n\\n```python\\n%%time\\npipe = get_lora_sd_pipeline(os.path.join(base_path, \\\"dog_dreambooth_updated\\\")...\"],[\"```\\n\\n\\n```python\\nset_adapter(pipe, adapter_name=\\\"toy_dog\\\")\\nprompt = \\\"sks dog rendered in the style of...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Configuration\\n\\nStart by running the following command to [create a DeepSpeed configuratio...\"],[\"```\\n\\nYou'll be asked a few questions about your setup, and configure the following arguments. In thi...\"],[\"```\\n\\nAn example [configuration file](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft\\u002fblob\\u002fmain\\u002fexamples\\u002fconditio...\"],[\"```\\n\\n## The important parts\\n\\nLet's dive a little deeper into the script so you can see what's going ...\"],[\"```\\n\\nThroughout the script, you'll see the [`~accelerate.Accelerator.main_process_first`] and [`~acc...\"],[\"```\\n\\nInside the training loop, the usual `loss.backward()` is replaced by ğŸ¤— Accelerate's [`~accelera...\"],[\"```\\n\\nYou'll see some output logs that track memory usage during training, and once it's completed, t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] tuners.multitask_prompt_tuning.config.MultitaskPromptTuningConfig\\n\\n## MultitaskPromptEmb...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThis should finish much quicker and allow faster iteration. Before creating the PR, however, pl...\"],[\"## Adding a new fine-tuning method\\n\\nNew parameter-efficient fine-tuning methods are developed all th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PromptTuningConfig\\n\\n[[autodoc]] tuners.prompt_tuning.config.PromptTuningConfig\\n\\n## PromptEmbeddin...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nimport peft\\nfrom peft import get_peft_config...\"],[\"```\\n\\n\\n```python\\nmodel\\n```\\n\\n\\n```python\\nmodel = get_peft_model(model, peft_config)\\nmodel.print_trainab...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present LLaMA-Adapter, a lightweight adaption method to efficie...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe last line is necessary if you want to activate both adapters, otherwise, only the first ada...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nYou'll be asked a few questions about your setup, and configure the following arguments. For th...\"],[\"```\\n\\nFor example, your FSDP configuration file may look like the following:\\n\\n```yaml\\ncommand_file: n...\"],[\"```\\n\\n## The important parts\\n\\nLet's dig a bit deeper into the training script to understand how it wo...\"],[\"```\\n\\nThroughout the script, you'll see the [`~accelerate.Accelerator.main_process_first`] and [`~acc...\"],[\"``python\\nfrom datasets import load_dataset\\nfrom transformers import set_seed, AutoModelForSeq2SeqLM,...\"],[\"```\\n\\n\\n```python\\ndef get_sst2(split: str):\\n    examples = load_dataset(\\\"sst2\\\")[split]\\n    result_exam...\"],[\"```\\n\\n\\n```python\\nfrom typing import Tuple\\nfrom torch.utils.data import Dataset, DataLoader\\nimport tor...\"],[\"return {\\n        \\\"input_ids\\\": input.input_ids,\\n        \\\"attention_mask\\\": input.attention_mask,\\n     ...\"],[\"```\\n\\n## source training\\n\\n\\n```python\\nfrom torch.optim.adamw import AdamW\\nfrom transformers import get...\"],[\"```\\n\\n\\n```python\\nPOSITIVE_TOKEN_ID = tokenizer(\\\" positive\\\", add_special_tokens=False)[\\\"input_ids\\\"][0]...\"],[\"step += 1\\n    batch = send_to_device(batch)\\n    loss = model(**batch).loss\\n    loss.backward()\\n    o...\"],[\"```\\n\\n## target training\\n\\n\\n```python\\ntrain = DataLoader(MyDataset(\\\"train\\\", \\\"target\\\"), shuffle=True, b...\"],[\"```\\n\\n\\n```python\\noptimizer = AdamW(model.parameters(), lr=1e-4)\\nscheduler = get_cosine_schedule_with_...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThere is also an option to set `init_lora_weights=False` which is useful for debugging and test...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nLearn more about how PEFT works with quantization in the [Quantization](quantization) gu...\"],[\"```\\n\\nIf you need to keep a copy of the weights so you can unmerge the adapter later or delete and lo...\"],[\"```\\n\\n## Load adapters\\n\\nAdapters can be loaded onto a pretrained model with [`~PeftModel.load_adapter...\"],[\"``python\\n!pip install -q git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n!pip install -q git+htt...\"],[\"```\\n\\n\\n```python\\nmodel\\n```\\n\\n\\n```python\\nmodel.to(\\\"cuda\\\")\\n```\\n\\n\\n```python\\nimport torch\\n\\ndevice = \\\"cuda\\\"...\"],[\"```\\n\\n\\n```python\\ninstruction = \\\"Tell me about alpacas.\\\"\\n\\nprint(evaluate(instruction))\\n```\\n\\n\\n```python...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import PeftModel, PeftConfig\\nimpor...\"],[\"```\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"ought\\u002fraft\\\", dataset_name...\"],[\"```\\n\\n\\n```python\\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\ntarget_max...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 15\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet ...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ntest_preds = []\\n\\nfor _, batch in enumerate(tqdm(test_dataloader)):\\n    ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nLearn more about the parameters you can configure for each PEFT method in their respective AP...\"],[\"```\\n\\nYou can create your own configuration for training by initializing a [`LoraConfig`].\\n\\n```py\\nfro...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## PEFT models\\n\\nWith a PEFT configuration in hand, you can now apply ...\"],[\"```\\n\\nTo load a [`PeftModel`] for inference, you'll need to provide the [`PeftConfig`] used to create...\"],[\"```\\n\\nTake a look at the [AutoPeftModel](package_reference\\u002fauto_class) API reference to learn more ab...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Fine-tune FLAN-T5 using `bitsandbytes`, `peft` & `transformers` ğŸ¤— \\n\\nIn this notebook we will see how...\"],[\"```\\n\\n## Prepare model for training\\n\\nSome pre-processing needs to be done before training such an int...\"],[\"```\\n\\nAs you can see, here we are only training 0.6% of the parameters of the model! This is a huge m...\"],[\"```\\n\\nLet's also apply some pre-processing of the input data, the labels needs to be pre-processed, t...\"],[\"```\\n\\n## Train our model! \\n\\nLet's now train our model, run the cells below.\\nNote that for T5 since so...\"],[\"```\\n\\n\\n```python\\nmodel.push_to_hub(\\\"ybelkada\\u002fflan-t5-large-financial-phrasebank-lora\\\", use_auth_token...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\npeft_config = PromptTuningConfig(task_type=\\\"SEQ_CLS\\\", num_virtual_tokens=10)\\nlr = 1e...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Share adapters on the ğŸ¤— Hub\\n\\n\\n```python\\nmodel.push_to_hub(\\\"smangrul\\u002froberta-large-peft-promp...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nInstall all the necessary required libraries with:\\n\\n```bash\\npip install -r requirements.txt\\n```...\"],[\"```\\n\\nHere's what a full set of script arguments may look like when running in Colab on a V100 GPU wi...\"],[\"```\\n\\n## Dataset for semantic similarity\\n\\nThe dataset we'll be using is a small subset of the [esci-d...\"],[\"For this task guide, we will explore the first stage of training an embedding model to predict seman...\"],[\"def __getattr__(self, name: str):\\n        \\\"\\\"\\\"Forward missing attributes to the wrapped module.\\\"\\\"\\\"\\n  ...\"],[\"```\\n\\nThe `get_cosine_embeddings` function computes the cosine similarity and the `get_loss` function...\"],[\"1. Get a list of ids to products which we can call `ids_to_products_dict`:\\n\\n```bash\\n{0: 'RamPro 10\\\" ...\"],[\"```\\n\\n2. Use the trained [smangrul\\u002fpeft_lora_e5_ecommerce_semantic_search_colab](https:\\u002f\\u002fhuggingface....\"],[\"```\\n\\n3. Create a search index using HNSWlib:\\n\\n```py\\ndef construct_search_index(dim, num_elements, da...\"],[\"```\\n\\n5. Let's test it out with the query `deep learning books`:\\n\\n```py\\nquery = \\\"deep learning books\\\"...\"],[\"```\\n\\nBooks on deep learning and machine learning are retrieved even though `machine learning` wasn't...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nif any(k in model_name_or_path for k in (\\\"gpt\\\", \\\"opt\\\", \\\"bloom\\\")):\\n    padding_side =...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Share adapters on the ğŸ¤— Hub\\n\\n\\n```python\\nmodel.push_to_hub(\\\"smangrul\\u002froberta-large-peft-p-tun...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nTo get started, import ğŸ¤— Transformers to create the base model, ğŸ¤— Datasets to load a ...\"],[\"```\\n\\nFrom ğŸ¤— Evaluate, load a metric for evaluating the model's performance. The evaluation module re...\"],[\"```\\n\\nUse [`~datasets.Dataset.map`] to apply the `tokenize_function` to the dataset, and remove the u...\"],[\"```\\n\\nCreate the base `roberta-large` model from [`~transformers.AutoModelForSequenceClassification`]...\"],[\"```\\n\\nThen pass the model, `TrainingArguments`, datasets, tokenizer, data collator, and evaluation fu...\"],[\"```\\n\\nGet some text and tokenize it:\\n\\n```py\\nclasses = [\\\"not equivalent\\\", \\\"equivalent\\\"]\\n\\nsentence1 = \\\"...\"],[\"Dreambooth with OFT\\nThis Notebook assumes that you already ran the train_dreambooth.py script to cre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Large text-to-image diffusion models have impressive capabilities ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Setup\\n\\nStart by defining the model and tokenizer, the dataset and the dataset columns to tra...\"],[\"```\\n\\n## Load dataset\\n\\nFor this guide, you'll load the `twitter_complaints` subset of the [RAFT](http...\"],[\"```\\n\\nCreate a `preprocess_function` to:\\n\\n1. Tokenize the input text and labels.\\n2. For each example ...\"],[\"```py\\ndef preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f...\"],[\"model_inputs[\\\"input_ids\\\"][i] = torch.tensor(model_inputs[\\\"input_ids\\\"][i][:max_length])\\n        model...\"],[\"```\\n\\nUse the [`~datasets.Dataset.map`] function to apply the `preprocess_function` to the entire dat...\"],[\"```\\n\\n## Train\\n\\nYou're almost ready to setup your model and start training!\\n\\nInitialize a base model ...\"],[\"```\\n\\nMove the model to the GPU, then write a training loop to start training!\\n\\n```py\\nmodel = model.t...\"],[\"```\\n\\nUse the [`~transformers.PreTrainedModel.push_to_hub`] function to upload your model to a model ...\"],[\"```\\n\\nPut the model on a GPU and *generate* the predicted label:\\n\\n```py\\nmodel.to(device)\\n\\nwith torch....\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import get_peft_config, get_peft_mo...\"],[\"```\\n\\n\\n```python\\nlen(test_dataloader)\\n```\\n\\n\\n```python\\nnext(iter(test_dataloader))\\n```\\n\\n\\n```python\\n# c...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 33\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Authenticate to share your model\\n\\nTo share the fine-tuned model at the end of the training w...\"],[\"```\\n\\nThe `image_processor` contains useful information on which size the training and evaluation ima...\"],[\"```\\n\\nFinally, set the transformation functions for the datasets accordingly:\\n\\n```python\\ntrain_ds.set...\"],[\"```\\n\\nBefore creating a `PeftModel`, you can check the number of trainable parameters in the original...\"],[\"```\\n\\nLet's unpack what's going on here.\\nTo use LoRA, you need to specify the target modules in `Lora...\"],[\"For model fine-tuning, use [`~transformers.Trainer`]. It accepts\\nseveral arguments which you can wra...\"],[\"```\\n\\nCompared to non-PEFT methods, you can use a larger batch size since there are fewer parameters ...\"],[\"```\\n\\nIn just a few minutes, the fine-tuned model shows 96% validation accuracy even on this small\\nsu...\"],[\"```\\n\\nWhen calling [`~transformers.PreTrainedModel.push_to_hub`] on the `lora_model`, only the LoRA p...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fsayakpaul\\u002fsampl...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nInstalling PEFT from source is useful for keeping up with the latest developments:\\n\\n```bash\\npyt...\"],[\"```\\n\\n### Randomly initialized layers\\n\\nFor some tasks, it is important to correctly configure `module...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nAs mentioned briefly earlier, [LoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2106.09685) is a technique that ac...\"],[\"## Low-Rank Hadamard Product (LoHa)\\n\\nLow-rank decomposition can impact performance because the weigh...\"],[\"## Orthogonal Finetuning (OFT)\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface....\"],[\"## Adaptive Low-Rank Adaptation (AdaLoRA)\\n\\n[AdaLoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2303.10512) manages the pa...\"],[\"To avoid adding noise to the tokens, the adapter uses zero-initialized attention. On top of this, th...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\npeft_config = PrefixTuningConfig(task_type=\\\"SEQ_CLS\\\", num_virtual_tokens=20)\\nlr = 1e...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Share adapters on the ğŸ¤— Hub\\n\\n\\n```python\\nmodel.push_to_hub(\\\"smangrul\\u002froberta-large-peft-prefi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nif any(k in model_name_or_path for k in (\\\"gpt\\\", \\\"opt\\\", \\\"bloom\\\")):\\n    padding_side =...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Share adapters on the ğŸ¤— Hub\\n\\n\\n```python\\nmodel.push_to_hub(\\\"SumanthRH\\u002froberta-large-peft-ia3\\\"...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Few-shot in-context learning (ICL) enables pre-trained language mo...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nLet's start by importing all the necessary libraries you'll need:\\n\\n- ğŸ¤— Transformers f...\"],[\"```\\n\\nThe `tags` values are defined in the label ids [dictionary](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftne...\"],[\"```\\n\\nNow you can write an evaluation function to compute the metrics from the model predictions and ...\"],[\"```\\n\\nYou'll also need to write a function to:\\n\\n1. Map each token to their respective word with the [...\"],[\"```\\n\\nFinally, create a data collator to pad the examples to the longest length in a batch:\\n\\n```py\\nda...\"],[\"```\\n\\nDefine the [`LoraConfig`] with:\\n\\n- `task_type`, token classification (`TaskType.TOKEN_CLS`)\\n- `...\"],[\"```\\n\\nFrom the ğŸ¤— Transformers library, create a [`~transformers.TrainingArguments`] class and specify...\"],[\"```\\n\\n## Inference\\n\\nTo use your model for inference, load the configuration and model:\\n\\n```py\\npeft_mo...\"],[\"```\\n\\nPass the inputs to the model, and print out the model prediction for each token:\\n\\n```py\\nwith to...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In principle, IA3 can be applied to any subset of weight matrices in a neural network to reduce the ...\"],[\"## Example Usage\\n\\nFor the task of sequence classification, one can initialize the IA3 config for a L...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"``python\\nimport os\\n\\nimport torch\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, defa...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\nt...\"],[\"```\\n\\n\\n```python\\n# optimizer and lr scheduler\\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 107\\ninput_ids = tokenizer(dataset[\\\"validation\\\"][text_column][i], re...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"``python\\nimport argparse\\nimport json\\nimport logging\\nimport math\\nimport os\\nimport random\\nfrom pathlib...\"],[\"```\\n\\n\\n```python\\nclass AutoModelForSentenceEmbedding(nn.Module):\\n    def __init__(self, model_name, t...\"],[\"```\\n\\n\\n```python\\nmodel_name_or_path = \\\"intfloat\\u002fe5-large-v2\\\"\\npeft_model_id = \\\"smangrul\\u002fpeft_lora_e5_s...\"],[\"```\\n\\n\\n```python\\n# base model\\nmodel = AutoModelForSentenceEmbedding(model_name_or_path, tokenizer)\\n\\n#...\"],[\"```\\n\\n\\n```python\\ndef construct_search_index(dim, num_elements, data):\\n    # Declaring index\\n    searc...\"],[\"```\\n\\n\\n```python\\nquery = \\\"NLP and ML books\\\"\\nk = 10\\nquery_embeddings = get_query_embeddings(query, mod...\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import PeftModel, PeftConfig\\nimport...\"],[\"processed_datasets = dataset.map(\\n    preprocess_function,\\n    batched=True,\\n    num_proc=1,\\n    rem...\"],[\"```\\n\\n\\n```python\\ndef test_preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n...\"],[\"```\\n\\nYou can load model from hub or local\\n\\n- Load model from Hugging Face Hub, you can change to you...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\neval_preds = []\\nfor _, batch in enumerate(tqdm(eval_dataloader)):\\n    b...\"],[\"Fine-tuning for semantic segmentation using LoRA and ğŸ¤— PEFT\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.researc...\"],[\"Using PEFT with custom models\\n\\n`peft` allows us to fine-tune models efficiently with LoRA. In this s...\"],[\"```\\n\\n## Model\\n\\nAs a model, we use a simple multilayer perceptron (MLP). For demonstration purposes, ...\"],[\"```\\n\\n\\n```python\\ndef train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\\n ...\"],[\"```\\n\\n\\n```python\\n%time train(module, optimizer, criterion, train_dataloader, eval_dataloader, epochs=...\"],[\"```\\n\\nChecking the numbers, we see that only ~1% of parameters are actually trained, which is what we...\"],[\"```\\n\\nSo we can see that apart from the new LoRA weights that were added, only the last layer was upd...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThis is a straightforward multilayer perceptron with an input layer, a hidden layer, and an out...\"],[\"```\\n\\nWith that, we can create our PEFT model and check the fraction of parameters trained:\\n\\n```pytho...\"],[\"```\\n[('', timm.models.metaformer.MetaFormer),\\n ('stem', timm.models.metaformer.Stem),\\n ('stem.conv',...\"],[\"('stages.0.blocks.0.mlp.drop1', torch.nn.modules.dropout.Dropout),\\n ('stages.0.blocks.0.mlp.norm', t...\"],[\"```\\n\\nUpon closer inspection, we see that the 2D conv layers have names such as `\\\"stages.0.blocks.0.m...\"],[\"```\\n\\nThis shows us that we only need to train less than 2% of all parameters, which is a huge effici...\"],[\"```\\n\\nIf that doesn't help, check the existing modules in your model architecture with the `named_mod...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom peft import LoraConfig, TaskType\\n\\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_S...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nSee the [`LoraConfig`] reference for more details about other parameters you can adjust,...\"],[\"```\\n\\nOut of [bigscience\\u002fmt0-large's](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fmt0-large) 1.2B parameters, y...\"],[\"```\\n\\nYou can also save your model to the Hub (make sure you're logged in to your Hugging Face accoun...\"],[\"```\\n\\nBoth methods only save the extra PEFT weights that were trained, meaning it is super efficient ...\"],[\"model = model.to(\\\"cuda\\\")\\nmodel.eval()\\ninputs = tokenizer(\\\"Preheat the oven to 350 degrees and place ...\"],[\"```\\n\\nFor other tasks that aren't explicitly supported with an `AutoPeftModelFor` class - such as aut...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.09685)\\n2. P...\"],[\"10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https:\\u002f\\u002farxiv.org\\u002f...\"],[\"## Getting started\\n\\n```python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_pe...\"],[\"```\\n\\n## Use Cases\\n\\n### Get comparable performance to full finetuning by adapting LLMs to downstream ...\"],[\"Performance of PEFT-LoRA tuned [`bigscience\\u002fT0_3B`](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fT0_3B) on [`ou...\"],[\"### Parameter Efficient Tuning of Diffusion Models\\n\\nGPU memory required by different settings during...\"],[\"accelerate launch train_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\\\\n  --instanc...\"],[\"```\\n\\nTry out the ğŸ¤— Gradio Space which should run seamlessly on a T4 instance:\\n[smangrul\\u002fpeft-lora-sd...\"],[\"### INT8 training of large models in Colab using PEFT LoRA and bitsandbytes\\n\\n- Here is now a demo on...\"],[\"An example of using LoRA for the task of adapting `LayoutLMForTokenClassification` on `FUNSD` datase...\"],[\"### Example of PEFT model training using ğŸ¤— Accelerate's DeepSpeed integration\\n\\nDeepSpeed version req...\"],[\"```\\n  b. run the below command to launch the example script\\n  ```bash\\n  accelerate launch --config_f...\"],[\"```\\n\\n### Example of PEFT model inference using ğŸ¤— Accelerate's Big Model Inferencing capabilities\\nAn ...\"],[\"### Sequence Classification\\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | ...\"],[\"### Image Classification\\n\\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | IA...\"],[\"## Caveats:\\n\\n1. Below is an example of using PyTorch FSDP for training. However, it doesn't lead to ...\"],[\"```\\n\\n  Example of parameter efficient tuning with [`mt0-xxl`](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fmt0-...\"],[\"```\\n\\n2. When using ZeRO3 with zero3_init_flag=True, if you find the gpu memory increase with trainin...\"],[\"```\\n\\nLearn more about the [low level API in the docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fpeft\\u002fdeveloper_gui...\"],[\"```\\n\\n## Contributing\\n\\nIf you would like to contribute to PEFT, please check out our [contributing gu...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Share adapters on the ğŸ¤— Hub\\n\\n\\n```python\\nmodel.push_to_hub(\\\"smangrul\\u002froberta-large-peft-lora\\\"...\"]],\"hovertemplate\":\"source=peft\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"peft, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"peft, circle\",\"showlegend\":true,\"x\":[-1.367982,-3.9034727,-2.014998,-5.905421,5.273869,5.3107767,5.754336,4.8973775,0.047302097,-0.66509515,-1.3835841,0.16798066,-5.693431,-0.6765496,-0.5451955,-1.3766336,-1.1490203,-1.124113,-0.8923787,-0.08311195,-0.5895352,-3.5647862,-4.9638634,-4.0872397,-1.191107,-2.2711093,-5.643396,-3.0311253,0.25115746,-0.97909504,-1.9101696,-1.0139482,0.78183,-1.0288919,-1.0837816,-0.9538013,-0.35759023,-1.0828971,0.5211565,-0.5953525,-5.6785855,1.1671398,2.0286324,-6.1591043,-6.466452,-6.3275976,-1.4635551,-2.8662405,-2.151962,-1.1042796,-0.74726987,0.15570861,-1.3259592,-0.10777714,-6.075891,-10.031127,1.4282506,0.2746517,0.25116768,0.7784664,0.3784381,-1.18927,-1.3454008,-1.8926214,-1.3783875,-1.9557589,-1.5807694,-0.3157019,0.16887426,-9.714732,-3.8580365,8.409406,8.372241,8.525211,-5.9205847,-5.960427,-5.737997,-5.8905807,8.931347,-4.1765547,-5.927726,-1.7667259,-2.5958157,-1.6726968,0.18164533,-1.4961817,-1.4034716,-0.4768388,-0.8367597,-1.4729466,-0.6698822,-6.0566096,-2.4491904,-3.3738127,-1.9472915,-1.5761911,-1.7148042,-1.0583743,-3.908874,-2.6031797,-2.8098428,-0.96398824,-1.9269915,-2.1763906,-2.1622446,-1.6909364,-1.2931582,-1.3693262,-0.24099106,0.4215142,-1.41974,-0.8851194,-0.8112982,-1.842007,-0.25613222,-1.4998573,-1.9967167,-0.26315206,-2.308929,-1.735095,-1.5542645,-0.26879773,-1.4220775,-0.76671535,-0.79639494,1.6931052,-0.8146908,-1.9586948,5.2212143,-4.337764,-4.6065216,-0.5171149,-4.6358213,-6.751139,-5.9647617,-6.518233,-0.34367618,-1.9335428,0.19135264,-1.8508497,-1.8604809,-0.96285355,-3.1009974,-3.6184845,-4.4873214,-1.8359739,-0.9288799,11.904472,11.050891,-1.9891398,-1.8466369,-1.4869547,-1.4257137,-1.0890108,-0.8563767,-2.5712159,-1.216001,-1.9774749,-1.5575029,-1.8634965,-1.3090492,-1.0681355,-2.1634164,5.9595942,-6.1187525,-2.4701996,5.6042995,0.23132221,0.20969506,-6.0439186,-2.4544718,-0.26335683,-0.9703598,-6.083447,-5.4326687,-1.0111924,-1.041161,-2.054425,-1.7461289,-1.411223,-1.7203343,-1.120139,-0.45250985,-1.1712297,-1.3162026,-1.0233847,-0.8073895,-1.5553308,-0.98648185,-0.87257874,-0.7836464,-3.3185282,-2.181815,-2.2534437,-1.0444981,-1.0171871,1.8477352,-2.0922246,-0.70141625,-0.20666824,0.48995495,-1.4819418,-1.8818375,-1.7462689,-2.3340318,-1.3416996,-1.5606619,-0.99727714,-0.8300206,-1.1434617,-4.425324,-1.2494165,-4.6710596,-1.4693583,-3.5089684,-1.6907501,-0.3089407,-0.34200242,-0.23556635,-1.3759001,-0.6470459,-0.49252474,-4.188445,-1.1693175,-1.4239649,-5.8518248,-5.9219937,-0.9254553,-4.567097,5.248514,-1.4293479,-0.13511401,-0.5847284,-5.602364,-0.29312092,-1.4351634,-0.6858264,-0.5347652,-5.415497,-0.7772552,0.5130085,-1.2706469,-1.2599401,-0.4551858,-2.4880342,-2.6676898,-4.8832283,-5.0568905,-5.8150396,-0.44123483,-1.0830902,-1.3223677,-1.3959078,-1.5687882,-0.4585139,-1.1271502,-0.26027593,-0.4982062,-1.9719195,-0.13813432,-0.76136583,1.8252829,-5.9481454,-3.5445175,-0.86873806,0.30546418,-1.2462339,-1.5681442,-1.9185835,-1.5021613,0.3620021,-1.3231483,-1.0735855,-0.21349584,5.5065336,-1.1172253,-1.8114711,-3.973539,-4.1047974,-4.337685,-4.5413632,-4.724015,-6.027181,-0.28670597,-1.3976252,-0.7383747,-0.48173785,-0.47614563,-0.36056247,-1.442754,-0.9060738,-0.3432136,-9.267622,-5.9378147,-4.096923,-1.2362953,-2.1864488,0.11144657,-1.4486343,-1.7574877,-2.2781224,0.08885657,-0.9681187,-1.4528356,-4.5468254,-3.5853956,-1.4843733,-6.2456193,-0.25000402,-1.4386846,-1.1365185,-1.8408408,-4.1066175,0.6304715,-1.4692708,-1.1728269,-0.71592414,-0.05832592,-0.50524116,-0.107657455,-0.23522809,-1.490334,-0.38548014,-1.5839483,-9.961126,-2.087427,-1.4803585,-0.95709664,-1.7780484,-1.4764698,-0.78207844,-4.830595,-2.3090546,-1.8223814,-0.7937173,-1.0681682,-1.2199405,-2.4682992,-1.8944544,-2.8667078,-1.1699834,-1.6003023,-1.0422817,2.5955677,-0.8500824,-1.9517214,-1.1297741,-4.4454274,-5.3393273,-5.784189,-1.1918651,-3.845122,-4.0174074,-3.6071398,-1.2187229,-3.967729,-4.1815667,-3.2762318,-2.0308435,-1.2856215,-4.427941,-9.379532,-9.959235,-2.2638206,-1.5234368,-1.9828283,-1.1331861,6.7867603,-0.2907425,-0.7373567,-0.41595763],\"xaxis\":\"x\",\"y\":[-1.1821319,-2.34669,-1.1334802,-0.22384205,0.14580268,2.3772998,2.5209906,2.0371768,2.6552198,1.7868108,3.9231544,2.7619421,1.7538478,-0.36614785,4.54577,4.2825894,-0.6735498,-0.58331794,-0.4644199,0.8056746,0.11262635,-1.4372641,-1.4782815,-0.6372289,-1.3910077,-1.4405155,-1.2547693,-2.3296916,-6.2544394,-0.31219944,-1.3184805,-1.495063,0.5007328,-2.3727162,1.4090979,-1.7886635,-0.42209545,4.27462,6.434862,-0.52993816,-1.3713573,-0.24567504,0.9923287,7.1309376,7.8200593,7.785989,4.203617,-0.004823923,-1.5614024,-0.15901414,-0.23026194,7.4400873,3.4687946,-1.0322522,7.0592666,-1.0763841,3.0638661,-6.257762,-6.2248793,6.5463758,7.267095,0.4138621,-1.4417495,-1.546265,-0.9320273,-1.7250663,-2.274272,-6.2835774,-6.269751,-1.239841,-1.6220374,2.6017137,2.6132786,2.349405,1.5574412,1.5516346,1.2233938,1.4782166,2.742436,-1.3041295,1.3867912,-1.4645311,4.4296436,1.6204287,4.09532,4.2612123,3.2064507,4.213708,-0.19469777,2.135084,-0.12100158,1.783469,-11.812881,-6.2784433,-5.965829,-5.705277,-1.8197457,-2.82113,-4.9217367,-4.768592,-4.679927,-2.69482,-5.635062,-6.2214575,-5.921983,-5.779118,-4.4499617,-6.1223073,-0.44610038,4.4305415,4.303882,-0.21331011,2.4549034,3.4580204,-0.46859056,4.383709,3.4182255,-0.48721543,4.2023735,4.241836,3.6917875,4.3159766,4.2532635,-0.58193684,-0.026806703,0.85871243,-0.8296543,3.3597264,-0.20965171,-1.123884,-0.50200546,-0.07296312,-0.618532,5.5792327,7.195434,7.9017534,4.1127167,4.673103,6.6139,0.77027255,-1.5291466,0.119600885,2.4034514,6.485769,-0.7870476,3.549327,-0.7101439,4.879337,3.2120488,-7.6609406,-5.944439,-5.8730817,-4.738235,-6.5148635,-6.8844466,-2.3422866,-2.3623292,-2.552222,-2.333283,-1.4752246,-1.2567106,-0.7259222,-1.320859,-0.7426369,1.6826506,-11.719791,-0.23055486,0.43015566,-0.11423351,1.6184729,-11.760582,-0.5223439,1.8471197,0.14111835,-0.43571758,-2.5575764,-2.9410095,-2.2307174,-0.485593,-2.0735536,-1.505664,-1.6957098,-0.15928873,5.6658444,3.3204834,3.663505,-0.20390831,3.308909,-0.16420071,-0.57128876,-0.31764954,-2.7955382,-1.489133,-1.6956196,-3.6415546,-2.852145,0.5723105,3.3170798,3.0990295,-0.51510936,4.2920275,4.309793,3.458628,3.6441314,-1.6971779,-1.9648318,-1.5043112,-1.4194787,-1.5849533,-1.543361,-2.0320685,-1.3966196,1.9624082,-1.4469965,2.6530614,4.2536397,-0.4005486,-0.9002813,-0.40644538,4.315484,-0.9468802,-1.7413442,-3.2359133,-1.8966693,-2.461449,3.1953826,3.0590065,2.3028076,-0.8647884,2.728881,3.2277591,4.6213603,5.218835,0.33904666,-0.5058899,4.383156,-0.9544834,-1.6696932,1.6461543,-0.067411065,6.6655974,4.3001776,-1.2742534,-0.20610635,4.0540576,-6.2825775,-6.2214627,-6.5282264,1.5227059,-0.43218648,4.327351,4.1306305,4.2882113,3.558244,4.0418687,-1.1120406,-0.5655311,-1.104928,2.8132536,-0.4366133,-0.4255899,0.8186005,1.7965068,-2.4546425,0.57912576,-6.232342,1.7350425,-1.4110866,-1.4720991,0.6278292,5.837597,0.6605557,-1.6300213,-6.1839356,-0.3712276,-1.3632538,0.8774919,-2.9629116,-2.8425047,-2.7944517,-5.068933,-1.8991945,0.8087182,-0.4869892,4.282083,-0.785129,-1.6561186,0.093146145,-0.43298683,4.3061113,-0.817596,-1.660777,1.6892732,1.2164358,-3.1828504,-1.1861551,5.09758,6.645987,4.878672,4.1924915,-1.5958694,-0.70104104,-0.7512133,2.612216,-0.9087981,-0.6402679,0.6600454,2.4842045,-0.53820586,4.3743634,-0.6645295,3.466556,-5.7985115,0.64790606,2.3097093,4.562915,0.11156518,4.65467,5.3464384,-0.46881402,4.3505616,4.3094077,-1.277372,3.0419059,-1.0500504,-1.5783579,-1.1961852,-0.26603362,-1.5148154,-0.7042466,-1.9570134,-0.12917446,-1.2873781,-1.6630857,2.2418556,1.6856636,0.2368992,-1.5835298,0.560544,-1.886771,-1.1052926,-1.5501913,-0.47049612,0.070394576,-1.6438398,2.9846287,-1.4514501,-1.0910362,0.17859443,-0.7817597,-1.2942967,-2.3513024,-2.1184735,-4.1645722,-3.155864,-2.2272656,-1.5191128,-1.8279415,-2.1776533,-1.7871842,-1.1148729,-0.26883757,-0.9094588,-2.0712962,-2.3107948,-1.8322568,-2.6159022,2.3943963,-0.48807052,-0.8762202,-1.6850221],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eHuggingface Hub à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤• à¤ªà¤¾à¤¯à¤¥à¤¨ à¤•à¥à¤²à¤¾à¤‡à¤‚à¤Ÿà¥¤\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"center\\\"...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## huggingface_hub à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤®à¥‡à¤‚ à¤†à¤ªà¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ\\n\\n`huggingface_hub` à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤†à¤ªà¤•à¥‹ [à¤¹à¤—à¤¿à¤‚à¤— à¤«à¥‡à¤¸ à¤¹à¤¬...\"],[\"## à¤ªà¥à¤°à¤®à¥à¤– à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤à¤‚\\n\\n- [à¤«à¤¼à¤¾à¤‡à¤²à¥‡à¤‚ à¤¡à¤¾à¤‰à¤¨à¤²à¥‹à¤¡ à¤•à¤°à¥‡à¤‚](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002f...\"],[\"```\\n\\nà¤¯à¤¦à¤¿ à¤†à¤ª à¤šà¤¾à¤¹à¥‡à¤‚, à¤¤à¥‹ à¤†à¤ª à¤‡à¤¸à¥‡ [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002finstallation#ins...\"],[\"```\\n\\nà¤¯à¤¾ à¤à¤• à¤¸à¤‚à¤ªà¥‚à¤°à¥à¤£ à¤­à¤‚à¤¡à¤¾à¤°\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\\\"st...\"],[\"```\\n\\n[à¤…à¤ªà¤²à¥‹à¤¡ à¤—à¤¾à¤‡à¤¡](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload) à¤®à¥‡à¤‚ à¤µà¤¿à¤µà¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤à¥¤\\n\\n#...\"],[\"à¤¯à¤¦à¤¿ à¤†à¤ª à¤…à¤ªà¤¨à¥€ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤•à¥‹ à¤à¤•à¥€à¤•à¥ƒà¤¤ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤šà¤°à¥à¤šà¤¾ à¤¶à¥à¤°à¥‚ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¬à¥‡à¤à¤¿à¤à¤• à¤à¤• à¤®à¥à¤¦à¥à¤¦à¤¾ à¤–à¥‹à¤²à¥‡à¤‚à¥¤ à¤¹à¤®...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"# Download from a dataset\\n\\u003e\\u003e\\u003e hf_hub_download(repo_id=\\\"google\\u002ffleurs\\\", filename=\\\"fleurs.py\\\", repo_ty...\"],[\"```\\n\\n### From specific version\\n\\nBy default, the latest version from the `main` branch is downloaded....\"],[\"```\\n\\n**Note:** When using the commit hash, it must be the full-length hash instead of a 7-character ...\"],[\"```\\n\\n### Filter files to download\\n\\n[`snapshot_download`] provides an easy way to download a reposito...\"],[\"```\\n\\n## Download file(s) to local folder\\n\\nThe recommended (and default) way to download files from t...\"],[\"However, in some cases you want to download files and move them to a specific folder. This is useful...\"],[\"Here is a table that summarizes the different options to help you choose the parameters that best su...\"],[\"## Download from the CLI\\n\\nYou can use the `huggingface-cli download` command from the terminal to di...\"],[\"```\\n\\nYou can download multiple files at once which displays a progress bar and returns the snapshot ...\"],[\"--\\n# For reference on dataset card metadata, see the spec: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fb...\"],[\"\\u003c!-- This section describes suitable use cases for the dataset. --\\u003e\\n\\n{{ direct_use | default(\\\"[More ...\"],[\"{{ annotation_process_section | default(\\\"[More Information Needed]\\\", true)}}\\n\\n#### Who are the annot...\"],[\"{{ glossary | default(\\\"[More Information Needed]\\\", true)}}\\n\\n## More Information [optional]\\n\\n{{ more_...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Upload a folder\\n\\nUse the [`upload_folder`] function to upload a local folder to an existing ...\"],[\"```\\n\\nBy default, the `.gitignore` file will be taken into account to know which files should be comm...\"],[\"```\\n\\nYou can also use the `delete_patterns` argument to specify files you want to delete from the re...\"],[\"```\\n\\n`local_path` and `path_in_repo` are optional and can be implicitly inferred. If `local_path` is...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nBackground jobs are queued when using `run_as_future=True`. This means that you are guar...\"],[\"```\\n\\n### Upload a folder by chunks\\n\\n[`upload_folder`] makes it easy to upload an entire folder to th...\"],[\"```\\n\\nIf you want a better control on the upload strategy (i.e. the commits that are created), you ca...\"],[\"# Schedule regular uploads. Remote repo and local folder are created if they don't already exist.\\n\\u003e\\u003e...\"],[\"```\\n\\nAnd that's it! User input\\u002foutputs and feedback will be available as a dataset on the Hub. By us...\"],[\"#### Space persistence demo\\n\\nPersisting data from a Space to a Dataset on the Hub is the main use ca...\"],[\"# 2. Zip png files in a single archive\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n       ...\"],[\"```\\n\\nWhen you overwrite `push_to_hub`, you have access to the attributes of [`CommitScheduler`] and ...\"],[\"For example, if you want to upload two files and delete a file in a Hub repository:\\n\\n1. Use the appr...\"],[\"```\\n\\n2. Pass your operations to [`create_commit`]:\\n\\n```py\\n\\u003e\\u003e\\u003e api.create_commit(\\n...     repo_id=\\\"ly...\"],[\"```\\n\\nIn addition to [`upload_file`] and [`upload_folder`], the following functions also use [`create...\"],[\"\\u003e\\u003e\\u003e repo_id = create_repo(\\\"test_preupload\\\").repo_id\\n\\n\\u003e\\u003e\\u003e operations = [] # List of all `CommitOperat...\"],[\"```\\n\\nFirst, we create the [`CommitOperationAdd`] objects one by one. In a real-world example, those ...\"],[\"- **Start small**: We recommend starting with a small amount of data to test your upload script. It'...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTT...\"],[\"```\\n\\nYou should install this for each repository that has a very large file. Once installed, you'll ...\"],[\"```\\n\\nYou can check the status of your push with the `command_queue` method:\\n\\n```python\\n\\u003e\\u003e\\u003e last_comm...\"],[\"```\\n\\nHowever, if you aren't ready to push a file yet, you can use [`~Repository.git_add`] and [`~Rep...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Do your best to follow these guidelines when submitting an issue or a feature\\nrequest. It will make ...\"],[\"Before writing code, we strongly advise you to search through the existing PRs or\\nissues to make sur...\"],[\"```\\n\\n3. Create a new branch to hold your development changes, and do this for every new PR you work ...\"],[\"```\\n\\n   Compared to `make style`, `make quality` will never update your code. In addition to the pre...\"],[\"```\\n\\n10. Once you are satisfied (**and the [checklist below](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingf...\"],[\"### Tests\\n\\nAn extensive test suite is included to test the library behavior and several examples. Li...\"],[\"```\\n\\nYou can specify a smaller set of tests in order to test only the feature you're working on.\\n\\nFo...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"All community leaders are obligated to respect the privacy and security of the\\nreporter of any incid...\"],[\"[homepage]: https:\\u002f\\u002fwww.contributor-covenant.org\\n\\nFor answers to common questions about this code of...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nIf the CLI is correctly installed, you should see a list of all the options available in the CL...\"],[\"```\\n_|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      ...\"],[\"```\\n\\nFor more details about authentication, check out [this section](..\\u002fquick-start#authentication)....\"],[\"```\\n\\n### Download a single file\\n\\nTo download a single file from a repo, simply provide the repo_id a...\"],[\"```\\n\\n### Download multiple files\\n\\nYou can also download a subset of the files from a repository with...\"],[\"```\\n\\nThe other approach is to provide patterns to filter which files you want to download using `--i...\"],[\"```\\n\\n### Download to a local folder\\n\\nThe recommended (and default) way to download files from the Hu...\"],[\"```\\n\\n### Quiet mode\\n\\nBy default, the `huggingface-cli download` command will be verbose. It will pri...\"],[\"```\\n\\nFinally, you can upload a folder to a specific destination on the repo:\\n\\n```bash\\n\\u003e\\u003e\\u003e huggingfac...\"],[\"```\\n\\n### Upload to a dataset or Space\\n\\nTo upload to a dataset or a Space, use the `--repo-type` opti...\"],[\"```\\n\\n### Upload at regular intervals\\n\\nIn some cases, you might want to push regular updates to a rep...\"],[\"```\\n\\n## huggingface-cli scan-cache\\n\\nScanning your cache directory is useful if you want to know whic...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli scan-cache\\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES L...\"],[\"Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\\nGot 1 warning(s) while scanning. Use -vvv to pr...\"],[\"```\\n\\nFor more details about how to scan your cache directory, please refer to the [Manage your cache...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli env\\n\\nCopy-and-paste the text below in your GitHub issue.\\n\\n- huggingface_...\"],[\"Running Tests\\n\\nTo run the test suite, please perform the following from the root directory of this r...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003e\\u003e\\u003e # Read the content of a remote file as a string\\n\\u003e\\u003e\\u003e train_data = fs.read_text(\\\"datasets\\u002fmy-usern...\"],[\"```\\n\\nThe optional `revision` argument can be passed to run an operation from a specific commit such ...\"],[\"```\\n\\nThe same workflow can also be used for [Dask](https:\\u002f\\u002fdocs.dask.org\\u002fen\\u002fstable\\u002fhow-to\\u002fconnect-to...\"],[\"```\\n\\n* Using the Hub as an array store with [Zarr](https:\\u002f\\u002fzarr.readthedocs.io\\u002fen\\u002fstable\\u002ftutorial.ht...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Let's fetch the collection with, `\\\"TheBloke\\u002frecent-models-64f9a55bb3115b4f513ec026\\\"`:\\n\\n```py\\n\\u003e\\u003e\\u003e fro...\"],[\"```\\n\\nThe [`Collection`] object returned by [`get_collection`] contains:\\n- high-level metadata: `slug...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nWhen listing collections, the item list per collection is truncated to 4 ...\"],[\"```\\n\\nParameter `sort` must be one of  `\\\"last_modified\\\"`,  `\\\"trending\\\"` or `\\\"upvotes\\\"`. Parameter `it...\"],[\"```\\n\\n## Manage items in a collection\\n\\nNow that we have a [`Collection`], we want to add items to it ...\"],[\"```\\n\\nIf an item already exists in a collection (same `item_id`\\u002f`item_type` pair), an HTTP 409 error ...\"],[\"```\\n\\n### Remove items\\n\\nFinally, you can also remove an item using [`delete_collection_item`].\\n\\n```py...\"],[\"--\\nlanguage:\\n- en\\nlicense: mit\\nlibrary_name: pytorch-lightning\\ntags:\\n- pytorch\\n- image-classificatio...\"],[\"--\\n[]\\n---\\n\\n# invalid-card-data\\n\\nThis card should fail when trying to load it in because the card dat...\"],[\"his document covers all steps that need to be done in order to do a release of the `huggingface_hub`...\"],[\"```\\n__version__ = \\\"\\u003cVERSION+1\\u003e.dev0\\\"  # For example, after releasing v0.5.0 or v0.5.1: \\\"0.6.0.dev0\\\"....\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nAt this step, your app should already be running on the Hub for free !\\nHowever, you might want ...\"],[\"```\\n\\n\\u003cTip\\u003e\\nFrom within your Space, secrets are available as environment variables (or\\nStreamlit Secr...\"],[\"```\\n\\n**4. Configure the hardware**\\n\\nBy default, your Space will run on a CPU environment for free. Y...\"],[\"```\\n```py\\n\\u003e\\u003e\\u003e api.duplicate_space(\\n...     from_id=repo_id,\\n...     hardware=\\\"cpu-upgrade\\\",\\n...     ...\"],[\"```\\n\\nNote: if you are using a 'cpu-basic' hardware, you cannot configure a custom sleep time. Your S...\"],[\"```\\n\\nYou can also delete your storage, losing all the data permanently.\\n```py\\n\\u003e\\u003e\\u003e api.delete_space_s...\"],[\"```\\n\\n## More advanced: temporarily upgrade your Space !\\n\\nSpaces allow for a lot of different use cas...\"],[\"# Space own repo_id\\nTRAINING_SPACE_ID = \\\"Wauplin\\u002fdreambooth-training\\\"\\n\\nfrom huggingface_hub import H...\"],[\"```\\n\\n### Task scheduler\\n\\nScheduling tasks can be done in many ways. Here is an example how it could ...\"],[\"api.upload_file(\\n        repo_id=repo_id,\\n        repo_type=repo_type,\\n        path_in_repo=\\\"tasks.c...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### eval_results_to_model_index\\n\\n[[autodoc]] huggingface_hub.repocard_data.eval_results_to_model_ind...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy mode...\"],[\"[[autodoc]] InferenceEndpoint\\n  - from_raw\\n  - client\\n  - async_client\\n  - all\\n\\n## InferenceEndpoint...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## HfApi\\n\\n[[autodoc]] HfApi\\n\\n[[autodoc]] plan_multi_commits\\n\\n## API Dataclasses\\n\\n### AccessRequ...\"],[\"[[autodoc]] huggingface_hub.hf_api.UserLikes\\n\\n## CommitOperation\\n\\nBelow are the supported values for...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eThe official Python client for the Huggingface Hub.\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"ce...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"## Key features\\n\\n- [Download files](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fdownload) ...\"],[\"```\\n\\nIf you prefer, you can also install it with [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub...\"],[\"```\\n\\n### Create a repository\\n\\n```py\\nfrom huggingface_hub import create_repo\\n\\ncreate_repo(repo_id=\\\"su...\"],[\"```\\n\\nFor details in the [upload guide](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload)...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[[autodoc]] huggingface_hub.WebhooksServer\\n\\n### @webhook_endpoint\\n\\n[[autodoc]] huggingface_hub.webho...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fp...\"],[\"## Contribute\\n\\nAll contributions to the `huggingface_hub` are welcomed and equally valued! ğŸ¤— Besides...\"],[\"--\\n# For reference on model card metadata, see the spec: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblo...\"],[\"## Uses\\n\\n\\u003c!-- Address questions around how the model is intended to be used, including the foreseeab...\"],[\"### Training Procedure \\n\\n\\u003c!-- This relates heavily to the Technical Specifications. Content here sho...\"],[\"## Model Examination [optional]\\n\\n\\u003c!-- Relevant interpretability work for the model goes here --\\u003e\\n\\n{{...\"],[\"**APA:**\\n\\n{{ citation_apa | default(\\\"[More Information Needed]\\\", true)}}\\n\\n## Glossary [optional]\\n\\n\\u003c!...\"],[\"MyCoolModel\\n\\nIn this example, we don't have any metadata at the top of the file. In cases like these...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```python\\n# app.py\\nfrom huggingface_hub import webhook_endpoint, WebhookPayload\\n\\n@webhook_endpoint\\na...\"],[\"```\\n\\nSave this snippet in a file called `'app.py'` and run it with `'python app.py'`. You should see...\"],[\"```\\n\\nGood job! You just launched a webhook server! Let's break down what happened exactly:\\n\\n1. By de...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n## Configure a Webhook\\n\\nNow that you have a webhook server running, you want to configure a...\"],[\"Your webhook server is now running on a public Space. If most cases, you will want to secure it with...\"],[\"```\\n\\nWhich will create two endpoints:\\n\\n```text\\n(...)\\nWebhooks are correctly setup and ready to use:\\n...\"],[\"```\\n\\n1. We define a custom UI using Gradio blocks. This UI will be displayed on the landing page of ...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThe docs will be viewable at [http:\\u002f\\u002flocalhost:3000](http:\\u002f\\u002flocalhost:3000). You can also previ...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"If you want to create a link to some internal class or function, you need to\\nprovide its path. For i...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n    Returns:\\n        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special tok...\"],[\"```\\n    Example:\\n\\n    ```python\\n    \\u003e\\u003e\\u003e from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\n ...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### Create a repository\\n\\nCreate an empty repository with [`create_repo`] and give it a name with the...\"],[\"```\\n\\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` paramet...\"],[\"```\\n\\n## Upload and download files\\n\\nNow that you have created your repository, you are interested in ...\"],[\"```\\n\\nYou can use the [`delete_branch`] and [`delete_tag`] functions in the same way to delete a bran...\"],[\"Some settings are specific to Spaces (hardware, environment variables,...). To configure those, plea...\"],[\"```\\n\\n### Rename your repository\\n\\nYou can rename your repository on the Hub using [`move_repo`]. Usin...\"],[\"```\\n\\n### Clone\\n\\nThe `clone_from` parameter clones a repository from a Hugging Face repository ID to ...\"],[\"```\\n\\n### Branch\\n\\nBranches are important for collaboration and experimentation without impacting your...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eHugging Face Hub Python å®¢æˆ·ç«¯\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"http...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"## ä¸»è¦ç‰¹ç‚¹\\n\\n- [ä»hugging face hubä¸‹è½½æ–‡ä»¶](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fdownload)\\n-...\"],[\"```\\n\\nå¦‚æœæ‚¨æ›´å–œæ¬¢ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ conda è¿›è¡Œå®‰è£…\\n\\nä¸ºäº†é»˜è®¤ä¿æŒåŒ…çš„æœ€å°åŒ–ï¼Œhuggingface_hub å¸¦æœ‰ä¸€äº›å¯é€‰çš„ä¾èµ–é¡¹ï¼Œé€‚ç”¨äºæŸäº›ç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³è¦å®Œæ•´çš„æ¨æ–­ä½“éªŒï¼Œè¯·è¿è¡Œï¼š\\n\\n`...\"],[\"```\\n\\n### ä¸Šä¼ æ–‡ä»¶\\n\\nä¸Šä¼ å•ä¸ªæ–‡ä»¶,è¯·è¿è¡Œä»¥ä¸‹ä»£ç \\n\\n```py\\nfrom huggingface_hub import upload_file\\n\\nupload_file(\\n    path_...\"],[\"```\\n\\næœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [ä¸Šä¼ æŒ‡å—](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload).\\n\\n## é›†æˆåˆ° Hub ä¸­\\n...\"],[\"## æ¬¢è¿å„ç§è´¡çŒ®ï¼ˆåŠŸèƒ½è¯·æ±‚ã€é”™è¯¯ç­‰ï¼‰ ğŸ’™ğŸ’šğŸ’›ğŸ’œğŸ§¡â¤ï¸\\n\\næ¬¢è¿æ¯ä¸ªäººæ¥è¿›è¡Œè´¡çŒ®ï¼Œæˆ‘ä»¬é‡è§†æ¯ä¸ªäººçš„è´¡çŒ®ã€‚ç¼–å†™ä»£ç å¹¶éå”¯ä¸€çš„å¸®åŠ©ç¤¾åŒºçš„æ–¹å¼ã€‚å›ç­”é—®é¢˜ã€å¸®åŠ©ä»–äººã€ç§¯æäº’åŠ¨å¹¶æ”¹å–„æ–‡æ¡£å¯¹ç¤¾åŒºæ¥è¯´éƒ½æ˜¯æå…¶æœ‰ä»·å€¼çš„...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eDer offizielle Python-Client fÃ¼r den Huggingface Hub.\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## Willkommen bei der huggingface_hub Bibliothek\\n\\nDie `huggingface_hub` Bibliothek ermÃ¶glicht I...\"],[\"## Installation\\n\\nInstallieren Sie das `huggingface_hub` Paket mit [pip](https:\\u002f\\u002fpypi.org\\u002fproject\\u002fhug...\"],[\"```\\n\\nWenn Sie mÃ¶chten, kÃ¶nnen Sie es auch mit [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fde...\"],[\"```\\n\\nOder eine gesamte Repository\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_dow...\"],[\"```\\n\\nWeitere Informationen finden Sie im [Upload-Leitfaden](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_...\"],[\"## BeitrÃ¤ge (Feature-Anfragen, Fehler usw.) sind super willkommen ğŸ’™ğŸ’šğŸ’›ğŸ’œğŸ§¡â¤ï¸\\n\\nJeder ist willkommen beiz...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Contrib test suite\\n\\nThe contrib folder contains simple end-to-end scripts to test integration of `hu...\"],[\"```\\n\\nThen tests can be run\\n\\n```sh\\n# Optional: -j4 to run in parallel. Output will be messy in that c...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"--\\nlanguage: en\\nlicense: mit\\nlibrary_name: timm\\ntags:\\n- pytorch\\n- image-classification\\ndatasets:\\n- b...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eê³µì‹ Huggingface Hub íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"htt...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## huggingface_hub ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œìš”\\n\\n`huggingface_hub` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co...\"],[\"## ì£¼ìš” ê¸°ëŠ¥\\n\\n- Hubì—ì„œ [íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fmain\\u002fko\\u002fguides\\u002fdownload)\\n- ...\"],[\"```\\n\\nì›í•œë‹¤ë©´ [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fko\\u002finstallation#install-with-conda)ë¥¼ ì´...\"],[\"```\\n\\níŒŒì¼ì€ ë¡œì»¬ ìºì‹œ í´ë”ì— ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì´ ê°€ì´ë“œ](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fko\\u002fguides\\u002fma...\"],[\"```\\n\\në ˆí¬ì§€í† ë¦¬ ì „ì²´ì˜ ê²½ìš°:\\n\\n```py\\nfrom huggingface_hub import upload_folder\\n\\nupload_folder(\\n    folder_path=...\"],[\"```\\n\\nìì„¸í•œ ë‚´ìš©ì€ [ì—…ë¡œë“œ ê°€ì´ë“œ](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fko\\u002fguides\\u002fupload)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\\n\\n## Hug...\"],[\"ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤:\\n\\n- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ìë“¤ì˜ ëª¨ë¸ì´ë‚˜ ë°ì´í„°ì…‹ì„ ë¬´ë£Œë¡œ í˜¸ìŠ¤íŒ…í•´ì¤ë‹ˆë‹¤.\\n- gitì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë°©ì‹ìœ¼ë¡œ, ì•„ì£¼ í° íŒŒì¼ë“¤ë„ ë²„ì „ì„ ê´€ë¦¬í• ...\"],[\"ì—¬ëŸ¬ë¶„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•©í•˜ê³  ì‹¶ë‹¤ë©´, ì´ìŠˆë¥¼ ì—´ì–´ì„œ ì˜ê²¬ì„ ë‚˜ëˆ ì£¼ì„¸ìš”. í†µí•© ê³¼ì •ì„ ì•ˆë‚´í•˜ê¸° ìœ„í•´ â¤ï¸ì„ ë‹´ì•„ [ë‹¨ê³„ë³„ ê°€ì´ë“œ](https:\\u002f\\u002fhuggingface.co\\u002fdocs...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"## Translating the `huggingface_hub` documentation into your language\\n\\nAs part of our mission to dem...\"],[\"```\\n\\nHere, `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https:...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nThe levels should be understood as follows:\\n\\n- `error`: only show critical logs about usage whi...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import snapshot_download\\n\\u003e\\u003e\\u003e from huggingface_hub.utils import are_pr...\"],[\"```\\n\\n### are_progress_bars_disabled\\n\\n[[autodoc]] huggingface_hub.utils.are_progress_bars_disabled\\n\\n#...\"],[\"```py\\nimport requests\\nfrom huggingface_hub.utils import hf_raise_for_status, HfHubHTTPError\\n\\nrespons...\"],[\"```\\n\\n[[autodoc]] huggingface_hub.utils.hf_raise_for_status\\n\\n### HTTP errors\\n\\nHere is a list of HTTP ...\"],[\"[[autodoc]] utils.send_telemetry\\n\\n\\n## Validators\\n\\n`huggingface_hub` includes custom validators to va...\"],[\"\\u003e\\u003e\\u003e my_cool_auth_method(token=\\\"a token\\\")\\n\\\"a token\\\"\\n\\n\\u003e\\u003e\\u003e my_cool_auth_method(use_auth_token=\\\"a use_au...\"],[\"```\\n\\n#### validate_hf_hub_args\\n\\n[[autodoc]] utils.validate_hf_hub_args\\n\\n#### HFValidationError\\n\\n[[au...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nè¦ä¸‹è½½æ–‡ä»¶çš„ç‰¹å®šç‰ˆæœ¬ï¼Œè¯·ä½¿ç”¨`revision`å‚æ•°æŒ‡å®šåˆ†æ”¯åç§°ã€æ ‡ç­¾æˆ–æäº¤å“ˆå¸Œã€‚å¦‚æœæ‚¨é€‰æ‹©ä½¿ç”¨æäº¤å“ˆå¸Œï¼Œå®ƒå¿…é¡»æ˜¯å®Œæ•´é•¿åº¦çš„å“ˆå¸Œï¼Œè€Œä¸æ˜¯è¾ƒçŸ­çš„7ä¸ªå­—ç¬¦çš„æäº¤å“ˆå¸Œï¼š\\n\\n```py\\n\\u003e\\u003e\\u003e fr...\"],[\"```\\n\\næ‚¨è¿˜å¯ä»¥ç›´æ¥å°†ä»¤ç‰Œä¼ é€’ç»™ [`login`]ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š`login(token=\\\"hf_xxx\\\")`ã€‚è¿™å°†ä½¿ç”¨æ‚¨çš„ç”¨æˆ·è®¿é—®ä»¤ç‰Œç™»å½•åˆ° Hugging Face æ¨¡å‹åº“ï¼Œè€Œæ— éœ€æ‚¨è¾“å…¥ä»»ä½•å†…å®¹...\"],[\"```\\nå¦‚æœæ‚¨æƒ³å°†å­˜å‚¨åº“è®¾ç½®ä¸ºç§æœ‰ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import HfApi  \\n\\u003e\\u003e\\u003e api = HfApi()  \\n\\u003e\\u003e\\u003e ...\"],[\"```\\n\\nè¦ä¸€æ¬¡ä¸Šä¼ å¤šä¸ªæ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹[ä¸Šä¼ æŒ‡å—](.\\u002fguides\\u002fupload) ,è¯¥æŒ‡å—å°†å‘æ‚¨ä»‹ç»å‡ ç§ä¸Šä¼ æ–‡ä»¶çš„æ–¹æ³•ï¼ˆæœ‰æˆ–æ²¡æœ‰ gitï¼‰ã€‚\\n\\n## ä¸‹ä¸€æ­¥\\n\\n`huggingface_hub`åº“ä¸º...\"],[\"Hugging Face Hub Client library\\n\\n## Download files from the Hub\\n\\nThe `hf_hub_download()` function is...\"],[\"```\\n\\n### `snapshot_download`\\n\\nUsing `hf_hub_download()` works well when you know which files you wan...\"],[\"\\u003cbr\\u003e\\n\\n## Publish files to the Hub\\n\\nIf you've used Git before, this will be very easy since Git is us...\"],[\"```\\n\\nWith the `HfApi` class there are methods to query models, datasets, and metrics by specific tag...\"],[\"```\\n\\nIf the repository you're cloning is one of yours or one of your organisation's,\\nthen having the...\"],[\"```\\n\\nThe repository can be managed through this object, through wrappers of\\ntraditional Git methods:...\"],[\"These two methods also have support for the `blocking` parameter.\\n\\nExamples using the `commit` conte...\"],[\"```\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e model = torch.nn.Transformer()\\n\\u003e\\u003e\\u003e with Repository(\\\"torch-model\\\"...\"],[\"```\\n\\nThis should be executed once for each model repo that contains a model file\\n\\u003e5GB. If you just t...\"],[\"```\\n\\nThis is an example of a task (`question-answering`) which requires a dictionary\\nas input thas h...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nOnce done, [check installation](#check-installation) is working correctly.\\n\\n### Install optiona...\"],[\"```\\n\\nWhen installing from source, you can also specify a specific branch. This is useful if you\\nwant...\"],[\"```\\n\\nThis command will fetch information from the Hub about the [gpt2](https:\\u002f\\u002fhuggingface.co\\u002fgpt2) ...\"],[\"```\\n\\n## Windows limitations\\n\\nWith our goal of democratizing good ML everywhere, we built `huggingfac...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n`HfApi.get_repo_discussions` returns a [generator](https:\\u002f\\u002fdocs.python.org\\u002f3.7\\u002fhowto\\u002ffunctional...\"],[\"```\\n\\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overvi...\"],[\"```\\n\\n[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclas...\"],[\"```\\n\\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to cr...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"If you are interested in Inference and Widgets, you can follow [this guide](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"```\\n\\n### push_to_hub\\n\\nThe `push_to_hub` method often requires a bit more complexity to handle repo c...\"],[\"```\\n\\nThis is of course only an example. If you are interested in more complex manipulations (delete ...\"],[\"In a lot of cases, a library already implements its model using a Python class. The class contains t...\"],[\"The advantage of using [`ModelHubMixin`] is that once you take care of the serialization\\u002floading of ...\"],[\"```\\n\\n#### Implementation\\n\\nThe implementation is actually very straightforward, and the full implemen...\"],[\"```\\n\\n3. Implement the `_from_pretrained` method:\\n\\n```python\\nclass PyTorchModelHubMixin(ModelHubMixin...\"],[\"```\\n\\nAnd that's it! Your library now enables users to upload and download files to and from the Hub....\"],[\"Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy any ...\"],[\"```\\n\\nIn this example, we created a `protected` Inference Endpoint named `\\\"my-endpoint-name\\\"`, to ser...\"],[\"```\\n\\nIt's a dataclass that holds information about the endpoint. You can access important attributes...\"],[\"```python\\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\\n\\u003e\\u003e\\u003e from huggingface_hub impor...\"],[\"```\\n\\nThe value to pass as `custom_image` is a dictionary containing a url to the docker container an...\"],[\"```\\n\\n## Check deployment status\\n\\nIn the rest of this guide, we will assume that we have a [`Inferenc...\"],[\"```\\n\\nInstead of fetching the Inference Endpoint status while waiting for it to run, you can directly...\"],[\"```\\n\\nIf the Inference Endpoint is not running, an [`InferenceEndpointError`] exception is raised:\\n\\n`...\"],[\"```\\n\\nFor more details about how to use the [`InferenceClient`], check out the [Inference guide](..\\u002fg...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```py\\n# Pause and resume endpoint\\n\\u003e\\u003e\\u003e endpoint.pause()\\nInferenceEndpoint(name='my-endpoint-n...\"],[\"```\\n\\n### Update model or hardware requirements\\n\\nIn some cases, you might also want to update your In...\"],[\"```\\n\\n### Delete the endpoint\\n\\nFinally if you won't use the Inference Endpoint anymore, you can simpl...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nWhile filtering, you can also sort the models and take only the top results. For example,\\nthe f...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## HfApi: a flexible and convenient HTTP client\\n\\nThe [`HfApi`] class was developed to provid...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nTo download a specific version of the file, use the `revision` parameter to specify the\\nbranch ...\"],[\"```\\n\\nThe command will tell you if you are already logged in and prompt you for your token. The token...\"],[\"```\\nfrom transformers import whoami\\n\\nuser = whoami(token=...)\\n```\\n\\nThis is usually discouraged excep...\"],[\"```\\n\\nPrivate repositories will not be visible to anyone except yourself.\\n\\n\\u003cTip\\u003e\\n\\nTo create a reposit...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nå¦‚æœä½ æƒ³åœ¨Hubä¸Šåˆ›å»ºå’Œç®¡ç†ä¸€ä¸ªä»“åº“ï¼Œä½ çš„è®¡ç®—æœºå¿…é¡»å¤„äºç™»å½•çŠ¶æ€ã€‚å¦‚æœå°šæœªç™»å½•ï¼Œè¯·å‚è€ƒ[æ­¤éƒ¨åˆ†](..\\u002fquick-start#login)ã€‚åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‡è®¾ä½ çš„è®¡ç®—æœºå·²...\"],[\"```\\n\\né»˜è®¤æƒ…å†µä¸‹ï¼Œ[`create_repo`] ä¼šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»“åº“ã€‚ä½†æ˜¯ä½ å¯ä»¥ä½¿ç”¨ `repo_type`å‚æ•°æ¥æŒ‡å®šå…¶ä»–ä»“åº“ç±»å‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ä»“åº“\\n\\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\\n\\n```p...\"],[\"```\\n\\n### å…‹éš†ä¸€ä¸ªä»“åº“ï¼ˆä»…é€‚ç”¨äº Spacesï¼‰\\n\\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½æƒ³è¦å¤åˆ¶åˆ«äººçš„ä»“åº“å¹¶æ ¹æ®è‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚å¯¹äº Spacesï¼Œä½ å¯ä»¥ä½¿ç”¨ [`duplicate_space`] æ–¹æ³•...\"],[\"```\\n\\n## ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶\\n\\næ—¢ç„¶æ‚¨å·²ç»åˆ›å»ºäº†æ‚¨çš„å­˜å‚¨åº“ï¼Œæ‚¨ç°åœ¨ä¹Ÿå¯ä»¥æ¨é€æ›´æ”¹è‡³å…¶ä¸­å¹¶ä»ä¸­ä¸‹è½½æ–‡ä»¶\\n\\nè¿™ä¸¤ä¸ªä¸»é¢˜æœ‰å®ƒä»¬è‡ªå·±çš„æŒ‡å—ã€‚è¯·[ä¸Šä¼ æŒ‡å—](.\\u002fupload) å’Œ[ä¸‹è½½æŒ‡å—](.\\u002fdownl...\"],[\"```\\n\\nåŒæ—¶,ä½ å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼ä½¿ç”¨ [`delete_branch`] å’Œ [`delete_tag`] å‡½æ•°æ¥åˆ é™¤åˆ†æ”¯æˆ–æ ‡ç­¾\\n\\n### åˆ—å‡ºæ‰€æœ‰çš„åˆ†æ”¯å’Œæ ‡ç­¾\\n\\nä½ è¿˜å¯ä»¥ä½¿ç”¨ [`list_rep...\"],[\"```\\n\\n## ä¿®æ”¹å­˜å‚¨åº“è®¾ç½®\\n\\nå­˜å‚¨åº“å…·æœ‰ä¸€äº›å¯é…ç½®çš„è®¾ç½®ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸ä¼šåœ¨æµè§ˆå™¨ä¸­çš„å­˜å‚¨åº“è®¾ç½®é¡µé¢ä¸Šæ‰‹åŠ¨é…ç½®è¿™äº›è®¾ç½®ã€‚è¦é…ç½®å­˜å‚¨åº“ï¼Œæ‚¨å¿…é¡»å…·æœ‰å¯¹å…¶çš„å†™è®¿é—®æƒé™ï¼ˆæ‹¥æœ‰å®ƒæˆ–å±äºç»„ç»‡ï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­...\"],[\"```\\n\\n## ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\\n\\nä¸Šè¿°æ‰€æœ‰æ“ä½œéƒ½å¯ä»¥é€šè¿‡HTTPè¯·æ±‚å®Œæˆã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨æœ¬åœ°æ‹¥æœ‰å­˜å‚¨åº“çš„å‰¯æœ¬ï¼Œå¹¶ä½¿ç”¨æ‚¨ç†Ÿæ‚‰çš„Gitå‘½ä»¤ä¸ä¹‹äº¤äº’ã€‚\\n\\n[`Repository`]...\"],[\"```\\n\\nä½ å¯ä»¥å°†`clone_from`å‚æ•°ä¸[`create_repo`]ç»“åˆä½¿ç”¨ï¼Œä»¥åˆ›å»ºå¹¶å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“ï¼š\\n\\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\\n\\n```py\\n\\u003e\\u003e\\u003e repo_url = create_repo...\"],[\"```\\n\\n### æ‹‰å–\\n\\n[`~Repository.git_pull`] å…è®¸ä½ ä½¿ç”¨è¿œç¨‹å­˜å‚¨åº“çš„æ›´æ”¹æ›´æ–°å½“å‰æœ¬åœ°åˆ†æ”¯ï¼š\\n\\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_h...\"],[\"--\\nlanguage: en\\nlicense: mit\\nlibrary_name: timm\\ntags:\\n- pytorch\\n- image-classification\\ndatasets:\\n- b...\"],[\"--\\nlicense: mit\\nlanguage: eo\\nthumbnail: https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fassets\\u002f01_how-to-train\\u002fEsperBERT...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nAnother way you might want to do this is with f-strings. In the following example, we:\\n\\n- Use [...\"],[\"```\\n---\\nlanguage: en\\nlicense: mit\\nlibrary: timm\\n---\\n\\n# My Model Card\\n\\nThis model created by [@natera...\"],[\"```\\n\\nNow, as you can see, the metadata header has been updated:\\n\\n```\\n---\\nlanguage: fr\\nlicense: apach...\"],[\"```\\n\\n## Share Model Cards\\n\\nIf you're authenticated with the Hugging Face Hub (either by using `huggi...\"],[\"```\\n\\nA resulting PR created from this command can be seen [here](https:\\u002f\\u002fhuggingface.co\\u002fnateraw\\u002fhf-h...\"],[\"```\\n\\n## Include Evaluation Results\\n\\nTo include evaluation results in the metadata `model-index`, you...\"],[\"```\\n\\nIf you have more than one evaluation result you'd like to share, just pass a list of `EvalResul...\"],[\"--\\nlanguage:\\n- en\\nlicense:\\n- bsd-3-clause\\nannotations_creators:\\n- crowdsourced\\n- expert-generated\\nla...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Defaults to `\\\"$HF_HOME\\u002fassets\\\"` (e.g. `\\\"~\\u002f.cache\\u002fhuggingface\\u002fassets\\\"` by default).\\n\\n### HF_TOKEN\\n\\nTo...\"],[\"## Boolean values\\n\\nThe following environment variables expect a boolean value. The variable will be ...\"],[\"### HF_HUB_DISABLE_PROGRESS_BARS\\n\\nFor time consuming tasks, `huggingface_hub` displays a progress ba...\"],[\"You can set `HF_HUB_DISABLE_TELEMETRY=1` as environment variable to globally disable telemetry.\\n\\n###...\"],[\"## From external tools\\n\\nSome environment variables are not specific to `huggingface_hub` but are sti...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"ä½¿ç”¨ [`Repository`] çš„æœ€å¤§ä¼˜ç‚¹æ˜¯å®ƒå…è®¸ä½ åœ¨æœ¬åœ°æœºå™¨ä¸Šç»´æŠ¤æ•´ä¸ªå­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬ã€‚è¿™ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªç¼ºç‚¹ï¼Œå› ä¸ºå®ƒéœ€è¦ä½ ä¸æ–­æ›´æ–°å’Œç»´æŠ¤è¿™ä¸ªæœ¬åœ°å‰¯æœ¬ã€‚è¿™ç±»ä¼¼äºä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­ï¼Œæ¯ä¸ªå¼€å‘äººå‘˜éƒ½ç»´æŠ¤è‡ªå·±...\"],[\"å¦‚æœæ‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„ git å·¥ä½œæµç¨‹å¹¶å®šæœŸæ¨é€æ›´æ–°å¯èƒ½æ›´æœ‰æ•ˆã€‚`Repository` è¢«ä¼˜åŒ–ä¸ºæ­¤ç±»æƒ…å†µï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨åå°è¿è¡Œã€‚\\nå¦‚æœæ‚¨éœ€è¦æ‰‹åŠ¨ç¼–è¾‘å¤§å‹æ–‡ä»¶ï¼Œ`git `æ˜¯æœ€ä½³é€‰æ‹©...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\u003cCACHE_DIR\\u003e\\nâ”œâ”€ datasets--glue\\nâ”‚  â”œâ”€ refs\\nâ”‚  â”œâ”€ blobs\\nâ”‚  â”œâ”€ snapshots\\n...\\n```\\n\\nEach folder is des...\"],[\"```\\n\\u003cCACHE_DIR\\u003e\\u002f\\u003cREPO_NAME\\u003e\\u002fsnapshots\\u002faaaaaa\\u002fREADME.md\\n```\\n\\nThat `README.md` file is actually a syml...\"],[\"```\\n\\u003cCACHE_DIR\\u003e\\u002f\\u003cREPO_NAME\\u003e\\u002f.no_exist\\u002faaaaaa\\u002fconfig_that_does_not_exist.json\\n```\\n\\nUnlike the `snapsh...\"],[\"```\\n\\n### In practice\\n\\nIn practice, your cache should look like the following tree:\\n\\n```text\\n    [  9...\"],[\"```\\n\\n### Limitations\\n\\nIn order to have an efficient cache-system, `huggingface-hub` uses symlinks. H...\"],[\"```py\\nfrom huggingface_hub import cached_assets_path\\n\\nassets_path = cached_assets_path(library_name=...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n[`cached_assets_path`] is the recommended way to store assets but is not mandatory. If\\ny...\"],[\"```\\n\\n## Scan your cache\\n\\nAt the moment, cached files are never deleted from your local directory: wh...\"],[\"The snippet below shows a scan report in a folder in which 4 models and 2 datasets are\\ncached.\\n\\n```t...\"],[\"```\\n\\nTo get a more detailed report, use the `--verbose` option. For each repo, you get a\\nlist of all...\"],[\"```text\\nâœ huggingface-cli scan-cache -v\\nREPO ID                     REPO TYPE REVISION              ...\"],[\"google\\u002ffleurs               dataset   129b6e96cf1967cd5d2b9b6aec75ce6cce7c89e8        25.4K        3...\"],[\"t5-base                     model     23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9        10.1K        3...\"],[\"```\\n\\n#### Grep example\\n\\nSince the output is in tabular format, you can combine it with any `grep`-li...\"],[\"```\\n\\n### Scan cache from Python\\n\\nFor a more advanced usage, use [`scan_cache_dir`] which is the pyth...\"],[\"Here is a simple usage example. See reference for details.\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import sc...\"],[\"```\\n\\n## Clean your cache\\n\\nScanning your cache is interesting but what you really want to do next is ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Clean cache from the terminal\\n\\nThe easiest way to delete some revisions from your HF cac...\"],[\"```\\npip install huggingface_hub[\\\"cli\\\"]\\n```\\n\\nThen run the command:\\n\\n```\\nhuggingface-cli delete-cache\\n...\"],[\"```\\n\\n#### Without TUI\\n\\nAs mentioned above, the TUI mode is currently in beta and is optional. It may...\"],[\"```\\n\\nExample of command file:\\n\\n```txt\\n# INSTRUCTIONS\\n# ------------\\n# This is a temporary file creat...\"],[\"# Dataset z-uo\\u002fmale-LJSpeech-italian (5.5G, used 5 days ago)\\n#    9cfa5647b32c0a30d0adfca06bf198d821...\"],[\"```\\n\\n### Clean cache from Python\\n\\nFor more flexibility, you can also use the [`~HFCacheInfo.delete_r...\"],[\"--\\n{{card_data}}\\n---\\n\\n# {{ model_name | default(\\\"MyModelName\\\", true)}}\\n\\n{{ some_data }}...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n[[autodoc]] AsyncInferenceClient\\n\\n## InferenceTimeoutError\\n\\n[[autodoc]] InferenceTimeoutError\\n\\n...\"],[\"[[autodoc]] huggingface_hub.inference._text_generation.TextGenerationParameters\\n\\n[[autodoc]] hugging...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"For web development, a [JS client](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface.js\\u002finference\\u002fREADME) has ...\"],[\"```\\n\\nWe initialized an [`InferenceClient`] with the default parameters. The only thing you need to k...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThere are more than 200k models on the Hugging Face Hub! Each task in the [`InferenceCli...\"],[\"```\\n\\n### Authentication\\n\\nCalls made with the [`InferenceClient`] can be authenticated using a [User ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nAuthentication is NOT mandatory when using the Inference API. However, authenticated use...\"],[\"| Domain | Task                           | Supported    | Documentation                            ...\"],[\"| | [Text-to-Image](https:\\u002f\\u002fhuggingface.co\\u002ftasks\\u002ftext-to-image)                  | âœ… | [`~InferenceC...\"],[\"| | [Text Classification](https:\\u002f\\u002fhuggingface.co\\u002ftasks\\u002ftext-classification)            | âœ… | [`~Infe...\"],[\"\\u003cTip\\u003e\\n\\nCheck out the [Tasks](https:\\u002f\\u002fhuggingface.co\\u002ftasks) page to learn more about each task, how t...\"],[\"```\\n\\n## Async client\\n\\nAn async version of the client is also provided, based on `asyncio` and `aioht...\"],[\"```\\n\\nFor more information about the `asyncio` module, please refer to the [official documentation](h...\"],[\"```\\n\\n### Binary inputs\\n\\nSome tasks require binary inputs, for example, when dealing with images or a...\"],[\"```\\n\\nto\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface_hub import InferenceClient\\n\\u003e\\u003e\\u003e inference = InferenceClient(m...\"],[\"```\\n\\n### Run with parameters\\n\\nChange from\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface_hub import InferenceApi\\n\\u003e\\u003e...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nå®Œæˆå,[æ£€æŸ¥å®‰è£…](#check-installation)æ˜¯å¦æ­£å¸¸å·¥ä½œ\\n\\n### å®‰è£…å¯é€‰ä¾èµ–é¡¹\\n\\n`huggingface_hub`çš„æŸäº›ä¾èµ–é¡¹æ˜¯ [å¯é€‰](https:\\u002f\\u002fsetup...\"],[\"```\\n\\nè¿™é‡Œåˆ—å‡ºäº† `huggingface_hub` çš„å¯é€‰ä¾èµ–é¡¹ï¼š\\n\\n- `cli`ï¼šä¸º `huggingface_hub` æä¾›æ›´æ–¹ä¾¿çš„å‘½ä»¤è¡Œç•Œé¢\\n\\n- `fastai`,` torch`, ...\"],[\"```\\n\\nå®Œæˆå®‰è£…åï¼Œè¯·[æ£€æŸ¥å®‰è£…](#check-installation)æ˜¯å¦æ­£å¸¸å·¥ä½œ\\n\\n### å¯ç¼–è¾‘å®‰è£…\\n\\nä»æºä»£ç å®‰è£…å…è®¸æ‚¨è®¾ç½®[å¯ç¼–è¾‘å®‰è£…](https:\\u002f\\u002fpip.pypa.io\\u002fen\\u002f...\"],[\"```\\nå®Œæˆå®‰è£…åï¼Œè¯·[æ£€æŸ¥å®‰è£…](#check-installation)æ˜¯å¦æ­£å¸¸å·¥ä½œ\\n\\n## éªŒè¯å®‰è£…\\n\\nå®‰è£…å®Œæˆåï¼Œé€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥`huggingface_hub`æ˜¯å¦æ­£å¸¸å·¥ä½œ:\\n\\n```...\"],[\"```\\n\\n## Windowså±€é™æ€§\\n\\nä¸ºäº†å®ç°è®©æ¯ä¸ªäººéƒ½èƒ½ä½¿ç”¨æœºå™¨å­¦ä¹ çš„ç›®æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº† `huggingface_hub`åº“ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªè·¨å¹³å°çš„åº“ï¼Œå°¤å…¶å¯ä»¥åœ¨ Unix å’Œ Windows ç³»ç»Ÿ...\"],[\"--\\n{card_data}\\n---\\n\\n# {{ pretty_name | default(\\\"Dataset Name\\\", true)}}\\n\\n{{ some_data }}...\"],[\"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"`\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002f...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\né€šè¿‡ `huggingface_hub`åº“ï¼Œæ‚¨å¯ä»¥ä¸é¢å‘æœºå™¨å­¦ä¹ å¼€å‘è€…å’Œåä½œè€…çš„å¹³å° [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"å½“ç„¶ï¼Œè´¡çŒ®è€…ä¹Ÿåº”è¯¥å°Šé‡æˆ‘ä»¬çš„[è¡Œä¸ºå‡†åˆ™](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmain\\u002fCODE_OF_CONDUCT.md)ï¼Œä»¥ä¾¿...\"]],\"hovertemplate\":\"source=huggingface_hub\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"huggingface_hub, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"huggingface_hub, circle\",\"showlegend\":true,\"x\":[6.903564,6.748269,7.039434,7.4735494,6.992873,6.6676126,5.8811007,7.3247786,7.6224647,3.9484963,3.3604028,3.0510392,3.436833,3.1709423,3.659155,3.394912,3.5053706,3.7188845,3.4204237,3.5823143,3.0401049,3.0131528,4.905821,3.9344585,4.128484,3.4862158,3.2866452,3.50896,3.1269023,3.3920083,3.3647265,3.0997431,3.5455363,3.5192735,3.2820148,3.182204,3.3786588,2.5936637,3.1309333,2.838054,3.355791,3.665027,3.604865,3.5058558,3.1454842,3.2904654,5.3961864,5.3043346,4.105463,3.615609,3.250772,4.4246664,2.018704,1.2680442,6.6811,6.845147,6.715802,4.4534364,4.010199,4.303513,4.054264,3.4539566,3.499335,3.339191,3.6193624,3.6358864,3.4365718,3.7700145,3.103185,4.125651,3.8566062,7.7174397,4.2284408,4.0579267,1.7579957,3.888672,2.652843,2.690759,3.1920025,3.0036817,4.3829145,4.486185,4.3271422,4.1102886,1.8970351,4.2842307,4.389607,4.3601213,4.5031843,-11.317451,4.2244473,3.6888778,3.2605705,5.081956,5.491865,5.414638,5.4177785,5.936899,5.9932065,5.788025,5.492172,5.9336743,6.0071554,2.2518659,2.8234847,5.4353824,4.0280447,8.31329,7.6514955,6.059456,3.6004765,3.1365418,4.3833013,3.6710715,3.4589462,4.325448,4.087855,4.2703576,6.088115,5.8631096,4.0656815,3.9616268,4.216275,4.1790996,3.6045732,5.322664,4.9100637,4.3791103,6.0277324,5.5425534,4.1873093,2.436564,-3.0424247,3.5237975,6.373778,4.146881,5.131825,4.1867805,4.9598055,5.305193,5.386118,5.210431,5.307746,5.5397363,5.3531756,5.265846,4.942694,0.1187746,-0.8198946,-1.9030771,-6.274173,4.0972238,3.8790786,4.165913,4.0015426,4.4447355,5.3335943,4.2515616,4.021554,4.120305,6.3412104,6.281411,6.684822,6.210033,5.516975,5.2651925,5.7493997,6.6443505,6.3090754,6.1682796,6.9021435,4.3108087,5.2749386,5.333249,4.8096657,4.588834,7.1654916,4.7782845,5.769785,2.4028382,1.2996032,4.718455,1.3478421,6.8133817,6.640918,7.099351,6.555202,6.1540155,5.887675,5.2022023,4.4454265,6.7650456,8.810323,6.6322093,4.602575,3.9024973,5.6500735,4.4304776,1.7406034,3.5794926,3.9359803,3.7681067,3.9768918,3.9682007,3.604839,3.4216757,5.6024723,5.1857057,5.3632307,4.9812183,5.6722865,3.9276342,3.5638092,3.989964,3.811487,4.1287436,3.5614505,2.5345595,3.072468,3.1256497,-1.7772192,4.965846,3.860592,4.1457944,-0.92660797,4.5503135,4.662952,4.445105,4.6684914,4.9971466,4.8288355,3.5806909,2.9603221,2.0862763,3.9467235,1.4709526,0.39298165,0.6061459,-0.5564809,3.9168165,3.5268753,3.7987347,3.7357385,3.5149617,3.6576545,3.7527785,3.527243,3.5559156,3.2255018,3.3789012,2.926032,3.3514721,4.2360573,3.4670057,2.8530538,1.8598542,5.4348536,4.379569,4.513764,3.712891,3.8589916,3.8732347,2.8259552,3.7212656,5.574577,5.4953895,5.066902,6.3918567,5.334403,5.5077667,5.5055747,5.284715,5.315404,5.110202,1.598278,7.550053,3.8454342,0.52196276,2.8875787,3.8314207,2.7016506,3.635413,1.2729021,1.0835418,-9.917122,1.2137011,4.339556,4.060559,4.036088,4.0296025,4.057699,4.116314,5.494202,5.5191135,5.5247536,4.507989,3.6884549,3.5484014,3.5999928,1.5025377,4.1102605,3.7301977,3.3407054,4.087876,3.850273,3.8389425,3.973341,7.782894,7.618089,0.32101718,3.8318152,3.987676,4.0597234,4.1893625,4.2239676,4.235448,3.8574076,8.202888,4.029962,4.884099,3.649552,3.4210427,3.628226,3.613511,4.1975145,3.3638306,3.4255974,3.8749273,4.0186443,-7.120502,-9.220259,-6.1128745,3.638841,3.4674494,3.311517,3.6023536,3.3251822,3.144642,5.4502935,5.44724,5.455574,5.494642,6.2786784,5.780163,4.9572287,5.8812428,6.1951175,6.2381835,10.819082],\"xaxis\":\"x\",\"y\":[0.25913817,0.23382062,0.23722966,0.68588245,0.26749122,0.1925492,0.33227035,0.4294735,0.61228186,1.0776186,2.4023397,1.5350977,1.7196226,1.5288044,1.2953684,1.8384684,2.115619,1.2406807,1.6738077,4.0854692,5.515653,5.5251207,3.5066118,1.4072301,1.1473439,1.4584389,1.400256,1.3288418,1.531152,1.6103414,1.4964064,1.6374238,1.6880488,1.6539755,2.6477532,1.5233595,1.6629666,1.3851765,1.551635,1.7747458,1.5927246,1.3998824,1.5264819,1.2531722,1.3220922,1.1915251,0.35246092,1.0917207,0.76219594,0.9976857,1.1729983,0.882864,0.58732814,1.4078696,1.4318755,1.544665,1.7046565,0.6803662,0.40125847,0.28946823,0.41460714,1.4816724,1.6490304,1.7548099,1.2771559,1.3150725,1.305657,1.5937597,1.3555636,1.5652405,1.6311586,2.58425,1.6409421,0.41635433,1.0740871,2.1795897,3.236486,2.054189,3.3048618,0.38667068,1.4556243,2.135427,2.5456493,2.5580926,3.633172,2.56589,2.33102,2.535207,2.0408595,-1.1220155,3.5125554,0.8455154,1.09559,1.7149503,1.4530313,1.4586385,1.3611938,1.5165671,1.4536844,1.6285557,1.5516455,1.5869066,1.6216872,2.8642063,1.9511398,0.32203165,1.5514383,2.729882,2.310306,1.732531,2.4909992,3.4784832,0.95259595,-1.7161937,-0.5826673,0.45604765,1.451615,1.8023679,0.14501338,0.19489749,1.2482308,1.061294,0.27506226,0.7270477,0.74251777,1.7566742,1.8080522,0.7640958,0.19701098,1.106784,2.85813,2.9433815,-1.002785,3.1030412,3.0925648,3.1042926,1.7344462,1.6136944,1.6355162,1.5199454,1.6126181,1.771514,1.5546106,1.6203455,0.17070928,2.2948997,1.8226826,2.48078,3.4928856,3.9483871,7.178345,0.8991268,0.7772078,1.1898171,1.0153236,1.1074557,1.4389094,1.1234344,0.85739636,0.9596173,0.18004684,0.17445883,0.22801465,0.24433137,0.22539249,0.4423462,0.4442722,0.43607837,0.17331107,0.20508946,0.20786703,0.8316886,0.13129339,0.2712662,0.5650489,0.8024158,0.6258518,0.5043249,1.7618437,1.0101181,1.2492142,0.6424629,3.2926428,0.17477125,0.19730918,0.23934954,0.1708024,0.27972186,0.13322565,0.44695655,1.0162268,0.29761368,2.231186,0.39643836,2.0893164,5.5643687,3.0023172,0.6863782,2.195729,1.1250046,1.1513131,1.151115,1.2499564,0.86572826,1.8426706,1.2129611,0.2078931,0.33844528,0.33608812,0.40421507,0.37623522,1.0769502,1.6325569,0.80685294,1.6904116,0.9901557,1.3542519,1.8489634,1.4352651,-0.6740444,4.4643703,0.3303068,0.5071988,0.5465392,3.3267477,0.85552204,1.5666499,1.9790777,1.7861135,1.555555,1.4920151,0.8686928,0.4365717,0.17328739,0.9279108,0.7077138,1.0831759,0.7936447,0.9858467,0.85338455,-1.5407658,-0.8850261,-0.84017336,-0.98331803,-0.858024,-0.79755443,-0.88335735,-0.9337559,-0.8682695,-0.8283267,-0.68082345,-0.7967241,1.0139029,2.3206472,3.3100188,0.55335605,0.2752122,0.83836293,0.8858399,0.89870846,0.88187355,0.34717533,0.6434122,0.90850586,0.4516378,0.371062,0.516026,1.5193732,0.4931716,0.43638098,0.41750908,0.41563353,0.4490865,0.5530867,3.1280468,1.8994561,2.557721,2.362816,2.546457,2.6458557,0.06317336,2.1453538,3.2039852,3.2091193,-0.49859548,0.7781552,0.87512994,1.2860001,0.9823226,1.2121868,1.1761341,1.0418687,0.33101833,0.42699996,0.42489627,0.96018094,1.7606622,1.732724,1.6610974,2.3759327,1.09584,1.2401124,2.0539927,1.7331302,1.7093388,1.9138165,1.7830716,2.5274537,2.475922,3.713611,1.9884897,1.80217,1.8263597,1.7628381,1.8485299,1.8174886,1.6678593,3.0991611,1.8883625,3.4593604,-1.5991114,-0.81821394,-1.2820349,-1.3317401,-0.43526006,-0.73320794,-1.3046198,-0.06844655,-1.064465,5.8894777,0.43728793,2.9578862,-0.8808749,-0.66032165,-0.9107003,-0.89264554,-0.49190858,0.00732493,0.23213667,0.2849127,0.28733078,0.1627598,0.14893481,0.41129848,3.6357093,0.2324142,0.25803515,0.12845081,2.075184],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"The tokenization pipeline\\n\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode_batch`, the input\\nte...\"],[\"## Normalization\\n\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to ...\"],[\"You can manually test that normalizer by applying it to any string:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython...\"],[\"When building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding att...\"],[\"An easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre...\"],[\"You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that wil...\"],[\"As we saw in the `quicktour`, you can\\ncustomize the pre-tokenizer of a `Tokenizer` by just changing ...\"],[\"This model is passed along when intializing the\\n`Tokenizer` so you already know how to\\ncustomize thi...\"],[\"Note that contrarily to the pre-tokenizer or the normalizer, you don't\\nneed to retrain a tokenizer a...\"],[\"Then we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode...\"],[\"The pre-tokenizer is just splitting on whitespace and punctuation:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e...\"],[\"And the post-processing uses the template we saw in the previous\\nsection:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003c...\"],[\"We can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:\\n\\n\\u003ctokenizerslangconte...\"],[\"The `decoder` will first convert the IDs back to tokens\\n(using the tokenizer's vocabulary) and remov...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"But by changing it to a proper decoder, we get:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{...\"],[\"Quicktour\\n\\nLet's have a quick look at the ğŸ¤— Tokenizers library features. The\\nlibrary provides an imp...\"],[\"```\\n\\n### Training the tokenizer\\n\\nIn this tour, we will build and train a Byte-Pair Encoding (BPE)\\nto...\"],[\"To train our tokenizer on the wikitext files, we will need to\\ninstantiate a [trainer]{.title-ref}, i...\"],[\"\\u003cTip\\u003e\\n\\nThe order in which you write the special tokens list matters: here `\\\"[UNK]\\\"` will get the ID ...\"],[\"Now, we can just call the `Tokenizer.train` method with any list of files we want to use:\\n\\n\\u003ctokenize...\"],[\"This should only take a few seconds to train our tokenizer on the full\\nwikitext dataset! To save the...\"],[\"and you can reload your tokenizer from that file with the\\n`Tokenizer.from_file`\\n`classmethod`:\\n\\n\\u003ctok...\"],[\"### Using the tokenizer\\n\\nNow that we have trained a tokenizer, we can use it on any text we want\\nwit...\"],[\"This `Encoding` object then has all the\\nattributes you need for your deep learning model (or other)....\"],[\"Similarly, the `ids` attribute will\\ncontain the index of each of those tokens in the tokenizer's\\nvoc...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"and those are the indices that correspond to the emoji in the original\\nsentence:\\n\\n\\u003ctokenizerslangcon...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"Here is how we can set the post-processing to give us the traditional\\nBERT inputs:\\n\\n\\u003ctokenizerslangc...\"],[\"Lastly, we specify the special tokens we used and their IDs in our\\ntokenizer's vocabulary.\\n\\nTo check...\"],[\"To check the results on a pair of sentences, we just pass the two\\nsentences to `Tokenizer.encode`:\\n\\n...\"],[\"You can then check the type IDs attributed to each token is correct with\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cp...\"],[\"### Encoding multiple sentences in a batch\\n\\nTo get the full speed of the ğŸ¤— Tokenizers library, it's ...\"],[\"To process a batch of sentences pairs, pass two lists to the\\n`Tokenizer.encode_batch` method: the\\nli...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"In this case, the `attention mask` generated by the\\ntokenizer takes the padding into account:\\n\\n\\u003ctoke...\"],[\"```\\n\\n### Importing a pretrained tokenizer from legacy vocabulary files\\n\\nYou can also import a pretra...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"## Bindings\\n\\nWe provide bindings to the following languages (more to come!):\\n  - [Rust](https:\\u002f\\u002fgith...\"],[\"```\\n\\nYou can customize how pre-tokenization (e.g., splitting into words) is done:\\n\\n```python\\nfrom to...\"],[\"`tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n  \\u003ch1\\u003e\\u003ccode\\u003ewasm-pack-template\\u003c\\u002fcode\\u003e\\u003c\\u002fh1\\u003e\\n\\n  \\u003cstrong\\u003eA template for kick start...\"],[\"Be sure to check out [other `wasm-pack` tutorials online][tutorials] for other\\ntemplates and usages ...\"],[\"```\\ncargo generate --git https:\\u002f\\u002fgithub.com\\u002frustwasm\\u002fwasm-pack-template.git --name my-project\\ncd my-...\"],[\"Post-processors\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertProcessing\\n\\n[[autodoc]] tokenizers.processo...\"],[\"Training from memory\\n\\nIn the [Quicktour](quicktour), we saw how to build and train a\\ntokenizer using...\"],[\"## Using the ğŸ¤— Datasets library\\n\\nAn awesome way to access one of the many datasets that exist out th...\"],[\"And that's it!\\n\\n## Using gzip files\\n\\nSince gzip files in Python can be used as iterators, it is extr...\"],[\"Models\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BPE\\n\\n[[autodoc]] tokenizers.models.BPE\\n\\n## Model\\n\\n[[auto...\"],[\"Added Tokens\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## AddedToken\\n\\n[[autodoc]] tokenizers.AddedToken\\n    ...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"```\\n\\n#### From sources:\\n\\nTo use this method, you need to have the Rust installed:\\n\\n```bash\\n# Install...\"],[\"```\\n\\nAnd you can train them just as simply:\\n\\n```python\\nfrom tokenizers import CharBPETokenizer\\n\\n# In...\"],[\"```\\n\\n#### Provided Tokenizers\\n\\n - `CharBPETokenizer`: The original BPE\\n - `ByteLevelBPETokenizer`: T...\"],[\"```\\n\\nNow, when you want to use this tokenizer, this is as simple as:\\n\\n```python\\nfrom tokenizers impo...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"## What is a Tokenizer\\n\\nA Tokenizer works as a pipeline, it processes some raw text as input and out...\"],[\"```\\n\\n### Deserialization and tokenization example\\n\\n```rust\\nuse tokenizers::tokenizer::{Result, Token...\"],[\"```\\n\\n### Training and serialization example\\n\\n```rust\\nuse tokenizers::decoders::DecoderWrapper;\\nuse t...\"],[\"let mut tokenizer = TokenizerBuilder::new()\\n        .with_model(BPE::default())\\n        .with_normal...\"],[\"```\\n\\n## Additional information\\n\\n- tokenizers is designed to leverage CPU parallelism when possible. ...\"],[\"`tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`...\"],[\"`tokenizers-freebsd-x64`\\n\\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`...\"],[\"`tokenizers-win32-ia32-msvc`\\n\\nThis is the **i686-pc-windows-msvc** binary for `tokenizers`...\"],[\"Visualizer\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Annotation\\n\\n[[autodoc]] tokenizers.tools.Annotation\\n...\"],[\"Components\\n\\nWhen building a Tokenizer, you can attach various types of components to\\nthis Tokenizer ...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| NFD | NFD...\"],[\"| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO á½ˆÎ”Î¥Î£Î£Î•ÎÎ£` \\u003cbr\\u003e Output: `hello`á½€Î´Ï…Ïƒ...\"],[\"| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input...\"],[\"## Pre-tokenizers\\n\\nThe `PreTokenizer` takes care of splitting the input according to a set\\nof rules....\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel...\"],[\"| CharDelimiterSplit | Splits on a given character | Example with `x`: \\u003cbr\\u003e Input: `\\\"Helloxthere\\\"` \\u003c...\"],[\"\\u003c\\u002fpython\\u003e\\n\\u003crust\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on whi...\"],[\"| CharDelimiterSplit | Splits on a given character | Example with `x`: \\u003cbr\\u003e Input: `\\\"Helloxthere\\\"` \\u003c...\"],[\"\\u003c\\u002frust\\u003e\\n\\u003cnode\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on white...\"],[\"| CharDelimiterSplit | Splits on a given character | Example with `x`: \\u003cbr\\u003e Input: `\\\"Helloxthere\\\"` \\u003c...\"],[\"## Models\\n\\nModels are the core algorithms used to actually tokenize, and therefore,\\nthey are the onl...\"],[\"After the whole pipeline, we sometimes want to insert some special\\ntokens before feed a tokenized st...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# Tokenizers\\n\\nFast State-of-the-art tokenizers, optimized for ...\"],[\"Decoders\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BPEDecoder\\n\\n[[autodoc]] tokenizers.decoders.BPEDecoder...\"],[\"`tokenizers-darwin-arm64`\\n\\nThis is the **aarch64-apple-darwin** binary for `tokenizers`...\"],[\"Input Sequences\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\nThese types represent all the different kinds of s...\"],[\"Encoding\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Encoding\\n\\n[[autodoc]] tokenizers.Encoding\\n    - all\\n  ...\"],[\"Pre-tokenizers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertPreTokenizer\\n\\n[[autodoc]] tokenizers.pre_tok...\"],[\"Normalizers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertNormalizer\\n\\n[[autodoc]] tokenizers.normalizers....\"],[\"Trainers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BpeTrainer\\n\\n[[autodoc]] tokenizers.trainers.BpeTrainer...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers-log...\"],[\"```\\n\\n## Basic example\\n\\n```ts\\nimport { Tokenizer } from \\\"tokenizers\\\";\\n\\nconst tokenizer = await Tokeni...\"],[\"# Requirements\\n\\nIn order to generate the documentation, it is necessary to have a Python environment...\"],[\"`tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`...\"],[\"Changelog\\nAll notable changes to this project will be documented in this file.\\n\\nThe format is based ...\"],[\"## [0.11.3]\\n\\n- [#919] Fixing single_word AddedToken. (regression from 0.11.2)\\n- [#916] Deserializing...\"],[\"### Changed\\n- [#234]: Completely changed the alignement mappings available on `Encoding`. Previous m...\"],[\"### Added\\n- [#236]: RobertaProcessing is now also taking care of trimming offsets, and works just as...\"],[\"### Fixed\\n- [#205]: Trim the decoded string in `BPEDecoder`\\n- [b770f36]: Fix a bug with added tokens...\"],[\"### Fixed\\n- [#193]: Fix some issues with the offsets being wrong with the `ByteLevel` BPE:\\n\\t- when `...\"],[\"[#1072]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f1072\\n[#956]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"[#868]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f868\\n[#860]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#236]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f236\\n[#234]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"`tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`...\"],[\"`tokenizers-android-arm-eabi`\\n\\nThis is the **armv7-linux-androideabi** binary for `tokenizers`...\"],[\"`tokenizers-linux-arm-gnueabihf`\\n\\nThis is the **armv7-unknown-linux-gnueabihf** binary for `tokenize...\"],[\"`tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`...\"],[\"`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`...\"],[\"`tokenizers-android-arm64`\\n\\nThis is the **aarch64-linux-android** binary for `tokenizers`...\"],[\"Changelog\\nAll notable changes to this project will be documented in this file.\\n\\nThe format is based ...\"],[\"## [0.11.6]\\n\\n- [#919] Fixing single_word AddedToken. (regression from 0.11.2)\\n- [#916] Deserializing...\"],[\"### Added\\n- [#693]: Add a CTC Decoder for Wave2Vec models\\n\\n### Removed\\n- [#714]: Removed support for...\"],[\"### Fixed\\n- [#519]: During training, the `Model` is now trained in-place. This fixes several bugs th...\"],[\"### Added\\n- [#379]: Add the ability to call `encode`\\u002f`encode_batch` with numpy arrays\\n- [#292]: Supp...\"],[\"### Fixed\\n- [#286]: Fix various crash when training a BPE model\\n- [#309]: Fixed a few bugs related t...\"],[\"## [0.7.0]\\n\\n### Changed\\n- Only one progress bar while reading files during training. This is better ...\"],[\"### Added\\n- [#188]: `ByteLevel` is also a `PostProcessor` now and handles trimming the offsets if ac...\"],[\"### How to migrate\\n- Add the `ByteLevel` `PostProcessor` to your byte-level BPE tokenizers if releva...\"],[\"## [0.5.2]\\n- [#163]: Do not open all files directly while training\\n\\n### Fixed\\n- We introduced a bug ...\"],[\"## [0.4.0]\\n\\n### Changed\\n- [#131]: Replaced all .new() class methods by a proper __new__ implementati...\"],[\"```\\noutput = tokenizer.encode(...)\\nprint(output.original_str.offsets(output.offsets[3]))...\"],[\"```\\n- [#99]: Exposed the vocabulary size on all tokenizers\\n\\n### Added\\n- Added `CharDelimiterSplit`: ...\"],[\"[#1096]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f1096\\n[#1072]: https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"[#895]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f895\\n[#884]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#686]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f686\\n[#674]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#519]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f519\\n[#509]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#394]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f394\\n[#389]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#273]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f273\\n[#272]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#156]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f156\\n[#152]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n  \\u003ch1\\u003e\\u003ccode\\u003ecreate-wasm-app\\u003c\\u002fcode\\u003e\\u003c\\u002fh1\\u003e\\n\\n  \\u003cstrong\\u003eAn \\u003ccode\\u003enpm init\\u003c\\u002fcode\\u003e tem...\"],[\"```\\nnpm init wasm-app\\n```\\n\\n## ğŸ”‹ Batteries Included\\n\\n- `.gitignore`: ignores `node_modules`\\n- `LICENS...\"],[\"Encode Inputs\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\nThese types represent all the different kinds of inp...\"],[\"alias of `Union[List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[st...\"],[\"Tokenizer\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Tokenizer\\n\\n[[autodoc]] tokenizers.Tokenizer\\n    - all...\"],[\"Installation\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\nğŸ¤— Tokenizers is tested on Python 3.5+.\\n\\nYou should in...\"],[\"```\\n\\u003c\\u002frust\\u003e\\n\\u003cnode\\u003e\\n## Installation with npm\\n\\nYou can simply install ğŸ¤— Tokenizers with npm using:\\n\\n``...\"],[\"`tokenizers-darwin-x64`\\n\\nThis is the **x86_64-apple-darwin** binary for `tokenizers`...\"],[\"# How to release\\n\\n# Before the release\\n\\nSimple checklist on how to make releases for `tokenizers`.\\n\\n...\"],[\"# Rust\\n\\n- `tokenizers` (rust, python & node) versions don't have to be in sync but it's\\n  very commo...\"],[\"# Python\\n\\n- Edit `bindings\\u002fpython\\u002fsetup.py` to reflect new version.\\n- Edit `bindings\\u002fpython\\u002fpy_src\\u002ft...\"],[\"# Node\\n\\n- Edit `bindings\\u002fnode\\u002fpackage.json` to reflect new version.\\n- Edit `CHANGELOG.md`:\\n    - Add...\"]],\"hovertemplate\":\"source=tokenizers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"tokenizers, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"tokenizers, circle\",\"showlegend\":true,\"x\":[-3.5373576,-3.613042,-3.4096828,-3.5409865,-3.538323,-3.493201,-3.4658237,-3.616436,-3.689471,-3.7149298,-3.570106,-3.5416656,-3.5125265,-3.3953624,-3.5191116,-3.4711323,-3.3242579,-3.5177073,-3.4827669,-3.4226174,-3.1746662,-3.2707415,-3.3527696,-3.4072158,-3.3874362,-3.3894453,-3.3623962,-3.4703066,-3.5059333,-3.434136,-3.4737635,-3.2801793,-3.4119732,-2.8473737,-2.8242338,-3.479763,-3.5118573,-3.4230819,-3.405751,-3.5681715,-3.5462694,-3.3977013,11.592749,5.1799197,4.512978,4.3637066,-3.699713,-3.3311045,1.248651,1.4705924,-3.6755638,-3.7491913,-3.6047206,-2.9263735,-3.1712186,-3.59322,-3.1117656,-3.7164822,-3.636256,-3.606294,-3.729037,-3.485156,-3.9417074,11.593741,11.592579,11.5917225,-3.7270236,-3.619728,-3.598118,-3.841873,-3.7707179,-3.5742571,-3.543426,-3.6190028,-3.5381331,-3.5380158,-3.5551054,-3.5298483,-3.5368476,-3.4730194,-3.49813,-3.6645975,11.591376,-3.5325165,-3.6011767,-3.679996,-3.7098434,-3.6981232,-3.629576,9.338279,4.4399877,11.592558,-5.407907,-5.411005,-5.411952,-5.409999,-5.411176,-5.4118967,5.9633355,5.984912,6.0942454,11.592438,11.592831,11.59255,11.592889,11.593854,11.592201,-5.404029,-5.4110346,-5.3999677,-3.8726478,-5.409415,-3.873614,-5.411732,-5.410724,-5.408195,-5.408137,-5.4105783,-2.8564036,-5.3988457,5.994795,5.990226,5.986941,5.9863,6.02981,5.9889565,5.9960585,4.7219915,4.8459806,-3.3163855,-3.6698596,-3.713732,2.896211,3.8910956,11.591605,2.5684597,3.323183,2.789236,3.5948653],\"xaxis\":\"x\",\"y\":[6.216038,6.4309726,6.616741,6.503825,6.4621387,6.486943,6.3747287,6.084619,6.223128,6.344671,6.43478,6.3795877,6.154688,6.2639728,6.433711,6.4279895,5.986013,6.0098906,6.2874,6.357342,6.036938,5.837959,6.310509,6.2209597,6.039252,6.2491155,6.317045,6.3363485,6.4001746,6.386618,6.4192615,6.315645,6.334099,5.6945505,5.1363387,6.27599,6.2124057,6.3440614,5.0436287,6.1911535,6.1290812,6.1965694,17.958338,2.2175379,1.7185522,1.6162173,6.7227015,6.0492105,3.716115,2.8313262,6.6665998,6.806519,6.1417804,5.1120887,5.8582516,5.607083,5.7324286,6.3057966,6.2408257,6.4928784,6.2420993,5.7594047,-2.7883186,17.95634,17.95883,17.958555,6.7721763,6.5053725,6.461458,6.3492,6.0907083,6.386874,6.338105,6.5202017,6.482832,6.461358,6.5161605,6.4109416,6.324139,6.236218,6.0370584,6.6971197,17.957127,6.5549836,6.5614233,6.7244387,6.8297195,6.6959987,6.149978,3.2801397,1.7882915,17.957949,13.71,13.724994,13.727331,13.722105,13.725695,13.728589,-1.0390254,-1.0918165,-1.0823857,17.95577,17.957445,17.958366,17.957363,17.9563,17.957443,13.694734,13.719712,13.652249,1.3507626,13.714589,0.2934466,13.714877,13.721044,13.715658,13.705074,13.726818,5.3776507,13.6822,-1.0704505,-1.0618687,-1.1262833,-1.0645849,-1.0760632,-1.0750561,-1.1090003,1.8577158,1.9965454,6.11671,6.649321,6.7192993,1.7231387,1.571039,17.958437,1.1896876,1.3343834,1.1738038,1.2446841],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, ...\"],[\"How to contribute to simulate?\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20C...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n\\t```bash\\n\\tgit checkout -b a-descripti...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Simulate with Godot\\n\\n### Install in Godot 4\\nThis integration has been developed for Godot 4.x. You...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\nfrom simulate import Scene\\n\\nscene = Scene.create_from('tests\\u002ftest_assets\\u002ffixtures\\u002fBox.gltf')  # ...\"],[\"```\\n\\nAn object (as well as the Scene) is just a node in a tree provided with optional mesh (as `pyvi...\"],[\"```\\n# Add two copy of the sphere to the scene as children of the root node (using list will add all ...\"],[\"```\\nscene.show()\\n```\\n\\nYou can find bridges to other rendering\\u002fsimulation engines in the `integration...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nor, they can be treated as individual assets, and added to the scene:\\n```\\nscene += sm.Asset.cre...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eMore discussion and explanation of the underlying concepts and ideas behind...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"docs\\u002fsource\\u002fassets\\u002fsimulate_library.png\\\" width=\\\"400\\\"\\u002f\\u003e\\n    ...\"],[\"```\\npip install --upgrade simulate\\n```\\nBefore you merge a PR, fix the style (we use `isort` + `black...\"],[\"```\\nfrom simulate import Scene\\n\\nscene = Scene.create_from('tests\\u002ftest_assets\\u002ffixtures\\u002fBox.gltf')  # ...\"],[\"```\\n\\nAn object (as well as the Scene) is just a node in a tree provided with optional mesh (under th...\"],[\"```\\npython examples\\u002fbasic\\u002fobjects.py\\n```\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fuser-imag...\"],[\"```\\n# Add two copy of the sphere to the scene as children of the root node (using list will add all ...\"],[\"```\\n\\nEditing objects:\\n- mesh of the object can be edited with all the manipulation operator provided...\"],[\"# Unity Integration\\n\\n### Install with the Unity editor\\nCurrently we use Unity version `2021.3.2f1` a...\"],[\"```\\nusing UnityEngine.Events;\\nusing Simulate;\\n\\npublic class MyCommand : Command {\\n    public string ...\"],[\"```\\n\\nThis currently only supports Box, Sphere, and Capsule colliders (the Unity\\u002fPhysX colliders).\\n\\nD...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\u003c!--\\nUse this section to tell people about which versions of ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"his package provides core backend functionality for the Hugging Face Simulate project: (https:\\u002f\\u002fgith...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\nfor i in range(60):\\n    event = scene.step()\\n```\\nYou should see the cube falling onto the plane....\"],[\"```\\nscene.config.return_nodes = False\\nscene.config.return_frames = False\\nscene.show()\\n```\\nFor advanc...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Interfacing with the git-versioning and hosting on the Hugging Face hub allow to download\\u002fupload sha...\"],[\"# Blender Integration\\n\\n### Install addon in Blender\\nThis integration has been developed for Blender ...\"],[\"Tests examples taken from the original great gltflib\\n\\nFind the great gltflib by Lukas Shawford here:...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Examples for Simulate\\n\\nThe examples are organized by level of complexity or application. \\nCurrentl...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nWe can now start an X server:\\n\\n```\\nsudo Xorg :0\\n```\\n\\nRun the following to confirm that offscree...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Reward function:\\n- A dense reward based on improvement in best euclidean distance to the object\\n- A ...\"],[\"Objective: Navigate to an object in a 3D maze, when the object is collected the environment resets.\\n...\"],[\"Parallel: 16 independent instances of the same environment configuration.\\n\\n\\n## Reward functions base...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"]],\"hovertemplate\":\"source=simulate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"simulate, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"simulate, circle\",\"showlegend\":true,\"x\":[7.6226544,1.3944291,4.2424555,3.498836,6.884699,6.5329866,7.5064163,7.21291,7.1144924,6.948052,7.023319,7.4426074,6.8129354,7.1740193,7.0562134,6.7337503,8.6470585,8.745532,7.2973,7.332022,7.1401644,6.9961586,6.940749,6.93891,7.062487,7.652647,10.752889,7.184933,6.9169893,5.2733784,5.266915,6.952605,6.596209,7.5145383,8.157577,7.2846365,7.137126,7.3126755,7.0418057,6.9295077,7.548669,7.5402617,7.3045797,7.2809997,6.7055435,7.2664165,6.901843,-4.9648757,5.359008,6.8563147,7.5176263,8.180094,8.163626,8.090592,8.050994,6.8217144,7.689458],\"xaxis\":\"x\",\"y\":[-9.622543,4.243874,1.3638157,1.0175468,-1.1440301,-0.6498534,-9.666107,-9.503437,-9.49334,-9.427471,-9.465737,-9.74945,-1.122834,-9.492972,-9.432473,-0.9571213,2.6139524,2.6226888,-9.513967,-9.553454,-9.519903,-9.438784,-9.401492,-9.507108,-9.472937,-9.687763,3.427198,-9.400418,-1.1824926,0.15758345,0.83772475,-1.032984,-0.9086754,-9.589985,-10.49897,-9.694546,-9.60628,-9.717481,0.37568521,-1.1885118,-9.663007,-9.407549,-9.55441,3.1255488,-0.98661965,-9.593135,-1.1589569,-1.5437678,1.3923395,-1.2383444,-9.597056,-10.362019,-10.499254,-10.32791,-10.481053,-1.0974529,-9.2090845],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"# How to release\\n\\n# Before the release\\n\\nSimple checklist on how to make releases for `safetensors`.\\n...\"],[\"# Rust\\n\\n- `safetensors` (rust, python & node) versions don't have to be in sync but it's\\n  very comm...\"],[\"# Python\\n\\n- Edit `bindings\\u002fpython\\u002fsetup.py` to reflect new version.\\n- Edit `bindings\\u002fpython\\u002fpy_src\\u002fs...\"],[\"# Node\\n\\n- Edit `bindings\\u002fnode\\u002fpackage.json` to reflect new version.\\n- Edit `CHANGELOG.md`:\\n    - Add...\"],[\"Flax API\\n\\n[[autodoc]] safetensors.flax.load_file\\n[[autodoc]] safetensors.flax.load\\n[[autodoc]] safet...\"],[\"Convert weights to safetensors\\n\\nPyTorch model weights are commonly saved and stored as `.bin` files ...\"],[\"Numpy API\\n\\n[[autodoc]] safetensors.numpy.load_file\\n[[autodoc]] safetensors.numpy.load\\n[[autodoc]] sa...\"],[\"Speed Comparison\\n\\n\\u003ca href=\\\"https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002f...\"],[\"```\\n\\n### CPU benchmark\\n\\n```py\\n\\u003e\\u003e\\u003e start_st = datetime.datetime.now()\\n\\u003e\\u003e\\u003e weights = load_file(sf_file...\"],[\"```\\n\\nThis speedup is due to the fact that this library avoids unnecessary copies by mapping the file...\"],[\"```\\n\\nThe speedup works because this library is able to skip unnecessary CPU allocations. It is unfor...\"],[\"# Installation\\n\\n```\\npip install safetensors\\n```\\n\\n\\n## Usage\\n\\n### Numpy\\n\\n```python\\nfrom safetensors.nu...\"],[\"PaddlePaddle API\\n\\n[[autodoc]] safetensors.paddle.load_file\\n[[autodoc]] safetensors.paddle.load\\n[[aut...\"],[\"he purpose of this directory is to showcase various attacks (and creating your own).\\n\\n\\n# Torch Arbit...\"],[\"```\\npython numpy_dos_create.py\\npython numpy_dos_get_pwned.py\\n```\\n\\n# Safetensors abuse attempts\\n\\nIn o...\"],[\"```\\npython safetensors_abuse_attempt_1.py\\npython safetensors_abuse_attempt_2.py\\npython safetensors_a...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cpicture\\u003e\\n    \\u003csource media=\\\"(prefers-color-scheme: dark)\\\" srcset=\\\"https:\\u002f\\u002fhuggi...\"],[\"Rust\\n[![Crates.io](https:\\u002f\\u002fimg.shields.io\\u002fcrates\\u002fv\\u002fsafetensors.svg)](https:\\u002f\\u002fcrates.io\\u002fcrates\\u002fsafete...\"],[\"```\\n\\n#### From source\\n\\nFor the sources, you need Rust\\n\\n```bash\\n# Install Rust\\ncurl --proto '=https' ...\"],[\"```\\n\\n[Python documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fsafetensors\\u002findex)\\n\\n\\n### Format\\n\\n- 8 bytes: ...\"],[\"Notes:\\n - Duplicate keys are disallowed. Not all parsers may respect this.\\n - In general the subset ...\"],[\"Let's take a look at alternatives and why this format is deemed interesting.\\nThis is my very persona...\"],[\"- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\\n- Zero-copy: Doe...\"],[\"### Main oppositions\\n\\n- Pickle: Unsafe, runs arbitrary code\\n- H5: Apparently now discouraged for TF\\u002f...\"],[\"- Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmome...\"],[\"Metadata Parsing\\n\\nGiven the simplicity of the format, it's very simple and efficient to fetch and pa...\"],[\"## Usage\\n\\n### JavaScript\\u002fTypeScript[[js]]\\n\\nUsing [`huggingface.js`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhugg...\"],[\"```\\n\\nDepending on whether the safetensors weights are sharded into multiple files or not, the output...\"],[\"```\\n\\n### Python\\n\\nIn this example python script, we are parsing metadata of [gpt2](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\n## Example output\\n\\nFor instance, here are the number of params per dtype for a few models on th...\"],[\"model | safetensors | params\\n--- | --- | ---\\n[gpt2](https:\\u002f\\u002fhuggingface.co\\u002fgpt2?show_tensors=true) |...\"],[\"[bigscience\\u002fbloom](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fbloom?show_tensors=true) | sharded | { 'BF16' =...\"],[\"Torch API\\n\\n[[autodoc]] safetensors.torch.load_file\\n[[autodoc]] safetensors.torch.load\\n[[autodoc]] sa...\"],[\"Tensorflow API\\n\\n[[autodoc]] safetensors.tensorflow.load_file\\n[[autodoc]] safetensors.tensorflow.load...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:h...\"],[\"```\\n\\n## Format\\n\\nLet's say you have safetensors file named `model.safetensors`, then `model.safetenso...\"],[\"* [huggingface\\u002ftransformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers)\\n* [AUTOMATIC1111\\u002fstable-di...\"],[\"* [brycedrennan\\u002fimaginAIry](https:\\u002f\\u002fgithub.com\\u002fbrycedrennan\\u002fimaginAIry)\\n* [comfyanonymous\\u002fComfyUI](h...\"],[\"Torch shared tensors\\n\\n\\n## TL;DR\\n\\nUsing specific functions, which should work in most cases for you.\\n...\"],[\"```\\n\\n## Why are shared tensors not saved in `safetensors` ?\\n\\nMultiple reasons for that:\\n\\n- *Not all ...\"],[\"```\\n\\nNow with all those reasons being mentioned, nothing is set in stone in there.\\nShared tensors do...\"]],\"hovertemplate\":\"source=safetensors\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"safetensors, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"safetensors, circle\",\"showlegend\":true,\"x\":[1.9762057,2.982493,3.0919442,3.4774764,1.6963663,-0.36281466,1.6599792,-2.731231,-2.6748078,-3.0603843,-2.676693,0.833218,1.8218231,0.8041706,1.0801245,1.1426535,6.67499,1.4057076,0.6689981,0.41364455,-0.3807718,1.2849835,0.4371586,0.6742755,-0.42018843,5.183871,4.9937477,1.6080625,1.6718262,0.40037572,0.801768,7.5731354,0.41059527,0.7309621,1.0491812,6.0852914,-4.312341,8.069582,-1.001502,-0.12824881,-0.065241136],\"xaxis\":\"x\",\"y\":[1.0169561,1.3266827,1.1378667,0.9640307,2.0317636,0.76051736,2.2199,-1.8783609,-1.5625001,-2.2949584,-1.9489497,1.2925124,2.3148434,1.1102713,1.9265141,2.1743882,1.0906627,1.5433725,1.0337136,2.184725,1.9051431,1.6212958,1.2829471,1.2011966,0.884586,2.7183068,3.169313,1.9994048,2.731647,2.747948,1.1594319,2.921384,1.2059758,1.4576727,1.3730408,2.379552,-6.7922673,2.3644001,0.9917316,0.9658797,1.4024211],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"How to create a pipeline object?\"]],\"hovertemplate\":\"source=User query\\u003cbr\\u003esymbol=star\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"User query, star\",\"marker\":{\"color\":\"black\",\"size\":[100],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"diamond\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"User query, star\",\"showlegend\":true,\"x\":[-2.6344566],\"xaxis\":\"x\",\"y\":[-8.9381],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eChunk source\\u003c\\u002fb\\u003e\"},\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"height\":700,\"width\":1000,\"title\":{\"text\":\"\\u003cb\\u003e2D Projection of Chunk Embeddings via PaCMAP\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7263b162-0362-4a40-a9d0-021e6d957803');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df = pd.DataFrame.from_dict(\n",
        "    [\n",
        "        {\n",
        "            \"x\": documents_projected[i, 0],\n",
        "            \"y\": documents_projected[i, 1],\n",
        "            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[1],\n",
        "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
        "            \"symbol\": \"circle\",\n",
        "            \"size_col\": 4,\n",
        "        }\n",
        "        for i in range(len(docs_processed))\n",
        "    ]\n",
        "    + [\n",
        "        {\n",
        "            \"x\": documents_projected[-1, 0],\n",
        "            \"y\": documents_projected[-1, 1],\n",
        "            \"source\": \"User query\",\n",
        "            \"extract\": user_query,\n",
        "            \"size_col\": 100,\n",
        "            \"symbol\": \"star\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Visualize the embedding\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"x\",\n",
        "    y=\"y\",\n",
        "    color=\"source\",\n",
        "    hover_data=\"extract\",\n",
        "    size=\"size_col\",\n",
        "    symbol=\"symbol\",\n",
        "    color_discrete_map={\"User query\": \"black\"},\n",
        "    width=1000,\n",
        "    height=700,\n",
        ")\n",
        "fig.update_traces(\n",
        "    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n",
        "    selector=dict(mode=\"markers\"),\n",
        ")\n",
        "fig.update_layout(\n",
        "    legend_title_text=\"<b>Chunk source</b>\",\n",
        "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWesCSGt9-9N"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/PaCMAP_embeddings.png\" height=\"700\">\n",
        "\n",
        "\n",
        "â¡ï¸ On the graph above, you can see a spatial representation of the knowledge base documents. As the vector embeddings represent the document's meaning, their closeness in meaning should be reflected in their embedding's closeness.\n",
        "\n",
        "The user query's embedding is also shown: we want to find the `k` documents that have the closest meaning, thus we pick the `k` closest vectors.\n",
        "\n",
        "In the LangChain vector database implementation, this search operation is performed by the method `vector_database.similarity_search(query)`.\n",
        "\n",
        "Here is the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VcjQzejH9-9N",
        "outputId": "b10a4bbd-6117-4af3-d50b-04b7a19174a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting retrieval for user_query='How to create a pipeline object?'...\n",
            "\n",
            "==================================Top document==================================\n",
            "```\n",
            "</tf>\n",
            "</frameworkcontent>\n",
            "\n",
            "## Pipeline\n",
            "\n",
            "<Youtube id=\"tiZFewofSLM\"/>\n",
            "\n",
            "The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
            "\n",
            "<Tip>\n",
            "\n",
            "For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).\n",
            "\n",
            "</Tip>\n",
            "==================================Metadata==================================\n",
            "{'source': 'huggingface/transformers/blob/main/docs/source/en/quicktour.md', 'start_index': 1585}\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "print(\n",
        "    \"\\n==================================Top document==================================\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Starting retrieval for user_query='How to create a pipeline object?'...\n",
        "\n",
        "==================================Top document==================================\n",
        "```\n",
        "</tf>\n",
        "</frameworkcontent>\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "<Youtube id=\"tiZFewofSLM\"/>\n",
        "\n",
        "The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
        "\n",
        "<Tip>\n",
        "\n",
        "For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).\n",
        "\n",
        "</Tip>\n",
        "==================================Metadata==================================\n",
        "{'source': 'huggingface/transformers/blob/main/docs/source/en/quicktour.md', 'start_index': 1585}"
      ],
      "metadata": {
        "id": "RClwA1ieeL-X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjVqmDGh9-9N"
      },
      "source": [
        "# 2. Reader - LLM ğŸ’¬\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved context to formulate its answer.__\n",
        "\n",
        "There are substeps that can all be tuned:\n",
        "1. The content of the retrieved documents is aggregated together into the \"context\", with many processing options like _prompt compression_.\n",
        "2. The context and the user query are aggregated into a prompt and then given to the LLM to generate its answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xiXcG269-9N"
      },
      "source": [
        "### 2.1. Reader model\n",
        "\n",
        "The choice of a reader model is important in a few aspects:\n",
        "- the reader model's `max_seq_length` must accommodate our prompt, which includes the context output by the retriever call: the context consists of 5 documents of 512 tokens each, so we aim for a context length of 4k tokens at least.\n",
        "- the reader model\n",
        "\n",
        "For this example, we chose [`HuggingFaceH4/zephyr-7b-beta`](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), a small but powerful model.\n",
        "\n",
        "With many models being released every week, you may want to substitute this model to the latest and greatest. The best way to keep track of open source LLMs is to check the [Open-source LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
        "\n",
        "To make inference faster, we will load the quantized version of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4554fb15ebb54f37aae57022e55fdef1",
            "00b7d2f8529743c082e89bfb6cd7ef84",
            "148f50555cb74dd78b22b40c5b0a5188",
            "45259304c4904fc9a9c18491c603be63",
            "da5a5b5042704d3a8df90620309f4b1c",
            "9de553c4dcbd4a6a90006516b9aca94b",
            "4d8ebe3226ed4902a3410a51ea175419",
            "3e6c796f367f4de89d757f49a8c13de1",
            "9964d81fb40449c4acf7705c0073cdf6",
            "6fe1a32d061447f9af100ddf63f820fb",
            "288f0fa13ac84fe8bb960e2812c3f10c"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "QX_ORK4l9-9N",
        "outputId": "3b99b64a-1e98-4b05-b400-5257cd068897"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4554fb15ebb54f37aae57022e55fdef1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    READER_MODEL_NAME, quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YTf_EGYj9-9O",
        "outputId": "00163711-d306-4d51-a777-104d5dadf417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': ' 8.\\nWhat is 3+5? Answer: 8.\\nWhat is 6-2? Answer: 4.\\nWhat is 10/2? Answer: 5.\\nWhat is 9 x 1? Answer: 9.\\nWhat is 11-3? Answer: 8.\\nWhat is 7 + 2? Answer: 9.\\nWhat is 8 Ã· 2? Answer: 4.\\nWhat is 10 â€“ 4? Answer: 6.\\nWhat is 6 Ã— 1? Answer: 6.\\nWhat is 9 Ã· 3? Answer: 3.\\nWhat is 11 â€“ 7? Answer: 4.\\nWhat is 7 + 3? Answer: 10.\\nWhat is 8 Ã· 4? Answer: 2.\\nWhat is 10 â€“ 5? Answer: 5.\\nWhat is 6 Ã— 2? Answer: 12.\\nWhat is 9 Ã· 1? Answer: 9.\\nWhat is 11 â€“ 9? Answer: 2.\\nWhat is 7 + 4? Answer: 11.\\nWhat is 8 Ã· 2? Answer: 4.\\nWhat is 10 â€“ 6? Answer: 4.\\nWhat is 6 Ã— 3? Answer: 18.\\nWhat is 9 Ã· 3? Answer: 3.\\nWhat is 11 â€“ 5? Answer: 6.\\nWhat is 7 + 5? Answer: 12.\\nWhat is 8 Ã· 4? Answer: 2.\\nWhat is 10 â€“ 7? Answer: 3.\\nWhat is 6 Ã— 4? Answer: 24.\\nWhat is 9 Ã· 1? Answer: 9.\\nWhat is 11 â€“ 9? Answer: 2.\\nWhat is 7 + 6? Answer: 13.\\nWhat is 8 Ã· 4? Answer: 2.\\nWhat is 10 â€“ 8? Answer: 2.\\nWhat is 6 Ã— 5? Answer: 30.\\nWhat is 9 Ã· 3? Answer'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "READER_LLM(\"What is 4+4? Answer:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlfHavRT9-9O"
      },
      "source": [
        "### 2.2. Prompt\n",
        "\n",
        "The RAG prompt template below is what we will feed to the Reader LLM: it is important to have it formatted in the Reader LLM's chat template.\n",
        "\n",
        "We give it our context and the user's question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Abn4gw5A9-9O",
        "outputId": "4051be4c-c76f-4e5f-b81f-ddebd3ac0a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "Using the information contained in the context,\n",
            "give a comprehensive answer to the question.\n",
            "Respond only to the question asked, response should be concise and relevant to the question.\n",
            "Provide the number of the source document when relevant.\n",
            "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
            "<|user|>\n",
            "Context:\n",
            "{context}\n",
            "---\n",
            "Now here is the question you need to answer.\n",
            "\n",
            "Question: {question}</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZRHLza-9-9O"
      },
      "source": [
        "Let's test our Reader on our previously retrieved documents!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G4XprIih9-9O",
        "outputId": "c76c28e0-884b-4a67-d6d7-19a1007d5338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To create a pipeline object, follow these steps:\n",
            "\n",
            "1. Define the inputs and outputs of your pipeline. These can be strings, raw bytes, dictionaries, or any other format that suits your use case.\n",
            "\n",
            "2. Inherit the `Pipeline` class from the `transformers` module and override the following methods:\n",
            "\n",
            "   - `_sanitize_parameters`: This method takes the parameters passed to the pipeline and returns a tuple containing the preprocess parameters, forward parameters, and additional parameters.\n",
            "\n",
            "   - `preprocess`: This method takes the inputs and any additional parameters and returns a dictionary containing the processed inputs.\n",
            "\n",
            "   - `_forward`: This method takes the processed inputs and returns the output of the diffusion model.\n",
            "\n",
            "   - `postprocess`: This method takes the output of the diffusion model and returns the final output of the pipeline.\n",
            "\n",
            "3. Implement the `_sanitize_parameters` method to extract the necessary parameters from the input dictionary. Here's an example implementation:\n",
            "\n",
            "   ```python\n",
            "   def _sanitize_parameters(self, **kwargs):\n",
            "       preprocess_kwargs = {}\n",
            "       if \"maybe_arg\" in kwargs:\n",
            "           preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
            "       return preprocess_kwargs, {}, {}\n",
            "   ```\n",
            "\n",
            "4. Implement the `preprocess` method to process the inputs and return a dictionary containing the processed inputs. Here's an example implementation:\n",
            "\n",
            "   ```python\n",
            "   def preprocess(self, inputs, maybe_arg=2):\n",
            "       model_input = Tensor(inputs[\"input_ids\"])\n",
            "       return {\"model_input\": model_input}\n",
            "   ```\n",
            "\n",
            "5. Implement the `_forward` method to pass the processed inputs to the diffusion model and return the output. Here's an example implementation:\n",
            "\n",
            "   ```python\n",
            "   def _forward(self, model_inputs, **kwargs):\n",
            "       # Use the diffusion model to generate the output\n",
            "       return output\n",
            "   ```\n",
            "\n",
            "6. Implement the `postprocess` method to further process the output of the diffusion model and return the final output of the pipeline. Here's an example implementation:\n",
            "\n",
            "   ```python\n",
            "   def postprocess(self, output, **kwargs):\n",
            "       # Process the output of the diffusion\n"
          ]
        }
      ],
      "source": [
        "retrieved_docs_text = [\n",
        "    doc.page_content for doc in retrieved_docs\n",
        "]  # We only need the text of the documents\n",
        "context = \"\\nExtracted documents:\\n\"\n",
        "context += \"\".join(\n",
        "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
        ")\n",
        "\n",
        "final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
        "    question=\"How to create a pipeline object?\", context=context\n",
        ")\n",
        "\n",
        "# Redact an answer\n",
        "answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a pipeline object, follow these steps:\n",
        "\n",
        "1. Define the inputs and outputs of your pipeline. This could be a dictionary, string, or any other Python data type.\n",
        "\n",
        "2. Create a subclass of the `Pipeline` class from the `transformers` module. This subclass should inherit from the `Pipeline` class and implement the following methods:\n",
        "   - `_sanitize_parameters`: This method sanitizes the parameters passed to the pipeline. It returns a tuple containing the preprocess, forward, and postprocess parameters.\n",
        "   - `preprocess`: This method preprocesses the input data according to the pipeline requirements. It returns a dictionary containing the processed data.\n",
        "   - `_forward`: This method performs the actual computation using the preprocessed data. It returns the output data.\n",
        "   - `postprocess`: This method postprocesses the output data according to the pipeline requirements. It returns the final output data.\n",
        "\n",
        "3. Implement the `_sanitize_parameters` method to sanitize the parameters passed to the pipeline. This method takes the parameters as keyword arguments and returns a tuple containing the preprocess, forward, and postprocess parameters.\n",
        "\n",
        "4. Implement the `preprocess` method to preprocess the input data according to the pipeline requirements. This method takes the inputs and any additional arguments as keyword arguments and returns a dictionary containing the preprocessed data.\n",
        "\n",
        "5. Implement the `_forward` method to perform the actual computation using the preprocessed data. This method takes the preprocessed data and any additional arguments as keyword arguments and returns the output data.\n",
        "\n",
        "6. Implement the `postprocess` method to postprocess the output data according to the pipeline requirements. This method takes the output data and any additional arguments as keyword arguments and returns the final output data.\n",
        "\n",
        "7. Instantiate the pipeline object by passing the preprocess, forward, and postprocess functions to the `Pipeline` constructor.\n",
        "\n",
        "Here's an example implementation:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import Pipeline\n",
        "\n",
        "class MyPipeline(Pipeline):\n",
        "    def __init__(self, preprocess, forward, postprocess):\n",
        "        super().__init__(preprocess, forward, postprocess)\n",
        "\n",
        "    def _sanitize_parameters(self, **kwargs):\n",
        "        # Sanitize parameters here"
      ],
      "metadata": {
        "id": "NHePksdTfSwI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhRHZoww9-9O"
      },
      "source": [
        "### 2.3. Reranking\n",
        "\n",
        "A good option for RAG is to retrieve more documents than you want in the end, then rerank the results with a more powerful retrieval model before keeping only the `top_k`.\n",
        "\n",
        "For this, [Colbertv2](https://arxiv.org/abs/2112.01488) is a great choice: instead of a bi-encoder like our classical embedding models, it is a cross-encoder that computes more fine-grained interactions between the query tokens and each document's tokens.\n",
        "\n",
        "It is easily usable thanks to [the RAGatouille library](https://github.com/bclavie/RAGatouille)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragatouille==0.0.9.post2"
      ],
      "metadata": {
        "id": "7CbgRqfUftgZ",
        "outputId": "0a6d1630-082e-4160-a234-90d1ba8d488e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ragatouille==0.0.9.post2\n",
            "  Using cached ragatouille-0.0.9.post2-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.11.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.53)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.19)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.20)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.18.0)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.4.8)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.6.0+cu124)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.7.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.1)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.11.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.51.3)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (5.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille==0.0.9.post2) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (23.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans->ragatouille==0.0.9.post2) (12.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.6.7)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille==0.0.9.post2) (1.33)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.42)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->ragatouille==0.0.9.post2) (5.29.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (11.2.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly->ragatouille==0.0.9.post2) (2.0.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille==0.0.9.post2) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (1.0.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.78.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (2.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.1.19)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (5.5.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.6.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille==0.0.9.post2) (3.2.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.70.15)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.3)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.44)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->fast-pytorch-kmeans->ragatouille==0.0.9.post2) (12.575.51)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->ragatouille==0.0.9.post2) (3.6.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.7.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (0.16.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.22 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.6.22)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (1.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.4.6)\n",
            "Using cached ragatouille-0.0.9.post2-py3-none-any.whl (46 kB)\n",
            "Installing collected packages: ragatouille\n",
            "  Attempting uninstall: ragatouille\n",
            "    Found existing installation: RAGatouille 0.0.8\n",
            "    Uninstalling RAGatouille-0.0.8:\n",
            "      Successfully uninstalled RAGatouille-0.0.8\n",
            "Successfully installed ragatouille-0.0.9.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "triOdqTV9-9O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "6883fcca-422d-4a56-c068-a95310fac802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-686d804d4597>:1: UserWarning: \n",
            "********************************************************************************\n",
            "RAGatouille WARNING: Future Release Notice\n",
            "--------------------------------------------\n",
            "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
            "instead of the current Stanford ColBERT backend.\n",
            "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
            "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
            "********************************************************************************\n",
            "  from ragatouille import RAGPretrainedModel\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-686d804d4597>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRERANKER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"colbert-ir/colbertv2.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.0.9post2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGPretrainedModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGTrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/RAGPretrainedModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mRAGatouilleLangChainRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"LateInteractionModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ColBERT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/colbert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msearcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_updater\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexUpdater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/training/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank_batcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRerankBatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ePgQAaSFiJ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$"
      ],
      "metadata": {
        "id": "jqH4XDIQiLIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nG2q-Tr2iSvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install a potentially more compatible transformers version first\n",
        "!pip install transformers==4.26.0\n",
        "\n",
        "# Then install the specific ragatouille version\n",
        "!pip install ragatouille==0.0.9.post2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "iMl3pmyniTta",
        "outputId": "f0befe02-3b4b-427c-c15d-2a4d86b07c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.26.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl.metadata (100 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/100.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.26.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.26.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.26.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.26.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.26.0) (2025.4.26)\n",
            "Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.28.0\n",
            "    Uninstalling transformers-4.28.0:\n",
            "      Successfully uninstalled transformers-4.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 2.7.0 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.26.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "d18d10db0e4d4e8b838ab827d33dd33f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille==0.0.9.post2 in /usr/local/lib/python3.11/dist-packages (0.0.9.post2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.11.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.53)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.19)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.20)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.18.0)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.4.8)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.6.0+cu124)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.7.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.1)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.11.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.26.0)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (5.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille==0.0.9.post2) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (23.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans->ragatouille==0.0.9.post2) (12.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.6.7)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille==0.0.9.post2) (1.33)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.42)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->ragatouille==0.0.9.post2) (5.29.4)\n",
            "Collecting transformers (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2)\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (11.2.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly->ragatouille==0.0.9.post2) (2.0.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille==0.0.9.post2) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (1.0.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.78.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (2.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.1.19)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (5.5.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.6.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille==0.0.9.post2) (3.2.2)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "s_FuC8b3hPwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert import Indexer\n",
        "\n",
        "if __name__=='__main__':\n",
        "    with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
        "\n",
        "        config = ColBERTConfig(\n",
        "            nbits=2,\n",
        "            root=\"/path/to/experiments\",\n",
        "        )\n",
        "        indexer = Indexer(checkpoint=\"/path/to/checkpoint\", config=config)\n",
        "        indexer.index(name=\"msmarco.nbits=2\", collection=\"/path/to/MSMARCO/collection.tsv\")\n"
      ],
      "metadata": {
        "id": "SDzrdV6Divnb",
        "outputId": "16581caa-bff7-40c5-db20-9b0511930207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[May 20, 02:34:37] #> Creating directory /content/experiments/msmarco/indexes/msmarco.nbits=2 \n",
            "\n",
            "\n",
            "#> Starting...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-32bd6c8382c2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         )\n\u001b[1;32m     11\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/path/to/checkpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"msmarco.nbits=2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/path/to/MSMARCO/collection.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/indexer.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, name, collection, overwrite)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex_does_not_exist\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'reuse'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/indexer.py\u001b[0m in \u001b[0;36m__launch\u001b[0;34m(self, collection)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Encodes collection into index using the CollectionIndexer class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_queues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/infra/launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, custom_config, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# TODO: If the processes crash upon join, raise an exception and don't block on .get() below!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mreturn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreturn_value_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_procs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mreturn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturn_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/infra/launcher.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# TODO: If the processes crash upon join, raise an exception and don't block on .get() below!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mreturn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreturn_value_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_procs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mreturn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturn_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZeoKEsHi9di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# ColBERTv2: Indexing & Search Notebook\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"GPU\" as your hardware accelerator in the runtime settings.\n",
        "\n",
        "First, we'll import the relevant classes. Note that `Indexer` and `Searcher` are the key actors here. Next, we'll download the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_YBBPTo5AZ",
        "outputId": "1d1250d8-396c-4758-b334-01a50e4d3b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'ColBERT/': No such file or directory\n",
            "Cloning into 'ColBERT'...\n",
            "remote: Enumerating objects: 2846, done.\u001b[K\n",
            "remote: Counting objects: 100% (1173/1173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (354/354), done.\u001b[K\n",
            "remote: Total 2846 (delta 953), reused 819 (delta 819), pack-reused 1673 (from 4)\u001b[K\n",
            "Receiving objects: 100% (2846/2846), 2.07 MiB | 21.24 MiB/s, done.\n",
            "Resolving deltas: 100% (1790/1790), done.\n"
          ]
        }
      ],
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9dI8G6Z8jORC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install -e ColBERT/[faiss-cpu','torch']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aLKcL0v2jOgT",
        "outputId": "39ac2338-13ed-49aa-f5a2-6c210ef39f0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmBi2UT5pxb3",
        "outputId": "b6e8bb21-dd60-4ee5-f40b-9a87158dbafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Obtaining file:///content/ColBERT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (3.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (3.1.1)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (3.1.44)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (1.11.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (4.26.0)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (5.10.0)\n",
            "INFO: pip is looking at multiple versions of colbert-ai[faiss-gpu,torch] to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu>=1.7.0; extra == \"faiss-gpu\" (from colbert-ai[faiss-gpu,torch]) (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu>=1.7.0; extra == \"faiss-gpu\"\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "try: # When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0jxbVar4kln"
      },
      "outputs": [],
      "source": [
        "import colbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQg9A-dtp1nB"
      },
      "outputs": [],
      "source": [
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLWjmlkVq9r0"
      },
      "source": [
        "We will use the dev set of the **LoTTE benchmark** we recently introduced in the ColBERTv2 paper. We'll download it from HuggingFace datasets. The dev and test sets contain several domain-specific corpora, and we'll use the smallest dev set corpus, namely lifestyle:dev.\n",
        "\n",
        "For the purposes of a quick demo, we will only run the `Indexer` on the first 10,000 passages. As we do this, let's also remove the queries whose relevant passages are all outside this small set of passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695,
          "referenced_widgets": [
            "e6722d76e201459a9ccc440436f000b8",
            "9f1cd00116c0467e916a6d3682d07004",
            "d40d5b134e2e4aab88af08f7491e5522",
            "284c44ad91b148d4b495c97449cf2bed",
            "d068ba465d07430ca8395b9bb748cb0a",
            "44e5cfe1fb9a450fb728b703cc1289cb",
            "28d0cec9dfd24bcdb9f0532ccbdb9652",
            "30ca3617e21148fcacce568cfebb764b",
            "c0ae8fc035bc449787123804638602c0",
            "91aff22d948243fc9f9fa7003e5680e7",
            "6c3463824fbc4ccf8ddad310457c9fd1",
            "12cdb95084dd4f279515029111052d83",
            "2f20693f93384563bab3f84701e46faf",
            "76dfe66ed1ae4e629f166e6c02edb682",
            "1fe07b5f8dc94f55b8a742e3f3aa7ba1",
            "59058501f8154b85bd10779240a4dd0c",
            "66c8d81ec1384564a37017c61e926174",
            "705a3ccbf7334721a44332d67d6d4c7f",
            "f798f407aabc419781b68fe6ce14eea2",
            "ce16414ea9ae45ad97d24d11f86072b5",
            "6106968cf3894965a1f8f30598e34bfc",
            "a9aecadb341b4a7184a9332d098190a5",
            "420b019cbcce4ad2b1bda07fead12975",
            "eb7e307d364e4529adf18a90be39be56",
            "5432e67447ef40ce85ad3bddea61c4ce",
            "d4f17acf58284f38af68d0de1c51d35d",
            "77a4832a6973471dbfede86098b2a33c",
            "fda1b69158f9475294a1214b3efdd805",
            "96b2b61403be419284df3ca384c505c7",
            "33c700ade3b746fe8febc064db3c4884",
            "17f912e294f1428994a314958773128a",
            "32e1d2e13f344e63bf2211cb6efea0a0",
            "f671b3623efc4432b1c7833854bca6ab",
            "acd2fff0e3a5482687339035db8196ce",
            "590e651d5ccc4653aeb8889eccb0d840",
            "7ca0022099f14ef7a257d2e8867f75a4",
            "1d7bd33e6a3f4cc3bb385673b45bbaa8",
            "bac7d01253d84bfdaebcf0ee3156c494",
            "dd98c5694800450a9ad13f3eb12b9288",
            "2918f1247ba24334a0e47402756bd823",
            "d2b48bbc25c64b699d90155c09ecf997",
            "18cad363030540a09a5d6164b8a1ef06",
            "3b5332b7245f414aa93aaf673afeb906",
            "7e9e46c5ca55497bb08315a6cfc9bad0",
            "894f77f5c21845649a52a3e696738743",
            "88377d67a940421aa3c621bf333228f7",
            "3b2885705ffe40d7a6b1c77ce78f0fc1",
            "2a73e0aba52f47baa0caee58762940a7",
            "08d8fdb151084ecd9e3454ab699c6861",
            "7934c4ade7164e9eb23e93b3ff642e41",
            "3314b2d6e30a4974b21c76b94d5524f9",
            "c514d61a7fdc4a8cacd369239cd18303",
            "c1bfc9ab842f4f83a17c4a5281fef84e",
            "49616b9b94374eacb7012b81ffa75028",
            "66ca07a022964bb59e37c45969ca93bd",
            "127da1beda60416f8dd5a265f99709d1",
            "b8588f17c06845edac97deb0e2aaf0c2",
            "3f56499e6dd148408ad84edf592bf03e",
            "d4139f50a990468bb0622d49df813f6d",
            "28304dbbf493450fae7ddbd3f8b36499",
            "3ef6f57adf4c4052b7aeeeebd0e78069",
            "977ffe9a73204abfb900bcbe692f58f7",
            "155da21c6f7e4a6ead2af6b08a4f3a0a",
            "8c13f61c7b8d4866a0a1ab1a9427799c",
            "f8e39a4fcb5b407cabc34c8d801176cf",
            "e9d4d35c7f3c43fca9fe1e7941a41edf",
            "40310cf9d3a2492d89c32c12c266d281",
            "dece032663254935a31374d8fb16da12",
            "ac866c1a923349658d2e1d6b378a195c",
            "7402a0e0e8804c5483f24ab0f9173feb",
            "773c8a4da5414fbd8ebc4a442650a0dd",
            "ea700308dbd549fbbea6ae9a21692987",
            "6d982d1cbfb8440491b867798eb5cf87",
            "0da445fda1aa4301aa7b0bfbe6f5913a",
            "4c4023841e6e4a8592fd644a2f971581",
            "0f18916ddef64343a1b0d6c858c3b0de",
            "dcce888fb5784b46a700a7294fe9d70c",
            "20dfa854644f43c7a42b3809f875fc32",
            "b3d76dedd1b54bb695d9dc88658ed585",
            "a219d02098a3407e8c21d08b0a25bb88",
            "94f4cb3ef8b04f918365c04311a685ac",
            "93815e42978e47119162216a71eca9cc",
            "0e226a73ba034970b1b652199548868c",
            "886eb695d41648e3ba96d317d072d1b6",
            "b5b056ef87134ef19263185d8ad93f81",
            "6b7b2b0bba654b8384771232a2c46651",
            "9d48db272f1e42b990ad27b3d713011b",
            "0bcedc7e0a444749971a825da17bbdae",
            "ae7ae52805054419b99ad4320142add8",
            "7bcf22b18c144a7991b6d8c8118fede5",
            "e0bdd1517891490f803ce2c2ed64a935",
            "cb3188256e22439c8906c6908d35f542",
            "a13f6dad45244af2a186a4adcc98acbb",
            "ce4da389c68c440a8cd10cdab7c209cb",
            "fd7b3b97df244b96a9ed461274e1789b",
            "21a44b6339e345ecb614b8c40d3c6d1e",
            "5440c10b0ad04b818b6a17d1cce2db51",
            "73931bd3bc764602bdeaf341df3d7496",
            "7789344bbf014a05ae8517143ed2976a",
            "d1e389a3ed44437294e4deb7e76af347",
            "65e6f933686e4ad6a1658682466890cd",
            "2d2cd34df25c4e26a161ded76fc42991",
            "be77b6c138e4474189acc4d1791e8d0a",
            "baf1c0421ff5426db5f11c3475666488",
            "c415a6ecfa0f4c599ab5b81fd3532195",
            "f3166a0c3c294a87a34b98fad2eacd2b",
            "5cc4d50f0f074a6382a443a79409b783",
            "f6ca2d5e0a34459aadc1769380288aca",
            "c20a5fde8a44439499f8920e82e074a9",
            "e5df74fd6ae243879f39b320775e3db1",
            "c5e8318e39e4427daa9a90b948ae6633",
            "79257861b08846deaadba2264aae0c99",
            "cda235c3789e40bfb15ca1eb89fc339d",
            "3094b03c22df46cebf3d5c99b0d38e76",
            "1b991942890d474db73a3b04a64ad314",
            "5ded5edb649244be984b14f52da93c78",
            "9799d124f5114927bd1c74f7212bc6e3",
            "5adb412c5dac4455bf061629fd1b0719",
            "8b5e6a8537564450b1359794f6a5a643",
            "86b1e99b12884e739a645029f6495504",
            "fa1e808820554e99ad9f722009b6f0e6",
            "dfdb3dd7e48a4d8b80b0920921d8a0ae",
            "ff714bdac1bc467fb2f98fbbca101040",
            "346541c0f7fb4298b425e092f7d478ff",
            "1463a83b521646e285d2b6548d9f5edd",
            "1dfe3b85d77f493cb4abd4c15f389b3f",
            "98706f53f6c74decb518f8e7d6a3d271",
            "4899661d424a4c968b5146f356ec742c",
            "5ae0eed94fe944ada31d7fc389d69b28",
            "dc9f1dee97f04b5795e4ad9731e23bae",
            "2e39ccfd9139469094ad809a5a67ab1b",
            "bf7ce015ddc84cdbbfa49ccb4fb00c54",
            "b66fd86eb69f47e9be103de911532fed",
            "2d505d899dc24841903e4a10fedced16",
            "8a57cbf25bb34c6cbd4adf80cf697c9d",
            "7fa9f2dee34d440c96ee3c24322fb160",
            "4fb11b1165644d3898b6f878dd245c86",
            "783d6e905631484ea2b3593152c0ae92",
            "aa629a0c89124c9a97e6e11b331a24ff",
            "c703b32e055a415da39ec0ea9d408885",
            "9e5ed2f1f0b642eba20a3a4af15b8fe9",
            "f466f3d675b047acaa40c248f3569e3b",
            "b83330b5839a4ee6b2b030385ef1a4db",
            "8bfcdd45822e47998abf163fad0944b7",
            "547e233c04d64a4eae9cf230b6d21fca",
            "1491c087a5b8401a945cc7f10fe5d7a9",
            "0075d5711f6749909f77394c7b6f81b4",
            "e851b8c3daca419aa9ef0c8ce8b5110f",
            "0491f40820f24ce297b40686fdf03b0b",
            "398dd7b40fd1441aa695d0a133a81b9f",
            "b1ce1dab37684d6bba9ba9cfdde673e0",
            "78a0d11fd1c1403b9ff771211047e94e",
            "9f5f10cc3fe940c9a0a11ac7b4e86454",
            "2863c48b3f934bca87769e1169aeb7bc",
            "7183fe72b89340268ad6d40818c56b8d",
            "8bca95bdba1e4674a0de0f341516ccaf",
            "93366ac76c39467a8a0b0b0875656fe5",
            "37787f1cf2c24cac942029b9090a7403",
            "072bd17cb240498a88400bdcb9974520",
            "ae4a3dac1d734e2fa6dc2e81d965cf0f",
            "3cbee04182d149288a0a1ad29a116423",
            "375d4f3ca57a484f9258abaf457ff845",
            "1ac9360f632f412bb99cf8fd357d0f63",
            "356bc69127d74f5c8bc72561d7d5ad3f",
            "d3b9838ee813401e99d030f40a486a77",
            "f42ebf40950e45249f58efc8ed7f8b0a",
            "272898254a4a4da096749448987705a3",
            "6c4ee0554f26467288ffd471172281af",
            "35626d359dbb4702af1650fabedba9a6",
            "d275cb5e6535430e993b09ed16cdf95e",
            "3edab2597e874412ba41cf116f5f63f4",
            "55ed2b597c234e38a08ceda79baacfa0",
            "178a063d96644b1cbbb6b42c99e98f96",
            "631ab166e51e40dd906934c9b4241ed9",
            "24141ab123884830a320db851bb822a9",
            "d40e43e76a9a40e8b299b2bf1c5b39e8",
            "c45e8442e79b4988beee9eb38fd5540c",
            "1501a70a39424a19b56024192825ae25",
            "e2502ea7524947a09ecae0afa746a700",
            "9abedfb1ba864a7bb4ee1337db039cec",
            "b075f9c221994e7b88016c68821ef136",
            "d1a16335af5c41c99d664c56f06a3d34",
            "a1a136289f6b4389a5e8e031f884b759",
            "f5bb56c01fb549228c4858c8458f5ab4",
            "138085b634d94265885de69572d605df",
            "5196757a3c964599874b72a7a81be85d",
            "9dfb7592594a4683b60a21ed9a38c2c3",
            "2f93bba205de4169a5c626209afdc3c6",
            "c9db269877714a21adc777a91a01de03",
            "6ca5ab9f234f4fd1b8c387816df20c9d",
            "480088aa47a14522a3810cbc5b6b008a",
            "74a17e5762524b1cb8ca76320daa154e",
            "165e633d344a4012a36fa45114319007",
            "2bc6a76bc4dd4db791ecc91112613bdc",
            "162567fbe159421492f419b7d6fd1194",
            "ab23c132e37945ecaed8606c103445f7",
            "92144ba571aa42c9870b8e8cd463255a",
            "39afaf85c4e54b2f8bc731501f414918",
            "ec42a1277cf7461d82f97a228b520310",
            "0eb5cccf3baa4162a26d98b1cd4ef242",
            "6a826540075f4555a533b23c89d9c816",
            "96ad62d4dc974edbac88ecc9f34bc534",
            "b22ecf7f90424cc0a4f77ec9232dbe1c",
            "a1aab5f69f4b4edcbd1324ef03e6da6d",
            "58353bc3c1a44787a1e067b76f5995ad",
            "2375b449a2d443779d01c44173766a74",
            "21879294b4b74431b087a42e30755db5",
            "1b84eb894fc642ad87799563ffe97d63",
            "5bdcd54c0f4a427087f109b31b7a4fb5",
            "c1ccc48f22f2452c91aac8f4db6bb93a",
            "3188b889ea8d4c52acbf21abd7862bf0",
            "f50e36f226de49afb4984b955fb0a1ec",
            "17cd50cef9574d56bf8eb9f4794e19d8",
            "3b4ccee503fd43f19f8887d5a3962de7",
            "5c7c7415b6ca45fea6e5f63452207b25",
            "bccf630d2a65436b838f7726c1109b1e",
            "98cfbc45f14f4da998a6d8f324fc7b1a",
            "e0a42570ae114de9a4d53cd209a8574e",
            "8087563759ab44c8b66d7bab09103697",
            "da9593e9e3c04f379391259c85382ff9",
            "95a5ab8b28684e7eb42499da6de2fc9c",
            "79bb0bbaacce42e3ac8deaf7c8a1f39a",
            "bc575c98deb043be9ab0901d966975bb",
            "a1deb270e67d4b959db8afe2c853ff2d",
            "496177a7e13648948e0445a91b9572b6",
            "37d9cdfebf10499c8c55a758c0847262",
            "85973680954c4d1eacc238c2a4744a20",
            "cd7577506f874e5483691060ddea6b0e",
            "19e90fe7d0034eb1b0277f0bfb66406a",
            "51b34d4f50664371b6ee9aeba572a24d",
            "7feef861eb6149eda65deaebc5fa536e",
            "aa78b59e548c47e58541e1de6e169102",
            "4e2aef6787194420a6e61413ea46b3f3",
            "281c54b7e93d4cae9ec4613af3a57390",
            "9cdf589279054d65b0573e43f30a429e",
            "09ca192368f947f0bf7fe3666b548d2d",
            "389b033a392b4b4385069e57e137f216",
            "094b2e05d5c944e393a2b8ec04938757",
            "28b13c389e934e99963a8879880ddf05",
            "82d2dd77b93246f5a48b28280850d904",
            "a8171f53498d4b18a69049787a176299",
            "f1537e86652e435aa72a62a826968c05"
          ]
        },
        "id": "rF7lv8jvq-ut",
        "outputId": "1370235b-5806-4396-c73b-dbce011fb0fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/14.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6722d76e201459a9ccc440436f000b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/895 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12cdb95084dd4f279515029111052d83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset lotte_passages/lifestyle to /root/.cache/huggingface/datasets/colbertv2___lotte_passages/lifestyle/1.1.0/8b0c6ee8d8641c804a3303248f83c490c72ec2df6c97cb48a9eb9caeea9d2fa0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "420b019cbcce4ad2b1bda07fead12975"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acd2fff0e3a5482687339035db8196ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/110M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "894f77f5c21845649a52a3e696738743"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "127da1beda60416f8dd5a265f99709d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating dev_collection split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40310cf9d3a2492d89c32c12c266d281"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test_collection split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20dfa854644f43c7a42b3809f875fc32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset lotte_passages downloaded and prepared to /root/.cache/huggingface/datasets/colbertv2___lotte_passages/lifestyle/1.1.0/8b0c6ee8d8641c804a3303248f83c490c72ec2df6c97cb48a9eb9caeea9d2fa0. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae7ae52805054419b99ad4320142add8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1e389a3ed44437294e4deb7e76af347"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/533 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5e8318e39e4427daa9a90b948ae6633"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset lotte/lifestyle to /root/.cache/huggingface/datasets/colbertv2___lotte/lifestyle/1.1.0/1d2984a7d651f29dfe45f7023a0c3b76d44bdef8ccbebdfa3a9522fb5cdbc7df...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfdb3dd7e48a4d8b80b0920921d8a0ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b66fd86eb69f47e9be103de911532fed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/393k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bfcdd45822e47998abf163fad0944b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/136k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7183fe72b89340268ad6d40818c56b8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/211k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f42ebf40950e45249f58efc8ed7f8b0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c45e8442e79b4988beee9eb38fd5540c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating forum_dev split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f93bba205de4169a5c626209afdc3c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating examples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating forum_test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec42a1277cf7461d82f97a228b520310"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating examples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating search_dev split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1ccc48f22f2452c91aac8f4db6bb93a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating examples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating search_test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95a5ab8b28684e7eb42499da6de2fc9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating examples\n",
            "Dataset lotte downloaded and prepared to /root/.cache/huggingface/datasets/colbertv2___lotte/lifestyle/1.1.0/1d2984a7d651f29dfe45f7023a0c3b76d44bdef8ccbebdfa3a9522fb5cdbc7df. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa78b59e548c47e58541e1de6e169102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Loaded 417 queries and 268,881 passages'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = 'lifestyle'\n",
        "datasplit = 'dev'\n",
        "\n",
        "collection_dataset = load_dataset(\"colbertv2/lotte_passages\", dataset)\n",
        "collection = [x['text'] for x in collection_dataset[datasplit + '_collection']]\n",
        "\n",
        "queries_dataset = load_dataset(\"colbertv2/lotte\", dataset)\n",
        "queries = [x['query'] for x in queries_dataset['search_' + datasplit]]\n",
        "\n",
        "f'Loaded {len(queries)} queries and {len(collection):,} passages'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXKC1oeUsnhk"
      },
      "source": [
        "This loaded 417 queries and 269k passages. Let's inspect one query and one passage to verify we have done so correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQFUHYTZs0aa",
        "outputId": "22f80aad-b6de-4df4-ffde-dd4b7718d78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "are blossom end rot tomatoes edible?\n",
            "\n",
            "I think the spraying thing is not after, it's during. The cold will freeze the mist, keeping the air around the trees at (but not below) freezing. See http://www.ehow.com/how_5805520_use-freeze-damage-fruit-trees.html for example which recommends a sprinkler. The \"releases heat\" thing is kind of an oversimplification, but basically as long as you have any liquid water around, it will keep things at zero. The sap of your tree is not pure water, and therefore freezes somewhat below zero. By having the water freeze instead you stay away from the temps that would damage your plants. That said, http://www.ehow.com/how-does_5245655_spraying-frost-protect-fruit-freezing_.html is total gibberish since evaporation doesn't generate heat, quite the opposite. There is a better explanation at http://www.gardenguides.com/135830-spray-water-plants-during-frost.html This is a picture from a blog entry that gives you details from the citrus farmer's point of view.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(queries[24])\n",
        "print()\n",
        "print(collection[19929])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "For an efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "max_id = 10000\n",
        "\n",
        "index_name = f'{dataset}.{datasplit}.{nbits}bits'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsImrM-rAUzi"
      },
      "source": [
        "To save space and time, we will only run the `Indexer` on the first 10,000 passages. To do so, we will filter out queries that do not contain passages with ids less than 10,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kDkbZCvY_f7x",
        "outputId": "16b2c417-4fdd-447c-8fa6-0a67486d7c94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Filtered down to 20 queries'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "answer_pids = [x['answers']['answer_pids'] for x in queries_dataset['search_' + datasplit]]\n",
        "filtered_queries = [q for q, apids in zip(queries, answer_pids) if any(x < max_id for x in apids)]\n",
        "\n",
        "f'Filtered down to {len(filtered_queries)} queries'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "Now run the `Indexer` on the collection subset. Assuming the use of only one GPU, this cell should take about six minutes to finish running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRiOnzxtwI0j",
        "outputId": "ddcd7c74-81fc-428d-b34a-3cdeb868c58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[Jun 28, 22:27:33] #> Creating directory /content/experiments/notebook/indexes/lifestyle.dev.2bits \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "#> Joined...\n"
          ]
        }
      ],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "                                                                                # Consider larger numbers for small datasets.\n",
        "\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=collection[:max_id], overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CTbP2LS1xHVZ",
        "outputId": "e20fed9f-14e8-4028-ec3b-34bba0a960c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/experiments/notebook/indexes/lifestyle.dev.2bits'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "indexer.get_index() # You can get the absolute path of the index, if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## Search\n",
        "\n",
        "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
        "\n",
        "We can use the `queries` set we loaded earlier â€” or you can supply your own questions. Feel free to get creative! But keep in mind this set of ~300k lifestyle passages can only answer a small, focused set of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3x_FnVnyB0n",
        "outputId": "739b356b-cc76-4b30-8e1e-a150d8719cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Jun 28, 22:34:54] #> Loading codec...\n",
            "[Jun 28, 22:34:54] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jun 28, 22:34:54] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jun 28, 22:34:55] #> Loading IVF...\n",
            "[Jun 28, 22:34:55] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1261.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Jun 28, 22:34:55] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.74it/s]\n"
          ]
        }
      ],
      "source": [
        "# To create the searcher using its relative name (i.e., not a full path), set\n",
        "# experiment=value_used_for_indexing in the RunConfig.\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "    searcher = Searcher(index=index_name, collection=collection)\n",
        "\n",
        "\n",
        "# If you want to customize the search latency--quality tradeoff, you can also supply a\n",
        "# config=ColBERTConfig(ncells=.., centroid_score_threshold=.., ndocs=..) argument.\n",
        "# The default settings with k <= 10 (1, 0.5, 256) gives the fastest search,\n",
        "# but you can gain more extensive search by setting larger values of k or\n",
        "# manually specifying more conservative ColBERTConfig settings (e.g. (4, 0.4, 4096))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JYA0N22yIeS",
        "outputId": "7dda63ff-24e3-407b-fd2d-db74b8b27f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#> are some cats just skinny?\n",
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . are some cats just skinny?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2024,  2070,  8870,  2074, 15629,  1029,   102,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103])\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "\t [1] \t\t 24.1 \t\t A cat can certainly be naturally skinny. I know one who was the runt of her litter and has been extremely thin all her life, to the point where you can easily count every bone. She is now 17 years old, having outlived two other cats in that household, so it certainly doesn't seem to have held her back.\n",
            "\t [2] \t\t 24.0 \t\t Yes. Just like us, cats vary in size and shape and weight. And like us, some of that is diet, some is health, some is genetics, some is age. One of my lady's cats was the runt of the litter; it wasn't certain she would survive, and she has always been both small and skinny. It hasn't seemed to limit her climbing/jumping much, if at all; I think she benefits from square/cube law to be stronger relative to her weight than you would expect. Another cat in the family was not only longer/taller/broader but also more solidly muscled. I think he may have weighed twice what the small one did, without being overweight. (The simplest rule-of-thumb test for whether a cat is overweight: if you can count every vertibra as you run your hand down their back, they're fine. If some or all of them are hard to feel through the skin, there's probably more fat there than there should be. The runt really is below ideal weight; not only can you feel every bone in her back, it's somewhat hard to believe there are muscles and tendons there. Especially true now that she's a senior catizen.)\n",
            "\t [3] \t\t 22.1 \t\t Thank you for your question. You have a few different issues within your one question, but let's start with your main question regarding cats eating each other's food. I have the same problem with my cats. I have one cat who gorges herself, and I have one skinny cat who likes to graze all day long. What do you do with this situation? It is not easy. You have to get a little creative. And unfortunately you will likely never get to the point where they will only eat their own food. However, you can get to a point where they mostly eat their own food. One thing to note is that cats tend to be dominant over certain things. Cats are not usually dominant over everything, like dogs tend to be. Some cats are dominant over food, which it sounds like your cat is. My thin cat will not eat unless my overweight cat is eating. You can see the obvious dominance there. Here are some ideas for you... KEEPING CATS FROM EATING EACH OTHER'S FOOD I feed my cats wet food at night. I feed the slightly overweight cat in one place and my thin cat on a high shelf where she can see the other cat eating. During the day, I feed my overweight cat using an interactive feeder, so she has to use her natural instincts to work for her food. I give my skinny cat a small bowl of dry food on another shelf, so she can graze. It's not that my overweight cat can't get to the other food, but she is content to eat her easier-to-get food. Each morning, I also feed my skinny cat dry food in our closet, which is at the end of our walk-in bathroom. So there is quite a buffer from the bathroom door. I turn on the fan, so the other cat (who is closed out) cannot hear my skinny cat crunching. The things we do for our cats! I also sneak my skinny cat treats now and then. CATS EATING DOG FOOD This is a real problem for inside cats. You have to stop this. Even though your cats may be healthy now, eating dog food will make any cat overweight and unhealthy. I have volunteered at an animal shelter for many years and have seen this problem over and over again with overweight cats given to the shelter. Feed your dog at times you can supervise and take up the bowl when your dog is finished. If you kennel your dog, feed him in his kennel, so the cats can't get to the food. CAN CATS EAT VEGAN FOOD This is the most difficult part of your question because I can tell you have strong convictions in this area. However, I must tell you that cats should not ever be on a vegan diet. I am very sorry. I know this is difficult to hear. Cats were built to be obligate carnivores. They must have the taurine from the meat because they cannot produce it on their own, like dogs can. I know some vegan pet foods have synthetic taurine, but I wouldn't chance this. It is just not the same. Cats on a vegan diet will likely have severe medical conditions. See the quote below from Pet WebMD If allowed to continue long enough, these dietary problems can lead to serious and sometimes irreversible medical conditions. The one veterinarians mention most often is taurine-related dilated cardiomyopathy (enlarged heart with weak contractions and poor pumping ability). Low taurine can also lead to reproductive failures, growth failures, and eye problems. I hope this helps and at least gives you some ideas. I wish you and your animal family the best!\n"
          ]
        }
      ],
      "source": [
        "query = filtered_queries[13] # try with an in-range query or supply your own\n",
        "print(f\"#> {query}\")\n",
        "\n",
        "# Find the top-3 passages for this query\n",
        "results = searcher.search(query, k=3)\n",
        "\n",
        "# Print out the top-k retrieved passages\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
      ]
    },
    {
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from ragatouille.utils import get_wikipedia_page\n",
        "\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "my_documents = [get_wikipedia_page(\"Hayao_Miyazaki\"), get_wikipedia_page(\"Studio_Ghibli\")]\n",
        "index_path = RAG.index(index_name=\"my_index\", collection=my_documents)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_5PiKWY5hTUM",
        "outputId": "4c024121-57ff-4c95-fc3a-028638c42684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-545d48bc11df>:1: UserWarning: \n",
            "********************************************************************************\n",
            "RAGatouille WARNING: Future Release Notice\n",
            "--------------------------------------------\n",
            "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
            "instead of the current Stanford ColBERT backend.\n",
            "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
            "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
            "********************************************************************************\n",
            "  from ragatouille import RAGPretrainedModel\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-545d48bc11df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_wikipedia_page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"colbert-ir/colbertv2.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_wikipedia_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hayao_Miyazaki\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wikipedia_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Studio_Ghibli\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.0.9post2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGPretrainedModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGTrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/RAGPretrainedModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mRAGatouilleLangChainRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"LateInteractionModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ColBERT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/colbert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msearcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_updater\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexUpdater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/training/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank_batcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRerankBatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "!pip install ragatouille==0.0.9.post2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jFb0tGWLhYcR",
        "outputId": "95d6cbf8-46e3-4102-d86c-f0fbf88cdb20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille==0.0.9.post2 in /usr/local/lib/python3.11/dist-packages (0.0.9.post2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.11.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.53)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.19)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.20)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.18.0)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.4.8)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.6.0+cu124)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.7.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.1)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.11.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.51.3)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (5.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille==0.0.9.post2) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (23.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans->ragatouille==0.0.9.post2) (12.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.6.7)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille==0.0.9.post2) (1.33)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.42)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->ragatouille==0.0.9.post2) (5.29.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (11.2.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly->ragatouille==0.0.9.post2) (2.0.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille==0.0.9.post2) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (1.0.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.78.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (2.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.1.19)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (5.5.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.6.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille==0.0.9.post2) (3.2.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (0.70.15)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.3)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.44)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->fast-pytorch-kmeans->ragatouille==0.0.9.post2) (12.575.51)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->ragatouille==0.0.9.post2) (3.6.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.7.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (0.16.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.22 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.6.22)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (1.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install RAGatouille==0.0.6"
      ],
      "metadata": {
        "id": "4FhJ_Q9cgvuZ",
        "outputId": "6cc9cfa3-23c9-4b3d-9c9e-cb69649a5373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement RAGatouille==0.0.6 (from versions: 0.0.1a0, 0.0.1b0, 0.0.1rc0, 0.0.2a0, 0.0.2b0, 0.0.2b1, 0.0.3a1, 0.0.3a2, 0.0.4a0, 0.0.4a2, 0.0.4a3, 0.0.4a4, 0.0.4b0, 0.0.4b1, 0.0.4b2, 0.0.5a0, 0.0.5a1, 0.0.5a2, 0.0.5b1, 0.0.5b2, 0.0.5b3, 0.0.6a1, 0.0.6a2, 0.0.6b0, 0.0.6b1, 0.0.6b2, 0.0.6b3, 0.0.6b4, 0.0.6b5, 0.0.6rc0, 0.0.6rc1, 0.0.6rc2, 0.0.7.post1, 0.0.7.post2, 0.0.7.post3, 0.0.7.post4, 0.0.7.post5, 0.0.7.post6, 0.0.7.post7, 0.0.7.post8, 0.0.7.post9, 0.0.7.post10, 0.0.7.post11, 0.0.8, 0.0.8.post1, 0.0.8.post2, 0.0.8.post3, 0.0.8.post4, 0.0.9, 0.0.9.post2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for RAGatouille==0.0.6\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pypi.org/project/RAGatouille/"
      ],
      "metadata": {
        "id": "mgnVawGQg0uh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZsix9xahg5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install \"ragatouille<0.0.10\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bEDxTHLDhhVS",
        "outputId": "5d51f718-5568-4a95-ad09-cde20d0e8c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille<0.0.10 in /usr/local/lib/python3.11/dist-packages (0.0.9.post2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (0.12.37)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (1.11.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (0.1.53)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (0.2.19)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (0.1.20)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (1.18.0)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (2.4.8)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (2.6.0+cu124)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (0.2.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from ragatouille<0.0.10) (2.7.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (3.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (3.1.1)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (1.11.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (4.51.3)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille<0.0.10) (5.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille<0.0.10) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille<0.0.10) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille<0.0.10) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille<0.0.10) (23.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans->ragatouille<0.0.10) (12.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (0.6.7)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (0.0.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille<0.0.10) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille<0.0.10) (1.33)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.4.7)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.3.42)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille<0.0.10) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->ragatouille<0.0.10) (5.29.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille<0.0.10) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille<0.0.10) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille<0.0.10) (11.2.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly->ragatouille<0.0.10) (2.0.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille<0.0.10) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille<0.0.10) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille<0.0.10) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille<0.0.10) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille<0.0.10) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille<0.0.10) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille<0.0.10) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille<0.0.10) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille<0.0.10) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille<0.0.10) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille<0.0.10) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille<0.0.10) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille<0.0.10) (1.0.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (1.78.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (2.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (1.6.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille<0.0.10) (0.1.19)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (5.5.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille<0.0.10) (0.6.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille<0.0.10) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille<0.0.10) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille<0.0.10) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille<0.0.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille<0.0.10) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille<0.0.10) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille<0.0.10) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille<0.0.10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille<0.0.10) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille<0.0.10) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille<0.0.10) (3.2.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille<0.0.10) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille<0.0.10) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille<0.0.10) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille<0.0.10) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille<0.0.10) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille<0.0.10) (0.70.15)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille<0.0.10) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille<0.0.10) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille<0.0.10) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille<0.0.10) (3.1.3)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai>=0.2.19->ragatouille<0.0.10) (3.1.44)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->fast-pytorch-kmeans->ragatouille<0.0.10) (12.575.51)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->ragatouille<0.0.10) (3.6.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (1.7.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille<0.0.10) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille<0.0.10) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->ragatouille<0.0.10) (0.16.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.22 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille<0.0.10) (0.6.22)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (1.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ragatouille<0.0.10) (1.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai>=0.2.19->ragatouille<0.0.10) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai>=0.2.19->ragatouille<0.0.10) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille<0.0.10) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille<0.0.10) (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel"
      ],
      "metadata": {
        "id": "Goa9endagqwx",
        "outputId": "9cb45bb0-4c2d-4490-c983-84d45fd94d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-75aaf6cf8545>:1: UserWarning: \n",
            "********************************************************************************\n",
            "RAGatouille WARNING: Future Release Notice\n",
            "--------------------------------------------\n",
            "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
            "instead of the current Stanford ColBERT backend.\n",
            "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
            "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
            "********************************************************************************\n",
            "  from ragatouille import RAGPretrainedModel\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-75aaf6cf8545>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.0.9post2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGPretrainedModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGTrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/RAGPretrainedModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mRAGatouilleLangChainRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"LateInteractionModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ColBERT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/colbert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msearcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_updater\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexUpdater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/training/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank_batcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRerankBatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from ragatouille.utils import get_wikipedia_page\n",
        "\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "my_documents = [get_wikipedia_page(\"Hayao_Miyazaki\"), get_wikipedia_page(\"Studio_Ghibli\")]\n",
        "index_path = RAG.index(index_name=\"my_index\", collection=my_documents)"
      ],
      "metadata": {
        "id": "qvfUApoaglYp",
        "outputId": "efc72739-d37d-4465-9cec-e065b08de862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-545d48bc11df>:1: UserWarning: \n",
            "********************************************************************************\n",
            "RAGatouille WARNING: Future Release Notice\n",
            "--------------------------------------------\n",
            "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
            "instead of the current Stanford ColBERT backend.\n",
            "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
            "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
            "********************************************************************************\n",
            "  from ragatouille import RAGPretrainedModel\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-545d48bc11df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_wikipedia_page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"colbert-ir/colbertv2.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_wikipedia_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hayao_Miyazaki\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wikipedia_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Studio_Ghibli\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.0.9post2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGPretrainedModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGTrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/RAGPretrainedModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mRAGatouilleLangChainRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"LateInteractionModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ColBERT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/colbert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msearcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_updater\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexUpdater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/training/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank_batcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRerankBatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ipython-input-21-686d804d4597>:1: UserWarning:\n",
        "\n",
        "\n",
        "********************************************************************************\n",
        "RAGatouille WARNING: Future Release Notice\n",
        "--------------------------------------------\n",
        "RAGatouille version 0.0.10 will be migrating to a PyLate backend\n",
        "instead of the current Stanford ColBERT backend.\n",
        "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
        "However, please pin version <0.0.10 if you require the Stanford ColBERT backend."
      ],
      "metadata": {
        "id": "M0pDXS_Xf5Fi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Minj2SV59-9O"
      },
      "source": [
        "# 3. Assembling it all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n11zYRfn9-9O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "2b822b64-a5ef-47fe-e5dc-09b9fa6c1159"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RAGPretrainedModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-174efff23007>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mknowledge_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mreranker\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnum_retrieved_docs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnum_docs_final\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RAGPretrainedModel' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import Pipeline\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMEgfY8PcrDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install -U ragatouille"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-40pIu7Vcra_",
        "outputId": "6d4d9108-a774-4b6e-a46f-5e711011fd01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille in /usr/local/lib/python3.11/dist-packages (0.0.9.post2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.12.37)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from ragatouille) (1.11.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.59)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.2.21)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.25)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from ragatouille) (1.18.0)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.5.1)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.6.0+cu124)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from ragatouille) (4.1.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.1.1)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.11.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.51.3)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille) (5.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille) (24.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille) (1.33)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.4.7)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.3.42)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->ragatouille) (5.29.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille) (11.2.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly->ragatouille) (2.0.10)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragatouille) (0.23.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille) (1.78.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.6.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille) (0.1.19)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (5.5.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille) (0.6.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille) (3.2.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.70.15)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (3.1.3)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai>=0.2.19->ragatouille) (3.1.44)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->ragatouille) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.20.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.7.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->ragatouille) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->ragatouille) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->ragatouille) (0.16.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.22 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille) (0.6.22)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille) (1.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (3.26.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai>=0.2.19->ragatouille) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai>=0.2.19->ragatouille) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index->ragatouille) (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import Pipeline\n",
        "from ragatouille import RAGPretrainedModel # Import RAGPretrainedModel here\n",
        "from typing import Optional, List, Tuple # Import List and Optional, Tuple from typing\n",
        "from langchain.docstore.document import Document as LangchainDocument # Import LangchainDocument here\n",
        "from langchain.vectorstores import FAISS # Import FAISS here\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    # Assuming RAG_PROMPT_TEMPLATE is defined globally or in an accessible scope\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BmgbaNr5ci9w",
        "outputId": "d1f43b2c-2cb6-4172-9271-f09016a20ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e701865208d1>:2: UserWarning: \n",
            "********************************************************************************\n",
            "RAGatouille WARNING: Future Release Notice\n",
            "--------------------------------------------\n",
            "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
            "instead of the current Stanford ColBERT backend.\n",
            "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
            "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
            "********************************************************************************\n",
            "  from ragatouille import RAGPretrainedModel # Import RAGPretrainedModel here\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e701865208d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m \u001b[0;31m# Import RAGPretrainedModel here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m \u001b[0;31m# Import List and Optional, Tuple from typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLangchainDocument\u001b[0m \u001b[0;31m# Import LangchainDocument here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m \u001b[0;31m# Import FAISS here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.0.9post2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGPretrainedModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGPretrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRAGTrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/RAGPretrainedModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mRAGatouilleLangChainRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragatouille\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLateInteractionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"LateInteractionModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ColBERT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragatouille/models/colbert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msearcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_updater\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexUpdater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colbert/training/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColBERTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank_batcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRerankBatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nA4nwRQ9-9P"
      },
      "source": [
        "Let's see how our RAG pipeline answers a user query."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Y1BCPgKh_O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install a compatible transformers version first\n",
        "!pip install transformers==4.28.0\n",
        "\n",
        "# Then install the specific ragatouille version\n",
        "!pip install ragatouille==0.0.9.post2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rm7B0BNQh_sj",
        "outputId": "d7ace6bb-215c-490b-9a11-cbafcca11d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/110.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (2025.4.26)\n",
            "Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 2.7.0 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "91cb072d6b554033812a2ae9cbf1e29f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille==0.0.9.post2 in /usr/local/lib/python3.11/dist-packages (0.0.9.post2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.11.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.53)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.19)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.1.20)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (1.18.0)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.4.8)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.6.0+cu124)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (0.2.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from ragatouille==0.0.9.post2) (2.7.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (3.1.1)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.11.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (4.28.0)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2) (5.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille==0.0.9.post2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille==0.0.9.post2) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->ragatouille==0.0.9.post2) (23.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans->ragatouille==0.0.9.post2) (12.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.6.7)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragatouille==0.0.9.post2) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core->ragatouille==0.0.9.post2) (1.33)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.12.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.42)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ragatouille==0.0.9.post2) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->ragatouille==0.0.9.post2) (5.29.4)\n",
            "Collecting transformers (from colbert-ai>=0.2.19->ragatouille==0.0.9.post2)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->ragatouille==0.0.9.post2) (11.2.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly->ragatouille==0.0.9.post2) (2.0.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->ragatouille==0.0.9.post2) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->ragatouille==0.0.9.post2) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragatouille==0.0.9.post2) (1.0.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (1.78.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (2.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.6.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.36->llama-index->ragatouille==0.0.9.post2) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.1.19)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (5.5.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9.post2) (0.6.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9.post2) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain->ragatouille==0.0.9.post2) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain->ragatouille==0.0.9.post2) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragatouille==0.0.9.post2) (3.2.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 174, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 162, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZTC1FtX9-9P"
      },
      "outputs": [],
      "source": [
        "question = \"how to create a pipeline object?\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(\n",
        "    question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwW0oqhZ9-9P"
      },
      "outputs": [],
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6iNo7lY9-9S"
      },
      "source": [
        "âœ… We now have a fully functional, performant RAG system. That's it for today! Congratulations for making it to the end ğŸ¥³\n",
        "\n",
        "\n",
        "# To go further ğŸ—ºï¸\n",
        "\n",
        "This is not the end of the journey! You can try many steps to improve your RAG system. We recommend doing so in an iterative way: bring small changes to the system and see what improves performance.\n",
        "\n",
        "### Setting up an evaluation pipeline\n",
        "\n",
        "- ğŸ’¬ \"You cannot improve the model performance that you do not measure\", said Gandhi... or at least Llama2 told me he said it. Anyway, you should absolutely start by measuring performance: this means building a small evaluation dataset, and then monitor the performance of your RAG system on this evaluation dataset.\n",
        "\n",
        "### Improving the retriever\n",
        "\n",
        "ğŸ› ï¸ __You can use these options to tune the results:__\n",
        "\n",
        "- Tune the chunking method:\n",
        "    - Size of the chunks\n",
        "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
        "- Change the embedding model\n",
        "\n",
        "ğŸ‘·â€â™€ï¸ __More could be considered:__\n",
        "- Try another chunking method, like semantic chunking\n",
        "- Change the index used (here, FAISS)\n",
        "- Query expansion: reformulate the user query in slightly different ways to retrieve more documents.\n",
        "\n",
        "### Improving the reader\n",
        "\n",
        "ğŸ› ï¸ __Here you can try the following options to improve results:__\n",
        "- Tune the prompt\n",
        "- Switch reranking on/off\n",
        "- Choose a more powerful reader model\n",
        "\n",
        "ğŸ’¡ __Many options could be considered here to further improve the results:__\n",
        "- Compress the retrieved context to keep only the most relevant parts to answer the query.\n",
        "- Extend the RAG system to make it more user-friendly:\n",
        "    - cite source\n",
        "    - make conversational"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c92afbcf51b64d0e946e920bfddad38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ccb7ea417bd4371adeed10bb7809673",
              "IPY_MODEL_b20ff3fd1bb347618cfeacacf0493bd4",
              "IPY_MODEL_cacc7cabba454ead9c5020a625dfa338"
            ],
            "layout": "IPY_MODEL_2c99550d032f4a5281de616f2b1ed1f6"
          }
        },
        "5ccb7ea417bd4371adeed10bb7809673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2513e870326845339341141dafcb428e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1e9d881821c740fda788b4d8a20be00c",
            "value": "100%"
          }
        },
        "b20ff3fd1bb347618cfeacacf0493bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bc4a0e7f6ef43e0b916dac3da75925f",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cba8586519714de68ec14df9dfa1c15d",
            "value": 2647
          }
        },
        "cacc7cabba454ead9c5020a625dfa338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a88baee21ed4cd2b9e683c88f33954a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b39d7eb01b0b447b8fa31c0fd039ef1b",
            "value": "â€‡2647/2647â€‡[00:00&lt;00:00,â€‡12405.07it/s]"
          }
        },
        "2c99550d032f4a5281de616f2b1ed1f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2513e870326845339341141dafcb428e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e9d881821c740fda788b4d8a20be00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bc4a0e7f6ef43e0b916dac3da75925f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba8586519714de68ec14df9dfa1c15d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a88baee21ed4cd2b9e683c88f33954a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b39d7eb01b0b447b8fa31c0fd039ef1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fa22cd3a3a142ffaf0953a358256bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40113e410df44dc48b524e77dc2a930a",
              "IPY_MODEL_f9205b86e9e84b888722224c5c12b63e",
              "IPY_MODEL_732e26a898b34458a1e25a206fdd96a5"
            ],
            "layout": "IPY_MODEL_90d7d9c827554de3a876c3791f9a0a14"
          }
        },
        "40113e410df44dc48b524e77dc2a930a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_861db66f8a4e46628d9877d2a521e79c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_033d139f1a7e4df4b8ac13bc5d929271",
            "value": "modules.json:â€‡100%"
          }
        },
        "f9205b86e9e84b888722224c5c12b63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac8bfc67b1f54b0c960d2d664ba7fd17",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6d019c7f36841a399a82cf62c666553",
            "value": 385
          }
        },
        "732e26a898b34458a1e25a206fdd96a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a95d67ea9e4745c2af96705ece91885e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_97c860e8d11c4edd8277e9289582282f",
            "value": "â€‡385/385â€‡[00:00&lt;00:00,â€‡36.4kB/s]"
          }
        },
        "90d7d9c827554de3a876c3791f9a0a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "861db66f8a4e46628d9877d2a521e79c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033d139f1a7e4df4b8ac13bc5d929271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac8bfc67b1f54b0c960d2d664ba7fd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d019c7f36841a399a82cf62c666553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a95d67ea9e4745c2af96705ece91885e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97c860e8d11c4edd8277e9289582282f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46319f2020c4484c87fd0ee044d11154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877d4a725a944be4b7a5885885973750",
              "IPY_MODEL_89364c7bb0ad4af98c3c4beedf6c8310",
              "IPY_MODEL_0da3d95ac0bc4d2399d9c0cad4f998f7"
            ],
            "layout": "IPY_MODEL_84a2c079c32b41e7bdc4351db25a881c"
          }
        },
        "877d4a725a944be4b7a5885885973750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68224d3707e4602b40fb5ea6cda4c13",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a55c621ce4a844b7ba4989e51feb06d8",
            "value": "README.md:â€‡100%"
          }
        },
        "89364c7bb0ad4af98c3c4beedf6c8310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d83ff7370df64a238653c81d8e6c74a5",
            "max": 68084,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d14e34c7bc14417fa0a8d76509c13741",
            "value": 68084
          }
        },
        "0da3d95ac0bc4d2399d9c0cad4f998f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98b05baf2dfe4fc5a8c90b1c27a07717",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_feff46742d4e4252b69731f7bc957670",
            "value": "â€‡68.1k/68.1kâ€‡[00:00&lt;00:00,â€‡7.18MB/s]"
          }
        },
        "84a2c079c32b41e7bdc4351db25a881c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68224d3707e4602b40fb5ea6cda4c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a55c621ce4a844b7ba4989e51feb06d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d83ff7370df64a238653c81d8e6c74a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d14e34c7bc14417fa0a8d76509c13741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98b05baf2dfe4fc5a8c90b1c27a07717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feff46742d4e4252b69731f7bc957670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6433164578154666a197c63079c4704d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d14a420d791946f39f437fbb4176bf69",
              "IPY_MODEL_c317c8747f514579bb6469f3e6c644d9",
              "IPY_MODEL_7432d11752d04280b063c798061e6ff8"
            ],
            "layout": "IPY_MODEL_3f70dc5340a440caadc32dc5ee1ae94c"
          }
        },
        "d14a420d791946f39f437fbb4176bf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_438613c7eae14075be433d9a72c70edc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_351a3ddc6c37490d95d7556927c4d573",
            "value": "sentence_bert_config.json:â€‡100%"
          }
        },
        "c317c8747f514579bb6469f3e6c644d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f5d203fd0b94266870b36159c7d9f59",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6470dc9ad674bc89a4f3a91e12aa269",
            "value": 57
          }
        },
        "7432d11752d04280b063c798061e6ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e72aa9440380409d8c5255b939455057",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2b85407534af473db459161367bf52eb",
            "value": "â€‡57.0/57.0â€‡[00:00&lt;00:00,â€‡6.30kB/s]"
          }
        },
        "3f70dc5340a440caadc32dc5ee1ae94c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438613c7eae14075be433d9a72c70edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351a3ddc6c37490d95d7556927c4d573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f5d203fd0b94266870b36159c7d9f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6470dc9ad674bc89a4f3a91e12aa269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e72aa9440380409d8c5255b939455057": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b85407534af473db459161367bf52eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6680c52c24b44318832f13507688580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd2eb63ea9394fd6b0d1cbe5d052f19e",
              "IPY_MODEL_bc7ce257adee4c3491a478e4117e8dea",
              "IPY_MODEL_199055a7b6ca43fe90dfd7c9fcdf5744"
            ],
            "layout": "IPY_MODEL_ac88f488ca554fcfaa06cd25a62134a8"
          }
        },
        "cd2eb63ea9394fd6b0d1cbe5d052f19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10825a808ecf43f4bfd84e2e6779fdd0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d26a7f1cabeb4fb29a68b0649a5b596a",
            "value": "config.json:â€‡100%"
          }
        },
        "bc7ce257adee4c3491a478e4117e8dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ee6d36ebcb344818988e2837d4c4202",
            "max": 583,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_847d2f0c03d541d7942dc3429cb908ad",
            "value": 583
          }
        },
        "199055a7b6ca43fe90dfd7c9fcdf5744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32089ee021cf456381edde2a6c28adcd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_073f0aee47a64818864ed9eca235f646",
            "value": "â€‡583/583â€‡[00:00&lt;00:00,â€‡62.5kB/s]"
          }
        },
        "ac88f488ca554fcfaa06cd25a62134a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10825a808ecf43f4bfd84e2e6779fdd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26a7f1cabeb4fb29a68b0649a5b596a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ee6d36ebcb344818988e2837d4c4202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847d2f0c03d541d7942dc3429cb908ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32089ee021cf456381edde2a6c28adcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "073f0aee47a64818864ed9eca235f646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db6ec575efaa4866833ffa7df4898d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c1804a77a264112a4625d3b3ebc9eff",
              "IPY_MODEL_d31e87be2bdc40aa9e2c11110ead05ce",
              "IPY_MODEL_9a2654c2dfd24fc7a8f1daf27a68be43"
            ],
            "layout": "IPY_MODEL_9560ea595e1b4a18b773344030fc6017"
          }
        },
        "9c1804a77a264112a4625d3b3ebc9eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe74ab37d3e649c3bbf68fd74dab9bc4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e7d2aa5e2774529bdc02202c4cb546f",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "d31e87be2bdc40aa9e2c11110ead05ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d69628cd98cf4c78bdd3c20777ae1517",
            "max": 66746168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a02c01305cbe4b94ba12754b44ac3d8d",
            "value": 66746168
          }
        },
        "9a2654c2dfd24fc7a8f1daf27a68be43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3580abd2c32d4357892730148470a001",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_762273b4dee54ef49ed0570f69992349",
            "value": "â€‡66.7M/66.7Mâ€‡[00:00&lt;00:00,â€‡255MB/s]"
          }
        },
        "9560ea595e1b4a18b773344030fc6017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe74ab37d3e649c3bbf68fd74dab9bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e7d2aa5e2774529bdc02202c4cb546f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d69628cd98cf4c78bdd3c20777ae1517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a02c01305cbe4b94ba12754b44ac3d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3580abd2c32d4357892730148470a001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "762273b4dee54ef49ed0570f69992349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a057640b556c43549efd0c2c333891ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e3d18047c0c4f6685722bf2dc3c546f",
              "IPY_MODEL_dc4986b64a824501970527e66735cfdd",
              "IPY_MODEL_a739c8fe9ec74a489b9ebc2b4dfea733"
            ],
            "layout": "IPY_MODEL_e5f783adf1cf406e84013b2e53767b36"
          }
        },
        "5e3d18047c0c4f6685722bf2dc3c546f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc0c882af0794af698e353b51e154639",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_781710943d1e4ce3b2124307dc4fb634",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "dc4986b64a824501970527e66735cfdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d0178a713bf4268a86ce8e0744c5d5f",
            "max": 394,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad9df7c3eb9c4017b2baebd30cf978a5",
            "value": 394
          }
        },
        "a739c8fe9ec74a489b9ebc2b4dfea733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b6607a65b8e43cfaac9c32e2282fa2b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c695fa9b175743c7acc972490200dc88",
            "value": "â€‡394/394â€‡[00:00&lt;00:00,â€‡17.7kB/s]"
          }
        },
        "e5f783adf1cf406e84013b2e53767b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0c882af0794af698e353b51e154639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "781710943d1e4ce3b2124307dc4fb634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d0178a713bf4268a86ce8e0744c5d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad9df7c3eb9c4017b2baebd30cf978a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b6607a65b8e43cfaac9c32e2282fa2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c695fa9b175743c7acc972490200dc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c4be8013fe04dceb145dbed4ea414bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc92dcc01841465eb8d5e191b0fc5567",
              "IPY_MODEL_297bd42339a249c892586fe279f444fc",
              "IPY_MODEL_671b82d3d4e44a389424a5680479bf48"
            ],
            "layout": "IPY_MODEL_34946db0d59746d9acb78689c435c5bb"
          }
        },
        "bc92dcc01841465eb8d5e191b0fc5567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efe9d1eaa3524a1ebc9e4c64503e8239",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b4ba7f9d62e94885a7afd536f8717fc1",
            "value": "vocab.txt:â€‡100%"
          }
        },
        "297bd42339a249c892586fe279f444fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4addb452efe2476da0f2f9d24a14b811",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bccbff19155d4e568c5e7a474764cbb7",
            "value": 231508
          }
        },
        "671b82d3d4e44a389424a5680479bf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39c8713aa3564bdbbf537935f90bce30",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d24f05c1c3f44394afa51ed1be714e50",
            "value": "â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡8.91MB/s]"
          }
        },
        "34946db0d59746d9acb78689c435c5bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe9d1eaa3524a1ebc9e4c64503e8239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ba7f9d62e94885a7afd536f8717fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4addb452efe2476da0f2f9d24a14b811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bccbff19155d4e568c5e7a474764cbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39c8713aa3564bdbbf537935f90bce30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24f05c1c3f44394afa51ed1be714e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1af5e2fc153d4e318fa6e50c1887798f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fb2954f79764fd0ba84eec07ca048b1",
              "IPY_MODEL_ea41dc9ceacb478f85b9be965b5afe38",
              "IPY_MODEL_edf5479146d0442ab1120a2ec4f68fd5"
            ],
            "layout": "IPY_MODEL_90a7a948c2fd4706bbe74d702911aafa"
          }
        },
        "2fb2954f79764fd0ba84eec07ca048b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c604f230cf6f4ab9b661ca37e79184e7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_accfc18e92aa473a99220e39fade35c9",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "ea41dc9ceacb478f85b9be965b5afe38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5849382e11483ab66b58243230221c",
            "max": 711661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a34e7d19f934c529f0fb04a923dd74c",
            "value": 711661
          }
        },
        "edf5479146d0442ab1120a2ec4f68fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef75e56db1fc48d59f3fe4da72c5a9f2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_601b195ceab84040886859486565fa94",
            "value": "â€‡712k/712kâ€‡[00:00&lt;00:00,â€‡13.8MB/s]"
          }
        },
        "90a7a948c2fd4706bbe74d702911aafa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c604f230cf6f4ab9b661ca37e79184e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "accfc18e92aa473a99220e39fade35c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5849382e11483ab66b58243230221c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a34e7d19f934c529f0fb04a923dd74c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef75e56db1fc48d59f3fe4da72c5a9f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "601b195ceab84040886859486565fa94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f6a6b881d7d4c38ae42a97a22e01eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f7583fe51b546198ec901c2dec3b6ca",
              "IPY_MODEL_2fc63d0d9ad146c5a11fd96a8d265966",
              "IPY_MODEL_6fd92beb5c7e4c0781d40b090a723133"
            ],
            "layout": "IPY_MODEL_e26802d6860d4f49b37a586281320e8e"
          }
        },
        "7f7583fe51b546198ec901c2dec3b6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49ff2d6a04a147899a171c20fb51a4a0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9dcc78a1063a4d2ba6d62c3ad096db0f",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "2fc63d0d9ad146c5a11fd96a8d265966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff12cce13b664d35b83c7e54300d3df4",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6797225b28334783a8b2f8d2b15ad1e1",
            "value": 125
          }
        },
        "6fd92beb5c7e4c0781d40b090a723133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a494af70e84505b4b43b620e4c8ce0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d59115c0e9374144baa07282052ba903",
            "value": "â€‡125/125â€‡[00:00&lt;00:00,â€‡5.26kB/s]"
          }
        },
        "e26802d6860d4f49b37a586281320e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ff2d6a04a147899a171c20fb51a4a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dcc78a1063a4d2ba6d62c3ad096db0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff12cce13b664d35b83c7e54300d3df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6797225b28334783a8b2f8d2b15ad1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6a494af70e84505b4b43b620e4c8ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d59115c0e9374144baa07282052ba903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb85d9aef8be44ebb1186a58c501c54b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b394afb8c9dd44ca9619c73620650199",
              "IPY_MODEL_90da8f25f914451fa40491dec9d4c552",
              "IPY_MODEL_67d0f0a2453f4a73912a13006e031f3d"
            ],
            "layout": "IPY_MODEL_f1c37e9658e746e889ebef3db607e7be"
          }
        },
        "b394afb8c9dd44ca9619c73620650199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40734e8590e6413483cc96bbc659d61d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_13c7fbd39f30460d9a832edcbe7fd9c8",
            "value": "config.json:â€‡100%"
          }
        },
        "90da8f25f914451fa40491dec9d4c552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3571aa91285c4c19a2a947b4c3d4fc1c",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_089b3bd121b54b06a95eaf6c95f5c6db",
            "value": 190
          }
        },
        "67d0f0a2453f4a73912a13006e031f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43b46ac6580a43dd96581a7919cd7b5b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3f2c7a5ca1a549e89d0424bc64190f31",
            "value": "â€‡190/190â€‡[00:00&lt;00:00,â€‡3.26kB/s]"
          }
        },
        "f1c37e9658e746e889ebef3db607e7be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40734e8590e6413483cc96bbc659d61d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c7fbd39f30460d9a832edcbe7fd9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3571aa91285c4c19a2a947b4c3d4fc1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "089b3bd121b54b06a95eaf6c95f5c6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43b46ac6580a43dd96581a7919cd7b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f2c7a5ca1a549e89d0424bc64190f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2267dca102e346548fbaca4f5ef2f455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2235d81bd394df08bffd909a3a4991d",
              "IPY_MODEL_4b59ed049be84b25ad17725bbff25140",
              "IPY_MODEL_33807852497d448da2979f290da318b7"
            ],
            "layout": "IPY_MODEL_00fb0e954c8443bb89f55d34b9f53fd4"
          }
        },
        "b2235d81bd394df08bffd909a3a4991d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4594e84e4bf4af99cd66bba3f456c03",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7eb70d98bf6f4381843a96cc37d184bd",
            "value": "100%"
          }
        },
        "4b59ed049be84b25ad17725bbff25140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a36ac9a7444f40c98df94205cc268263",
            "max": 31085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8afae79c18f347a0b1a8dab34abf3849",
            "value": 31085
          }
        },
        "33807852497d448da2979f290da318b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_834b697beca14883b0cbddc9f19fb092",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7eb31c03fd61491dba8e556033aa3046",
            "value": "â€‡31085/31085â€‡[00:19&lt;00:00,â€‡1623.34it/s]"
          }
        },
        "00fb0e954c8443bb89f55d34b9f53fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4594e84e4bf4af99cd66bba3f456c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eb70d98bf6f4381843a96cc37d184bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a36ac9a7444f40c98df94205cc268263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8afae79c18f347a0b1a8dab34abf3849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "834b697beca14883b0cbddc9f19fb092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eb31c03fd61491dba8e556033aa3046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adf709a94f63457590fbd0f6ecc1dce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5de3ceed147446fabc1862fd9620c78",
              "IPY_MODEL_e3bf1dc095844bf7aadf67e188287c7a",
              "IPY_MODEL_757d7b2fb1984f388ca24ed1e329165d"
            ],
            "layout": "IPY_MODEL_61e8a3c008384612b1947cf92543d58e"
          }
        },
        "f5de3ceed147446fabc1862fd9620c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c8464c9a3a4b93a676c28bd1945b3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_907d4bb764554778a49fcceebaebc1d3",
            "value": "100%"
          }
        },
        "e3bf1dc095844bf7aadf67e188287c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72101cbe483b4d78b8b0c4244413a653",
            "max": 31085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75d8bf5808514831bde7329f500c5e92",
            "value": 31085
          }
        },
        "757d7b2fb1984f388ca24ed1e329165d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5ed4c3264a459fbc89c902ead0deb5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_edee2264920042ada4d9554f43ef6ebe",
            "value": "â€‡31085/31085â€‡[00:18&lt;00:00,â€‡1902.41it/s]"
          }
        },
        "61e8a3c008384612b1947cf92543d58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c8464c9a3a4b93a676c28bd1945b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "907d4bb764554778a49fcceebaebc1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72101cbe483b4d78b8b0c4244413a653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d8bf5808514831bde7329f500c5e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b5ed4c3264a459fbc89c902ead0deb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edee2264920042ada4d9554f43ef6ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1484f4dc9cae4b9d820f789c7b44f291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98cdf507861245ecae4bde37a21e6db0",
              "IPY_MODEL_0b30b713a8fa47bda943f1b49f138d3c",
              "IPY_MODEL_837c385289ca4ae3b0be5afb85853215"
            ],
            "layout": "IPY_MODEL_a4db75e28ffe4e5ebd448b1175db99db"
          }
        },
        "98cdf507861245ecae4bde37a21e6db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63d9cbff0334b859c885668bda47ec3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dcbe0862509d477ab3f440ddb9adb991",
            "value": "100%"
          }
        },
        "0b30b713a8fa47bda943f1b49f138d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7005f98fe22e4446a9978dc767afeac9",
            "max": 31085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b98f796b2f27410f90c8739d8141a95e",
            "value": 31085
          }
        },
        "837c385289ca4ae3b0be5afb85853215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_694eb29da59e4577a48bebd7eb8236a8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ba8cce4a12974d288000549afa143e7a",
            "value": "â€‡31085/31085â€‡[00:19&lt;00:00,â€‡1771.34it/s]"
          }
        },
        "a4db75e28ffe4e5ebd448b1175db99db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b63d9cbff0334b859c885668bda47ec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcbe0862509d477ab3f440ddb9adb991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7005f98fe22e4446a9978dc767afeac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b98f796b2f27410f90c8739d8141a95e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "694eb29da59e4577a48bebd7eb8236a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba8cce4a12974d288000549afa143e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60598824aa664b81b88ff30622f3de7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb4ea892be7f40fba941f9b02c930f4c",
              "IPY_MODEL_223b31681720423db92dcf844a5e52c6",
              "IPY_MODEL_178e696659f94c888f0703b3b265c5d6"
            ],
            "layout": "IPY_MODEL_de9e8e18aa4e4bc199e93979d4e08c8c"
          }
        },
        "cb4ea892be7f40fba941f9b02c930f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_991985d888f54512b020e9cae80a7475",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9e652fc588074b5ea9e26d1448bc34b9",
            "value": "100%"
          }
        },
        "223b31681720423db92dcf844a5e52c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74498868eb049d2882a26356a98477c",
            "max": 16776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9db2e9e27e5e4832b2aa0ce1e76f6f78",
            "value": 16776
          }
        },
        "178e696659f94c888f0703b3b265c5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da8bbef218ba4784b9873446a822583b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b8bb4140712421087e1e47e80a6558b",
            "value": "â€‡16776/16776â€‡[00:18&lt;00:00,â€‡933.12it/s]"
          }
        },
        "de9e8e18aa4e4bc199e93979d4e08c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991985d888f54512b020e9cae80a7475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e652fc588074b5ea9e26d1448bc34b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e74498868eb049d2882a26356a98477c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9db2e9e27e5e4832b2aa0ce1e76f6f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da8bbef218ba4784b9873446a822583b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b8bb4140712421087e1e47e80a6558b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4554fb15ebb54f37aae57022e55fdef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00b7d2f8529743c082e89bfb6cd7ef84",
              "IPY_MODEL_148f50555cb74dd78b22b40c5b0a5188",
              "IPY_MODEL_45259304c4904fc9a9c18491c603be63"
            ],
            "layout": "IPY_MODEL_da5a5b5042704d3a8df90620309f4b1c"
          }
        },
        "00b7d2f8529743c082e89bfb6cd7ef84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9de553c4dcbd4a6a90006516b9aca94b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4d8ebe3226ed4902a3410a51ea175419",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "148f50555cb74dd78b22b40c5b0a5188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e6c796f367f4de89d757f49a8c13de1",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9964d81fb40449c4acf7705c0073cdf6",
            "value": 8
          }
        },
        "45259304c4904fc9a9c18491c603be63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fe1a32d061447f9af100ddf63f820fb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_288f0fa13ac84fe8bb960e2812c3f10c",
            "value": "â€‡8/8â€‡[01:11&lt;00:00,â€‡â€‡7.90s/it]"
          }
        },
        "da5a5b5042704d3a8df90620309f4b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de553c4dcbd4a6a90006516b9aca94b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d8ebe3226ed4902a3410a51ea175419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e6c796f367f4de89d757f49a8c13de1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9964d81fb40449c4acf7705c0073cdf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fe1a32d061447f9af100ddf63f820fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "288f0fa13ac84fe8bb960e2812c3f10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}